.........................................Table Of Contents...............................................................
1. Conversions (bases) <URL:#tn=1. Conversions (bases)>
	1.1 convert decimal to hexa <URL:#tn=	1.1 convert decimal to hexa>
	1.2 Convert hex to decimal <URL:#tn=	1.2 Convert hex to decimal>
	1.3 Denary (base 10) to binary (base 2) <URL:#tn=	1.3 Denary (base 10) to binary (base 2)>
	1.4 String to int and vise versa <URL:#tn=	1.4 String to int and vise versa>
	1.5 Convert string  to hex (Python) <URL:#tn=	1.5 Convert string  to hex (Python)>
2. Parse configuration file. <URL:#tn=2. Parse configuration file.>
	2.1 ConfigParser, ver 2.7.3 <URL:#tn=	2.1 ConfigParser, ver 2.7.3>
		2.1.1  Manual <URL:#tn=		2.1.1  Manual>
		2.1.2 ConfigParser – Work with configuration files <URL:#tn=		2.1.2 ConfigParser – Work with configuration files>
	2.2 <URL:#tn=	2.2>
3. Tools <URL:#tn=3. Tools>
	3.1 GUI, tags: python GUI UX UI <URL:#tn=	3.1 GUI, tags: python GUI UX UI>
		3.1.1 Tkinter, tags: tk interface, Tkinter, python GUI UX UI <URL:#tn=		3.1.1 Tkinter, tags: tk interface, Tkinter, python GUI UX UI>
			3.1.1.1 Create a popup window <URL:#tn=			3.1.1.1 Create a popup window>
			3.1.1.2 Building menues dynamically and passing an argument to callback <URL:#tn=			3.1.1.2 Building menues dynamically and passing an argument to callback>
			3.1.1.3 Dialogs, request additional information <URL:#tn=			3.1.1.3 Dialogs, request additional information>
				3.1.1.3.1 Standard (Common) Dialogs <URL:#tn=				3.1.1.3.1 Standard (Common) Dialogs>
				3.1.1.3.2 Custom (Common) Dialogs <URL:#tn=				3.1.1.3.2 Custom (Common) Dialogs>
			3.1.1.4 Display image in label <URL:#tn=			3.1.1.4 Display image in label>
				3.1.1.4.1 PhotoImage <URL:#tn=				3.1.1.4.1 PhotoImage>
			3.1.1.5 Tooltip <URL:#tn=			3.1.1.5 Tooltip>
			3.1.1.6  clean widgets, remove/destroy frame <URL:#tn=			3.1.1.6  clean widgets, remove/destroy frame>
				3.1.1.6.1 In grid geometry <URL:#tn=				3.1.1.6.1 In grid geometry>
			3.1.1.7 scrollbar <URL:#tn=			3.1.1.7 scrollbar>
				3.1.1.7.1 Add scrollbar to frame <URL:#tn=				3.1.1.7.1 Add scrollbar to frame>
			3.1.1.7 <URL:#tn=			3.1.1.7>
		3.1.2 more on TKinter <URL:#tn=		3.1.2 more on TKinter>
		    3.1.2.1 TKinter getting started with Chatgpt <URL:#tn=		    3.1.2.1 TKinter getting started with Chatgpt>
		    3.1.2.2 <URL:#tn=		    3.1.2.2>
		3.1.3 <URL:#tn=		3.1.3>
	3.2 Pygame <URL:#tn=	3.2 Pygame>
	3.3 PyopenGL <URL:#tn=	3.3 PyopenGL>
	3.4 Twisted <URL:#tn=	3.4 Twisted>
	3.5 Pyui <URL:#tn=	3.5 Pyui>
4. tutorial <URL:#tn=4. tutorial>
    4.1 My tutorial <URL:#tn=    4.1 My tutorial>
        4.1.1  basic <URL:#tn=        4.1.1  basic>
1.25 <URL:#tn=1.25>
        4.1.2 memory model <URL:#tn=        4.1.2 memory model>
        4.1.3 <URL:#tn=        4.1.3>
    4.2 https://wiki.python.org/moin/BeginnersGuide <URL:#tn=    4.2 https://wiki.python.org/moin/BeginnersGuide>
    4.3 <URL:#tn=    4.3>
5. CLI <URL:#tn=5. CLI>
    5.1 Build CLI fast with cmd module <URL:#tn=    5.1 Build CLI fast with cmd module>
    5.2 How to create a menu for a python console application <URL:#tn=    5.2 How to create a menu for a python console application>
    5.3 https://pypi.org/project/simple-term-menu/ <URL:#tn=    5.3 https://pypi.org/project/simple-term-menu/>
        5.3.1 Installation <URL:#tn=        5.3.1 Installation>
        5.3.2 Usage <URL:#tn=        5.3.2 Usage>
        5.3.3 Styling <URL:#tn=        5.3.3 Styling>
        5.3.4 Searching <URL:#tn=        5.3.4 Searching>
        5.3.5 Shortcuts <URL:#tn=        5.3.5 Shortcuts>
        5.3.6 Custom accept keys <URL:#tn=        5.3.6 Custom accept keys>
        5.3.7 Multi-select <URL:#tn=        5.3.7 Multi-select>
        5.3.8 Preview window <URL:#tn=        5.3.8 Preview window>
        5.3.9 Skipping empty entries <URL:#tn=        5.3.9 Skipping empty entries>
        5.3.10 Additional settings <URL:#tn=        5.3.10 Additional settings>
        5.3.11 More advanced example <URL:#tn=        5.3.11 More advanced example>
        5.3.12 Similar projects <URL:#tn=        5.3.12 Similar projects>
    5.4 How to implement a dynamic command line menu in Python <URL:#tn=    5.4 How to implement a dynamic command line menu in Python>
6. Compile python <URL:#tn=6. Compile python>
	6.1 for windows <URL:#tn=	6.1 for windows>
		6.1.1 get help on configuration options of py2exe <URL:#tn=		6.1.1 get help on configuration options of py2exe>
		6.1.2 <URL:#tn=		6.1.2>
7. Run system shell commands. <URL:#tn=7. Run system shell commands.>
	7.1 du example: <URL:#tn=	7.1 du example:>
8. Bit manipulation: <URL:#tn=8. Bit manipulation:>
	8.1 http://wiki.python.org/moin/BitManipulation <URL:#tn=	8.1 http://wiki.python.org/moin/BitManipulation>
	8.2 http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/113799 <URL:#tn=	8.2 http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/113799>
	8.3 Binary buffers. <URL:#tn=	8.3 Binary buffers.>
		8.3.1	Strings in python can contain 0 and control chars, Unlike C, C++ and Jave! <URL:#tn=		8.3.1	Strings in python can contain 0 and control chars, Unlike C, C++ and Jave!>
		8.3.2 struct <URL:#tn=		8.3.2 struct>
9. Network programming <URL:#tn=9. Network programming>
	9.1 simple TCP server and client <URL:#tn=	9.1 simple TCP server and client>
		9.1.1 simple TCP server <URL:#tn=		9.1.1 simple TCP server>
		9.1.2 simple TCP client <URL:#tn=		9.1.2 simple TCP client>
	9.2 byte buffers. <URL:#tn=	9.2 byte buffers.>
	9.3 Data structures, common tyes and idioms <URL:#tn=	9.3 Data structures, common tyes and idioms>
		9.3.1 hexa strings <URL:#tn=		9.3.1 hexa strings>
		9.3.2 Struct <URL:#tn=		9.3.2 Struct>
			9.3.2.1 Functions and ExceptionsÂ¶ <URL:#tn=			9.3.2.1 Functions and ExceptionsÂ¶>
			9.3.2.2 Format Strings <URL:#tn=			9.3.2.2 Format Strings>
				9.3.2.2.1 Byte Order, Size, and AlignmentÂ¶ <URL:#tn=				9.3.2.2.1 Byte Order, Size, and AlignmentÂ¶>
				9.3.2.2.2 Format CharactersÂ¶ <URL:#tn=				9.3.2.2.2 Format CharactersÂ¶>
				9.3.2.2.3 ExamplesÂ¶ <URL:#tn=				9.3.2.2.3 ExamplesÂ¶>
			9.3.2.3 <URL:#tn=			9.3.2.3>
		9.3.3 deque <URL:#tn=		9.3.3 deque>
		    9.3.3.0  basics and API <URL:#tn=		    9.3.3.0  basics and API>
		    9.3.3.1 deque bfs usage <URL:#tn=		    9.3.3.1 deque bfs usage>
		    9.3.3.2 deque vs list performance <URL:#tn=		    9.3.3.2 deque vs list performance>
		    9.3.3.3 more on deque from Chatgpt <URL:#tn=		    9.3.3.3 more on deque from Chatgpt>
		    9.3.3.4 <URL:#tn=		    9.3.3.4>
		9.3.4 <URL:#tn=		9.3.4>
	9.4 Examples, Recipes <URL:#tn=	9.4 Examples, Recipes>
		9.4.1 TLV parser <URL:#tn=		9.4.1 TLV parser>
		9.4.2 PEZ RADIUS Request composing <URL:#tn=		9.4.2 PEZ RADIUS Request composing>
		9.4.3 Copy hex stream of packet from wireshark and parsing, verifying it <URL:#tn=		9.4.3 Copy hex stream of packet from wireshark and parsing, verifying it>
		9.4.4 RADIUS CoA server <URL:#tn=		9.4.4 RADIUS CoA server>
		9.4.5 <URL:#tn=		9.4.5>
	9.5 libs <URL:#tn=	9.5 libs>
		9.5.1 dpkt <URL:#tn=		9.5.1 dpkt>
		9.5.2 <URL:#tn=		9.5.2>
	9.6 <URL:#tn=	9.6>
10. Python CLI <URL:#tn=10. Python CLI>
	10.1 CMD module, http://effbot.org/librarybook/cmd.htm <URL:#tn=	10.1 CMD module, http://effbot.org/librarybook/cmd.htm>
11. Doc strings, <URL:#tn=11. Doc strings,>
12. Logging, tags: Logging <URL:#tn=12. Logging, tags: Logging>
    12.1 Chatgpt: describe common ways to add logging capabilities to python code, tags Chatgpt: describe common ways to add logging capabilities to python code <URL:#tn=    12.1 Chatgpt: describe common ways to add logging capabilities to python code, tags Chatgpt: describe common ways to add logging capabilities to python code>
    12.2 The logging package, tags: The logging package <URL:#tn=    12.2 The logging package, tags: The logging package>
13. Debug Python <URL:#tn=13. Debug Python>
	13.1 PDB <URL:#tn=	13.1 PDB>
   13.1.1. Let's start with a simple program, epdb1.py. <URL:#tn=   13.1.1. Let's start with a simple program, epdb1.py.>
   13.1.2. Insert the following statement at the beginning of your Python program. This statement imports the Python debugger module, pdb. <URL:#tn=   13.1.2. Insert the following statement at the beginning of your Python program. This statement imports the Python debugger module, pdb.>
   13.1.3. Now find a spot where you would like tracing to begin, and insert the following code: <URL:#tn=   13.1.3. Now find a spot where you would like tracing to begin, and insert the following code:>
   13.1.4. Now run your program from the command line as you usually do, which will probably look something like this: <URL:#tn=   13.1.4. Now run your program from the command line as you usually do, which will probably look something like this:>
	13.2 <URL:#tn=	13.2>
14. Reading, Handling of command line arguments, getopt: options, arguments, file(s) content iterator <URL:#tn=14. Reading, Handling of command line arguments, getopt: options, arguments, file(s) content iterator>
	14.1 Example from http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/302262 <URL:#tn=	14.1 Example from http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/302262>
    14.2 Example from http://docs.python.org/lib/module-optparse.html <URL:#tn=    14.2 Example from http://docs.python.org/lib/module-optparse.html>
	14.3 Example from http://wiki.python.org/moin/OptParse <URL:#tn=	14.3 Example from http://wiki.python.org/moin/OptParse>
	14.4 argparse <URL:#tn=	14.4 argparse>
		14.4.1 https://blog.oyam.dev/python-argparse-cheatsheet/ <URL:#tn=		14.4.1 https://blog.oyam.dev/python-argparse-cheatsheet/>
		    14.4.1.1   Basic argparse usage <URL:#tn=		    14.4.1.1   Basic argparse usage>
		    14.4.1.2  cmd arg - a single positional argument <URL:#tn=		    14.4.1.2  cmd arg - a single positional argument>
		    14.4.1.3 <URL:#tn=		    14.4.1.3>
		    14.4.1.4 <URL:#tn=		    14.4.1.4>
		    14.4.1.5 <URL:#tn=		    14.4.1.5>
		    14.4.1.6 <URL:#tn=		    14.4.1.6>
		    14.4.1.7 <URL:#tn=		    14.4.1.7>
		    14.4.1.8 <URL:#tn=		    14.4.1.8>
		    14.4.1.9 <URL:#tn=		    14.4.1.9>
		    14.4.1.10 <URL:#tn=		    14.4.1.10>
		14.4.2 https://docs.python.org/3/howto/argparse.html <URL:#tn=		14.4.2 https://docs.python.org/3/howto/argparse.html>
		    14.4.2.1 positional arguments <URL:#tn=		    14.4.2.1 positional arguments>
		    14.4.2.2 add optional argument <URL:#tn=		    14.4.2.2 add optional argument>
		14.4.3 args restrict values <URL:#tn=		14.4.3 args restrict values>
		14.4.4 count arg repetition <URL:#tn=		14.4.4 count arg repetition>
		14.4.5 Python argparse mutual exclusive Groups <URL:#tn=		14.4.5 Python argparse mutual exclusive Groups>
		14.4.6 <URL:#tn=		14.4.6>
	14.5 <URL:#tn=	14.5>
15. Lists <URL:#tn=15. Lists>
    15.1 An Introduction to Python Lists <URL:#tn=    15.1 An Introduction to Python Lists>
    15.2 Delete from list <URL:#tn=    15.2 Delete from list>
        15.2.1 delete range from list <URL:#tn=        15.2.1 delete range from list>
            15.2.1.1 delete arbitrary indices in one go from list <URL:#tn=            15.2.1.1 delete arbitrary indices in one go from list>
            15.2.1.2 <URL:#tn=            15.2.1.2>
        15.2.2 <URL:#tn=        15.2.2>
    15.3 <URL:#tn=    15.3>
16. Validate input <URL:#tn=16. Validate input>
	16.1 Tkinter widget validators <URL:#tn=	16.1 Tkinter widget validators>
17. Files <URL:#tn=17. Files>
	17.1 delete lines from file <URL:#tn=	17.1 delete lines from file>
	17.2 Get list of files in dir <URL:#tn=	17.2 Get list of files in dir>
		17.2.1  os way <URL:#tn=		17.2.1  os way>
		17.2.2 Using glob ( > 2.6) <URL:#tn=		17.2.2 Using glob ( > 2.6)>
	17.3 Read files <URL:#tn=	17.3 Read files>
	    17.3.1  read file character by character <URL:#tn=	    17.3.1  read file character by character>
	    17.3.2  read file byte by byte <URL:#tn=	    17.3.2  read file byte by byte>
	    17.3.3 3 Ways to Read A Text File Line by Line in Python <URL:#tn=	    17.3.3 3 Ways to Read A Text File Line by Line in Python>
	    17.3.3 <URL:#tn=	    17.3.3>
	17.4 <URL:#tn=	17.4>
18. String represntation of classes <URL:#tn=18. String represntation of classes>
	18.1 use or override built in method __str__ <URL:#tn=	18.1 use or override built in method __str__>
19. tags: Create installation, distribution, build, packages <URL:#tn=19. tags: Create installation, distribution, build, packages>
	19.1 Tutorial <URL:#tn=	19.1 Tutorial>
		19.1.1 Python 2.3 documentation <URL:#tn=		19.1.1 Python 2.3 documentation>
		19.1.2 A Simple Example <URL:#tn=		19.1.2 A Simple Example>
		19.1.3 Determine the package installation path. <URL:#tn=		19.1.3 Determine the package installation path.>
		19.1.4 Package lore <URL:#tn=		19.1.4 Package lore>
		19.1.5 Manual, http://docs.python.org/dist/intro.html <URL:#tn=		19.1.5 Manual, http://docs.python.org/dist/intro.html>
		19.1.6 tags: creating package python3 <URL:#tn=		19.1.6 tags: creating package python3>
		    19.1.6.1 https://packaging.python.org/en/latest/tutorials/packaging-projects/ <URL:#tn=		    19.1.6.1 https://packaging.python.org/en/latest/tutorials/packaging-projects/>
		    19.1.6.2 <URL:#tn=		    19.1.6.2>
		19.1.7 <URL:#tn=		19.1.7>
    19.2 PIP <URL:#tn=    19.2 PIP>
        19.2.1  installation guide <URL:#tn=        19.2.1  installation guide>
        19.2.2 creating a pip package <URL:#tn=        19.2.2 creating a pip package>
        19.2.3 <URL:#tn=        19.2.3>
    19.3 pipenv <URL:#tn=    19.3 pipenv>
        19.3.1  Basic <URL:#tn=        19.3.1  Basic>
        19.3.2 <URL:#tn=        19.3.2>
    19.4 <URL:#tn=    19.4>
20. Tips and Tricks <URL:#tn=20. Tips and Tricks>
	20.1 Running in background instead of swapning thread <URL:#tn=	20.1 Running in background instead of swapning thread>
	20.2 Printing without newline (print 'a',) prints a space, how to remove? <URL:#tn=	20.2 Printing without newline (print 'a',) prints a space, how to remove?>
	20.3 Print with colors, bold etc <URL:#tn=	20.3 Print with colors, bold etc>
	    20.3 Print with colors, bold etc <URL:#tn=	    20.3 Print with colors, bold etc>
	        20.3.1 My example <URL:#tn=	        20.3.1 My example>
	        20.3.2 <URL:#tn=	        20.3.2>
	20.4 <URL:#tn=	20.4>
21. Information from Learning Python book (3rd edition). <URL:#tn=21. Information from Learning Python book (3rd edition).>
	21.1 System Programming <URL:#tn=	21.1 System Programming>
		21.1.1 overview <URL:#tn=		21.1.1 overview>
		21.1.2 Python System Modules <URL:#tn=		21.1.2 Python System Modules>
			21.1.2.1 Module Documentation Sources <URL:#tn=			21.1.2.1 Module Documentation Sources>
				21.1.2.2 Paging Documentation Strings <URL:#tn=				21.1.2.2 Paging Documentation Strings>
				21.1.2.3 Introducing String Methods <URL:#tn=				21.1.2.3 Introducing String Methods>
				21.1.2.4  File Operation Basics <URL:#tn=				21.1.2.4  File Operation Basics>
				21.1.2.5  Using Programs in Two Ways <URL:#tn=				21.1.2.5  Using Programs in Two Ways>
				21.1.2.6   Python Library Manuals <URL:#tn=				21.1.2.6   Python Library Manuals>
			21.1.2.2 sys module <URL:#tn=			21.1.2.2 sys module>
				21.1.2.2.1 Platforms and Versions <URL:#tn=				21.1.2.2.1 Platforms and Versions>
				21.1.2.2.2 The Module Search Path <URL:#tn=				21.1.2.2.2 The Module Search Path>
				21.1.2.2.3 The Loaded Modules Table <URL:#tn=				21.1.2.2.3 The Loaded Modules Table>
				21.1.2.2.4 Exception Details <URL:#tn=				21.1.2.2.4 Exception Details>
				21.1.2.2.5  Other sys Module Exports <URL:#tn=				21.1.2.2.5  Other sys Module Exports>
			21.1.2.3 os module <URL:#tn=			21.1.2.3 os module>
				21.1.2.3.1  The Big os Lists <URL:#tn=				21.1.2.3.1  The Big os Lists>
				21.1.2.3.2  Administrative Tools <URL:#tn=				21.1.2.3.2  Administrative Tools>
				21.1.2.3.3  Portability Constants <URL:#tn=				21.1.2.3.3  Portability Constants>
				21.1.2.3.4  Basic os.path Tools <URL:#tn=				21.1.2.3.4  Basic os.path Tools>
				21.1.2.3.5   Running Shell Commands from Scripts <URL:#tn=				21.1.2.3.5   Running Shell Commands from Scripts>
					21.1.2.3.5.1  What's a shell command? <URL:#tn=					21.1.2.3.5.1  What's a shell command?>
					21.1.2.3.5.2   Running shell commands <URL:#tn=					21.1.2.3.5.2   Running shell commands>
					21.1.2.3.5.3   Communicating with shell commands <URL:#tn=					21.1.2.3.5.3   Communicating with shell commands>
					21.1.2.3.5.4  Shell command limitations <URL:#tn=					21.1.2.3.5.4  Shell command limitations>
				21.1.2.3.6  Other os Module Exports <URL:#tn=				21.1.2.3.6  Other os Module Exports>
			21.1.2.4 Script Execution Context <URL:#tn=			21.1.2.4 Script Execution Context>
			21.1.2.5 Current Working Directory <URL:#tn=			21.1.2.5 Current Working Directory>
				21.1.2.5.1 CWD, Files, and Import Paths <URL:#tn=				21.1.2.5.1 CWD, Files, and Import Paths>
				21.1.2.5.2 CWD and Command Lines <URL:#tn=				21.1.2.5.2 CWD and Command Lines>
			21.1.2.6 Command-Line Arguments <URL:#tn=			21.1.2.6 Command-Line Arguments>
			21.1.2.7 Shell Environment Variables <URL:#tn=			21.1.2.7 Shell Environment Variables>
				21.1.2.7.1 Fetching Shell Variables <URL:#tn=				21.1.2.7.1 Fetching Shell Variables>
				21.1.2.7.2 Changing Shell Variables <URL:#tn=				21.1.2.7.2 Changing Shell Variables>
				21.1.2.7.3  Shell Variable Details <URL:#tn=				21.1.2.7.3  Shell Variable Details>
			21.1.2.8  Standard Streams <URL:#tn=			21.1.2.8  Standard Streams>
				21.1.2.8.1 Redirecting Streams to Files and Programs <URL:#tn=				21.1.2.8.1 Redirecting Streams to Files and Programs>
					21.1.2.8.1.1  Redirecting streams to files <URL:#tn=					21.1.2.8.1.1  Redirecting streams to files>
					21.1.2.8.1.2  Chaining programs with pipes <URL:#tn=					21.1.2.8.1.2  Chaining programs with pipes>
					21.1.2.8.1.3   Coding alternatives <URL:#tn=					21.1.2.8.1.3   Coding alternatives>
					21.1.2.8.1.4 Redirected streams and user interaction <URL:#tn=					21.1.2.8.1.4 Redirected streams and user interaction>
					21.1.2.8.1.5  Reading keyboard input <URL:#tn=					21.1.2.8.1.5  Reading keyboard input>
				21.1.2.8.2  Redirecting Streams to Python Objects <URL:#tn=				21.1.2.8.2  Redirecting Streams to Python Objects>
				21.1.2.8.3  The StringIO Module <URL:#tn=				21.1.2.8.3  The StringIO Module>
				21.1.2.8.4  Capturing the stderr Stream <URL:#tn=				21.1.2.8.4  Capturing the stderr Stream>
				21.1.2.8.5   Redirection Syntax in Print Statements <URL:#tn=				21.1.2.8.5   Redirection Syntax in Print Statements>
				21.1.2.8.6  Other Redirection Options <URL:#tn=				21.1.2.8.6  Other Redirection Options>
		21.1.3 File and Directory Tools <URL:#tn=		21.1.3 File and Directory Tools>
		21.1.4 <URL:#tn=		21.1.4>
		21.1.5 <URL:#tn=		21.1.5>
		21.1.6 <URL:#tn=		21.1.6>
	21.2 <URL:#tn=	21.2>
22. Core Python Programming, Second Edition <URL:#tn=22. Core Python Programming, Second Edition>
	22.1 Getting Started <URL:#tn=	22.1 Getting Started>
		22.1.1  Program Output, the print Statement, and "Hello World!" <URL:#tn=		22.1.1  Program Output, the print Statement, and "Hello World!">
		22.1.2  Program Input and the raw_input()Built-in Function <URL:#tn=		22.1.2  Program Input and the raw_input()Built-in Function>
		22.1.3  Comments <URL:#tn=		22.1.3  Comments>
		22.1.4  Operators <URL:#tn=		22.1.4  Operators>
		22.1.5  Variables and Assignment <URL:#tn=		22.1.5  Variables and Assignment>
		22.1.6  Numbers <URL:#tn=		22.1.6  Numbers>
		22.1.7  Strings <URL:#tn=		22.1.7  Strings>
		22.1.8  Lists and Tuples <URL:#tn=		22.1.8  Lists and Tuples>
		22.1.9  Dictionaries <URL:#tn=		22.1.9  Dictionaries>
		22.1.10 Code Blocks Use Indentation <URL:#tn=		22.1.10 Code Blocks Use Indentation>
		22.1.11 if Statement <URL:#tn=		22.1.11 if Statement>
		22.1.12 while Loop <URL:#tn=		22.1.12 while Loop>
		22.1.13 for Loop and the range() Built-in Function <URL:#tn=		22.1.13 for Loop and the range() Built-in Function>
		22.1.14 List Comprehensions <URL:#tn=		22.1.14 List Comprehensions>
		22.1.15 Files and the open() and file() Built-in Functions <URL:#tn=		22.1.15 Files and the open() and file() Built-in Functions>
		22.1.16 Errors and Exceptions <URL:#tn=		22.1.16 Errors and Exceptions>
		22.1.17 Functions <URL:#tn=		22.1.17 Functions>
		22.1.18 Classes <URL:#tn=		22.1.18 Classes>
		22.1.19 Modules <URL:#tn=		22.1.19 Modules>
		22.1.20 Useful Functions <URL:#tn=		22.1.20 Useful Functions>
		22.1.21 Exercises <URL:#tn=		22.1.21 Exercises>
23. Programming Python, 3rd Edition <URL:#tn=23. Programming Python, 3rd Edition>
24. Python Cookbook, 2nd Edition <URL:#tn=24. Python Cookbook, 2nd Edition>
25. Python in a Nutshell, 2nd Edition <URL:#tn=25. Python in a Nutshell, 2nd Edition>
26. Python Phrasebook: Essential Code and Commands <URL:#tn=26. Python Phrasebook: Essential Code and Commands>
27. Python Pocket Reference, 3rd Edition <URL:#tn=27. Python Pocket Reference, 3rd Edition>
28. Python: Essential Reference, Third Edition <URL:#tn=28. Python: Essential Reference, Third Edition>
29. Rapid GUI Programming with Python and Qt: The Definitive Guide to PyQt Programming <URL:#tn=29. Rapid GUI Programming with Python and Qt: The Definitive Guide to PyQt Programming>
30. learning python, 3rd edition <URL:#tn=30. learning python, 3rd edition>
	30.1. Types and Operations <URL:#tn=	30.1. Types and Operations>
		30.1.1. Introducing Python Object Types <URL:#tn=		30.1.1. Introducing Python Object Types>
			30.1.1.1 Why Use Built-in Types? <URL:#tn=			30.1.1.1 Why Use Built-in Types?>
				30.1.1.1.1 Python's Core Data Types <URL:#tn=				30.1.1.1.1 Python's Core Data Types>
		30.1.2. Numbers <URL:#tn=		30.1.2. Numbers>
			30.1.2.1 Python Numeric Types <URL:#tn=			30.1.2.1 Python Numeric Types>
				30.1.2.1.1 Numeric Literals <URL:#tn=				30.1.2.1.1 Numeric Literals>
				30.1.2.1.2 Built-in Numeric Tools and Extensions <URL:#tn=				30.1.2.1.2 Built-in Numeric Tools and Extensions>
			30.1.2.2 Python Expression Operators <URL:#tn=			30.1.2.2 Python Expression Operators>
				30.1.2.2.1 Mixed Operators Follow Operator Precedence <URL:#tn=				30.1.2.2.1 Mixed Operators Follow Operator Precedence>
				30.1.2.2.2 Parentheses Group Subexpressions <URL:#tn=				30.1.2.2.2 Parentheses Group Subexpressions>
				30.1.2.2.3 Mixed Types Are Converted Up <URL:#tn=				30.1.2.2.3 Mixed Types Are Converted Up>
				30.1.2.2.4 Preview: Operator Overloading <URL:#tn=				30.1.2.2.4 Preview: Operator Overloading>
			30.1.2.3 Numbers in Action <URL:#tn=			30.1.2.3 Numbers in Action>
				30.1.2.3.1 Variables and Basic Expressions <URL:#tn=				30.1.2.3.1 Variables and Basic Expressions>
				30.1.2.3.2 Numeric Display Formats <URL:#tn=				30.1.2.3.2 Numeric Display Formats>
				30.1.2.3.3 Division: Classic, Floor, and True <URL:#tn=				30.1.2.3.3 Division: Classic, Floor, and True>
				30.1.2.3.4 Bitwise Operations <URL:#tn=				30.1.2.3.4 Bitwise Operations>
				30.1.2.3.5 Long Integers <URL:#tn=				30.1.2.3.5 Long Integers>
				30.1.2.3.6 Complex Numbers <URL:#tn=				30.1.2.3.6 Complex Numbers>
				30.1.2.3.7 Hexadecimal and Octal Notation <URL:#tn=				30.1.2.3.7 Hexadecimal and Octal Notation>
				30.1.2.3.8 Other Built-in Numeric Tools <URL:#tn=				30.1.2.3.8 Other Built-in Numeric Tools>
			30.1.2.4 Other Numeric Types <URL:#tn=			30.1.2.4 Other Numeric Types>
				30.1.2.4.1 Decimal Numbers <URL:#tn=				30.1.2.4.1 Decimal Numbers>
				30.1.2.4.2 Sets <URL:#tn=				30.1.2.4.2 Sets>
				30.1.2.4.3 Booleans <URL:#tn=				30.1.2.4.3 Booleans>
				30.1.2.4.4 Third-Party Extensions <URL:#tn=				30.1.2.4.4 Third-Party Extensions>
			30.1.2.5 Chapter Summary <URL:#tn=			30.1.2.5 Chapter Summary>
		30.1.3. The Dynamic Typing Interlude <URL:#tn=		30.1.3. The Dynamic Typing Interlude>
			30.1.3.1 The Case of the Missing Declaration Statements <URL:#tn=			30.1.3.1 The Case of the Missing Declaration Statements>
				30.1.3.1.1 Variables, Objects, and References <URL:#tn=				30.1.3.1.1 Variables, Objects, and References>
				30.1.3.1.2 Types Live with Objects, Not Variables <URL:#tn=				30.1.3.1.2 Types Live with Objects, Not Variables>
				30.1.3.1.3 Objects Are Garbage-Collected <URL:#tn=				30.1.3.1.3 Objects Are Garbage-Collected>
			30.1.3.2 Shared References <URL:#tn=			30.1.3.2 Shared References>
				30.1.3.2.1 Shared References and In-Place Changes <URL:#tn=				30.1.3.2.1 Shared References and In-Place Changes>
				30.1.3.2.2 Shared References and Equality <URL:#tn=				30.1.3.2.2 Shared References and Equality>
			30.1.3.3 Dynamic Typing Is Everywhere <URL:#tn=			30.1.3.3 Dynamic Typing Is Everywhere>
			30.1.3.4 Chapter Summary <URL:#tn=			30.1.3.4 Chapter Summary>
		30.1.4. ofsmallStrings <URL:#tn=		30.1.4. ofsmallStrings>
			30.1.4.1 String Literals <URL:#tn=			30.1.4.1 String Literals>
				30.1.4.1.1 Single- and Double-Quoted Strings Are the Same <URL:#tn=				30.1.4.1.1 Single- and Double-Quoted Strings Are the Same>
				30.1.4.1.2 Escape Sequences Represent Special Bytes <URL:#tn=				30.1.4.1.2 Escape Sequences Represent Special Bytes>
				30.1.4.1.3 Raw Strings Suppress Escapes <URL:#tn=				30.1.4.1.3 Raw Strings Suppress Escapes>
				30.1.4.1.4 Triple Quotes Code Multiline Block Strings <URL:#tn=				30.1.4.1.4 Triple Quotes Code Multiline Block Strings>
				30.1.4.1.5 Unicode Strings Encode Larger Character Sets <URL:#tn=				30.1.4.1.5 Unicode Strings Encode Larger Character Sets>
			30.1.4.2 Strings in Action <URL:#tn=			30.1.4.2 Strings in Action>
				30.1.4.2.1 Basic Operations <URL:#tn=				30.1.4.2.1 Basic Operations>
				30.1.4.2.2 Indexing and Slicing <URL:#tn=				30.1.4.2.2 Indexing and Slicing>
					30.1.4.2.2.1 Extended slicing: the third limit <URL:#tn=					30.1.4.2.2.1 Extended slicing: the third limit>
				30.1.4.2.3 String Conversion Tools <URL:#tn=				30.1.4.2.3 String Conversion Tools>
					30.1.4.2.3.1 Character code conversions <URL:#tn=					30.1.4.2.3.1 Character code conversions>
				30.1.4.2.4 Changing Strings <URL:#tn=				30.1.4.2.4 Changing Strings>
			30.1.4.3 String Formatting <URL:#tn=			30.1.4.3 String Formatting>
				30.1.4.3.1 Advanced String Formatting <URL:#tn=				30.1.4.3.1 Advanced String Formatting>
				30.1.4.3.2 Dictionary-Based String Formatting <URL:#tn=				30.1.4.3.2 Dictionary-Based String Formatting>
			30.1.4.4 String Methods <URL:#tn=			30.1.4.4 String Methods>
				30.1.4.4.1 String Method Examples: Changing Strings <URL:#tn=				30.1.4.4.1 String Method Examples: Changing Strings>
				30.1.4.4.2 String Method Examples: Parsing Text <URL:#tn=				30.1.4.4.2 String Method Examples: Parsing Text>
				30.1.4.4.3 Other Common String Methods in Action <URL:#tn=				30.1.4.4.3 Other Common String Methods in Action>
				30.1.4.4.4 The Original string Module <URL:#tn=				30.1.4.4.4 The Original string Module>
			30.1.4.5 General Type Categories <URL:#tn=			30.1.4.5 General Type Categories>
				30.1.4.5.1 Types Share Operation Sets by Categories <URL:#tn=				30.1.4.5.1 Types Share Operation Sets by Categories>
				30.1.4.5.2 Mutable Types Can Be Changed In-Place <URL:#tn=				30.1.4.5.2 Mutable Types Can Be Changed In-Place>
			30.1.4.6 Chapter Summary <URL:#tn=			30.1.4.6 Chapter Summary>
		30.1.5. Lists and Dictionaries <URL:#tn=		30.1.5. Lists and Dictionaries>
			30.1.5.1 Lists <URL:#tn=			30.1.5.1 Lists>
			30.1.5.2 Lists in Action <URL:#tn=			30.1.5.2 Lists in Action>
				30.1.5.2.1 Basic List Operations <URL:#tn=				30.1.5.2.1 Basic List Operations>
				30.1.5.2.2 Indexing, Slicing, and Matrixes <URL:#tn=				30.1.5.2.2 Indexing, Slicing, and Matrixes>
				30.1.5.2.3 Changing Lists In-Place <URL:#tn=				30.1.5.2.3 Changing Lists In-Place>
					30.1.5.2.3.1 Index and slice assignments <URL:#tn=					30.1.5.2.3.1 Index and slice assignments>
					30.1.5.2.3.2 List method calls <URL:#tn=					30.1.5.2.3.2 List method calls>
					30.1.5.2.3.3 Other common list operations <URL:#tn=					30.1.5.2.3.3 Other common list operations>
			30.1.5.3 Dictionaries <URL:#tn=			30.1.5.3 Dictionaries>
			30.1.5.4 Dictionaries in Action <URL:#tn=			30.1.5.4 Dictionaries in Action>
				30.1.5.4.1 Basic Dictionary Operations <URL:#tn=				30.1.5.4.1 Basic Dictionary Operations>
				30.1.5.4.2 Changing Dictionaries In-Place <URL:#tn=				30.1.5.4.2 Changing Dictionaries In-Place>
				30.1.5.4.3 More Dictionary Methods <URL:#tn=				30.1.5.4.3 More Dictionary Methods>
				30.1.5.4.4 A Languages Table <URL:#tn=				30.1.5.4.4 A Languages Table>
				30.1.5.4.5 Dictionary Usage Notes <URL:#tn=				30.1.5.4.5 Dictionary Usage Notes>
					30.1.5.4.5.1 Using dictionaries to simulate flexible lists <URL:#tn=					30.1.5.4.5.1 Using dictionaries to simulate flexible lists>
					30.1.5.4.5.2 Using dictionaries for sparse data structures <URL:#tn=					30.1.5.4.5.2 Using dictionaries for sparse data structures>
					30.1.5.4.5.3 Avoiding missing-key errors <URL:#tn=					30.1.5.4.5.3 Avoiding missing-key errors>
					30.1.5.4.5.4 Using dictionaries as "records" <URL:#tn=					30.1.5.4.5.4 Using dictionaries as "records">
					30.1.5.4.5.5 Other ways to make dictionaries <URL:#tn=					30.1.5.4.5.5 Other ways to make dictionaries>
			30.1.5.5 Chapter Summary <URL:#tn=			30.1.5.5 Chapter Summary>
		30.1.6. Tuples, Files, and Everything Else <URL:#tn=		30.1.6. Tuples, Files, and Everything Else>
			30.1.6.1  Tuples <URL:#tn=			30.1.6.1  Tuples>
				30.1.6.1.1  Tuples in Action <URL:#tn=				30.1.6.1.1  Tuples in Action>
					30.1.6.1.1.1  Tuple syntax peculiarities: commas and parentheses <URL:#tn=					30.1.6.1.1.1  Tuple syntax peculiarities: commas and parentheses>
					30.1.6.1.1.2 Conversions and immutability <URL:#tn=					30.1.6.1.1.2 Conversions and immutability>
				30.1.6.1.2 Why Lists and Tuples? <URL:#tn=				30.1.6.1.2 Why Lists and Tuples?>
			30.1.6.2  Files <URL:#tn=			30.1.6.2  Files>
				30.1.6.2.1  Opening Files <URL:#tn=				30.1.6.2.1  Opening Files>
				30.1.6.2.2 Using Files <URL:#tn=				30.1.6.2.2 Using Files>
				30.1.6.2.3 Files in Action <URL:#tn=				30.1.6.2.3 Files in Action>
					30.1.6.2.3.1 Storing and parsing Python objects in files <URL:#tn=					30.1.6.2.3.1 Storing and parsing Python objects in files>
					30.1.6.2.3.2 Storing native Python objects with pickle <URL:#tn=					30.1.6.2.3.2 Storing native Python objects with pickle>
					30.1.6.2.3.3 Storing and parsing packed binary data in files <URL:#tn=					30.1.6.2.3.3 Storing and parsing packed binary data in files>
					30.1.6.2.3.4 Other File Tools <URL:#tn=					30.1.6.2.3.4 Other File Tools>
			30.1.6.3  Type Categories Revisited <URL:#tn=			30.1.6.3  Type Categories Revisited>
			30.1.6.4  Object Flexibility <URL:#tn=			30.1.6.4  Object Flexibility>
			30.1.6.5  References Versus Copies <URL:#tn=			30.1.6.5  References Versus Copies>
			30.1.6.6  Comparisons, Equality, and Truth <URL:#tn=			30.1.6.6  Comparisons, Equality, and Truth>
				30.1.6.6.1  The Meaning of True and False in Python <URL:#tn=				30.1.6.6.1  The Meaning of True and False in Python>
			30.1.6.7  Python's Type Hierarchies <URL:#tn=			30.1.6.7  Python's Type Hierarchies>
			30.1.6.8  Other Types in Python <URL:#tn=			30.1.6.8  Other Types in Python>
			30.1.6.9  Built-in Type Gotchas <URL:#tn=			30.1.6.9  Built-in Type Gotchas>
				30.1.6.9.1  Assignment Creates References, Not Copies <URL:#tn=				30.1.6.9.1  Assignment Creates References, Not Copies>
				30.1.6.9.2 Repetition Adds One Level Deep <URL:#tn=				30.1.6.9.2 Repetition Adds One Level Deep>
				30.1.6.9.3 Beware of Cyclic Data Structures <URL:#tn=				30.1.6.9.3 Beware of Cyclic Data Structures>
				30.1.6.9.4 Immutable Types Can't Be Changed In-Place <URL:#tn=				30.1.6.9.4 Immutable Types Can't Be Changed In-Place>
			30.1.6.10 Chapter Summary <URL:#tn=			30.1.6.10 Chapter Summary>
			30.1.6.11 BRAIN BUILDER <URL:#tn=			30.1.6.11 BRAIN BUILDER>
	30.2. Statements and Syntax <URL:#tn=	30.2. Statements and Syntax>
		30.2.1. Introducing Python Statements <URL:#tn=		30.2.1. Introducing Python Statements>
		30.2.2.  Python Program Structure Revisited <URL:#tn=		30.2.2.  Python Program Structure Revisited>
		30.2.1.1 Python's Statements <URL:#tn=		30.2.1.1 Python's Statements>
		30.2.1.2 A Tale of Two ifs <URL:#tn=		30.2.1.2 A Tale of Two ifs>
			30.2.1.2.1 What Python Adds <URL:#tn=			30.2.1.2.1 What Python Adds>
			30.2.1.2.2 What Python Removes <URL:#tn=			30.2.1.2.2 What Python Removes>
				30.2.1.2.2.1 Parentheses are optional <URL:#tn=				30.2.1.2.2.1 Parentheses are optional>
				30.2.1.2.2.2 End of line is end of statement <URL:#tn=				30.2.1.2.2.2 End of line is end of statement>
				30.2.1.2.2.3 End of indentation is end of block <URL:#tn=				30.2.1.2.2.3 End of indentation is end of block>
			30.2.1.2.3 Why Indentation Syntax? <URL:#tn=			30.2.1.2.3 Why Indentation Syntax?>
			30.2.1.2.4 A Few Special Cases <URL:#tn=			30.2.1.2.4 A Few Special Cases>
				30.2.1.2.4.1 Statement rule special cases <URL:#tn=				30.2.1.2.4.1 Statement rule special cases>
				30.2.1.2.4.2 Block rule special case <URL:#tn=				30.2.1.2.4.2 Block rule special case>
		30.2.1.3 A Quick Example: Interactive Loops <URL:#tn=		30.2.1.3 A Quick Example: Interactive Loops>
			30.2.1.3.1 A Simple Interactive Loop <URL:#tn=			30.2.1.3.1 A Simple Interactive Loop>
			30.2.1.3.2 Doing Math on User Inputs <URL:#tn=			30.2.1.3.2 Doing Math on User Inputs>
			30.2.1.3.3 Handling Errors by Testing Inputs <URL:#tn=			30.2.1.3.3 Handling Errors by Testing Inputs>
			30.2.1.3.4 Handling Errors with try Statements <URL:#tn=			30.2.1.3.4 Handling Errors with try Statements>
			30.2.1.3.5 Nesting Code Three Levels Deep <URL:#tn=			30.2.1.3.5 Nesting Code Three Levels Deep>
		30.2.1.4 Chapter Summary <URL:#tn=		30.2.1.4 Chapter Summary>
		30.2.2. rwordsAssignment, Expressions, and print <URL:#tn=		30.2.2. rwordsAssignment, Expressions, and print>
			30.2.2.1 Assignment Statements <URL:#tn=			30.2.2.1 Assignment Statements>
				30.2.2.1.1 Assignment Statement Forms <URL:#tn=				30.2.2.1.1 Assignment Statement Forms>
				30.2.2.1.2 Sequence Assignments <URL:#tn=				30.2.2.1.2 Sequence Assignments>
					30.2.2.1.2.1 Advanced sequence assignment patterns <URL:#tn=					30.2.2.1.2.1 Advanced sequence assignment patterns>
				30.2.2.1.3 Multiple-Target Assignments <URL:#tn=				30.2.2.1.3 Multiple-Target Assignments>
					30.2.2.1.3.1 Multiple-target assignment and shared references <URL:#tn=					30.2.2.1.3.1 Multiple-target assignment and shared references>
				30.2.2.1.4 Augmented Assignments <URL:#tn=				30.2.2.1.4 Augmented Assignments>
					30.2.2.1.4.1 Augmented assignment and shared references <URL:#tn=					30.2.2.1.4.1 Augmented assignment and shared references>
				30.2.2.1.5 Variable Name Rules <URL:#tn=				30.2.2.1.5 Variable Name Rules>
					30.2.2.1.5.1 Naming conventions <URL:#tn=					30.2.2.1.5.1 Naming conventions>
					30.2.2.1.5.2 Names have no type, but objects do <URL:#tn=					30.2.2.1.5.2 Names have no type, but objects do>
			30.2.2.2 Expression Statements <URL:#tn=			30.2.2.2 Expression Statements>
				30.2.2.2.1 Expression Statements and In-Place Changes <URL:#tn=				30.2.2.2.1 Expression Statements and In-Place Changes>
			30.2.2.3 print Statements <URL:#tn=			30.2.2.3 print Statements>
				30.2.2.3.1 The Python "Hello World" Program <URL:#tn=				30.2.2.3.1 The Python "Hello World" Program>
				30.2.2.3.2 Redirecting the Output Stream <URL:#tn=				30.2.2.3.2 Redirecting the Output Stream>
				30.2.2.3.3 The print >> file Extension <URL:#tn=				30.2.2.3.3 The print >> file Extension>
			30.2.2.4 Chapter Summary <URL:#tn=			30.2.2.4 Chapter Summary>
		30.2.3. if Tests <URL:#tn=		30.2.3. if Tests>
			30.2.3.1  if statements <URL:#tn=			30.2.3.1  if statements>
				30.2.3.1.1  General Format <URL:#tn=				30.2.3.1.1  General Format>
				30.2.3.1.2 Basic Examples <URL:#tn=				30.2.3.1.2 Basic Examples>
				30.2.3.1.3 Multiway Branching <URL:#tn=				30.2.3.1.3 Multiway Branching>
			30.2.3.2 Python Syntax Rules <URL:#tn=			30.2.3.2 Python Syntax Rules>
				30.2.3.2.1 Block Delimiters <URL:#tn=				30.2.3.2.1 Block Delimiters>
				30.2.3.2.2 Statement Delimiters <URL:#tn=				30.2.3.2.2 Statement Delimiters>
				30.2.3.2.3 A Few Special Cases <URL:#tn=				30.2.3.2.3 A Few Special Cases>
			30.2.3.3 Truth Tests <URL:#tn=			30.2.3.3 Truth Tests>
			30.2.3.4 Chapter Summary <URL:#tn=			30.2.3.4 Chapter Summary>
		30.2.4. while and for Loops <URL:#tn=		30.2.4. while and for Loops>
			30.2.4.1 while Loops <URL:#tn=			30.2.4.1 while Loops>
				30.2.4.1.1 General Format <URL:#tn=				30.2.4.1.1 General Format>
				30.2.4.1.2 Examples <URL:#tn=				30.2.4.1.2 Examples>
			30.2.4.2 break, continue, pass, and the Loop else <URL:#tn=			30.2.4.2 break, continue, pass, and the Loop else>
				30.2.4.2.1 General Loop Format <URL:#tn=				30.2.4.2.1 General Loop Format>
				30.2.4.2.2 Examples <URL:#tn=				30.2.4.2.2 Examples>
					30.2.4.2.2.1 pass <URL:#tn=					30.2.4.2.2.1 pass>
					30.2.4.2.2.2 continue <URL:#tn=					30.2.4.2.2.2 continue>
					30.2.4.2.2.3 break <URL:#tn=					30.2.4.2.2.3 break>
					30.2.4.2.2.4 else <URL:#tn=					30.2.4.2.2.4 else>
					30.2.4.2.2.5 More on the loop else clause <URL:#tn=					30.2.4.2.2.5 More on the loop else clause>
				30.2.4.2.3 for Loops <URL:#tn=				30.2.4.2.3 for Loops>
				30.2.4.2.4 General Format <URL:#tn=				30.2.4.2.4 General Format>
				30.2.4.2.5 Examples <URL:#tn=				30.2.4.2.5 Examples>
					30.2.4.2.5.1 Basic usage <URL:#tn=					30.2.4.2.5.1 Basic usage>
					30.2.4.2.5.2 Other data types <URL:#tn=					30.2.4.2.5.2 Other data types>
					30.2.4.2.5.3 Tuple assignment in for <URL:#tn=					30.2.4.2.5.3 Tuple assignment in for>
					30.2.4.2.5.4 Nested for loops <URL:#tn=					30.2.4.2.5.4 Nested for loops>
				30.2.4.2.4 Iterators: A First Look <URL:#tn=				30.2.4.2.4 Iterators: A First Look>
					30.2.4.2.4.1 File Iterators <URL:#tn=					30.2.4.2.4.1 File Iterators>
					30.2.4.2.4.2 Other Built-in Type Iterators <URL:#tn=					30.2.4.2.4.2 Other Built-in Type Iterators>
					30.2.4.2.4.3 Other Iteration Contexts <URL:#tn=					30.2.4.2.4.3 Other Iteration Contexts>
					30.2.4.2.4.4 User-Defined Iterators <URL:#tn=					30.2.4.2.4.4 User-Defined Iterators>
				30.2.4.2.5 Loop Coding Techniques <URL:#tn=				30.2.4.2.5 Loop Coding Techniques>
					30.2.4.2.5.1 Counter Loops: while and range <URL:#tn=					30.2.4.2.5.1 Counter Loops: while and range>
					30.2.4.2.5.2 Nonexhaustive Traversals: range <URL:#tn=					30.2.4.2.5.2 Nonexhaustive Traversals: range>
					30.2.4.2.5.3 Changing Lists: range <URL:#tn=					30.2.4.2.5.3 Changing Lists: range>
					30.2.4.2.5.4 Parallel Traversals: zip and map <URL:#tn=					30.2.4.2.5.4 Parallel Traversals: zip and map>
						30.2.4.2.5.4.1 Dictionary construction with zip <URL:#tn=						30.2.4.2.5.4.1 Dictionary construction with zip>
						30.2.4.2.5.5 Generating Both Offsets and Items: enumerate <URL:#tn=						30.2.4.2.5.5 Generating Both Offsets and Items: enumerate>
						30.2.4.2.5.6 List Comprehensions: A First Look <URL:#tn=						30.2.4.2.5.6 List Comprehensions: A First Look>
							30.2.4.2.5.6.1 List Comprehension Basics <URL:#tn=							30.2.4.2.5.6.1 List Comprehension Basics>
							30.2.4.2.5.6.2 Using List Comprehensions on Files <URL:#tn=							30.2.4.2.5.6.2 Using List Comprehensions on Files>
							30.2.4.2.5.6.3 Extended List Comprehension Syntax <URL:#tn=							30.2.4.2.5.6.3 Extended List Comprehension Syntax>
				30.2.4.2.7 Chapter Summary <URL:#tn=				30.2.4.2.7 Chapter Summary>
		30.2.5. The Documentation Interlude <URL:#tn=		30.2.5. The Documentation Interlude>
			30.2.5.1 Python Documentation Sources <URL:#tn=			30.2.5.1 Python Documentation Sources>
				30.2.5.1.1 # Comments <URL:#tn=				30.2.5.1.1 # Comments>
				30.2.5.1.2 The dir Function <URL:#tn=				30.2.5.1.2 The dir Function>
				30.2.5.1.3 Docstrings: __doc__ <URL:#tn=				30.2.5.1.3 Docstrings: __doc__>
					30.2.5.1.3.1 User-defined docstrings <URL:#tn=					30.2.5.1.3.1 User-defined docstrings>
					30.2.5.1.3.2 Docstring standards <URL:#tn=					30.2.5.1.3.2 Docstring standards>
14.1.3.3. Built-in docstrings <URL:#tn=14.1.3.3. Built-in docstrings>
				30.2.5.1.4 PyDoc: The help Function <URL:#tn=				30.2.5.1.4 PyDoc: The help Function>
				30.2.5.1.5 PyDoc: HTML Reports <URL:#tn=				30.2.5.1.5 PyDoc: HTML Reports>
				30.2.5.1.6 Standard Manual Set <URL:#tn=				30.2.5.1.6 Standard Manual Set>
				30.2.5.1.7 Web Resources <URL:#tn=				30.2.5.1.7 Web Resources>
14.1.8. Published Books <URL:#tn=14.1.8. Published Books>
			30.2.5.2 Common Coding Gotchas <URL:#tn=			30.2.5.2 Common Coding Gotchas>
			30.2.5.3 Chapter Summary <URL:#tn=			30.2.5.3 Chapter Summary>
			30.2.5.4 BRAIN BUILDER <URL:#tn=			30.2.5.4 BRAIN BUILDER>
1. <URL:#tn=1.>
	30.3. Functions <URL:#tn=	30.3. Functions>
 	 	30.3.1. Function Basics <URL:#tn= 	 	30.3.1. Function Basics>
			30.3.1.1 Why Use Functions? <URL:#tn=			30.3.1.1 Why Use Functions?>
				30.3.1.1.1 Coding Functions <URL:#tn=				30.3.1.1.1 Coding Functions>
				30.3.1.1.2 def Statements <URL:#tn=				30.3.1.1.2 def Statements>
				30.3.1.1.3 def Executes at Runtime <URL:#tn=				30.3.1.1.3 def Executes at Runtime>
			30.3.1.2 A First Example: Definitions and Calls <URL:#tn=			30.3.1.2 A First Example: Definitions and Calls>
				30.3.1.2.1 Definition <URL:#tn=				30.3.1.2.1 Definition>
				30.3.1.2.2 Calls <URL:#tn=				30.3.1.2.2 Calls>
				30.3.1.2.3 Polymorphism in Python <URL:#tn=				30.3.1.2.3 Polymorphism in Python>
			30.3.1.3 A Second Example: Intersecting Sequences <URL:#tn=			30.3.1.3 A Second Example: Intersecting Sequences>
				30.3.1.3.1 Definition <URL:#tn=				30.3.1.3.1 Definition>
				30.3.1.3.2 Calls <URL:#tn=				30.3.1.3.2 Calls>
				30.3.1.3.3 Polymorphism Revisited <URL:#tn=				30.3.1.3.3 Polymorphism Revisited>
				30.3.1.3.4 Local Variables <URL:#tn=				30.3.1.3.4 Local Variables>
			30.3.1.4 Chapter Summary <URL:#tn=			30.3.1.4 Chapter Summary>
			30.3.1.5 <URL:#tn=			30.3.1.5>
 	 	30.3.2. Scopes and Arguments <URL:#tn= 	 	30.3.2. Scopes and Arguments>
			30.3.2.1 Scope Rules <URL:#tn=			30.3.2.1 Scope Rules>
				30.3.2.1.1 Python Scope Basics <URL:#tn=				30.3.2.1.1 Python Scope Basics>
				30.3.2.1.2 Name Resolution: The LEGB Rule <URL:#tn=				30.3.2.1.2 Name Resolution: The LEGB Rule>
				30.3.2.1.3 Scope Example <URL:#tn=				30.3.2.1.3 Scope Example>
				30.3.2.1.4 The Built-in Scope <URL:#tn=				30.3.2.1.4 The Built-in Scope>
			30.3.2.2 The global Statement <URL:#tn=			30.3.2.2 The global Statement>
				30.3.2.2.1 Minimize Global Variables <URL:#tn=				30.3.2.2.1 Minimize Global Variables>
				30.3.2.2.2 Minimize Cross-File Changes <URL:#tn=				30.3.2.2.2 Minimize Cross-File Changes>
				30.3.2.2.3 Other Ways to Access Globals <URL:#tn=				30.3.2.2.3 Other Ways to Access Globals>
			30.3.2.3 Scopes and Nested Functions <URL:#tn=			30.3.2.3 Scopes and Nested Functions>
				30.3.2.3.1 Nested Scope Details <URL:#tn=				30.3.2.3.1 Nested Scope Details>
				30.3.2.3.2 Nested Scope Examples <URL:#tn=				30.3.2.3.2 Nested Scope Examples>
					30.3.2.3.2.1 Factory functions <URL:#tn=					30.3.2.3.2.1 Factory functions>
					30.3.2.3.2.2 Retaining enclosing scopes' state with defaults <URL:#tn=					30.3.2.3.2.2 Retaining enclosing scopes' state with defaults>
				30.3.2.3.3 Nested scopes and lambdas <URL:#tn=				30.3.2.3.3 Nested scopes and lambdas>
				30.3.2.3.4 Scopes versus defaults with loop variables <URL:#tn=				30.3.2.3.4 Scopes versus defaults with loop variables>
				30.3.2.3.5 Arbitrary scope nesting <URL:#tn=				30.3.2.3.5 Arbitrary scope nesting>
			30.3.2.4 Passing Arguments <URL:#tn=			30.3.2.4 Passing Arguments>
				30.3.2.4.1 Arguments and Shared References <URL:#tn=				30.3.2.4.1 Arguments and Shared References>
				30.3.2.4.2 Avoiding Mutable Argument Changes <URL:#tn=				30.3.2.4.2 Avoiding Mutable Argument Changes>
				30.3.2.4.3 Simulating Output Parameters <URL:#tn=				30.3.2.4.3 Simulating Output Parameters>
			30.3.2.5 Special Argument-Matching Modes <URL:#tn=			30.3.2.5 Special Argument-Matching Modes>
				30.3.2.5.1 Keyword and Default Examples <URL:#tn=				30.3.2.5.1 Keyword and Default Examples>
					30.3.2.5.1.1 Keywords <URL:#tn=					30.3.2.5.1.1 Keywords>
					30.3.2.5.1.2 Defaults <URL:#tn=					30.3.2.5.1.2 Defaults>
				30.3.2.5.2 Arbitrary Arguments Examples <URL:#tn=				30.3.2.5.2 Arbitrary Arguments Examples>
					30.3.2.5.2.1 Collecting arguments <URL:#tn=					30.3.2.5.2.1 Collecting arguments>
					30.3.2.5.2.2 Unpacking arguments <URL:#tn=					30.3.2.5.2.2 Unpacking arguments>
				30.3.2.5.3 Combining Keywords and Defaults <URL:#tn=				30.3.2.5.3 Combining Keywords and Defaults>
				30.3.2.5.4 The min Wakeup Call <URL:#tn=				30.3.2.5.4 The min Wakeup Call>
					30.3.2.5.4.1 Full credit <URL:#tn=					30.3.2.5.4.1 Full credit>
					30.3.2.5.4.2 Bonus points <URL:#tn=					30.3.2.5.4.2 Bonus points>
					30.3.2.5.4.3 The punch line <URL:#tn=					30.3.2.5.4.3 The punch line>
				30.3.2.5.5 A More Useful Example: General Set Functions <URL:#tn=				30.3.2.5.5 A More Useful Example: General Set Functions>
				30.3.2.5.6 Argument Matching: The Gritty Details <URL:#tn=				30.3.2.5.6 Argument Matching: The Gritty Details>
			30.3.2.6 Chapter Summary <URL:#tn=			30.3.2.6 Chapter Summary>
 	 	30.3.3. Advanced Function Topics <URL:#tn= 	 	30.3.3. Advanced Function Topics>
			30.3.3.1  Anonymous Functions: lambda <URL:#tn=			30.3.3.1  Anonymous Functions: lambda>
				30.3.3.1.1  lambda Expressions <URL:#tn=				30.3.3.1.1  lambda Expressions>
				30.3.3.1.2 Why Use lambda? <URL:#tn=				30.3.3.1.2 Why Use lambda?>
				30.3.3.1.3 How (Not) to Obfuscate Your Python Code <URL:#tn=				30.3.3.1.3 How (Not) to Obfuscate Your Python Code>
				30.3.3.1.4 Nested lambdas and Scopes <URL:#tn=				30.3.3.1.4 Nested lambdas and Scopes>
			30.3.3.2  Applying Functions to Arguments <URL:#tn=			30.3.3.2  Applying Functions to Arguments>
				30.3.3.2.1  The apply Built-in <URL:#tn=				30.3.3.2.1  The apply Built-in>
					30.3.3.2.1.1  Passing keyword arguments <URL:#tn=					30.3.3.2.1.1  Passing keyword arguments>
				30.3.3.2.2 apply-Like Call Syntax <URL:#tn=				30.3.3.2.2 apply-Like Call Syntax>
			30.3.3.3  Mapping Functions over Sequences: map <URL:#tn=			30.3.3.3  Mapping Functions over Sequences: map>
			30.3.3.4  Functional Programming Tools: filter and reduce <URL:#tn=			30.3.3.4  Functional Programming Tools: filter and reduce>
			30.3.3.5  List Comprehensions Revisited: Mappings <URL:#tn=			30.3.3.5  List Comprehensions Revisited: Mappings>
				30.3.3.5.1  List Comprehension Basics <URL:#tn=				30.3.3.5.1  List Comprehension Basics>
				30.3.3.5.2 Adding Tests and Nested Loops, tags: double for nested list comprehension <URL:#tn=				30.3.3.5.2 Adding Tests and Nested Loops, tags: double for nested list comprehension>
				30.3.3.5.3 List Comprehensions and Matrixes <URL:#tn=				30.3.3.5.3 List Comprehensions and Matrixes>
				30.3.3.5.4 Comprehending List Comprehensions <URL:#tn=				30.3.3.5.4 Comprehending List Comprehensions>
			30.3.3.6  Iterators Revisited: Generators <URL:#tn=			30.3.3.6  Iterators Revisited: Generators>
				30.3.3.6.1  Generator Function Example <URL:#tn=				30.3.3.6.1  Generator Function Example>
				30.3.3.6.2 Extended Generator Function Protocol: send Versus next <URL:#tn=				30.3.3.6.2 Extended Generator Function Protocol: send Versus next>
				30.3.3.6.3 Iterators and Built-in Types <URL:#tn=				30.3.3.6.3 Iterators and Built-in Types>
				30.3.3.6.4 Generator Expressions: Iterators Meet List Comprehensions <URL:#tn=				30.3.3.6.4 Generator Expressions: Iterators Meet List Comprehensions>
			30.3.3.7  Timing Iteration Alternatives <URL:#tn=			30.3.3.7  Timing Iteration Alternatives>
2.6.5 (r265:79063, Jun 12 2010, 17:07:01) <URL:#tn=2.6.5 (r265:79063, Jun 12 2010, 17:07:01)>
2.5 (r25:51908, Sep 19 2006, 09:52:17) [MSC v.1310 32 bit (Intel)] <URL:#tn=2.5 (r25:51908, Sep 19 2006, 09:52:17) [MSC v.1310 32 bit (Intel)]>
2.5 (r25:51908, Sep 19 2006, 09:52:17) [MSC v.1310 32 bit (Intel)] <URL:#tn=2.5 (r25:51908, Sep 19 2006, 09:52:17) [MSC v.1310 32 bit (Intel)]>
			30.3.3.8  Function Design Concepts <URL:#tn=			30.3.3.8  Function Design Concepts>
				30.3.3.8.1  Functions Are Objects: Indirect Calls <URL:#tn=				30.3.3.8.1  Functions Are Objects: Indirect Calls>
			30.3.3.9  Function Gotchas <URL:#tn=			30.3.3.9  Function Gotchas>
				30.3.3.9.1  Local Names Are Detected Statically <URL:#tn=				30.3.3.9.1  Local Names Are Detected Statically>
				30.3.3.9.2 Defaults and Mutable Objects <URL:#tn=				30.3.3.9.2 Defaults and Mutable Objects>
				30.3.3.9.3 Functions Without returns <URL:#tn=				30.3.3.9.3 Functions Without returns>
				30.3.3.9.4 Enclosing Scope Loop Variables <URL:#tn=				30.3.3.9.4 Enclosing Scope Loop Variables>
			30.3.3.10 Chapter Summary <URL:#tn=			30.3.3.10 Chapter Summary>
			30.3.3.11 BRAIN BUILDER <URL:#tn=			30.3.3.11 BRAIN BUILDER>
11.91 <URL:#tn=11.91>
11.91 <URL:#tn=11.91>
11.91 <URL:#tn=11.91>
          13.0 is prime <URL:#tn=          13.0 is prime>
          15.0 has factor 5.0 <URL:#tn=          15.0 has factor 5.0>
13.0 is prime <URL:#tn=13.0 is prime>
15.0 has factor 5.0 <URL:#tn=15.0 has factor 5.0>
	30.4. Modules <URL:#tn=	30.4. Modules>
 	 	30.4.1. Modules: The Big Picture <URL:#tn= 	 	30.4.1. Modules: The Big Picture>
			30.4.1.1 Why Use Modules? <URL:#tn=			30.4.1.1 Why Use Modules?>
			30.4.1.2 Python Program Architecture <URL:#tn=			30.4.1.2 Python Program Architecture>
				30.4.1.2.1 How to Structure a Program <URL:#tn=				30.4.1.2.1 How to Structure a Program>
				30.4.1.2.2 Imports and Attributes <URL:#tn=				30.4.1.2.2 Imports and Attributes>
				30.4.1.2.3 Standard Library Modules <URL:#tn=				30.4.1.2.3 Standard Library Modules>
			30.4.1.3 How Imports Work <URL:#tn=			30.4.1.3 How Imports Work>
				30.4.1.3.1 1. Find It <URL:#tn=				30.4.1.3.1 1. Find It>
					30.4.1.3.1.1 The module search path <URL:#tn=					30.4.1.3.1.1 The module search path>
					30.4.1.3.1.2 The sys.path list <URL:#tn=					30.4.1.3.1.2 The sys.path list>
					30.4.1.3.1.3 Module file selection <URL:#tn=					30.4.1.3.1.3 Module file selection>
					30.4.1.3.1.4 Advanced module selection concepts <URL:#tn=					30.4.1.3.1.4 Advanced module selection concepts>
				30.4.1.3.2 2. Compile It (Maybe) <URL:#tn=				30.4.1.3.2 2. Compile It (Maybe)>
				30.4.1.3.3 3. Run It <URL:#tn=				30.4.1.3.3 3. Run It>
			30.4.1.4 Summary <URL:#tn=			30.4.1.4 Summary>
 	 	30.4.2. Module Coding Basics <URL:#tn= 	 	30.4.2. Module Coding Basics>
			30.4.2.1 Module Creation <URL:#tn=			30.4.2.1 Module Creation>
			30.4.2.2 Module Usage <URL:#tn=			30.4.2.2 Module Usage>
				30.4.2.2.1 The import Statement <URL:#tn=				30.4.2.2.1 The import Statement>
				30.4.2.2.2 The from statement <URL:#tn=				30.4.2.2.2 The from statement>
				30.4.2.2.3 The from * Statement <URL:#tn=				30.4.2.2.3 The from * Statement>
				30.4.2.2.4 Imports Happen Only Once <URL:#tn=				30.4.2.2.4 Imports Happen Only Once>
				30.4.2.2.5 import and from Are Assignments <URL:#tn=				30.4.2.2.5 import and from Are Assignments>
				30.4.2.2.6 Cross-File Name Changes <URL:#tn=				30.4.2.2.6 Cross-File Name Changes>
				30.4.2.2.7 import and from Equivalence <URL:#tn=				30.4.2.2.7 import and from Equivalence>
				30.4.2.2.8 Potential Pitfalls of the from Statement <URL:#tn=				30.4.2.2.8 Potential Pitfalls of the from Statement>
					30.4.2.2.8.1 When import is required <URL:#tn=					30.4.2.2.8.1 When import is required>
			30.4.2.3 Module Namespaces <URL:#tn=			30.4.2.3 Module Namespaces>
				30.4.2.3.1 Files Generate Namespaces <URL:#tn=				30.4.2.3.1 Files Generate Namespaces>
				30.4.2.3.2 Attribute Name Qualification <URL:#tn=				30.4.2.3.2 Attribute Name Qualification>
				30.4.2.3.3 Imports Versus Scopes <URL:#tn=				30.4.2.3.3 Imports Versus Scopes>
				30.4.2.3.4 Namespace Nesting <URL:#tn=				30.4.2.3.4 Namespace Nesting>
			30.4.2.4 Reloading Modules <URL:#tn=			30.4.2.4 Reloading Modules>
				30.4.2.4.1 reload Basics <URL:#tn=				30.4.2.4.1 reload Basics>
				30.4.2.4.2 reload Example <URL:#tn=				30.4.2.4.2 reload Example>
			30.4.2.5 Chapter Summary <URL:#tn=			30.4.2.5 Chapter Summary>
 	 	30.4.3. Module Packages <URL:#tn= 	 	30.4.3. Module Packages>
			30.4.3.1 Package Import Basics <URL:#tn=			30.4.3.1 Package Import Basics>
				30.4.3.1.1 Packages and Search Path Settings <URL:#tn=				30.4.3.1.1 Packages and Search Path Settings>
				30.4.3.1.2 Package __init__.py Files <URL:#tn=				30.4.3.1.2 Package __init__.py Files>
			30.4.3.2 Package Import Example <URL:#tn=			30.4.3.2 Package Import Example>
				30.4.3.2.1 from Versus import with Packages <URL:#tn=				30.4.3.2.1 from Versus import with Packages>
			30.4.3.3 Why Use Package Imports? <URL:#tn=			30.4.3.3 Why Use Package Imports?>
				30.4.3.3.1 A Tale of Three Systems <URL:#tn=				30.4.3.3.1 A Tale of Three Systems>
			30.4.3.4 Chapter Summary <URL:#tn=			30.4.3.4 Chapter Summary>
 	 	30.4.4. Advanced Module Topics <URL:#tn= 	 	30.4.4. Advanced Module Topics>
			30.4.4.1 . Data Hiding in Modules <URL:#tn=			30.4.4.1 . Data Hiding in Modules>
				30.4.4.1.1 Minimizing from * Damage: _X and __all__ <URL:#tn=				30.4.4.1.1 Minimizing from * Damage: _X and __all__>
			30.4.4.2 . Enabling Future Language Features <URL:#tn=			30.4.4.2 . Enabling Future Language Features>
			30.4.4.3 . Mixed Usage Modes: __name__ and __main__ <URL:#tn=			30.4.4.3 . Mixed Usage Modes: __name__ and __main__>
				30.4.4.3.1 Unit Tests with __name__ <URL:#tn=				30.4.4.3.1 Unit Tests with __name__>
			30.4.4.4 . Changing the Module Search Path <URL:#tn=			30.4.4.4 . Changing the Module Search Path>
			30.4.4.5 . The import as Extension <URL:#tn=			30.4.4.5 . The import as Extension>
			30.4.4.6 . Relative Import Syntax <URL:#tn=			30.4.4.6 . Relative Import Syntax>
				30.4.4.6.1 Why Relative Imports? <URL:#tn=				30.4.4.6.1 Why Relative Imports?>
			30.4.4.7 . Module Design Concepts <URL:#tn=			30.4.4.7 . Module Design Concepts>
				30.4.4.7.1 Modules Are Objects: Metaprograms <URL:#tn=				30.4.4.7.1 Modules Are Objects: Metaprograms>
			30.4.4.8 . Module Gotchas <URL:#tn=			30.4.4.8 . Module Gotchas>
				30.4.4.8.1 Statement Order Matters in Top-Level Code <URL:#tn=				30.4.4.8.1 Statement Order Matters in Top-Level Code>
				30.4.4.8.2 Importing Modules by Name String <URL:#tn=				30.4.4.8.2 Importing Modules by Name String>
				30.4.4.8.3 from Copies Names but Doesn't Link <URL:#tn=				30.4.4.8.3 from Copies Names but Doesn't Link>
				30.4.4.8.4 from * Can Obscure the Meaning of Variables <URL:#tn=				30.4.4.8.4 from * Can Obscure the Meaning of Variables>
				30.4.4.8.5 reload May Not Impact from Imports <URL:#tn=				30.4.4.8.5 reload May Not Impact from Imports>
				30.4.4.8.6 reload, from, and Interactive Testing <URL:#tn=				30.4.4.8.6 reload, from, and Interactive Testing>
				30.4.4.8.7 reload Isn't Applied Transitively <URL:#tn=				30.4.4.8.7 reload Isn't Applied Transitively>
				30.4.4.8.8 Recursive from Imports May Not Work <URL:#tn=				30.4.4.8.8 Recursive from Imports May Not Work>
			30.4.4.9 . Chapter Summary <URL:#tn=			30.4.4.9 . Chapter Summary>
			30.4.4.10. BRAIN BUILDER <URL:#tn=			30.4.4.10. BRAIN BUILDER>
	30.5. Classes and OOP <URL:#tn=	30.5. Classes and OOP>
		30.5.1. OOP: The Big Picture <URL:#tn=		30.5.1. OOP: The Big Picture>
			30.5.1.1 Why Use Classes? <URL:#tn=			30.5.1.1 Why Use Classes?>
			30.5.1.2 OOP from 30,000 Feet <URL:#tn=			30.5.1.2 OOP from 30,000 Feet>
				30.5.1.2.1 Attribute Inheritance Search <URL:#tn=				30.5.1.2.1 Attribute Inheritance Search>
			30.5.1.3 Chapter Summary <URL:#tn=			30.5.1.3 Chapter Summary>
		30.5.2. Class Coding Basis <URL:#tn=		30.5.2. Class Coding Basis>
			30.5.2.1 Classes Generate Multiple Instance Objects <URL:#tn=			30.5.2.1 Classes Generate Multiple Instance Objects>
				30.5.2.1.1 Class Objects Provide Default Behavior <URL:#tn=				30.5.2.1.1 Class Objects Provide Default Behavior>
				30.5.2.1.2 Instance Objects Are Concrete Items <URL:#tn=				30.5.2.1.2 Instance Objects Are Concrete Items>
				30.5.2.1.3 A First Example <URL:#tn=				30.5.2.1.3 A First Example>
3.14159 <URL:#tn=3.14159>
			30.5.2.2 Classes Are Customized by Inheritance <URL:#tn=			30.5.2.2 Classes Are Customized by Inheritance>
				30.5.2.2.1 A Second Example <URL:#tn=				30.5.2.2.1 A Second Example>
				30.5.2.2.2 Classes Are Attributes in Modules <URL:#tn=				30.5.2.2.2 Classes Are Attributes in Modules>
			30.5.2.3 Classes Can Intercept Python Operators <URL:#tn=			30.5.2.3 Classes Can Intercept Python Operators>
				30.5.2.3.1 A Third Example <URL:#tn=				30.5.2.3.1 A Third Example>
				30.5.2.3.2 Why Use Operator Overloading? <URL:#tn=				30.5.2.3.2 Why Use Operator Overloading?>
			30.5.2.4 The World's Simplest Python Class <URL:#tn=			30.5.2.4 The World's Simplest Python Class>
			30.5.2.5 Chapter Summary <URL:#tn=			30.5.2.5 Chapter Summary>
		30.5.3. Class Coding Details <URL:#tn=		30.5.3. Class Coding Details>
			30.5.3.1 The class Statement <URL:#tn=			30.5.3.1 The class Statement>
				30.5.3.1.1 General Form <URL:#tn=				30.5.3.1.1 General Form>
				30.5.3.1.2 Example <URL:#tn=				30.5.3.1.2 Example>
			30.5.3.2 Methods <URL:#tn=			30.5.3.2 Methods>
				30.5.3.2.1 Example <URL:#tn=				30.5.3.2.1 Example>
				30.5.3.2.2 Calling Superclass Constructors <URL:#tn=				30.5.3.2.2 Calling Superclass Constructors>
				30.5.3.2.3 Other Method Call Possibilities <URL:#tn=				30.5.3.2.3 Other Method Call Possibilities>
			30.5.3.3 Inheritance <URL:#tn=			30.5.3.3 Inheritance>
				30.5.3.3.1 Attribute Tree Construction <URL:#tn=				30.5.3.3.1 Attribute Tree Construction>
				30.5.3.3.2 Specializing Inherited Methods <URL:#tn=				30.5.3.3.2 Specializing Inherited Methods>
				30.5.3.3.3 Class Interface Techniques <URL:#tn=				30.5.3.3.3 Class Interface Techniques>
				30.5.3.3.4 Abstract Superclasses <URL:#tn=				30.5.3.3.4 Abstract Superclasses>
			30.5.3.4 Operator Overloading <URL:#tn=			30.5.3.4 Operator Overloading>
				30.5.3.4.1 Common Operator Overloading Methods <URL:#tn=				30.5.3.4.1 Common Operator Overloading Methods>
				30.5.3.4.2 __getitem__ Intercepts Index References <URL:#tn=				30.5.3.4.2 __getitem__ Intercepts Index References>
				30.5.3.4.3 __getitem__ and __iter__ Implement Iteration <URL:#tn=				30.5.3.4.3 __getitem__ and __iter__ Implement Iteration>
				30.5.3.4.4 User-Defined Iterators <URL:#tn=				30.5.3.4.4 User-Defined Iterators>
					30.5.3.4.4.1 Multiple iterators on one object <URL:#tn=					30.5.3.4.4.1 Multiple iterators on one object>
				30.5.3.4.5 __getattr__ and __setattr__ Catch Attribute References <URL:#tn=				30.5.3.4.5 __getattr__ and __setattr__ Catch Attribute References>
				30.5.3.4.6 Emulating Privacy for Instance Attributes <URL:#tn=				30.5.3.4.6 Emulating Privacy for Instance Attributes>
				30.5.3.4.7 __repr__ and __str__ Return String Representations <URL:#tn=				30.5.3.4.7 __repr__ and __str__ Return String Representations>
				30.5.3.4.8 __radd__ Handles Right-Side Addition <URL:#tn=				30.5.3.4.8 __radd__ Handles Right-Side Addition>
				30.5.3.4.9 __call__ Intercepts Calls <URL:#tn=				30.5.3.4.9 __call__ Intercepts Calls>
				30.5.3.4.10 Function Interfaces and Callback-Based Code <URL:#tn=				30.5.3.4.10 Function Interfaces and Callback-Based Code>
				30.5.3.4.11 __del__ Is a Destructor <URL:#tn=				30.5.3.4.11 __del__ Is a Destructor>
			30.5.3.5 Namespaces: The Whole Story <URL:#tn=			30.5.3.5 Namespaces: The Whole Story>
				30.5.3.5.1 Simple Names: Global Unless Assigned <URL:#tn=				30.5.3.5.1 Simple Names: Global Unless Assigned>
				30.5.3.5.2 Attribute Names: Object Namespaces <URL:#tn=				30.5.3.5.2 Attribute Names: Object Namespaces>
				30.5.3.5.3 The "Zen" of Python Namespaces: Assignments Classify Names <URL:#tn=				30.5.3.5.3 The "Zen" of Python Namespaces: Assignments Classify Names>
				30.5.3.5.4 Namespace Dictionaries <URL:#tn=				30.5.3.5.4 Namespace Dictionaries>
				30.5.3.5.5 Namespace Links <URL:#tn=				30.5.3.5.5 Namespace Links>
			30.5.3.6 A More Realistic Example <URL:#tn=			30.5.3.6 A More Realistic Example>
			30.5.3.7 Chapter Summary <URL:#tn=			30.5.3.7 Chapter Summary>
		30.5.4. Designing with Classes <URL:#tn=		30.5.4. Designing with Classes>
			30.5.4.1  Python and OOP <URL:#tn=			30.5.4.1  Python and OOP>
				30.5.4.1.1  Overloading by Call Signatures (or Not) <URL:#tn=				30.5.4.1.1  Overloading by Call Signatures (or Not)>
			30.5.4.2  Classes As Records <URL:#tn=			30.5.4.2  Classes As Records>
			30.5.4.3  OOP and Inheritance: "Is-a" Relationships <URL:#tn=			30.5.4.3  OOP and Inheritance: "Is-a" Relationships>
			30.5.4.4  OOP and Composition: "Has-a" Relationships <URL:#tn=			30.5.4.4  OOP and Composition: "Has-a" Relationships>
				30.5.4.4.1  Stream Processors Revisited <URL:#tn=				30.5.4.4.1  Stream Processors Revisited>
			30.5.4.5  OOP and Delegation <URL:#tn=			30.5.4.5  OOP and Delegation>
			30.5.4.6  Multiple Inheritance <URL:#tn=			30.5.4.6  Multiple Inheritance>
			30.5.4.7  Classes Are Objects: Generic Object Factories <URL:#tn=			30.5.4.7  Classes Are Objects: Generic Object Factories>
				30.5.4.7.1  Why Factories? <URL:#tn=				30.5.4.7.1  Why Factories?>
			30.5.4.8  Methods Are Objects: Bound or Unbound <URL:#tn=			30.5.4.8  Methods Are Objects: Bound or Unbound>
			30.5.4.9  Documentation Strings Revisited <URL:#tn=			30.5.4.9  Documentation Strings Revisited>
			30.5.4.10 Classes Versus Modules <URL:#tn=			30.5.4.10 Classes Versus Modules>
			30.5.4.11 Chapter Summary <URL:#tn=			30.5.4.11 Chapter Summary>
		30.5.5. Advanced Class Topics <URL:#tn=		30.5.5. Advanced Class Topics>
			30.5.5.1 Extending Built-in Types <URL:#tn=			30.5.5.1 Extending Built-in Types>
				30.5.5.1.1 Extending Types by Embedding <URL:#tn=				30.5.5.1.1 Extending Types by Embedding>
				30.5.5.1.2 Extending Types by Subclassing <URL:#tn=				30.5.5.1.2 Extending Types by Subclassing>
			30.5.5.2 Pseudoprivate Class Attributes <URL:#tn=			30.5.5.2 Pseudoprivate Class Attributes>
				30.5.5.2.1 Name Mangling Overview <URL:#tn=				30.5.5.2.1 Name Mangling Overview>
				30.5.5.2.2 Why Use Pseudoprivate Attributes? <URL:#tn=				30.5.5.2.2 Why Use Pseudoprivate Attributes?>
			30.5.5.3 New-Style Classes <URL:#tn=			30.5.5.3 New-Style Classes>
				30.5.5.3.1 Diamond Inheritance Change <URL:#tn=				30.5.5.3.1 Diamond Inheritance Change>
					30.5.5.3.1.1 Diamond inheritance example <URL:#tn=					30.5.5.3.1.1 Diamond inheritance example>
					30.5.5.3.1.2 Explicit conflict resolution <URL:#tn=					30.5.5.3.1.2 Explicit conflict resolution>
				30.5.5.3.2 Other New-Style Class Extensions <URL:#tn=				30.5.5.3.2 Other New-Style Class Extensions>
					30.5.5.3.2.1 Static and class methods <URL:#tn=					30.5.5.3.2.1 Static and class methods>
					30.5.5.3.2.2 Instance slots <URL:#tn=					30.5.5.3.2.2 Instance slots>
					30.5.5.3.2.3 Class properties <URL:#tn=					30.5.5.3.2.3 Class properties>
					30.5.5.3.2.4 New __getattribute__ overloading method <URL:#tn=					30.5.5.3.2.4 New __getattribute__ overloading method>
			30.5.5.4 Static and Class Methods <URL:#tn=			30.5.5.4 Static and Class Methods>
				30.5.5.4.1 Using Static and Class Methods <URL:#tn=				30.5.5.4.1 Using Static and Class Methods>
			30.5.5.5 Function Decorators <URL:#tn=			30.5.5.5 Function Decorators>
				30.5.5.5.1 Decorator Example <URL:#tn=				30.5.5.5.1 Decorator Example>
			30.5.5.6 Class Gotchas <URL:#tn=			30.5.5.6 Class Gotchas>
				30.5.5.6.1 Changing Class Attributes Can Have Side Effects <URL:#tn=				30.5.5.6.1 Changing Class Attributes Can Have Side Effects>
				30.5.5.6.2 Multiple Inheritance: Order Matters <URL:#tn=				30.5.5.6.2 Multiple Inheritance: Order Matters>
				30.5.5.6.3 Methods, Classes, and Nested Scopes <URL:#tn=				30.5.5.6.3 Methods, Classes, and Nested Scopes>
				30.5.5.6.4 "Overwrapping-itis" <URL:#tn=				30.5.5.6.4 "Overwrapping-itis">
			30.5.5.7 Chapter Summary <URL:#tn=			30.5.5.7 Chapter Summary>
			30.5.5.8 BRAIN BUILDER <URL:#tn=			30.5.5.8 BRAIN BUILDER>
				30.5.5.8.1 Exercises <URL:#tn=				30.5.5.8.1 Exercises>
					30.5.5.8.1.1 Inheritance. <URL:#tn=					30.5.5.8.1.1 Inheritance.>
					30.5.5.8.1.2 Operator overloading. <URL:#tn=					30.5.5.8.1.2 Operator overloading.>
					30.5.5.8.1.3 Subclassing. <URL:#tn=					30.5.5.8.1.3 Subclassing.>
					30.5.5.8.1.4 Metaclass methods. <URL:#tn=					30.5.5.8.1.4 Metaclass methods.>
					30.5.5.8.1.5 Set objects. <URL:#tn=					30.5.5.8.1.5 Set objects.>
					30.5.5.8.1.6 Class tree links. <URL:#tn=					30.5.5.8.1.6 Class tree links.>
					30.5.5.8.1.7 Composition. <URL:#tn=					30.5.5.8.1.7 Composition.>
					30.5.5.8.1.8 Zoo animal hierarchy. <URL:#tn=					30.5.5.8.1.8 Zoo animal hierarchy.>
					30.5.5.8.1.9 The Dead Parrot Sketch. <URL:#tn=					30.5.5.8.1.9 The Dead Parrot Sketch.>
				30.5.5.8.2 Answers <URL:#tn=				30.5.5.8.2 Answers>
					30.5.5.8.2.1 Inheritance. <URL:#tn=					30.5.5.8.2.1 Inheritance.>
					30.5.5.8.2.2 Operator overloading. <URL:#tn=					30.5.5.8.2.2 Operator overloading.>
					30.5.5.8.2.3 Subclassing. <URL:#tn=					30.5.5.8.2.3 Subclassing.>
					30.5.5.8.2.4 Metaclass methods. <URL:#tn=					30.5.5.8.2.4 Metaclass methods.>
					30.5.5.8.2.5 Set objects. <URL:#tn=					30.5.5.8.2.5 Set objects.>
					30.5.5.8.2.6 Class tree links. <URL:#tn=					30.5.5.8.2.6 Class tree links.>
					30.5.5.8.2.7 Composition. <URL:#tn=					30.5.5.8.2.7 Composition.>
					30.5.5.8.2.8 Zoo animal hierarchy. <URL:#tn=					30.5.5.8.2.8 Zoo animal hierarchy.>
					30.5.5.8.2.9 The Dead Parrot Sketch. <URL:#tn=					30.5.5.8.2.9 The Dead Parrot Sketch.>
				30.5.5.8.3 <URL:#tn=				30.5.5.8.3>
	30.6. Exceptions and Tools <URL:#tn=	30.6. Exceptions and Tools>
		30.6.1. Exception Basics <URL:#tn=		30.6.1. Exception Basics>
			30.6.1.1 Why Use Exceptions? <URL:#tn=			30.6.1.1 Why Use Exceptions?>
			30.6.1.2 Exception Handling: The Short Story <URL:#tn=			30.6.1.2 Exception Handling: The Short Story>
			30.6.1.3 The try/except/else Statement <URL:#tn=			30.6.1.3 The try/except/else Statement>
			30.6.1.4 The try/finally Statement <URL:#tn=			30.6.1.4 The try/finally Statement>
			30.6.1.5 Unified try/except/finally <URL:#tn=			30.6.1.5 Unified try/except/finally>
			30.6.1.6 The raise Statement <URL:#tn=			30.6.1.6 The raise Statement>
			30.6.1.7 The assert Statement <URL:#tn=			30.6.1.7 The assert Statement>
			30.6.1.8 with/as Context Managers <URL:#tn=			30.6.1.8 with/as Context Managers>
			30.6.1.9 Chapter Summary <URL:#tn=			30.6.1.9 Chapter Summary>
		30.6.2. Exception Objects <URL:#tn=		30.6.2. Exception Objects>
			30.6.2.1 String-Based Exceptions <URL:#tn=			30.6.2.1 String-Based Exceptions>
			30.6.2.2 Class-Based Exceptions <URL:#tn=			30.6.2.2 Class-Based Exceptions>
			30.6.2.3 General raise Statement Forms <URL:#tn=			30.6.2.3 General raise Statement Forms>
			30.6.2.4 Chapter Summary <URL:#tn=			30.6.2.4 Chapter Summary>
		30.6.3. Designing with Exceptions <URL:#tn=		30.6.3. Designing with Exceptions>
			30.6.3.1 Nesting Exception Handlers <URL:#tn=			30.6.3.1 Nesting Exception Handlers>
			30.6.3.2 Exception Idioms <URL:#tn=			30.6.3.2 Exception Idioms>
			30.6.3.3 Exception Design Tips <URL:#tn=			30.6.3.3 Exception Design Tips>
			30.6.3.4 Exception Gotchas <URL:#tn=			30.6.3.4 Exception Gotchas>
			30.6.3.5 Core Language Summary <URL:#tn=			30.6.3.5 Core Language Summary>
			30.6.3.6 Chapter Summary <URL:#tn=			30.6.3.6 Chapter Summary>
			30.6.3.7 BRAIN BUILDER <URL:#tn=			30.6.3.7 BRAIN BUILDER>
		30.6.3. <URL:#tn=		30.6.3.>
	30.7. Appendixes <URL:#tn=	30.7. Appendixes>
		30.7.1. Installation and Configuration <URL:#tn=		30.7.1. Installation and Configuration>
31. Python Pocket reference, 4th edition <URL:#tn=31. Python Pocket reference, 4th edition>
	31.1 Common idioms <URL:#tn=	31.1 Common idioms>
32. Recipes <URL:#tn=32. Recipes>
	32.1 Sorting <URL:#tn=	32.1 Sorting>
		32.1.1 Sorting Mini-HOW TO <URL:#tn=		32.1.1 Sorting Mini-HOW TO>
		32.1.2 "Sort" Dictionaries <URL:#tn=		32.1.2 "Sort" Dictionaries>
			32.1.2.1 Python 2.4 <URL:#tn=			32.1.2.1 Python 2.4>
			32.1.2.2 <URL:#tn=			32.1.2.2>
		32.1.3 Sort lists that contain class objects <URL:#tn=		32.1.3 Sort lists that contain class objects>
			32.1.3.1  Override cmp operator <URL:#tn=			32.1.3.1  Override cmp operator>
			32.1.3.2 Pass comparison function to sort() <URL:#tn=			32.1.3.2 Pass comparison function to sort()>
		32.1.4 <URL:#tn=		32.1.4>
	32.2 Operator overloading <URL:#tn=	32.2 Operator overloading>
		32.2.1 equality operator, ==, comparison <URL:#tn=		32.2.1 equality operator, ==, comparison>
	32.3 <URL:#tn=	32.3>
33. Environment, Custimization, IDLE <URL:#tn=33. Environment, Custimization, IDLE>
	33.1 .pythonrc <URL:#tn=	33.1 .pythonrc>
		33.1.1 Example 1 <URL:#tn=		33.1.1 Example 1>
		33.1.2 <URL:#tn=		33.1.2>
	33.2 <URL:#tn=	33.2>
34. Tips, Tricks, Quick guides <URL:#tn=34. Tips, Tricks, Quick guides>
	34.1 Python Tips, Tricks, and Hacks <URL:#tn=	34.1 Python Tips, Tricks, and Hacks>
		34.1.1 1   Quick Tricks <URL:#tn=		34.1.1 1   Quick Tricks>
			34.1.1.1 Four Kinds of Quotes <URL:#tn=			34.1.1.1 Four Kinds of Quotes>
			34.1.1.2 Truthfulness of Various Objects <URL:#tn=			34.1.1.2 Truthfulness of Various Objects>
			34.1.1.3 Checking if a String Contains a Substring <URL:#tn=			34.1.1.3 Checking if a String Contains a Substring>
			34.1.1.4 Pretty-Printing a List <URL:#tn=			34.1.1.4 Pretty-Printing a List>
			34.1.1.5 Integer vs. Float Division <URL:#tn=			34.1.1.5 Integer vs. Float Division>
			34.1.1.6 Lambda Functions <URL:#tn=			34.1.1.6 Lambda Functions>
		34.1.2 Lists <URL:#tn=		34.1.2 Lists>
			34.1.2.1 List Comprehensions <URL:#tn=			34.1.2.1 List Comprehensions>
			34.1.2.2 Mapping the List <URL:#tn=			34.1.2.2 Mapping the List>
			34.1.2.3 Filtering the List <URL:#tn=			34.1.2.3 Filtering the List>
			34.1.2.4 Map and Filter at Once <URL:#tn=			34.1.2.4 Map and Filter at Once>
			34.1.2.5 Generator Expressions <URL:#tn=			34.1.2.5 Generator Expressions>
			34.1.2.6 Nested 'for' Statements <URL:#tn=			34.1.2.6 Nested 'for' Statements>
			34.1.2.7 Conclusion <URL:#tn=			34.1.2.7 Conclusion>
			34.1.2.8 Reducing a List <URL:#tn=			34.1.2.8 Reducing a List>
			34.1.2.9 Iterating over a List: range, xrange and enumerate <URL:#tn=			34.1.2.9 Iterating over a List: range, xrange and enumerate>
			34.1.2.10 Checking a Condition on Any or Every List Element <URL:#tn=			34.1.2.10 Checking a Condition on Any or Every List Element>
			34.1.2.11 Combining Multiple Lists, Item by Item <URL:#tn=			34.1.2.11 Combining Multiple Lists, Item by Item>
			34.1.2.12 A Few More List Operators <URL:#tn=			34.1.2.12 A Few More List Operators>
			34.1.2.13 Advanced Logic With Sets <URL:#tn=			34.1.2.13 Advanced Logic With Sets>
		34.1.3 Dictionaries <URL:#tn=		34.1.3 Dictionaries>
			34.1.3.1 Constructing Dictionaries with Keyword Arguments <URL:#tn=			34.1.3.1 Constructing Dictionaries with Keyword Arguments>
			34.1.3.2 Dicts to Lists <URL:#tn=			34.1.3.2 Dicts to Lists>
			34.1.3.3 Lists to Dicts <URL:#tn=			34.1.3.3 Lists to Dicts>
			34.1.3.4 'Dictionary Comprehensions' <URL:#tn=			34.1.3.4 'Dictionary Comprehensions'>
		34.1.4 Selecting Values <URL:#tn=		34.1.4 Selecting Values>
			34.1.4.1 The Right Way <URL:#tn=			34.1.4.1 The Right Way>
			34.1.4.2 The and/or Trick <URL:#tn=			34.1.4.2 The and/or Trick>
			34.1.4.3 Using True and False as Indexes <URL:#tn=			34.1.4.3 Using True and False as Indexes>
		34.1.5 Functions <URL:#tn=		34.1.5 Functions>
			34.1.5.1 Default Argument Values are Only Evaluated Once <URL:#tn=			34.1.5.1 Default Argument Values are Only Evaluated Once>
			34.1.5.2 Force Default Arguments to be Evaluated Each Time <URL:#tn=			34.1.5.2 Force Default Arguments to be Evaluated Each Time>
			34.1.5.3 Arbitrary Numbers of Arguments <URL:#tn=			34.1.5.3 Arbitrary Numbers of Arguments>
			34.1.5.4 Caveat <URL:#tn=			34.1.5.4 Caveat>
			34.1.5.5 Passing a List or Dictionary as Arguments <URL:#tn=			34.1.5.5 Passing a List or Dictionary as Arguments>
			34.1.5.6 Decorators <URL:#tn=			34.1.5.6 Decorators>
			34.1.5.7 'Switch Statements' using Dictionaries of Functions <URL:#tn=			34.1.5.7 'Switch Statements' using Dictionaries of Functions>
		34.1.6 Classes <URL:#tn=		34.1.6 Classes>
			34.1.6.1 Passing 'self' Manually <URL:#tn=			34.1.6.1 Passing 'self' Manually>
			34.1.6.2 Checking for Property and Method Existence <URL:#tn=			34.1.6.2 Checking for Property and Method Existence>
			34.1.6.3 Modifying Classes After Creation <URL:#tn=			34.1.6.3 Modifying Classes After Creation>
			34.1.6.4 Creating Class Methods <URL:#tn=			34.1.6.4 Creating Class Methods>
35. riddles <URL:#tn=35. riddles>
	35.1 http://www.pythonchallenge.com/ <URL:#tn=	35.1 http://www.pythonchallenge.com/>
		35.1.1  2^34 <URL:#tn=		35.1.1  2^34>
		35.1.2 translation <URL:#tn=		35.1.2 translation>
		35.1.3 <URL:#tn=		35.1.3>
	35.2 Find which digit is represented by which letter so that following is true: <URL:#tn=	35.2 Find which digit is represented by which letter so that following is true:>
	35.3 <URL:#tn=	35.3>
36. My examples, recipes <URL:#tn=36. My examples, recipes>
	36.1  strings <URL:#tn=	36.1  strings>
		36.1.1   Translate strings <URL:#tn=		36.1.1   Translate strings>
			36.1.1.1    using ord and chr <URL:#tn=			36.1.1.1    using ord and chr>
			36.1.1.2 Using string.maketrans() <URL:#tn=			36.1.1.2 Using string.maketrans()>
			36.1.1.3 <URL:#tn=			36.1.1.3>
		36.1.2 Extract filename from full path in Python? <URL:#tn=		36.1.2 Extract filename from full path in Python?>
		36.1.3 in python how to remove this \n from string or list <URL:#tn=		36.1.3 in python how to remove this \n from string or list>
		36.1.4 slices <URL:#tn=		36.1.4 slices>
		36.1.5 namedtuple <URL:#tn=		36.1.5 namedtuple>
		36.1.6 Run code Snipplets from pycharm <URL:#tn=		36.1.6 Run code Snipplets from pycharm>
	36.2 directories <URL:#tn=	36.2 directories>
		36.2.1 simple directory related tasks <URL:#tn=		36.2.1 simple directory related tasks>
			36.2.1.1 File and Directory Access <URL:#tn=			36.2.1.1 File and Directory Access>
				36.2.1.1.1 os.path — Common pathname manipulations <URL:#tn=				36.2.1.1.1 os.path — Common pathname manipulations>
				36.2.1.1.2 fileinput — Iterate over lines from multiple input streams <URL:#tn=				36.2.1.1.2 fileinput — Iterate over lines from multiple input streams>
			36.2.1.2 get current working directory <URL:#tn=			36.2.1.2 get current working directory>
			36.2.1.3 <URL:#tn=			36.2.1.3>
		36.2.2 <URL:#tn=		36.2.2>
	36.3 Time <URL:#tn=	36.3 Time>
	    36.3.1 Simple measure elapsed time <URL:#tn=	    36.3.1 Simple measure elapsed time>
	    36.3.2 <URL:#tn=	    36.3.2>
	36.4 dates <URL:#tn=	36.4 dates>
	    36.4.1 pretty print date and time <URL:#tn=	    36.4.1 pretty print date and time>
	    36.4.2 <URL:#tn=	    36.4.2>
	36.5 tips and tricks <URL:#tn=	36.5 tips and tricks>
	    36.5.1 redirect prints to stdout <URL:#tn=	    36.5.1 redirect prints to stdout>
	36.6 <URL:#tn=	36.6>
37. Concepts, syntax constructs <URL:#tn=37. Concepts, syntax constructs>
	37.1 List comprehension <URL:#tn=	37.1 List comprehension>
	37.2 With as statement <URL:#tn=	37.2 With as statement>
		37.2.1 Understanding Python's "with" statement <URL:#tn=		37.2.1 Understanding Python's "with" statement>
		37.2.2 The with statement <URL:#tn=		37.2.2 The with statement>
		37.2.3 <URL:#tn=		37.2.3>
	37.3 <URL:#tn=	37.3>
38. Graphs <URL:#tn=38. Graphs>
	38.1  networkx <URL:#tn=	38.1  networkx>
		38.1.1  Tutorial <URL:#tn=		38.1.1  Tutorial>
		38.1.2 <URL:#tn=		38.1.2>
		38.1.3 <URL:#tn=		38.1.3>
		38.1.4 <URL:#tn=		38.1.4>
		38.1.5 <URL:#tn=		38.1.5>
		38.1.6 <URL:#tn=		38.1.6>
		38.1.7 <URL:#tn=		38.1.7>
		38.1.8 <URL:#tn=		38.1.8>
		38.1.9 <URL:#tn=		38.1.9>
		38.1.10 <URL:#tn=		38.1.10>
		38.1.11 <URL:#tn=		38.1.11>
		38.1.12 <URL:#tn=		38.1.12>
		38.1.13 <URL:#tn=		38.1.13>
		38.1.14 <URL:#tn=		38.1.14>
		38.1.15 <URL:#tn=		38.1.15>
		38.1.16 <URL:#tn=		38.1.16>
		38.1.17 <URL:#tn=		38.1.17>
		38.1.18 <URL:#tn=		38.1.18>
		38.1.19 <URL:#tn=		38.1.19>
		38.1.20 <URL:#tn=		38.1.20>
		38.1.21 <URL:#tn=		38.1.21>
		38.1.22 <URL:#tn=		38.1.22>
		38.1.23 <URL:#tn=		38.1.23>
		38.1.24 <URL:#tn=		38.1.24>
		38.1.25 <URL:#tn=		38.1.25>
		38.1.26 <URL:#tn=		38.1.26>
		38.1.27 <URL:#tn=		38.1.27>
		38.1.28 <URL:#tn=		38.1.28>
		38.1.29 <URL:#tn=		38.1.29>
		38.1.30 <URL:#tn=		38.1.30>
		38.1.31 <URL:#tn=		38.1.31>
		38.1.32 <URL:#tn=		38.1.32>
		38.1.33 <URL:#tn=		38.1.33>
		38.1.34 <URL:#tn=		38.1.34>
		38.1.35 <URL:#tn=		38.1.35>
		38.1.36 <URL:#tn=		38.1.36>
		38.1.37 <URL:#tn=		38.1.37>
		38.1.38 <URL:#tn=		38.1.38>
		38.1.39 <URL:#tn=		38.1.39>
		38.1.40 <URL:#tn=		38.1.40>
		38.1.41 <URL:#tn=		38.1.41>
	38.2   graphviz <URL:#tn=	38.2   graphviz>
	38.3 <URL:#tn=	38.3>
	38.4 <URL:#tn=	38.4>
	38.5 <URL:#tn=	38.5>
	38.6 <URL:#tn=	38.6>
	38.7 <URL:#tn=	38.7>
	38.8 <URL:#tn=	38.8>
	38.9 <URL:#tn=	38.9>
	38.10 <URL:#tn=	38.10>
	38.11 <URL:#tn=	38.11>
	38.12 <URL:#tn=	38.12>
	38.13 <URL:#tn=	38.13>
	38.14 <URL:#tn=	38.14>
	38.15 <URL:#tn=	38.15>
	38.16 <URL:#tn=	38.16>
	38.17 <URL:#tn=	38.17>
	38.18 <URL:#tn=	38.18>
	38.19 <URL:#tn=	38.19>
	38.20 <URL:#tn=	38.20>
	38.21 <URL:#tn=	38.21>
	38.22 <URL:#tn=	38.22>
	38.23 <URL:#tn=	38.23>
	38.24 <URL:#tn=	38.24>
	38.25 <URL:#tn=	38.25>
	38.26 <URL:#tn=	38.26>
	38.27 <URL:#tn=	38.27>
	38.28 <URL:#tn=	38.28>
	38.29 <URL:#tn=	38.29>
	38.30 <URL:#tn=	38.30>
	38.31 <URL:#tn=	38.31>
39. One-liners <URL:#tn=39. One-liners>
	39.1 Powerful Python One-Liners <URL:#tn=	39.1 Powerful Python One-Liners>
		39.1.1 Want to know many bytes a terabyte is? If you know further abbreviations, you can extend the list. <URL:#tn=		39.1.1 Want to know many bytes a terabyte is? If you know further abbreviations, you can extend the list.>
		39.1.2 Convert decimal to hexadecimal <URL:#tn=		39.1.2 Convert decimal to hexadecimal>
		39.1.3 Convert decimal to binary <URL:#tn=		39.1.3 Convert decimal to binary>
		39.1.4 And what's the largest number that can be represented by 8 Byte? <URL:#tn=		39.1.4 And what's the largest number that can be represented by 8 Byte?>
		39.1.5 Find prime numbers <URL:#tn=		39.1.5 Find prime numbers>
		39.1.6 find length of each word in sentence <URL:#tn=		39.1.6 find length of each word in sentence>
		39.1.7 Get list of mounts <URL:#tn=		39.1.7 Get list of mounts>
		39.1.8 Decode a base64 encoded file <URL:#tn=		39.1.8 Decode a base64 encoded file>
		39.1.9 Editing a list of files in place <URL:#tn=		39.1.9 Editing a list of files in place>
		39.1.10 Reimplementing cut <URL:#tn=		39.1.10 Reimplementing cut>
		39.1.11 Set of all subsets <URL:#tn=		39.1.11 Set of all subsets>
		39.1.12 Apply regular expression to lines from stdin <URL:#tn=		39.1.12 Apply regular expression to lines from stdin>
		39.1.13 More one-liners <URL:#tn=		39.1.13 More one-liners>
		39.1.14 <URL:#tn=		39.1.14>
	39.2 <URL:#tn=	39.2>
40. Python Intermiddiate course <URL:#tn=40. Python Intermiddiate course>
	40.1 Search Engine demo <URL:#tn=	40.1 Search Engine demo>
		40.1.1 DB <URL:#tn=		40.1.1 DB>
		40.1.2 REST demo <URL:#tn=		40.1.2 REST demo>
		40.1.3 Run <URL:#tn=		40.1.3 Run>
		40.1.4 <URL:#tn=		40.1.4>
	40.2 Lucene based, whoosh local files search engine <URL:#tn=	40.2 Lucene based, whoosh local files search engine>
	    40.2.1 whoosh blog <URL:#tn=	    40.2.1 whoosh blog>
1. Creating Indexed Data: Whoosh <URL:#tn=1. Creating Indexed Data: Whoosh>
2. Querying Indexed Data : Whoosh <URL:#tn=2. Querying Indexed Data : Whoosh>
3. Glossary <URL:#tn=3. Glossary>
	    40.2.2 my implementation <URL:#tn=	    40.2.2 my implementation>
	    40.2.3 <URL:#tn=	    40.2.3>
	40.3 <URL:#tn=	40.3>
41. machine learning <URL:#tn=41. machine learning>
    41.1  tutorial point python machine learning <URL:#tn=    41.1  tutorial point python machine learning>
        41.1.1   setup <URL:#tn=        41.1.1   setup>
        41.1.2 What is machine_learning <URL:#tn=        41.1.2 What is machine_learning>
        41.1.3 data preprocessing <URL:#tn=        41.1.3 data preprocessing>
            41.1.3.1 mean, std, scale <URL:#tn=            41.1.3.1 mean, std, scale>
            41.1.3.2 Normalization <URL:#tn=            41.1.3.2 Normalization>
            41.1.3.3 One Hot Encoding <URL:#tn=            41.1.3.3 One Hot Encoding>
                41.1.3.3.1 One Hot Encoding explained <URL:#tn=                41.1.3.3.1 One Hot Encoding explained>
                41.1.3.3.2 <URL:#tn=                41.1.3.3.2>
            41.1.3.4 Label Encoding <URL:#tn=            41.1.3.4 Label Encoding>
            41.1.3.5 <URL:#tn=            41.1.3.5>
    41.2 Data analysis <URL:#tn=    41.2 Data analysis>
        41.2.1 Loading the Dataset <URL:#tn=        41.2.1 Loading the Dataset>
        41.2.2 Data Visualization <URL:#tn=        41.2.2 Data Visualization>
        41.2.3 <URL:#tn=        41.2.3>
    41.3 <URL:#tn=    41.3>
42. Important packages, modules, tags: Important packages, modules libraries <URL:#tn=42. Important packages, modules, tags: Important packages, modules libraries>
    42.1 flask, lightweight web framework <URL:#tn=    42.1 flask, lightweight web framework>
        42.1.1 http://flask.pocoo.org/docs/1.0/tutorial/ <URL:#tn=        42.1.1 http://flask.pocoo.org/docs/1.0/tutorial/>
        42.1.2 https://www.tutorialspoint.com/flask <URL:#tn=        42.1.2 https://www.tutorialspoint.com/flask>
127.0.0.1 - - [29/May/2019 16:45:30] "GET / HTTP/1.1" 200 - <URL:#tn=127.0.0.1 - - [29/May/2019 16:45:30] "GET / HTTP/1.1" 200 ->
        42.1.3 flask tutorial <URL:#tn=        42.1.3 flask tutorial>
            42.1.3.1  https://www.tutorialspoint.com/flask/index.htm <URL:#tn=            42.1.3.1  https://www.tutorialspoint.com/flask/index.htm>
                42.1.3.1.1  basics <URL:#tn=                42.1.3.1.1  basics>
                    42.1.3.1.1.1  hello flask <URL:#tn=                    42.1.3.1.1.1  hello flask>
                    42.1.3.1.1.2 <URL:#tn=                    42.1.3.1.1.2>
            42.1.3.2 <URL:#tn=            42.1.3.2>
        42.1.4 <URL:#tn=        42.1.4>
    42.2 pramiko, ssh, ftp, execute commands remotely <URL:#tn=    42.2 pramiko, ssh, ftp, execute commands remotely>
        42.2.1  https://medium.com/@keagileageek/paramiko-how-to-ssh-and-file-transfers-with-python-75766179de73 <URL:#tn=        42.2.1  https://medium.com/@keagileageek/paramiko-how-to-ssh-and-file-transfers-with-python-75766179de73>
        42.2.2 <URL:#tn=        42.2.2>
    42.3 Data science, AI, big data <URL:#tn=    42.3 Data science, AI, big data>
        42.3.1 overview <URL:#tn=        42.3.1 overview>
            42.3.1.1 Numpy <URL:#tn=            42.3.1.1 Numpy>
            42.3.1.2 Pandas, tags: Pandas <URL:#tn=            42.3.1.2 Pandas, tags: Pandas>
                42.3.1.2.1 chatgpt overview of pandas, tags: chatgpt overview of pandas <URL:#tn=                42.3.1.2.1 chatgpt overview of pandas, tags: chatgpt overview of pandas>
                42.3.1.2.2 udemy 100 day python course pandas chapter (day 25) <URL:#tn=                42.3.1.2.2 udemy 100 day python course pandas chapter (day 25)>
                    42.3.1.2.2.1 pandas101 <URL:#tn=                    42.3.1.2.2.1 pandas101>
                    42.3.1.2.2.2 chatgpt pandas count number of rows with a specific value in a DataFrame <URL:#tn=                    42.3.1.2.2.2 chatgpt pandas count number of rows with a specific value in a DataFrame>
                    42.3.1.2.2.3 <URL:#tn=                    42.3.1.2.2.3>
                42.3.1.2.3 <URL:#tn=                42.3.1.2.3>
            42.3.1.3 Matplotlib <URL:#tn=            42.3.1.3 Matplotlib>
                42.3.1.3.1 chatpgpt overview of matplotlib in python <URL:#tn=                42.3.1.3.1 chatpgpt overview of matplotlib in python>
                42.3.1.3.2 <URL:#tn=                42.3.1.3.2>
            42.3.1.4 Scikit-Learn <URL:#tn=            42.3.1.4 Scikit-Learn>
            42.3.1.5 Scipy <URL:#tn=            42.3.1.5 Scipy>
        42.3.2 <URL:#tn=        42.3.2>
    42.4 20 Python libraries you can’t live without <URL:#tn=    42.4 20 Python libraries you can’t live without>
        42.4.1 Requests. The most famous http library written by kenneth reitz. It’s a must have for every python developer. <URL:#tn=        42.4.1 Requests. The most famous http library written by kenneth reitz. It’s a must have for every python developer.>
            42.4.1.1 q. provide explanation about python requests package with some code examples <URL:#tn=            42.4.1.1 q. provide explanation about python requests package with some code examples>
            42.4.1.2 perform outh2 authentication using requests <URL:#tn=            42.4.1.2 perform outh2 authentication using requests>
            42.4.1.3 upload a file using requests <URL:#tn=            42.4.1.3 upload a file using requests>
            42.4.1.4 <URL:#tn=            42.4.1.4>
        42.4.2. Scrapy. If you are involved in webscraping then this is a must have library for you. After using this library you won’t use any other. <URL:#tn=        42.4.2. Scrapy. If you are involved in webscraping then this is a must have library for you. After using this library you won’t use any other.>
            42.4.2.1 provide explanation about python Scrapy package with some code examples <URL:#tn=            42.4.2.1 provide explanation about python Scrapy package with some code examples>
            42.4.2.2 provide a more complex example using middleware pipeline and settings customization <URL:#tn=            42.4.2.2 provide a more complex example using middleware pipeline and settings customization>
1. Middleware: <URL:#tn=1. Middleware:>
2. Pipelines: <URL:#tn=2. Pipelines:>
3. Settings Customization: <URL:#tn=3. Settings Customization:>
            42.4.2.3 <URL:#tn=            42.4.2.3>
        42.4.3. wxPython. A gui toolkit for python. I have primarily used it in place of tkinter. You will really love it. <URL:#tn=        42.4.3. wxPython. A gui toolkit for python. I have primarily used it in place of tkinter. You will really love it.>
        42.4.4. Pillow. A friendly fork of PIL (Python Imaging Library). It is more user friendly than PIL and is a must have for anyone who works with images. <URL:#tn=        42.4.4. Pillow. A friendly fork of PIL (Python Imaging Library). It is more user friendly than PIL and is a must have for anyone who works with images.>
        42.4.5. SQLAlchemy. A database library. Many love it and many hate it. The choice is yours. <URL:#tn=        42.4.5. SQLAlchemy. A database library. Many love it and many hate it. The choice is yours.>
        42.4.6. BeautifulSoup. I know it’s slow but this xml and html parsing library is very useful for beginners. <URL:#tn=        42.4.6. BeautifulSoup. I know it’s slow but this xml and html parsing library is very useful for beginners.>
        42.4.7. Twisted. The most important tool for any network application developer. It has a very beautiful api and is used by a lot of famous python developers. <URL:#tn=        42.4.7. Twisted. The most important tool for any network application developer. It has a very beautiful api and is used by a lot of famous python developers.>
        42.4.8. NumPy. How can we leave this very important library ? It provides some advance math functionalities to python. <URL:#tn=        42.4.8. NumPy. How can we leave this very important library ? It provides some advance math functionalities to python.>
        42.4.9. SciPy. When we talk about NumPy then we have to talk about scipy. It is a library of algorithms and mathematical tools for python and has caused many scientists to switch from ruby to python. <URL:#tn=        42.4.9. SciPy. When we talk about NumPy then we have to talk about scipy. It is a library of algorithms and mathematical tools for python and has caused many scientists to switch from ruby to python.>
        42.4.10. matplotlib. A numerical plotting library. It is very useful for any data scientist or any data analyzer. <URL:#tn=        42.4.10. matplotlib. A numerical plotting library. It is very useful for any data scientist or any data analyzer.>
        42.4.11. Pygame. Which developer does not like to play games and develop them ? This library will help you achieve your goal of 2d game development. <URL:#tn=        42.4.11. Pygame. Which developer does not like to play games and develop them ? This library will help you achieve your goal of 2d game development.>
        42.4.12. Pyglet. A 3d animation and game creation engine. This is the engine in which the famous python port of minecraft was made <URL:#tn=        42.4.12. Pyglet. A 3d animation and game creation engine. This is the engine in which the famous python port of minecraft was made>
        42.4.13. pyQT. A GUI toolkit for python. It is my second choice after wxpython for developing GUI’s for my python scripts. <URL:#tn=        42.4.13. pyQT. A GUI toolkit for python. It is my second choice after wxpython for developing GUI’s for my python scripts.>
        42.4.14. pyGtk. Another python GUI library. It is the same library in which the famous Bittorrent client is created. <URL:#tn=        42.4.14. pyGtk. Another python GUI library. It is the same library in which the famous Bittorrent client is created.>
        42.4.15. Scapy. A packet sniffer and analyzer for python made in python. <URL:#tn=        42.4.15. Scapy. A packet sniffer and analyzer for python made in python.>
        42.4.16. pywin32. A python library which provides some useful methods and classes for interacting with windows. <URL:#tn=        42.4.16. pywin32. A python library which provides some useful methods and classes for interacting with windows.>
        42.4.17. nltk. Natural Language Toolkit – I realize most people won’t be using this one, but it’s generic enough. It is a very useful library if you want to manipulate strings. But it’s capacity is beyond that. Do check it out. <URL:#tn=        42.4.17. nltk. Natural Language Toolkit – I realize most people won’t be using this one, but it’s generic enough. It is a very useful library if you want to manipulate strings. But it’s capacity is beyond that. Do check it out.>
        42.4.18. nose. A testing framework for python. It is used by millions of python developers. It is a must have if you do test driven development. <URL:#tn=        42.4.18. nose. A testing framework for python. It is used by millions of python developers. It is a must have if you do test driven development.>
        42.4.19. SymPy. SymPy can do algebraic evaluation, differentiation, expansion, complex numbers, etc. It is contained in a pure Python distribution. <URL:#tn=        42.4.19. SymPy. SymPy can do algebraic evaluation, differentiation, expansion, complex numbers, etc. It is contained in a pure Python distribution.>
        42.4.20. IPython. I just can’t stress enough how useful this tool is. It is a python prompt on steroids. It has completion, history, shell capabilities, and a lot more. Make sure that you take a look at it. <URL:#tn=        42.4.20. IPython. I just can’t stress enough how useful this tool is. It is a python prompt on steroids. It has completion, history, shell capabilities, and a lot more. Make sure that you take a look at it.>
    42.5 IPython <URL:#tn=    42.5 IPython>
        42.5.1 Introducing IPython <URL:#tn=        42.5.1 Introducing IPython>
    42.6 colorgram, tags: colorgram <URL:#tn=    42.6 colorgram, tags: colorgram>
    42.7 pendulum, tags: pendulum <URL:#tn=    42.7 pendulum, tags: pendulum>
    42.8 pypdf, tags: pypdf <URL:#tn=    42.8 pypdf, tags: pypdf>
    42.9 CSV <URL:#tn=    42.9 CSV>
        42.9.1 csv description from ChatGPT <URL:#tn=        42.9.1 csv description from ChatGPT>
        42.9.2 <URL:#tn=        42.9.2>
    42.10 seaborn <URL:#tn=    42.10 seaborn>
        42.10.1 chatgpt overview of seaborn library in python <URL:#tn=        42.10.1 chatgpt overview of seaborn library in python>
        42.10.2 <URL:#tn=        42.10.2>
    42.11 <URL:#tn=    42.11>
43. My sap Course <URL:#tn=43. My sap Course>
    43.1 getting started <URL:#tn=    43.1 getting started>
        43.1.1 Run python <URL:#tn=        43.1.1 Run python>
            43.1.1.1 python REPL <URL:#tn=            43.1.1.1 python REPL>
            43.1.1.2 Run as script <URL:#tn=            43.1.1.2 Run as script>
            43.1.1.3 IDLE <URL:#tn=            43.1.1.3 IDLE>
            43.1.1.4 pycharm, vscode <URL:#tn=            43.1.1.4 pycharm, vscode>
            43.1.1.5 ipython <URL:#tn=            43.1.1.5 ipython>
            43.1.1.6 <URL:#tn=            43.1.1.6>
        43.1.2 <URL:#tn=        43.1.2>
    43.2 SAP intro <URL:#tn=    43.2 SAP intro>
        43.2.1  Python strong features <URL:#tn=        43.2.1  Python strong features>
        43.2.2 Python weak features <URL:#tn=        43.2.2 Python weak features>
    43.3 <URL:#tn=    43.3>
44. Web programming <URL:#tn=44. Web programming>
    44.1 graphql <URL:#tn=    44.1 graphql>
        44.1.1 graphql flask tutorial <URL:#tn=        44.1.1 graphql flask tutorial>
            44.1.1.1 Creating a GraphQL server with flask <URL:#tn=            44.1.1.1 Creating a GraphQL server with flask>
                44.1.1.1.1 Setting up your project <URL:#tn=                44.1.1.1.1 Setting up your project>
        44.1.2 <URL:#tn=        44.1.2>
    44.2 tags: local Web server <URL:#tn=    44.2 tags: local Web server>
        44.2.1  one liner http <URL:#tn=        44.2.1  one liner http>
    44.3 authentication <URL:#tn=    44.3 authentication>
        44.3.1 JWT based authentication <URL:#tn=        44.3.1 JWT based authentication>
            44.3.1.1 https://realpython.com/token-based-authentication-with-flask/ <URL:#tn=            44.3.1.1 https://realpython.com/token-based-authentication-with-flask/>
                44.3.1.1.1 Introduction <URL:#tn=                44.3.1.1.1 Introduction>
                44.3.1.1.2 Project Setup <URL:#tn=                44.3.1.1.2 Project Setup>
                44.3.1.1.3 <URL:#tn=                44.3.1.1.3>
            44.3.1.2 JWT based authentication step by step <URL:#tn=            44.3.1.2 JWT based authentication step by step>
            44.3.1.3 <URL:#tn=            44.3.1.3>
        44.3.2 HTTP basic authentication <URL:#tn=        44.3.2 HTTP basic authentication>
            44.3.2.1  HTTP basic authentication step by step <URL:#tn=            44.3.2.1  HTTP basic authentication step by step>
            44.3.2.2 <URL:#tn=            44.3.2.2>
        44.3.3 <URL:#tn=        44.3.3>
    44.4 making http requests, tags: making http requests <URL:#tn=    44.4 making http requests, tags: making http requests>
        44.4.1 Chatgpt <URL:#tn=        44.4.1 Chatgpt>
        44.4.2 <URL:#tn=        44.4.2>
    44.5 <URL:#tn=    44.5>
45. Virtual Environment, venv <URL:#tn=45. Virtual Environment, venv>
    45.1  tutorial <URL:#tn=    45.1  tutorial>
        45.1.1  https://realpython.com/python-virtual-environments-a-primer/ <URL:#tn=        45.1.1  https://realpython.com/python-virtual-environments-a-primer/>
            45.1.1.1  venv <URL:#tn=            45.1.1.1  venv>
            45.1.1.2 virtualenvwrapper <URL:#tn=            45.1.1.2 virtualenvwrapper>
            45.1.1.3 pyenv <URL:#tn=            45.1.1.3 pyenv>
            45.1.1.4 <URL:#tn=            45.1.1.4>
        45.1.2 <URL:#tn=        45.1.2>
    45.2 <URL:#tn=    45.2>
46. Python and vim <URL:#tn=46. Python and vim>
    46.1  https://realpython.com/vim-and-python-a-match-made-in-heaven/ <URL:#tn=    46.1  https://realpython.com/vim-and-python-a-match-made-in-heaven/>
    46.2 <URL:#tn=    46.2>
47. oneliners <URL:#tn=47. oneliners>
    47.1 https://blog.finxter.com/10-python-one-liners/ <URL:#tn=    47.1 https://blog.finxter.com/10-python-one-liners/>
    47.2 <URL:#tn=    47.2>
48. JSON support <URL:#tn=48. JSON support>
    48.1 Validate JSON one liner <URL:#tn=    48.1 Validate JSON one liner>
    48.2 python jwt decoder <URL:#tn=    48.2 python jwt decoder>
        48.2.1  my usage examples <URL:#tn=        48.2.1  my usage examples>
        48.2.2 <URL:#tn=        48.2.2>
    48.3 tutorials <URL:#tn=    48.3 tutorials>
        48.3.1 https://realpython.com/python-json/#a-very-brief-history-of-json <URL:#tn=        48.3.1 https://realpython.com/python-json/#a-very-brief-history-of-json>
            48.3.1.1  introduction <URL:#tn=            48.3.1.1  introduction>
            48.3.1.2 Serializing JSON <URL:#tn=            48.3.1.2 Serializing JSON>
        48.3.2 deserialization <URL:#tn=        48.3.2 deserialization>
        48.3.3 <URL:#tn=        48.3.3>
    48.4 <URL:#tn=    48.4>
49. signals <URL:#tn=49. signals>
    49.1 capture SIGINT, ctrl-c <URL:#tn=    49.1 capture SIGINT, ctrl-c>
    49.2 <URL:#tn=    49.2>
50. Multi threading, parallelism, concurrency <URL:#tn=50. Multi threading, parallelism, concurrency>
    50.1  intro to python multi threading <URL:#tn=    50.1  intro to python multi threading>
        50.1.1  https://realpython.com/intro-to-python-threading/ <URL:#tn=        50.1.1  https://realpython.com/intro-to-python-threading/>
            50.1.1.1  single thread <URL:#tn=            50.1.1.1  single thread>
            50.1.1.2 multiple threads <URL:#tn=            50.1.1.2 multiple threads>
            50.1.1.3 ThreadPoolExecutor <URL:#tn=            50.1.1.3 ThreadPoolExecutor>
            50.1.1.4 More <URL:#tn=            50.1.1.4 More>
            50.1.1.5 <URL:#tn=            50.1.1.5>
        50.1.2 <URL:#tn=        50.1.2>
    50.2 <URL:#tn=    50.2>
51. requests <URL:#tn=51. requests>
    51.1 https://realpython.com/python-requests/ <URL:#tn=    51.1 https://realpython.com/python-requests/>
        51.1.1 install <URL:#tn=        51.1.1 install>
        51.1.2 basic <URL:#tn=        51.1.2 basic>
        51.1.3 response object <URL:#tn=        51.1.3 response object>
        51.1.4 Query params <URL:#tn=        51.1.4 Query params>
        51.1.5 Request Headers <URL:#tn=        51.1.5 Request Headers>
        51.1.6 Other HTTP Methods <URL:#tn=        51.1.6 Other HTTP Methods>
        51.1.7 The Message Body <URL:#tn=        51.1.7 The Message Body>
        51.1.8 Inspecting Your Request <URL:#tn=        51.1.8 Inspecting Your Request>
        51.1.9 Authentication <URL:#tn=        51.1.9 Authentication>
        51.1.10 SSL Certificate Verification <URL:#tn=        51.1.10 SSL Certificate Verification>
        51.1.11 Performance <URL:#tn=        51.1.11 Performance>
        51.1.12 The Session Object <URL:#tn=        51.1.12 The Session Object>
        51.1.13 Max Retries <URL:#tn=        51.1.13 Max Retries>
        51.1.14 <URL:#tn=        51.1.14>
    51.2 <URL:#tn=    51.2>
52. regexp <URL:#tn=52. regexp>
    52.1  https://www.geeksforgeeks.org/regular-expression-python-examples-set-1/ <URL:#tn=    52.1  https://www.geeksforgeeks.org/regular-expression-python-examples-set-1/>
    52.2 Regular Expression Basics <URL:#tn=    52.2 Regular Expression Basics>
    52.3 https://www.tutorialspoint.com/python/python_reg_expressions.htm <URL:#tn=    52.3 https://www.tutorialspoint.com/python/python_reg_expressions.htm>
    52.4 <URL:#tn=    52.4>
53. my scripts, tags: scripts, tools <URL:#tn=53. my scripts, tags: scripts, tools>
    53.1 tocify.py <URL:#tn=    53.1 tocify.py>
    53.2 pdf to text <URL:#tn=    53.2 pdf to text>
    53.3 <URL:#tn=    53.3>
54. itertools <URL:#tn=54. itertools>
    54.1 itertools basics <URL:#tn=    54.1 itertools basics>
    54.2 itertools accumulate <URL:#tn=    54.2 itertools accumulate>
    54.3 <URL:#tn=    54.3>
55. tags: python regex <URL:#tn=55. tags: python regex>
    55.1  tags: python regex tutorial <URL:#tn=    55.1  tags: python regex tutorial>
        55.1.1  simple examples <URL:#tn=        55.1.1  simple examples>
            55.1.1.1  Extract number from string <URL:#tn=            55.1.1.1  Extract number from string>
            55.1.1.2 <URL:#tn=            55.1.1.2>
        55.1.2 <URL:#tn=        55.1.2>
    55.2 <URL:#tn=    55.2>
56. Chatgpt <URL:#tn=56. Chatgpt>
    56.1 Query:  python3     def put(self, elem: T): self.q.append(elem) generates error NameError: name 'T' is not defined <URL:#tn=    56.1 Query:  python3     def put(self, elem: T): self.q.append(elem) generates error NameError: name 'T' is not defined>
    56.2 q. list different ways to iterate a list in reverse in python <URL:#tn=    56.2 q. list different ways to iterate a list in reverse in python>
    56.3 <URL:#tn=    56.3>
57. algorithms, data-structures tags: algorithms <URL:#tn=57. algorithms, data-structures tags: algorithms>
    57.1  BFS, Dijkstra, astar, tags: BFS, Dijkstra, astar <URL:#tn=    57.1  BFS, Dijkstra, astar, tags: BFS, Dijkstra, astar>
        57.1.1  https://www.redblobgames.com/pathfinding/a-star/implementation.html <URL:#tn=        57.1.1  https://www.redblobgames.com/pathfinding/a-star/implementation.html>
        57.1.2 astar Chatgpt <URL:#tn=        57.1.2 astar Chatgpt>
        57.1.3 <URL:#tn=        57.1.3>
    57.2 Fibonacci heaps <URL:#tn=    57.2 Fibonacci heaps>
    57.3 <URL:#tn=    57.3>
58. turtle, tags: turtle <URL:#tn=58. turtle, tags: turtle>
    58.1  documentation <URL:#tn=    58.1  documentation>
        58.1.1  https://docs.python.org/3/library/turtle.html <URL:#tn=        58.1.1  https://docs.python.org/3/library/turtle.html>
        58.1.2 Chatgpt <URL:#tn=        58.1.2 Chatgpt>
            58.1.2.1 python turtle most used API <URL:#tn=            58.1.2.1 python turtle most used API>
            58.1.2.2 begin_fill example <URL:#tn=            58.1.2.2 begin_fill example>
        58.1.3 circle star example <URL:#tn=        58.1.3 circle star example>
        58.1.4 spiral <URL:#tn=        58.1.4 spiral>
        58.1.5 rainbound circle <URL:#tn=        58.1.5 rainbound circle>
        58.1.6 <URL:#tn=        58.1.6>
    58.2 turtle examples <URL:#tn=    58.2 turtle examples>
    58.3 turtle shapes <URL:#tn=    58.3 turtle shapes>
    58.4 turtle colors <URL:#tn=    58.4 turtle colors>
    58.5 <URL:#tn=    58.5>
59. python packages repositories <URL:#tn=59. python packages repositories>
    59.1 pypi <URL:#tn=    59.1 pypi>
        59.1.1 prettytable <URL:#tn=        59.1.1 prettytable>
        59.1.2 <URL:#tn=        59.1.2>
    59.2 <URL:#tn=    59.2>
60. strings, tags: strings <URL:#tn=60. strings, tags: strings>
    60.1 q. python check char is a-z <URL:#tn=    60.1 q. python check char is a-z>
    60.2 <URL:#tn=    60.2>
61. python data structures and collections, tags: python data structures and collections <URL:#tn=61. python data structures and collections, tags: python data structures and collections>
    61.1  named tuples <URL:#tn=    61.1  named tuples>
    61.2 <URL:#tn=    61.2>
62. Python versions, tags: versions <URL:#tn=62. Python versions, tags: versions>
    62.1  python version 2, tags: python version 2 <URL:#tn=    62.1  python version 2, tags: python version 2>
    62.2  python version 3, tags: python version 3 <URL:#tn=    62.2  python version 3, tags: python version 3>
        62.2.1   placeholder <URL:#tn=        62.2.1   placeholder>
        62.2.2   placeholder <URL:#tn=        62.2.2   placeholder>
        62.2.3   placeholder <URL:#tn=        62.2.3   placeholder>
        62.2.4   placeholder <URL:#tn=        62.2.4   placeholder>
        62.2.5   placeholder <URL:#tn=        62.2.5   placeholder>
        62.2.6   placeholder <URL:#tn=        62.2.6   placeholder>
        62.2.7   placeholder <URL:#tn=        62.2.7   placeholder>
        62.2.8   placeholder <URL:#tn=        62.2.8   placeholder>
        62.2.9   placeholder <URL:#tn=        62.2.9   placeholder>
        62.2.10  placeholder <URL:#tn=        62.2.10  placeholder>
        62.2.11  placeholder <URL:#tn=        62.2.11  placeholder>
        62.2.12  python version 3.12, tags: python version 3.12 <URL:#tn=        62.2.12  python version 3.12, tags: python version 3.12>
            62.2.12.1  better error messages <URL:#tn=            62.2.12.1  better error messages>
            62.2.12.2 comprehension performance improvements <URL:#tn=            62.2.12.2 comprehension performance improvements>
            62.2.12.3 optimizations <URL:#tn=            62.2.12.3 optimizations>
            62.2.12.4 f-strings inner quotes <URL:#tn=            62.2.12.4 f-strings inner quotes>
            62.2.12.5 better typing for kwargs (add unpack) <URL:#tn=            62.2.12.5 better typing for kwargs (add unpack)>
            62.2.12.6 @override <URL:#tn=            62.2.12.6 @override>
            62.2.12.7 generics syntax simplified <URL:#tn=            62.2.12.7 generics syntax simplified>
            62.2.12.8 <URL:#tn=            62.2.12.8>
        62.2.13  placeholder <URL:#tn=        62.2.13  placeholder>
        62.2.14  placeholder <URL:#tn=        62.2.14  placeholder>
        62.2.15  placeholder <URL:#tn=        62.2.15  placeholder>
        62.2.16  placeholder <URL:#tn=        62.2.16  placeholder>
        62.2.17  placeholder <URL:#tn=        62.2.17  placeholder>
        62.2.18  placeholder <URL:#tn=        62.2.18  placeholder>
        62.2.19  placeholder <URL:#tn=        62.2.19  placeholder>
        62.2.20  placeholder <URL:#tn=        62.2.20  placeholder>
        62.2.21  placeholder <URL:#tn=        62.2.21  placeholder>
        62.2.22  placeholder <URL:#tn=        62.2.22  placeholder>
        62.2.23  placeholder <URL:#tn=        62.2.23  placeholder>
        62.2.24  placeholder <URL:#tn=        62.2.24  placeholder>
        62.2.25  placeholder <URL:#tn=        62.2.25  placeholder>
        62.2.26  placeholder <URL:#tn=        62.2.26  placeholder>
        62.2.27  placeholder <URL:#tn=        62.2.27  placeholder>
        62.2.28  placeholder <URL:#tn=        62.2.28  placeholder>
        62.2.29  placeholder <URL:#tn=        62.2.29  placeholder>
        62.2.30  placeholder <URL:#tn=        62.2.30  placeholder>
        62.2.31  placeholder <URL:#tn=        62.2.31  placeholder>
    62.3 <URL:#tn=    62.3>
63. <URL:#tn=63.>
.................................................END TOC..............................................










































































Description: 	Python knowledge. Python is an extremly feature rich langauge that implements OOB paradigm to perfection (IMHO).
Author:		Yosi Izaq.







1. Conversions (bases)
	1.1 convert decimal to hexa
	print "%x"%98

	1.2 Convert hex to decimal
	>>> int ('6e', 16)  

	1.3 Denary (base 10) to binary (base 2)
	def Denary2Binary(n):
    '''convert denary integer n to binary string bStr'''
    bStr = ''
    if n < 0:  raise ValueError, "must be a positive integer"
    if n == 0: return '0'
    while n > 0:
        bStr = str(n % 2) + bStr
        n = n >> 1
    return bStr
 
def int2bin(n, count=24):
    """returns the binary of integer n, using count number of digits"""
    return "".join([str((n >> y) & 1) for y in range(count-1, -1, -1)])
110

	1.4 String to int and vise versa

		string to int: int(str_value)
		int to string: str(int_value)

	1.5 Convert string  to hex (Python) 
## {{{ http://code.activestate.com/recipes/496969/ (r1)
#convert string to hex
def toHex(s):
    lst = []
    for ch in s:
        hv = hex(ord(ch)).replace('0x', '')
        if len(hv) == 1:
            hv = '0'+hv
        lst.append(hv)
    
    return reduce(lambda x,y:x+y, lst)

#convert hex repr to string
def toStr(s):
    return s and chr(atoi(s[:2], base=16)) + toStr(s[2:]) or ''
## end of http://code.activestate.com/recipes/496969/ }}}

5 comments
sasa sasa 3 years, 10 months ago  # | flag

list comprehensions. Though your snippet is nice, you should really have a look at least comprehensions. With a few built-in methods you can reduce your 8 lines of code (which is pretty slow because of the "extra work" I guess) to one:

toHex = lambda x:"".join([hex(ord(c))[2:].zfill(2) for c in x])

The builtin string-method "join" joins every element of the list to the string by re-using the string. ";;".join(['a', 'b', 'c']) would result in 'a;;b;;c'. Note that you can enhance the speed of the above snippet by replacing the [] with () what will change it from a list comprehension to a generator. Same enhancements can be done with your second function.

Regards, Stargaming
tomer filiba 3 years, 10 months ago  # | flag

encode and decode. this functionality already exists with the encodings library (which is built-in):

>>> "hello".encode("hex")
'68656c6c6f'
>>> "68656c6c6f".decode("hex")
'hello'
>>>

Adam Monsen 3 years, 2 months ago  # | flag

base 16 int conversion. The hex() and int() builtins should do everything you need...

>>> int('0x10AFCC', 16)
1093580
>>> hex(1093580)
'0x10afcc'

Adam Monsen 3 years, 2 months ago  # | flag

Nevermind, I should've read the original post more closely. As the other commenter said, str.encode() and str.decode() do the same thing as your code.
a 1 month, 3 weeks ago  # | flag

atoi() is deprecated. you should use int() instead

2. Parse configuration file.

	2.1 ConfigParser, ver 2.7.3 

		2.1.1  Manual


 ConfigParser — Configuration file parser

Note

The ConfigParser module has been renamed to configparser in Python 3. The 2to3 tool will automatically adapt imports when converting your sources to Python 3.

This module defines the class ConfigParser. The ConfigParser class implements a basic configuration file parser language which provides a structure similar to what you would find on Microsoft Windows INI files. You can use this to write Python programs which can be customized by end users easily.

Note

This library does not interpret or write the value-type prefixes used in the Windows Registry extended version of INI syntax.

See also

Module shlex
    Support for a creating Unix shell-like mini-languages which can be used as an alternate format for application configuration files.
Module json
    The json module implements a subset of JavaScript syntax which can also be used for this purpose.

The configuration file consists of sections, led by a [section] header and followed by name: value entries, with continuations in the style of RFC 822 (see section 3.1.1, “LONG HEADER FIELDS”); name=value is also accepted. Note that leading whitespace is removed from values. The optional values can contain format strings which refer to other values in the same section, or values in a special DEFAULT section. Additional defaults can be provided on initialization and retrieval. Lines beginning with '#' or ';' are ignored and may be used to provide comments.

Configuration files may include comments, prefixed by specific characters (# and ;). Comments may appear on their own in an otherwise empty line, or may be entered in lines holding values or section names. In the latter case, they need to be preceded by a whitespace character to be recognized as a comment. (For backwards compatibility, only ; starts an inline comment, while # does not.)

On top of the core functionality, SafeConfigParser supports interpolation. This means values can contain format strings which refer to other values in the same section, or values in a special DEFAULT section. Additional defaults can be provided on initialization.

For example:

[My Section]
foodir: %(dir)s/whatever
dir=frob
long: this value continues
   in the next line

would resolve the %(dir)s to the value of dir (frob in this case). All reference expansions are done on demand.

Default values can be specified by passing them into the ConfigParser constructor as a dictionary. Additional defaults may be passed into the get() method which will override all others.

Sections are normally stored in a built-in dictionary. An alternative dictionary type can be passed to the ConfigParser constructor. For example, if a dictionary type is passed that sorts its keys, the sections will be sorted on write-back, as will be the keys within each section.

class ConfigParser.RawConfigParser([defaults[, dict_type[, allow_no_value]]])

    The basic configuration object. When defaults is given, it is initialized into the dictionary of intrinsic defaults. When dict_type is given, it will be used to create the dictionary objects for the list of sections, for the options within a section, and for the default values. When allow_no_value is true (default: False), options without values are accepted; the value presented for these is None.

    This class does not support the magical interpolation behavior.

    All option names are passed through the optionxform() method. Its default implementation converts option names to lower case.

    New in version 2.3.

    Changed in version 2.6: dict_type was added.

    Changed in version 2.7: The default dict_type is collections.OrderedDict. allow_no_value was added.

class ConfigParser.ConfigParser([defaults[, dict_type[, allow_no_value]]])

    Derived class of RawConfigParser that implements the magical interpolation feature and adds optional arguments to the get() and items() methods. The values in defaults must be appropriate for the %()s string interpolation. Note that __name__ is an intrinsic default; its value is the section name, and will override any value provided in defaults.

    All option names used in interpolation will be passed through the optionxform() method just like any other option name reference. Using the default implementation of optionxform(), the values foo %(bar)s and foo %(BAR)s are equivalent.

    New in version 2.3.

    Changed in version 2.6: dict_type was added.

    Changed in version 2.7: The default dict_type is collections.OrderedDict. allow_no_value was added.

class ConfigParser.SafeConfigParser([defaults[, dict_type[, allow_no_value]]])

    Derived class of ConfigParser that implements a more-sane variant of the magical interpolation feature. This implementation is more predictable as well. New applications should prefer this version if they don’t need to be compatible with older versions of Python.

    New in version 2.3.

    Changed in version 2.6: dict_type was added.

    Changed in version 2.7: The default dict_type is collections.OrderedDict. allow_no_value was added.

exception ConfigParser.Error

    Base class for all other configparser exceptions.

exception ConfigParser.NoSectionError

    Exception raised when a specified section is not found.

exception ConfigParser.DuplicateSectionError

    Exception raised if add_section() is called with the name of a section that is already present.

exception ConfigParser.NoOptionError

    Exception raised when a specified option is not found in the specified section.

exception ConfigParser.InterpolationError

    Base class for exceptions raised when problems occur performing string interpolation.

exception ConfigParser.InterpolationDepthError

    Exception raised when string interpolation cannot be completed because the number of iterations exceeds MAX_INTERPOLATION_DEPTH. Subclass of InterpolationError.

exception ConfigParser.InterpolationMissingOptionError

    Exception raised when an option referenced from a value does not exist. Subclass of InterpolationError.

    New in version 2.3.

exception ConfigParser.InterpolationSyntaxError

    Exception raised when the source text into which substitutions are made does not conform to the required syntax. Subclass of InterpolationError.

    New in version 2.3.

exception ConfigParser.MissingSectionHeaderError

    Exception raised when attempting to parse a file which has no section headers.

exception ConfigParser.ParsingError

    Exception raised when errors occur attempting to parse a file.

ConfigParser.MAX_INTERPOLATION_DEPTH

    The maximum depth for recursive interpolation for get() when the raw parameter is false. This is relevant only for the ConfigParser class.

See also

Module shlex
    Support for a creating Unix shell-like mini-languages which can be used as an alternate format for application configuration files.


	More advanced module that also write configuration:
		ConfigObj

		more info: http://www.voidspace.org.uk/python/configobj.html#introduction

		2.1.2 ConfigParser – Work with configuration files
Purpose:	Read/write configuration files similar to Windows INI files
Available In:	1.5

Use the ConfigParser module to manage user-editable configuration files for an application. The configuration files are organized into sections, and each section can contain name-value pairs for configuration data. Value interpolation using Python formatting strings is also supported, to build values that depend on one another (this is especially handy for URLs and message strings).
Configuration File Format

The file format used by ConfigParser is similar to the format used by older versions of Microsoft Windows. It consists of one or more named sections, each of which can contain individual options with names and values.

Config file sections are identified by looking for lines starting with [ and ending with ]. The value between the square brackets is the section name, and can contain any characters except square brackets.

Options are listed one per line within a section. The line starts with the name of the option, which is separated from the value by a colon (:) or equal sign (=). Whitespace around the separator is ignored when the file is parsed.

A sample configuration file with section “bug_tracker” and three options would look like:

[bug_tracker]
url = http://localhost:8080/bugs/
username = dhellmann
password = SECRET

Reading Configuration Files

The most common use for a configuration file is to have a user or system administrator edit the file with a regular text editor to set application behavior defaults, and then have the application read the file, parse it, and act based on its contents. Use the read() method of SafeConfigParser to read the configuration file.

from ConfigParser import SafeConfigParser

parser = SafeConfigParser()
parser.read('simple.ini')

print parser.get('bug_tracker', 'url')

This program reads the simple.ini file from the previous section and prints the value of the url option from the bug_tracker section.

$ python ConfigParser_read.py

http://localhost:8080/bugs/

The read() method also accepts a list of filenames. Each name in turn is scanned, and if the file exists it is opened and read.

from ConfigParser import SafeConfigParser
import glob

parser = SafeConfigParser()

candidates = ['does_not_exist.ini', 'also-does-not-exist.ini',
              'simple.ini', 'multisection.ini',
              ]

found = parser.read(candidates)

missing = set(candidates) - set(found)

print 'Found config files:', sorted(found)
print 'Missing files     :', sorted(missing)

read() returns a list containing the names of the files successfully loaded, so the program can discover which configuration files are missing and decide whether to ignore them.

$ python ConfigParser_read_many.py

Found config files: ['multisection.ini', 'simple.ini']
Missing files     : ['also-does-not-exist.ini', 'does_not_exist.ini']

Unicode Configuration Data

Configuration files containing Unicode data should be opened using the codecs module to set the proper encoding value.

Changing the password value of the original input to contain Unicode characters and saving the results in UTF-8 encoding gives:

[bug_tracker]
url = http://localhost:8080/bugs/
username = dhellmann
password = ßéç®é†

The codecs file handle can be passed to readfp(), which uses the readline() method of its argument to get lines from the file and parse them.

from ConfigParser import SafeConfigParser
import codecs

parser = SafeConfigParser()

# Open the file with the correct encoding
with codecs.open('unicode.ini', 'r', encoding='utf-8') as f:
    parser.readfp(f)

password = parser.get('bug_tracker', 'password')

print 'Password:', password.encode('utf-8')
print 'Type    :', type(password)
print 'repr()  :', repr(password)

The value returned by get() is a unicode object, so in order to print it safely it must be re-encoded as UTF-8.

$ python ConfigParser_unicode.py

Password: ßéç®é†
Type    : <type 'unicode'>
repr()  : u'\xdf\xe9\xe7\xae\xe9\u2020'

Accessing Configuration Settings

SafeConfigParser includes methods for examining the structure of the parsed configuration, including listing the sections and options, and getting their values. This configuration file includes two sections for separate web services:

[bug_tracker]
url = http://localhost:8080/bugs/
username = dhellmann
password = SECRET

[wiki]
url = http://localhost:8080/wiki/
username = dhellmann
password = SECRET

And this sample program exercies some of the methods for looking at the configuration data, including sections(), options(), and items().

from ConfigParser import SafeConfigParser

parser = SafeConfigParser()
parser.read('multisection.ini')

for section_name in parser.sections():
    print 'Section:', section_name
    print '  Options:', parser.options(section_name)
    for name, value in parser.items(section_name):
        print '  %s = %s' % (name, value)
    print

Both sections() and options() return lists of strings, while items() returns a list of tuples containing the name-value pairs.

$ python ConfigParser_structure.py

Section: bug_tracker
  Options: ['url', 'username', 'password']
  url = http://localhost:8080/bugs/
  username = dhellmann
  password = SECRET

Section: wiki
  Options: ['url', 'username', 'password']
  url = http://localhost:8080/wiki/
  username = dhellmann
  password = SECRET

Testing whether values are present

To test if a section exists, use has_section(), passing the section name.

from ConfigParser import SafeConfigParser

parser = SafeConfigParser()
parser.read('multisection.ini')

for candidate in [ 'wiki', 'bug_tracker', 'dvcs' ]:
    print '%-12s: %s' % (candidate, parser.has_section(candidate))

Testing if a section exists before calling get() avoids exceptions for missing data.

$ python ConfigParser_has_section.py

wiki        : True
bug_tracker : True
dvcs        : False

Use has_option() to test if an option exists within a section.

from ConfigParser import SafeConfigParser

parser = SafeConfigParser()
parser.read('multisection.ini')

for section in [ 'wiki', 'none' ]:
    print '%s section exists: %s' % (section, parser.has_section(section))
    for candidate in [ 'username', 'password', 'url', 'description' ]:
        print '%s.%-12s  : %s' % (section, candidate, parser.has_option(section, candidate))
    print

If the section does not exist, has_option() returns False.

$ python ConfigParser_has_option.py

wiki section exists: True
wiki.username      : True
wiki.password      : True
wiki.url           : True
wiki.description   : False

none section exists: False
none.username      : False
none.password      : False
none.url           : False
none.description   : False

Value Types

All section and option names are treated as strings, but option values can be strings, integers, floating point numbers, or booleans. There are a range of possible boolean values that are converted true or false. This example file includes one of each:

[ints]
positive = 1
negative = -5

[floats]
positive = 0.2
negative = -3.14

[booleans]
number_true = 1
number_false = 0
yn_true = yes
yn_false = no
tf_true = true
tf_false = false
onoff_true = on
onoff_false = false

SafeConfigParser does not make any attempt to understand the option type. The application is expected to use the correct method to fetch the value as the desired type. get() always returns a string. Use getint() for integers, getfloat() for floating point numbers, and getboolean() for boolean values.

from ConfigParser import SafeConfigParser

parser = SafeConfigParser()
parser.read('types.ini')

print 'Integers:'
for name in parser.options('ints'):
    string_value = parser.get('ints', name)
    value = parser.getint('ints', name)
    print '  %-12s : %-7r -> %d' % (name, string_value, value)

print '\nFloats:'
for name in parser.options('floats'):
    string_value = parser.get('floats', name)
    value = parser.getfloat('floats', name)
    print '  %-12s : %-7r -> %0.2f' % (name, string_value, value)

print '\nBooleans:'
for name in parser.options('booleans'):
    string_value = parser.get('booleans', name)
    value = parser.getboolean('booleans', name)
    print '  %-12s : %-7r -> %s' % (name, string_value, value)

Running this program with the example input produces:

$ python ConfigParser_value_types.py

Integers:
  positive     : '1'     -> 1
  negative     : '-5'    -> -5

Floats:
  positive     : '0.2'   -> 0.20
  negative     : '-3.14' -> -3.14

Booleans:
  number_true  : '1'     -> True
  number_false : '0'     -> False
  yn_true      : 'yes'   -> True
  yn_false     : 'no'    -> False
  tf_true      : 'true'  -> True
  tf_false     : 'false' -> False
  onoff_true   : 'on'    -> True
  onoff_false  : 'false' -> False

Options as Flags

Usually the parser requires an explicit value for each option, but with the SafeConfigParser parameter allow_no_value set to True an option can appear by itself on a line in the input file, and be used as a flag.

import ConfigParser

# Requre values
try:
    parser = ConfigParser.SafeConfigParser()
    parser.read('allow_no_value.ini')
except ConfigParser.ParsingError, err:
    print 'Could not parse:', err

# Allow stand-alone option names
print '\nTrying again with allow_no_value=True'
parser = ConfigParser.SafeConfigParser(allow_no_value=True)
parser.read('allow_no_value.ini')
for flag in [ 'turn_feature_on', 'turn_other_feature_on' ]:
    print
    print flag
    exists = parser.has_option('flags', flag)
    print '  has_option:', exists
    if exists:
        print '         get:', parser.get('flags', flag)

When an option has no explicit value, has_option() reports that the option exists and get() returns None.

$ python ConfigParser_allow_no_value.py

Could not parse: File contains parsing errors: allow_no_value.ini
        [line  2]: 'turn_feature_on\n'

Trying again with allow_no_value=True

turn_feature_on
  has_option: True
         get: None

turn_other_feature_on
  has_option: False

Modifying Settings

While SafeConfigParser is primarily intended to be configured by reading settings from files, settings can also be populated by calling add_section() to create a new section, and set() to add or change an option.

import ConfigParser

parser = ConfigParser.SafeConfigParser()

parser.add_section('bug_tracker')
parser.set('bug_tracker', 'url', 'http://localhost:8080/bugs')
parser.set('bug_tracker', 'username', 'dhellmann')
parser.set('bug_tracker', 'password', 'secret')

for section in parser.sections():
    print section
    for name, value in parser.items(section):
        print '  %s = %r' % (name, value)

All options must be set as strings, even if they will be retrieved as integer, float, or boolean values.

$ python ConfigParser_populate.py

bug_tracker
  url = 'http://localhost:8080/bugs'
  username = 'dhellmann'
  password = 'secret'

Sections and options can be removed from a SafeConfigParser with remove_section() and remove_option().

from ConfigParser import SafeConfigParser

parser = SafeConfigParser()
parser.read('multisection.ini')

print 'Read values:\n'
for section in parser.sections():
    print section
    for name, value in parser.items(section):
        print '  %s = %r' % (name, value)

parser.remove_option('bug_tracker', 'password')
parser.remove_section('wiki')
        
print '\nModified values:\n'
for section in parser.sections():
    print section
    for name, value in parser.items(section):
        print '  %s = %r' % (name, value)

Removing a section deletes any options it contains.

$ python ConfigParser_remove.py

Read values:

bug_tracker
  url = 'http://localhost:8080/bugs/'
  username = 'dhellmann'
  password = 'SECRET'
wiki
  url = 'http://localhost:8080/wiki/'
  username = 'dhellmann'
  password = 'SECRET'

Modified values:

bug_tracker
  url = 'http://localhost:8080/bugs/'
  username = 'dhellmann'

Saving Configuration Files

Once a SafeConfigParser is populated with desired data, it can be saved to a file by calling the write() method. This makes it possible to provide a user interface for editing the configuration settings, without having to write any code to manage the file.

import ConfigParser
import sys

parser = ConfigParser.SafeConfigParser()

parser.add_section('bug_tracker')
parser.set('bug_tracker', 'url', 'http://localhost:8080/bugs')
parser.set('bug_tracker', 'username', 'dhellmann')
parser.set('bug_tracker', 'password', 'secret')

parser.write(sys.stdout)

The write() method takes a file-like object as argument. It writes the data out in the INI format so it can be parsed again by SafeConfigParser.

$ python ConfigParser_write.py

[bug_tracker]
url = http://localhost:8080/bugs
username = dhellmann
password = secret

Option Search Path

SafeConfigParser uses a multi-step search process when looking for an option.

Before starting the option search, the section name is tested. If the section does not exist, and the name is not the special value DEFAULT, then NoSectionError is raised.

    If the option name appears in the vars dictionary passed to get(), the value from vars is returned.
    If the option name appears in the specified section, the value from that section is returned.
    If the option name appears in the DEFAULT section, that value is returned.
    If the option name appears in the defaults dictionary passed to the constructor, that value is returned.

If the name is not found in any of those locations, NoOptionError is raised.

The search path behavior can be demonstrated using this configuration file:

[DEFAULT]
file-only = value from DEFAULT section
init-and-file = value from DEFAULT section
from-section = value from DEFAULT section
from-vars = value from DEFAULT section

[sect]
section-only = value from section in file
from-section = value from section in file
from-vars = value from section in file

and this test program:

import ConfigParser

# Define the names of the options
option_names =  [
    'from-default',
    'from-section', 'section-only',
    'file-only', 'init-only', 'init-and-file',
    'from-vars',
    ]

# Initialize the parser with some defaults
parser = ConfigParser.SafeConfigParser(
    defaults={'from-default':'value from defaults passed to init',
              'init-only':'value from defaults passed to init',
              'init-and-file':'value from defaults passed to init',
              'from-section':'value from defaults passed to init',
              'from-vars':'value from defaults passed to init',
              })

print 'Defaults before loading file:'
defaults = parser.defaults()
for name in option_names:
    if name in defaults:
        print '  %-15s = %r' % (name, defaults[name])

# Load the configuration file
parser.read('with-defaults.ini')

print '\nDefaults after loading file:'
defaults = parser.defaults()
for name in option_names:
    if name in defaults:
        print '  %-15s = %r' % (name, defaults[name])

# Define some local overrides
vars = {'from-vars':'value from vars'}

# Show the values of all of the options
print '\nOption lookup:'
for name in option_names:
    value = parser.get('sect', name, vars=vars)
    print '  %-15s = %r' % (name, value)

# Show error messages for options that do not exist
print '\nError cases:'
try:
    print 'No such option :', parser.get('sect', 'no-option')
except ConfigParser.NoOptionError, err:
    print str(err)

try:
    print 'No such section:', parser.get('no-sect', 'no-option')
except ConfigParser.NoSectionError, err:
    print str(err)

The output shows the origin for the value of each option, and illustrates the way defaults from different sources override existing values.

$ python ConfigParser_defaults.py

Defaults before loading file:
  from-default    = 'value from defaults passed to init'
  from-section    = 'value from defaults passed to init'
  init-only       = 'value from defaults passed to init'
  init-and-file   = 'value from defaults passed to init'
  from-vars       = 'value from defaults passed to init'

Defaults after loading file:
  from-default    = 'value from defaults passed to init'
  from-section    = 'value from DEFAULT section'
  file-only       = 'value from DEFAULT section'
  init-only       = 'value from defaults passed to init'
  init-and-file   = 'value from DEFAULT section'
  from-vars       = 'value from DEFAULT section'

Option lookup:
  from-default    = 'value from defaults passed to init'
  from-section    = 'value from section in file'
  section-only    = 'value from section in file'
  file-only       = 'value from DEFAULT section'
  init-only       = 'value from defaults passed to init'
  init-and-file   = 'value from DEFAULT section'
  from-vars       = 'value from vars'

Error cases:
No such option : No option 'no-option' in section: 'sect'
No such section: No section: 'no-sect'

Combining Values with Interpolation

SafeConfigParser provides a feature called interpolation that can be used to combine values together. Values containing standard Python format strings trigger the interpolation feature when they are retrieved with get(). Options named within the value being fetched are replaced with their values in turn, until no more substitution is necessary.

The URL examples from earlier in this section can be rewritten to use interpolation to make it easier to change only part of the value. For example, this configuration file separates the protocol, hostname, and port from the URL as separate options.

[bug_tracker]
protocol = http
server = localhost
port = 8080
url = %(protocol)s://%(server)s:%(port)s/bugs/
username = dhellmann
password = SECRET

Interpolation is performed by default each time get() is called. Pass a true value in the raw argument to retrieve the original value, without interpolation.

from ConfigParser import SafeConfigParser

parser = SafeConfigParser()
parser.read('interpolation.ini')

print 'Original value       :', parser.get('bug_tracker', 'url')

parser.set('bug_tracker', 'port', '9090')
print 'Altered port value   :', parser.get('bug_tracker', 'url')

print 'Without interpolation:', parser.get('bug_tracker', 'url', raw=True)

Because the value is computed by get(), changing one of the settings being used by the url value changes the return value.

$ python ConfigParser_interpolation.py

Original value       : http://localhost:8080/bugs/
Altered port value   : http://localhost:9090/bugs/
Without interpolation: %(protocol)s://%(server)s:%(port)s/bugs/

Using Defaults

Values for interpolation do not need to appear in the same section as the original option. Defaults can be mixed with override values. Using this config file:

[DEFAULT]
url = %(protocol)s://%(server)s:%(port)s/bugs/
protocol = http
server = bugs.example.com
port = 80

[bug_tracker]
server = localhost
port = 8080
username = dhellmann
password = SECRET

The url value comes from the DEFAULT section, and the substitution starts by looking in bug_tracker and falling back to DEFAULT for pieces not found.

from ConfigParser import SafeConfigParser

parser = SafeConfigParser()
parser.read('interpolation_defaults.ini')

print 'URL:', parser.get('bug_tracker', 'url')

The hostname and port values come from the bug_tracker section, but the protocol comes from DEFAULT.

$ python ConfigParser_interpolation_defaults.py

URL: http://localhost:8080/bugs/

Substitution Errors

Substitution stops after MAX_INTERPOLATION_DEPTH steps to avoid problems due to recursive references.

import ConfigParser

parser = ConfigParser.SafeConfigParser()

parser.add_section('sect')
parser.set('sect', 'opt', '%(opt)s')

try:
    print parser.get('sect', 'opt')
except ConfigParser.InterpolationDepthError, err:
    print 'ERROR:', err

An InterpolationDepthError exception is raised if there are too many substitution steps.

$ python ConfigParser_interpolation_recursion.py

ERROR: Value interpolation too deeply recursive:
        section: [sect]
        option : opt
        rawval : %(opt)s

Missing values result in an InterpolationMissingOptionError exception.

import ConfigParser

parser = ConfigParser.SafeConfigParser()

parser.add_section('bug_tracker')
parser.set('bug_tracker', 'url', 'http://%(server)s:%(port)s/bugs')

try:
    print parser.get('bug_tracker', 'url')
except ConfigParser.InterpolationMissingOptionError, err:
    print 'ERROR:', err

Since no server value is defined, the url cannot be constructed.

$ python ConfigParser_interpolation_error.py

ERROR: Bad value substitution:
        section: [bug_tracker]
        option : url
        key    : server
        rawval : :%(port)s/bugs


	2.2

3. Tools
	
	3.1 GUI, tags: python GUI UX UI
		3.1.1 Tkinter, tags: tk interface, Tkinter, python GUI UX UI

			3.1.1.1 Create a popup window
			First using SimpleDialog:
			#!/usr/bin/python

from Tkinter import *
from SimpleDialog import SimpleDialog

# initialize GUI toolkit

root = Tk()

# pop up a dialog window with some text

SimpleDialog(root,
             text="Hi there\nHere is some text",
             buttons=["OK"],
             default=0,
             title="Demo Dialog").go()

# notice the go()

		
			Second, using simple costum dialog
from Tkinter import *

class MyDialog:
    def __init__(self, parent):

        top = self.top = Toplevel(parent)

        Label(top, text="Value").pack()

        self.e = Entry(top)
        self.e.pack(padx=5)

        b = Button(top, text="OK", command=self.ok)
        b.pack(pady=5)

    def ok(self):

        print "value is", self.e.get()

        self.top.destroy()


root = Tk()

d = MyDialog(root)

root.wait_window(d.top)


			3.1.1.2 Building menues dynamically and passing an argument to callback

				One form is the use a lambda function for the callback. example:
		gamesmenu = Menu(menubar, tearoff=0)
		for game in self.m_cfg.m_games.keys():
			gamesmenu.add_command(label=game, command= lambda p_game = game: self.query_game(p_game))
		menubar.add_cascade(label="Games", menu=gamesmenu)

					

			3.1.1.3 Dialogs, request additional information
			Dialogs are windows popped up by a script to provide or request additional information. They come in two flavors, modal and nonmodal:
Modal
These dialogs block the rest of the interface until the dialog window is dismissed; users must reply to the dialog before the program continues.

Nonmodal
These dialogs can remain on-screen indefinitely without interfering with other windows in the interface; they can usually accept inputs at any time.

Regardless of their modality, dialogs are generally implemented with the Toplevel window object we met in the prior section, whether you make the Toplevel or not. There are essentially three ways to present pop-up dialogs to users with Tkinter: by using common dialog calls, by using the now-dated Dialog object, and by creating custom dialog windows with Toplevels and other kinds of widgets. Let's explore the basics of all three schemes.


				3.1.1.3.1 Standard (Common) Dialogs
			Tkinter supports many pre made default dialog options.
			See some basic examples:

			dlg1.py
			from Tkinter import *
from tkMessageBox import *

def callback( ):
    if askyesno('Verify', 'Do you really want to quit?'):
        showwarning('Yes', 'Quit not yet implemented')
    else:
        showinfo('No', 'Quit has been cancelled')

errmsg = 'Sorry, no Spam allowed!'
Button(text='Quit', command=callback).pack(fill=X)
Button(text='Spam', command=(lambda: showerror('Spam', errmsg))).pack(fill=X)
mainloop( )

		quitter.py:
#############################################
# a Quit button that verifies exit requests;
# to reuse, attach an instance to other GUIs
#############################################

from Tkinter import *                          # get widget classes
from tkMessageBox import askokcancel           # get canned std dialog

class Quitter(Frame):                          # subclass our GUI
    def __init__(self, parent=None):             # constructor method
        Frame.__init__(self, parent)
        self.pack( )
        widget = Button(self, text='Quit', command=self.quit)
        widget.pack(side=LEFT)
    def quit(self):
        ans = askokcancel('Verify exit', "Really quit?")
        if ans: Frame.quit(self)

if __name__ == '__main__':  Quitter().mainloop( )


		dialogTable.py:
# define a name:callback demos table

from tkFileDialog   import askopenfilename        # get standard dialogs
from tkColorChooser import askcolor               # they live in Lib/lib-tk
from tkMessageBox   import askquestion, showerror
from tkSimpleDialog import askfloat

demos = {
    'Open':  askopenfilename,
    'Color': askcolor,
    'Query': lambda: askquestion('Warning', 'You typed "rm *"\nConfirm?'),
    'Error': lambda: showerror('Error!', "He's dead, Jim"),
    'Input': lambda: askfloat('Entry', 'Enter credit card number')
}

		demoDlg.py:
from Tkinter import *              # get base widget set
from dialogTable import demos      # button callback handlers
from quitter import Quitter        # attach a quit object to me

class Demo(Frame):
    def __init__(self, parent=None):
        Frame.__init__(self, parent)
        self.pack( )
        Label(self, text="Basic demos").pack( )
        for (key, value) in demos.items( ):
            Button(self, text=key, command=value).pack(side=TOP, fill=BOTH)
        Quitter(self).pack(side=TOP, fill=BOTH)

if __name__ == '__main__': Demo().mainloop( )

		demoDlg-print.py (Also actually use the input values from the dialogs):
##########################################################################
# same, but show return values of dialog calls;  the lambda saves
# data from the local scope to be passed to the handler (button press
# handlers normally get no arguments) and works just like a nested def
# statement of this form: def func(key=key): self.printit(key)
##########################################################################

from Tkinter import *              # get base widget set
from dialogTable import demos      # button callback handlers
from quitter import Quitter        # attach a quit object to me

class Demo(Frame):
    def __init__(self, parent=None):
        Frame.__init__(self, parent)
        self.pack( )
        Label(self, text="Basic demos").pack( )
        for (key, value) in demos.items( ):
            func = (lambda key=key: self.printit(key))
            Button(self, text=key, command=func).pack(side=TOP, fill=BOTH)
        Quitter(self).pack(side=TOP, fill=BOTH)
    def printit(self, name):
        print name, 'returns =>', demos[name]( )      # fetch, call, print

if __name__ == '__main__': Demo().mainloop( )

Dynamically change background color, setcolor.py
from Tkinter import *
from tkColorChooser import askcolor

def setBgColor( ):
    (triple, hexstr) = askcolor( )
    if hexstr:
        print hexstr
        push.config(bg=hexstr)

root = Tk( )
push = Button(root, text='Set Background Color', command=setBgColor)
push.config(height=3, font=('times', 20, 'bold'))
push.pack(expand=YES, fill=BOTH)
root.mainloop( )

				3.1.1.3.2 Custom (Common) Dialogs
				The dialogs we've seen so far have a standard appearance and interaction. They are fine for many purposes, but often we need something a bit more custom. For example, forms that request multiple field inputs (e.g., name, age, shoe size) aren't directly addressed by the common dialog library. We could pop up one single-input dialog in turn for each requested field, but that isn't exactly user friendly.

Custom dialogs support arbitrary interfaces, but they are also the most complicated to program. Even so, there's not much to itsimply create a pop-up window as a Toplevel with attached widgets, and arrange a callback handler to fetch user inputs entered in the dialog (if any) and to destroy the window. To make such a custom dialog modal, we also need to wait for a reply by giving the window input focus, making other windows inactive, and waiting for an event. 


			3.1.1.4 Display image in label
			http://effbot.org/tkinterbook/label.htm

photo = PhotoImage(file="icon.gif")
w = Label(parent, image=photo)
w.photo = photo
w.pack()

				3.1.1.4.1 PhotoImage
				http://effbot.org/tkinterbook/photoimage.htm

			3.1.1.5 Tooltip
				#format python
'''Michael Lange <klappnase at 8ung dot at>
The ToolTip class provides a flexible tooltip widget for Tkinter; it is based on IDLE's ToolTip
module which unfortunately seems to be broken (at least the version I saw).
INITIALIZATION OPTIONS:
anchor :        where the text should be positioned inside the widget, must be on of "n", "s", "e", "w", "nw" and so on;
                default is "center"
bd :            borderwidth of the widget; default is 1 (NOTE: don't use "borderwidth" here)
bg :            background color to use for the widget; default is "lightyellow" (NOTE: don't use "background")
delay :         time in ms that it takes for the widget to appear on the screen when the mouse pointer has
                entered the parent widget; default is 1500
fg :            foreground (i.e. text) color to use; default is "black" (NOTE: don't use "foreground")
follow_mouse :  if set to 1 the tooltip will follow the mouse pointer instead of being displayed
                outside of the parent widget; this may be useful if you want to use tooltips for
                large widgets like listboxes or canvases; default is 0
font :          font to use for the widget; default is system specific
justify :       how multiple lines of text will be aligned, must be "left", "right" or "center"; default is "left"
padx :          extra space added to the left and right within the widget; default is 4
pady :          extra space above and below the text; default is 2
relief :        one of "flat", "ridge", "groove", "raised", "sunken" or "solid"; default is "solid"
state :         must be "normal" or "disabled"; if set to "disabled" the tooltip will not appear; default is "normal"
text :          the text that is displayed inside the widget
textvariable :  if set to an instance of Tkinter.StringVar() the variable's value will be used as text for the widget
width :         width of the widget; the default is 0, which means that "wraplength" will be used to limit the widgets width
wraplength :    limits the number of characters in each line; default is 150

WIDGET METHODS:
configure(**opts) : change one or more of the widget's options as described above; the changes will take effect the
                    next time the tooltip shows up; NOTE: follow_mouse cannot be changed after widget initialization

Other widget methods that might be useful if you want to subclass ToolTip:
enter() :           callback when the mouse pointer enters the parent widget
leave() :           called when the mouse pointer leaves the parent widget
motion() :          is called when the mouse pointer moves inside the parent widget if follow_mouse is set to 1 and the
                    tooltip has shown up to continually update the coordinates of the tooltip window
coords() :          calculates the screen coordinates of the tooltip window
create_contents() : creates the contents of the tooltip window (by default a Tkinter.Label)
'''
# Ideas gleaned from PySol

import Tkinter

class ToolTip:
    def __init__(self, master, text='Your text here', delay=1500, **opts):
        self.master = master
        self._opts = {'anchor':'center', 'bd':1, 'bg':'lightyellow', 'delay':delay, 'fg':'black',\
                      'follow_mouse':0, 'font':None, 'justify':'left', 'padx':4, 'pady':2,\
                      'relief':'solid', 'state':'normal', 'text':text, 'textvariable':None,\
                      'width':0, 'wraplength':150}
        self.configure(**opts)
        self._tipwindow = None
        self._id = None
        self._id1 = self.master.bind("<Enter>", self.enter, '+')
        self._id2 = self.master.bind("<Leave>", self.leave, '+')
        self._id3 = self.master.bind("<ButtonPress>", self.leave, '+')
        self._follow_mouse = 0
        if self._opts['follow_mouse']:
            self._id4 = self.master.bind("<Motion>", self.motion, '+')
            self._follow_mouse = 1
    
    def configure(self, **opts):
        for key in opts:
            if self._opts.has_key(key):
                self._opts[key] = opts[key]
            else:
                KeyError = 'KeyError: Unknown option: "%s"' %key
                raise KeyError
    
    ##----these methods handle the callbacks on "<Enter>", "<Leave>" and "<Motion>"---------------##
    ##----events on the parent widget; override them if you want to change the widget's behavior--##
    
    def enter(self, event=None):
        self._schedule()
        
    def leave(self, event=None):
        self._unschedule()
        self._hide()
    
    def motion(self, event=None):
        if self._tipwindow and self._follow_mouse:
            x, y = self.coords()
            self._tipwindow.wm_geometry("+%d+%d" % (x, y))
    
    ##------the methods that do the work:---------------------------------------------------------##
    
    def _schedule(self):
        self._unschedule()
        if self._opts['state'] == 'disabled':
            return
        self._id = self.master.after(self._opts['delay'], self._show)

    def _unschedule(self):
        id = self._id
        self._id = None
        if id:
            self.master.after_cancel(id)

    def _show(self):
        if self._opts['state'] == 'disabled':
            self._unschedule()
            return
        if not self._tipwindow:
            self._tipwindow = tw = Tkinter.Toplevel(self.master)
            # hide the window until we know the geometry
            tw.withdraw()
            tw.wm_overrideredirect(1)

            if tw.tk.call("tk", "windowingsystem") == 'aqua':
                tw.tk.call("::tk::unsupported::MacWindowStyle", "style", tw._w, "help", "none")

            self.create_contents()
            tw.update_idletasks()
            x, y = self.coords()
            tw.wm_geometry("+%d+%d" % (x, y))
            tw.deiconify()
    
    def _hide(self):
        tw = self._tipwindow
        self._tipwindow = None
        if tw:
            tw.destroy()
                
    ##----these methods might be overridden in derived classes:----------------------------------##
    
    def coords(self):
        # The tip window must be completely outside the master widget;
        # otherwise when the mouse enters the tip window we get
        # a leave event and it disappears, and then we get an enter
        # event and it reappears, and so on forever :-(
        # or we take care that the mouse pointer is always outside the tipwindow :-)
        tw = self._tipwindow
        twx, twy = tw.winfo_reqwidth(), tw.winfo_reqheight()
        w, h = tw.winfo_screenwidth(), tw.winfo_screenheight()
        # calculate the y coordinate:
        if self._follow_mouse:
            y = tw.winfo_pointery() + 20
            # make sure the tipwindow is never outside the screen:
            if y + twy > h:
                y = y - twy - 30
        else:
            y = self.master.winfo_rooty() + self.master.winfo_height() + 3
            if y + twy > h:
                y = self.master.winfo_rooty() - twy - 3
        # we can use the same x coord in both cases:
        x = tw.winfo_pointerx() - twx / 2
        if x < 0:
            x = 0
        elif x + twx > w:
            x = w - twx
        return x, y

    def create_contents(self):
        opts = self._opts.copy()
        for opt in ('delay', 'follow_mouse', 'state'):
            del opts[opt]
        label = Tkinter.Label(self._tipwindow, **opts)
        label.pack()

##---------demo code-----------------------------------##

def demo():
    root = Tkinter.Tk(className='ToolTip-demo')
    l = Tkinter.Listbox(root)
    l.insert('end', "I'm a listbox")
    l.pack(side='top')
    t1 = ToolTip(l, follow_mouse=1, text="I'm a tooltip with follow_mouse set to 1, so I won't be placed outside my parent")
    b = Tkinter.Button(root, text='Quit', command=root.quit)
    b.pack(side='bottom')
    t2 = ToolTip(b, text='Enough of this')
    root.mainloop()

if __name__ == '__main__':
    demo()

			3.1.1.6  clean widgets, remove/destroy frame 

				3.1.1.6.1 In grid geometry
				f = Frame(parent)
b = Button(f, text="Foobar")
b.grid(row=0, col=0)

if you then want to get rid of b, you do
f.grid_forget()

NOT
b.grid_forget()

		
			3.1.1.7 scrollbar
			
			Courtesy of: http://effbot.org/zone/tkinter-scrollbar-patterns.htm

Tkinter provides a number of scrollable widgets, but unlike many other toolkits, the widgets do not maintain their own scrollbars. To add a scrollbar to a scrollable widget, you have to create separate Scrollbar widget instances, and attach them to the widget. This article explains how the scrollbar interface works, and shows you how to add scrollbars to standard widgets.
The Scrollbar Interface #

In Tkinter, the scrollbar is a separate widget that can be attached to any widget that support the standard scrollbar interface. Such widgets include:

    * the Listbox widget.
    * the Text widget.
    * the Canvas widget
    * the Entry widget

To attach a scrollbar to a scrolled widget, set the scrollbarÂ¿s command option to point to a method that will be called when the scrollbar is changed, and set the widgetÂ¿s scroll command option to the scrollbarÂ¿s set method. This method is called by the widget when when the view is changed (for example, when new items are added, or the widget is resized).

In standard Tkinter, these methods and options have standard names; the scrollbar should call the widgetÂ¿s xview or yview method when the scrollbar is changed, and the widgets provide xscrollcommand and yscrollcommand command options that are called when the scrollbar needs to be updated.

In the following example, a listbox containing 100 integers is equipped with a scrollbar:
Example: A listbox with a scrollbar

from Tkinter import *

root = Tk()

scrollbar = Scrollbar(root)
scrollbar.pack(side=RIGHT, fill=Y)

listbox = Listbox(root)
listbox.pack()

for i in range(100):
    listbox.insert(END, i)

# attach listbox to scrollbar
listbox.config(yscrollcommand=scrollbar.set)
scrollbar.config(command=listbox.yview)

mainloop()

To watch the traffic between the listbox and the scrollbar, you can replace the Scrollbar and Listbox classes with versions that log the relevant method calls:

class DebugScrollbar(Scrollbar):
    def set(self, *args):
        print "SCROLLBAR SET", args
        Scrollbar.set(self, *args)

class DebugListbox(Listbox):
    def yview(self, *args):
        print "LISTBOX YVIEW", args
        Listbox.yview(self, *args)

scrollbar = DebugScrollbar()
scrollbar.pack(side=RIGHT, fill=Y)

listbox = DebugListbox(yscrollcommand=scrollbar.set)
listbox.pack()

scrollbar.config(command=listbox.yview)

When you run the example using these widgets, youÂ¿ll get a stream of SCROLLBAR and LISTBOX messages in the console window.

When the listbox is first displayed, the listbox calls the scrollbar to inform it about the current view (in this example, 10 out of 100 lines are displayed). The scrollbar calls back, telling the listbox that the scrollbar is in its topmost position:

SCROLLBAR SET ('0', '0.1')
LISTBOX YVIEW ('moveto', '0')

Note that all arguments are strings, and that the values are normalized to fit in the 0.0 to 1.0 range.

When you move the scrollbar thumb, the scrollbar sends moveto messages to the listbox. The listbox updates the view, and calls the scrollbarÂ¿s set method with the resulting values:

LISTBOX YVIEW ('moveto', '0.1041')
SCROLLBAR SET ('0.1', '0.2')
LISTBOX YVIEW ('moveto', '0.186')
SCROLLBAR SET ('0.19', '0.29')
LISTBOX YVIEW ('moveto', '0.3124')
SCROLLBAR SET ('0.31', '0.41')
LISTBOX YVIEW ('moveto', '0.4166')
SCROLLBAR SET ('0.42', '0.52')

Note that the listbox rounds the scrollbar value to the nearest full line.

If you click outside the scrollbar thumb, the scrollbar generates scroll events.

LISTBOX YVIEW ('scroll', '1', 'pages')
SCROLLBAR SET ('0.5', '0.6')
LISTBOX YVIEW ('scroll', '1', 'pages')
SCROLLBAR SET ('0.58', '0.68')
LISTBOX YVIEW ('scroll', '1', 'units')
SCROLLBAR SET ('0.59', '0.69')
LISTBOX YVIEW ('scroll', '1', 'units')
SCROLLBAR SET ('0.6', '0.7')

For scroll events, the scrollbar provides both a value and a unit, and itÂ¿s up to the listbox to interpret the units in a way that makes sense to the user. The value is usually -1 (scroll up/left) or 1 (scroll down/right), and the unit is either pages or units.

In a listbox, the basic unit is usually a single item, and a page is as many items that fit into the widgetÂ¿s window. Other widgets may use different definitions.
Patterns #
Adding Scrollbars to Listbox Widgets #

WeÂ¿ve already seen how to add scrollbars to a listbox widget, but hereÂ¿s the relevant part again:

scrollbar = Scrollbar(master)
scrollbar.pack(side=RIGHT, fill=Y)

listbox = Listbox(master, yscrollcommand=scrollbar.set)
listbox.pack()

scrollbar.config(command=listbox.yview)

Note that this example uses pack to add the scrollbar and the listbox to the parent widget. This can get extremely tricky if you plan to add more than just a scrolled listbox to the parent; to solve this, you can either use grid to get the individual widgets in their right location, or pack the widgets into an extra frame, and use pack or grid to put the frame where you want it.

Another problem with the original example is that the scrollbar has no border, but the standard listbox is drawn with a Â¿sunkenÂ¿ appearance. At least on Windows, things look a lot better if the listbox contents and the scrollbar are placed at the same level. In the following example, IÂ¿ve removed the border from the listbox (by setting the border width to zero), and added a border to the extra frame widget:

frame = Frame(root, bd=2, relief=SUNKEN)

scrollbar = Scrollbar(frame)
scrollbar.pack(side=RIGHT, fill=Y)

listbox = Listbox(frame, bd=0, yscrollcommand=scrollbar.set)
listbox.pack()

scrollbar.config(command=listbox.yview)

frame.pack()

The Listbox widget also supports horizontal scrolling. For examples, see the next section.
Adding Scrollbars to Text Widgets #

The Text widget implements the same scrollbar interface as the listbox; create the scrollbar, and attach it to the text widget by setting the appropriate options:

scrollbar = Scrollbar(master)
scrollbar.pack(side=RIGHT, fill=Y)

text = Text(master, wrap=WORD, yscrollcommand=scrollbar.set)
text.pack()

scrollbar.config(command=text.yview)

The wrap option controls how to handle long lines in the text widget. The default value is CHAR, which tells the widget that itÂ¿s okay to add line breaks between individual characters, for lines that are longer than the widget is wide. If youÂ¿re displaying text in the widget, itÂ¿s usually better to set the wrap option to WORD, like in the example above. This tells the widget to avoid breaking lines inside words.

You can also switch off line wrapping, by setting the wrap option to NONE. When you do, lines that are longer than the widget is wide will be truncated. To allow the user to display wide text, you can add a second, horizontal scrollbar:

xscrollbar = Scrollbar(master, orient=HORIZONTAL)
xscrollbar.pack(side=BOTTOM, fill=X)

yscrollbar = Scrollbar(master)
yscrollbar.pack(side=RIGHT, fill=Y)

text = Text(master, wrap=NONE,
            xscrollcommand=xscrollbar.set,
            yscrollcommand=yscrollbar.set)
text.pack()

xscrollbar.config(command=text.xview)
yscrollbar.config(command=text.yview)

Note that by using pack to display the widgets, youÂ¿ll end up with either a horizontal scrollbar thatÂ¿s wider than the text widget, or a vertical scrollbar thatÂ¿s taller than the text widget. You can probably get around this by packing a small frame in the lower left corner, and use additional frames to get the right grouping, but itÂ¿s easier to put everything in a separate frame, and use the grid manager to display the widgets:

frame = Frame(master, bd=2, relief=SUNKEN)

frame.grid_rowconfigure(0, weight=1)
frame.grid_columnconfigure(0, weight=1)

xscrollbar = Scrollbar(frame, orient=HORIZONTAL)
xscrollbar.grid(row=1, column=0, sticky=E+W)

yscrollbar = Scrollbar(frame)
yscrollbar.grid(row=0, column=1, sticky=N+S)

text = Text(frame, wrap=NONE, bd=0,
            xscrollcommand=xscrollbar.set,
            yscrollcommand=yscrollbar.set)

text.grid(row=0, column=0, sticky=N+S+E+W)

xscrollbar.config(command=text.xview)
yscrollbar.config(command=text.yview)

frame.pack()

Note the use of grid_rowconfigure and grid_columnconfigure. If you leave out those calls, the widget wonÂ¿t behave properly if you pack the frame into an expanding parent; if the parent is made smaller, the scrollbars may disappear out of sight. If the parent is made larger, youÂ¿ll end up with lots of padding.
Adding Scrollbars to Canvas Widgets #

As expected, the Canvas widget also implements the standard scrollbar interface, but with a twist. HereÂ¿s an example:

frame = Frame(root, bd=2, relief=SUNKEN)

frame.grid_rowconfigure(0, weight=1)
frame.grid_columnconfigure(0, weight=1)

xscrollbar = Scrollbar(frame, orient=HORIZONTAL)
xscrollbar.grid(row=1, column=0, sticky=E+W)

yscrollbar = Scrollbar(frame)
yscrollbar.grid(row=0, column=1, sticky=N+S)

canvas = Canvas(frame, bd=0,
                xscrollcommand=xscrollbar.set,
                yscrollcommand=yscrollbar.set)

canvas.grid(row=0, column=0, sticky=N+S+E+W)

xscrollbar.config(command=canvas.xview)
yscrollbar.config(command=canvas.yview)

frame.pack()

If you leave it like this, you get a nice scrolling canvas, but the scrollbars do not behave like expected; the scrollbar thumbs stay small and centered no matter how you scroll around. The reason is that the canvas coordinate space is unbounded; at any time, youÂ¿re only seeing a very small fraction of the available coordinate space.

To fix this, you can tell the canvas to limit the scrolling to a given area. To do this, set the scrollregion option to a rectangle (given as a 4-tuple). For example, if you know that youÂ¿re going to draw things in a 1000x1000 area, you can set the scroll region when you create the canvas:

canvas = Canvas(frame, bd=0, scrollregion=(0, 0, 1000, 1000),
                xscrollcommand=xscrollbar.set,
                yscrollcommand=yscrollbar.set)

If you donÂ¿t know the scroll region until later, use the config method to update the region:

canvas.config(scrollregion=(left, top, right, bottom))

You can use the bbox method to get a bounding box for a given object, or a group of objects; canvas.bbox(ALL) returns the bounding box for all objects on the canvas:

canvas.config(scrollregion=canvas.bbox(ALL))

Patterns: A Scrolled Widget Helper

To be continued.
 

def Scrolled(_widget, _master, _mode='y', **options):
    frame = Frame(_master, bd=2, relief=SUNKEN)
    xscrollbar = yscrollbar = None
    if 'x' in _mode: xscrollbar = Scrollbar(frame, orient=HORIZONTAL)
    if 'y' in _mode: yscrollbar = Scrollbar(frame)
    if not options.has_key("bd"):
        options["bd"] = 0
    widget = _widget(frame, **options)
    ... grid the scrollbars and the widget ...
    return widget

widget = Scrolled(Listbox, master, ...options...)
widget = Scrolled(Text, master, ...options...)
widget = Scrolled(Canvas, master, 'xy', ...options...)

def ScrolledListbox(master, _mode='y', **options):
    return Scrolled(Listbox, master, _mode, **options)

def ScrolledText(master, _mode='y', **options):
    return Scrolled(Text, master, _mode, **options)

def ScrolledCanvas(master, _mode='xy', **options):
    return Scrolled(Canvas, master, _mode, **options)

widget = ScrolledListbox(master, ...options...)
widget = ScrolledText(master, ...options...)
widget = ScrolledCanvas(master, ...options...)


				3.1.1.7.1 Add scrollbar to frame
				Impossible, but see solution below (from http://mail.python.org/pipermail/python-list/1999-June/005180.html):

				> I have only an easy question. I think it is possible to bind a scrollbar
> and an other widget e.g. a frame with buttons. But I have not found any
> literature or programming examples. I want to bind the scrollbar to an
> frame and I don't know how. Please help me.

you cannot.  period.

but you can put the frame in a canvas, and
add scrollbars to the canvas.    the following
example does this (and more).

</F>

#
# tk045 -- scrolled frame with autohiding scrollbars
#
# fredrik lundh, august 1998
#
# fredrik at pythonware.com
# http://www.pythonware.com
#

from Tkinter import *

class AutoScrollbar(Scrollbar):
    # a scrollbar that hides itself if it's not needed.  only
    # works if you use the grid geometry manager.
    def set(self, lo, hi):
        if float(lo) <= 0.0 and float(hi) >= 1.0:
            # grid_remove is currently missing from Tkinter!
            self.tk.call("grid", "remove", self)
        else:
            self.grid()
        Scrollbar.set(self, lo, hi)
    def pack(self, **kw):
        raise TclError, "cannot use pack with this widget"
    def place(self, **kw):
        raise TclError, "cannot use place with this widget"

#
# create scrolled canvas

root = Tk()

vscrollbar = AutoScrollbar(root)
vscrollbar.grid(row=0, column=1, sticky=N+S)
hscrollbar = AutoScrollbar(root, orient=HORIZONTAL)
hscrollbar.grid(row=1, column=0, sticky=E+W)

canvas = Canvas(root, 
                yscrollcommand=vscrollbar.set, 
                xscrollcommand=hscrollbar.set)
canvas.grid(row=0, column=0, sticky=N+S+E+W)

vscrollbar.config(command=canvas.yview)
hscrollbar.config(command=canvas.xview)

# make the canvas expandable
root.grid_rowconfigure(0, weight=1)
root.grid_columnconfigure(0, weight=1)

#
# create canvas contents

frame = Frame(canvas)
frame.rowconfigure(1, weight=1)
frame.columnconfigure(1, weight=1)

rows = 5 
for i in range(1,rows):
    for j in range(1,10):
        button = Button(frame, padx=7, pady=7, text="[%d,%d]" % (i,j))
        button.grid(row=i, column=j, sticky='news')

canvas.create_window(0, 0, anchor=NW, window=frame)

frame.update_idletasks()

canvas.config(scrollregion=canvas.bbox("all"))

root.mainloop()


			3.1.1.7 
		3.1.2 more on TKinter

		    3.1.2.1 TKinter getting started with Chatgpt
To get started with Python `tkinter`, you first need to make sure that it is installed on your system. `tkinter` is usually included in Python distributions, so you may not need to install it separately. However, you can check if `tkinter` is installed by running the following command in your terminal or command prompt:

```
python -m tkinter
```

If `tkinter` is installed, a small window should appear. You can close this window to exit the `tkinter` application.

Once you have confirmed that `tkinter` is installed, you can start building graphical user interfaces (GUIs) using `tkinter`. Here's a simple "Hello, World!" example to get you started:

```python
import tkinter as tk

root = tk.Tk()

label = tk.Label(root, text="Hello, World!")
label.pack()

root.mainloop()
```

In this example, we import `tkinter` and create a new `Tk` object, which represents the main window of the GUI. We then create a `Label` widget that displays the text "Hello, World!" and pack it into the window using the `pack()` method. Finally, we start the GUI event loop using the `mainloop()` method, which keeps the window open until it is closed by the user.

You can save this code in a file with a `.py` extension, such as `hello.py`, and run it from the terminal or command prompt using the following command:

```
python hello.py
```

This will open a new window with the "Hello, World!" label.

From here, you can explore the `tkinter` module and its various widgets and methods to create more complex GUIs. The Python documentation for `tkinter` provides a comprehensive reference to all the available widgets and methods: https://docs.python.org/3/library/tk.html
		    3.1.2.2

		3.1.3
	3.2 Pygame

	3.3 PyopenGL

	3.4 Twisted

	3.5 Pyui

4. tutorial

    4.1 My tutorial

        4.1.1  basic
interpreter. python, python3
REPL, IDLE,
$python
#truncate, convert to float, floor, mod and exp:
>>> 5/4
1
>>> 5/4.0
1.25
>>> 5//4
1
>>> 5%4
1
>>> 2**3
8

#last result, _
>>> _
8
>>> _+1
9
>>> print('python supports int, float, Decimal, Fraction and complex numbers')
python supports int, float, Decimal, Fraction and complex numbers
>>> 2+5j
(2+5j)

#escaping
>>> 'quo'te literaly'
  File "<stdin>", line 1
    'quo'te literaly'
          ^
SyntaxError: invalid syntax
>>> 'quo\te literaly'
'quo\te literaly'
>>> 'quo\'te literaly'
"quo'te literaly"
>>> r'a raw string\\/"'
'a raw string\\\\/"'
>>> """
... a multi line
... string
... """
'\na multi line\nstring\n'
>>> 'hel' + 'lo world'
'hello world'
>>> 'hel' 'lo world'
'hello world'

#indexing
>>> st='abc'
>>> st[0]
'a'
>>> st[-1]
'c'
>>> st[:]
'abc'
>>> st[:-1]
'ab'
>>> st[1:]
'bc'
>>> st='aaabbbccc'
>>> st[1::3]
'abc'
>>> st[1:-1:3]
'abc'
>>> st[1:-4:2]
'ab'
>>> st[1::2]
'abbc'
>>> st[1]='f'
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: 'str' object does not support item assignment
>>> len(st)
9

#lists, list comprehension
>>> range(len(st))
[0, 1, 2, 3, 4, 5, 6, 7, 8]
>>> [x**2 for x in range(len(st))]
[0, 1, 4, 9, 16, 25, 36, 49, 64]
>>> sq = [x**2 for x in range(len(st))]
>>> sq
[0, 1, 4, 9, 16, 25, 36, 49, 64]
>>> sq_shallowCP = sq[:]
>>> sq[0]=-1
>>> sq
[-1, 1, 4, 9, 16, 25, 36, 49, 64]
>>> sq_shallowCP
[0, 1, 4, 9, 16, 25, 36, 49, 64]
>>> sq_shallowCP+sq
[0, 1, 4, 9, 16, 25, 36, 49, 64, -1, 1, 4, 9, 16, 25, 36, 49, 64]
>>> sq[1:3]=[1,2,3]
>>> sq
[-1, 1, 2, 3, 9, 16, 25, 36, 49, 64]
>>> sq[1:3]=[]
>>> sq
[-1, 3, 9, 16, 25, 36, 49, 64]


        4.1.2 memory model
https://cdn.oreillystatic.com/en/assets/1/event/95/Python%20103_%20Memory%20Model%20_%20Best%20Practices%20Presentation.pdf

Python has several implementations. The oldest one CPython uses reference-count for memory management. All objects are actually references (aliases) to realy memory
Allocation happens on assignment
Object is GCd when RC(Reference Count) hits 0

>>> foo=[]
>>> import sys
>>> print(sys.getrefcount(foo))
2
>>> print('ref count 2. foo in scope and call sys.getrefcount')
ref count 2. foo in scope and call sys.getrefcount
>>> def bar(f):
...     print(sys.getrefcount(foo)) #4, foo var sys.getrefcount, bar argument and function stack
... 
>>> bar(foo)
4

ref-count GC fails to clean circular dependencies. e.g.
o={}
o['reflexive_ref']=o

or
o1={}
o2={}
o1['o2'] = o2
o2['o1'] = o1

For this python has generational GC
import gc

# We are using ctypes to access our unreachable objects by memory address.
class PyObject(ctypes.Structure):
    _fields_ = [("refcnt", ctypes.c_long)]


gc.disable()  # Disable generational gc

lst = []
lst.append(lst)

# Store address of the list
lst_address = id(lst)

# Destroy the lst reference
del lst

object_1 = {}
object_2 = {}
object_1['obj2'] = object_2
object_2['obj1'] = object_1

obj_address = id(object_1)

# Destroy references
del object_1, object_2

# Uncomment if you want to manually run garbage collection process 
# gc.collect()

# Check the reference count
print(PyObject.from_address(obj_address).refcnt)
print(PyObject.from_address(lst_address).refcnt)

        Performance tips
Cycles can easily happen in real life. Typically you encounter them in graphs, linked lists or in structures, in which you need to keep track of relations between objects. If your program has an intensive workload and requires low latency, you should avoid reference cycles as possible.

To avoid circular references in your code, you need to use weak references, which are implemented in the weakref module. Unlike the usual references, the weakref.ref doesn't increase the reference count and returns None if an object was destroyed.

In some cases, it is useful to disable GC and use it manually. The automatic collection can be disabled by calling gc.disable(). To manually run collection process you need to use gc.collect().

arguments are passed by value (can't be changed)
e.g.
>>> def add1(a):
...     a = a+1
...     return a
... 
>>> a=1
>>> a
1
>>> print(add1(a))
2
>>> a
1

        4.1.3

        with as

    4.2 https://wiki.python.org/moin/BeginnersGuide
    https://docs.python.org/3/tutorial/

    4.3

id=__python_cli__
5. CLI

id=__cli_menu_python__
    5.1 Build CLI fast with cmd module
	use cmd module:

	Example:
	import cmd
	import string, sys

	class CLI(cmd.Cmd):

		def __init__(self):
		cmd.Cmd.__init__(self)
		self.prompt = '> '

		def do_hello(self, arg):
		print "hello again", arg, "!"

		def help_hello(self):
		print "syntax: hello [message]",
		print "-- prints a hello message"

		def do_quit(self, arg):
		sys.exit(1)

		def help_quit(self):
		print "syntax: quit",
		print "-- terminates the application"

# shortcuts
		do_q = do_quit

#
# try it out

		cli = CLI()
		cli.cmdloop()
		
    5.2 How to create a menu for a python console application 
https://computinglearner.com/how-to-create-a-menu-for-a-python-console-application/
TLDR:
menu_options = {
    1: 'Option 1',
    2: 'Option 2',
    3: 'Option 3',
    4: 'Exit',
}

def print_menu():
    for key in menu_options.keys():
        print (key, '--', menu_options[key] )

def option1():
     print('Handle option \'Option 1\'')

def option2():
     print('Handle option \'Option 2\'')

def option3():
     print('Handle option \'Option 3\'')

if __name__=='__main__':
    while(True):
        print_menu()
        option = ''
        try:
            option = int(input('Enter your choice: '))
        except:
            print('Wrong input. Please enter a number ...')
        #Check what choice was entered and act accordingly
        if option == 1:
           option1()
        elif option == 2:
            option2()
        elif option == 3:
            option3()
        elif option == 4:
            print('Thanks message before exiting')
            exit()
        else:
            print('Invalid option. Please enter a number between 1 and 4.')


    5.3 https://pypi.org/project/simple-term-menu/ 
        5.3.1 Installation
simple-term-menu is available on PyPI for Python 3.5+ and can be installed with pip:

python3 -m pip install simple-term-menu
If you use Arch Linux or one of its derivatives, you can also install simple-term-menu from the AUR:

yay -S python-simple-term-menu
You also find self-contained executables for 64-bit Linux distributions and macOS High Sierra and newer on the releases page. They are created with PyInstaller and only require glibc >= 2.17 on Linux (should be fine on any recent Linux system).

        5.3.2 Usage
Create a menu with the default style
Create an instance of the class TerminalMenu and pass the menu entries as a list of strings to the constructor. Call the show method to output the menu and wait for keyboard input:

#!/usr/bin/env python3

from simple_term_menu import TerminalMenu

def main():
    options = ["entry 1", "entry 2", "entry 3"]
    terminal_menu = TerminalMenu(options)
    menu_entry_index = terminal_menu.show()
    print(f"You have selected {options[menu_entry_index]}!")

if __name__ == "__main__":
    main()
You will get an output like:

screenshot_basic

You can now select a menu entry with the arrow keys or j/k (vim motions) and accept your choice by hitting enter or cancel the menu with escape, q or <Ctrl>-C. show returns the selected menu entry index or None if the menu was canceled.

You can pass an optional title to the TerminalMenu constructor which will be placed above the menu. title can be a simple string, a multiline string (with \n newlines) or a list of strings. The same applies to the status_bar parameter, which places a status bar below the menu. Moreover, you can use a callable as status_bar parameter which takes the currently selected entry and returns a status bar string.

        5.3.3 Styling
You can pass styling arguments to the TerminalMenu constructor. Each style is a tuple of keyword strings. Currently the following keywords are accepted:

bg_black
bg_blue
bg_cyan
bg_gray
bg_green
bg_purple
bg_red
bg_yellow
fg_black
fg_blue
fg_cyan
fg_gray
fg_green
fg_purple
fg_red
fg_yellow
bold
italics
standout
underline
You can alter the following styles:

menu_cursor_style: The style of the shown cursor. The default style is ("fg_red", "bold").

menu_highlight_style: The style of the selected menu entry. The default style is ("standout",).

search_highlight_style: The style of matched search strings. The default style is ("fg_black", "bg_yellow", "bold").

shortcut_key_highlight_style: The style of shortcut keys. The default style is ("fg_blue",).

shortcut_brackets_highlight_style: The style of brackets enclosing shortcut keys. The default style is ("fg_gray",).

status_bar_style: The style of the status bar below the menu. The default style is ("fg_yellow", "bg_black").

multi_select_cursor_style: The style of the cursor which pins a selected entry in a multi-selection. The default style is ("fg_yellow", "bold"). This style excludes brackets (see below).

multi_select_cursor_brackets_style: The style of brackets in the multi_select_cursor (([{<)]}>). The default style is ("fg_gray",).

By setting menu_cursor you can define another cursor or disable it (None). The default cursor is "> ". The parameter multi_select_cursor customizes the multi-select cursor (the default is "[*] ").

        5.3.4 Searching
simple_term_menu has a built-in search feature to filter shown menu entries. The default key to activate search mode is / (like in Vim, less and other terminal programs). If you prefer another search key, pass a search_key parameter to the TerminalMenu constructor. None can be passed to activate the search on every letter key. Obviously, j and k cannot be used for cursor motions in that mode. Use <Ctrl-j> and <Ctrl-k> instead.

The search mode supports Python regex syntax. Visit the Python re documentation for more details.

String parts of the menu entries which match the given search pattern are highlighted. Use the search_highlight_style parameter to adjust the highlight style to your liking.

By default, the search is case insensitive. Set search_case_sensitive to True if you prefer a case sensitive search line.

Pass show_search_hint=True to the TerminalMenu constructor to activate a search hint in the search line (like (Press "/" to search)).

        5.3.5 Shortcuts
You can define shortcuts for selected menu entries by prepending a single character enclosed in square brackets (like [a]). Pass shortcut_key_highlight_style and/or shortcut_brackets_highlight_style to the TerminalMenu constructor to change the default highlight style of the shortcuts.

By default, the show method returns when a shortcut key is pressed. If you only want the selection to jump the shortcut target, pass exit_on_shortcut=False to the TerminalMenu constructor.

If you configured the search to be activated on every letter key, the shortcut feature will be disabled.

Pass show_shortcut_hints=True to the TerminalMenu constructor to display shortcut hints in the status bar (useful for very long menus which need scrolling). Additionally pass show_shortcut_hints_in_status_bar=False if you prefer shortcut hints in the menu title.

Shortcuts example
Create a menu of some fruits and use the first letter as shortcut key:

#!/usr/bin/env python3

import os
from simple_term_menu import TerminalMenu


def main():
    fruits = ["[a] apple", "[b] banana", "[o] orange"]
    terminal_menu = TerminalMenu(fruits, title="Fruits")
    menu_entry_index = terminal_menu.show()


if __name__ == "__main__":
    main()
screenshot_shortcuts

        5.3.6 Custom accept keys
The default key to accept a menu item is enter. Pass the accept_keys parameter with a tuple of keys (as strings) to the TerminalMenu constructor to use a different set of accept keys. Custom accept keys can be plain ascii letters or ascii letters with a modifier key (prepend ctrl- or alt- to an ascii letter). Use the chosen_accept_key property of the TerminalMenu instance to query which accept key was pressed by the user.

Be aware that not all combinations of modifier and ascii keys will work depending on your terminal emulator and graphical user interface. In addition, some combinations generate other existing keys (e.g. ctrl-m is enter / carriage return).

Custom accept keys example
#!/usr/bin/env python3

from simple_term_menu import TerminalMenu


def main():
    terminal_menu = TerminalMenu(["entry 1", "entry 2", "entry 3"], accept_keys=("enter", "alt-d", "ctrl-i"))
    menu_entry_index = terminal_menu.show()
    print(terminal_menu.chosen_accept_key)


if __name__ == "__main__":
    main()

        5.3.7 Multi-select
Pass multi_select=True to the TerminalMenu constructor to enable the multi-select mode. Press space or tab on an arbitrary menu item to add it to your selection. Press enter (or any other configured accept_key) to add the currently selected entry as the last item to the selection and to return from the show method as usual. In multi-select mode, the show method returns a sorted tuple of all your selected menu indices instead of a single int. Use the chosen_menu_entries property to get a tuple of the menu entry strings instead. By setting multi_select_keys you can define another set of keys to toggle a selected item. By passing show_multi_select_hint=True a multi-select mode hint is shown in the status bar. If you don't want the accept_key to also select the last highlighted item you can pass multi_select_select_on_accept=False. If no menu item is explicitly selected, the last highlighted menu item will still be added to the selection unless you also pass multi_select_empty_ok=True.

An optional list (or any other iterable object) preselected_entries can also be passed to have items already selected when the menu is displayed. This list can be composed of either integers representing indexes of the menu_entries list, or strings matching the elements of menu_entries. Integers and strings can be mixed.

Multi-select example
#!/usr/bin/env python3

from simple_term_menu import TerminalMenu


def main():
    terminal_menu = TerminalMenu(
        ["dog", "cat", "mouse", "squirrel"],
        multi_select=True,
        show_multi_select_hint=True,
    )
    menu_entry_indices = terminal_menu.show()
    print(menu_entry_indices)
    print(terminal_menu.chosen_menu_entries)


if __name__ == "__main__":
    main()

        5.3.8 Preview window
simple-term-menu can show a preview for each menu entry. Pass a preview_command to the TerminalMenu constructor to activate this optional feature. preview_command either takes a command string which will be executed as a subprocess or a Python callable which converts a given menu entry string into the preview output. If a command string is given, the pattern {} is replaced with the current menu entry string. If a menu entry has an additional data component (separated by |), it is passed instead to the preview command. \| can be used for a literal |. If you simply append a | (without a data component), the preview window will be disabled for this entry.

The additional keyword argument preview_size can be used to control the height of the preview window. It is given as fraction of the complete terminal height (default: 0.25). The width cannot be set, it is always the complete width of the terminal window.

Pass preview_title with a string of your choice to customize the preview window title (default: "preview") or preview_border=False to deactivate the border around the preview window (also deactivates the title string).

Preview commands are allowed to generate ANSI escape color codes.

Preview examples
Create a menu for all files in the current directory and preview their contents with the bat command:

#!/usr/bin/env python3

import os
from simple_term_menu import TerminalMenu


def list_files(directory="."):
    return (file for file in os.listdir(directory) if os.path.isfile(os.path.join(directory, file)))


def main():
    terminal_menu = TerminalMenu(list_files(), preview_command="bat --color=always {}", preview_size=0.75)
    menu_entry_index = terminal_menu.show()


if __name__ == "__main__":
    main()

Another file preview example using the Pygments api:

#!/usr/bin/env python3

import os
from pygments import formatters, highlight, lexers
from pygments.util import ClassNotFound
from simple_term_menu import TerminalMenu


def highlight_file(filepath):
    with open(filepath, "r") as f:
        file_content = f.read()
    try:
        lexer = lexers.get_lexer_for_filename(filepath, stripnl=False, stripall=False)
    except ClassNotFound:
        lexer = lexers.get_lexer_by_name("text", stripnl=False, stripall=False)
    formatter = formatters.TerminalFormatter(bg="dark")  # dark or light
    highlighted_file_content = highlight(file_content, lexer, formatter)
    return highlighted_file_content


def list_files(directory="."):
    return (file for file in os.listdir(directory) if os.path.isfile(os.path.join(directory, file)))


def main():
    terminal_menu = TerminalMenu(list_files(), preview_command=highlight_file, preview_size=0.75)
    menu_entry_index = terminal_menu.show()


if __name__ == "__main__":
    main()
screenshot_preview_pygments

Preview the active pane of each running tmux session (the session ids are appended to the menu entries with the | separator):

#!/usr/bin/env python3

import subprocess
from simple_term_menu import TerminalMenu


def list_tmux_sessions():
    tmux_command_output = subprocess.check_output(
        ["tmux", "list-sessions", "-F#{session_id}:#{session_name}"], universal_newlines=True
    )
    tmux_sessions = []
    for line in tmux_command_output.split("\n"):
        line = line.strip()
        if not line:
            continue
        session_id, session_name = tuple(line.split(":"))
        tmux_sessions.append((session_name, session_id))
    return tmux_sessions


def main():
    terminal_menu = TerminalMenu(
        ("|".join(session) for session in list_tmux_sessions()),
        preview_command="tmux capture-pane -e -p -t {}",
        preview_size=0.75,
    )
    menu_entry_index = terminal_menu.show()


if __name__ == "__main__":
    main()
screenshot_preview_tmux_sessions

        5.3.9 Skipping empty entries
Use the constructor parameter skip_empty_entries or the flag --skip-empty-entries to interpret an empty string value in the menu entries as an empty menu entry (will be skipped when iterating over the entries). A None value is always considered as an empty menu entry independently from the skip_empty_entries parameter.

from simple_term_menu import TerminalMenu

def main():                                                         # Or use `None` instead of `""`:
    options = ["entry 1", "entry 2", "", "add", "edit"]             # ["entry 1", "entry 2", None, "add", "edit"]
    terminal_menu = TerminalMenu(options, skip_empty_entries=True)  # TerminalMenu(options)
    menu_entry_index = terminal_menu.show()
    print(f"You have selected {options[menu_entry_index]}!")

if __name__ == "__main__":
    main()
screenshot_skip_empty_entries

Localization / Text modification
Use the constructor parameters

show_search_hint_text and
show_multi_select_hint_text
to modify the corresponding texts. Use the placeholder {key} for the search key in show_search_hint_text and both {accept_keys} and {multi_select_keys} in show_multi_select_hint_text if appropriately.

        5.3.10 Additional settings
Furthermore, the TerminalMenu constructor takes these additional parameters to change the menu behavior:

clear_menu_on_exit: A bool value which indicates if the menu will be cleared after the show method. Defaults to True.
clear_screen: A bool value which indicates if the screen will be cleared before the menu is shown. Defaults to False.
cursor_index: The initially selected item index.
cycle_cursor: A bool value which indicates if the menu cursor cycles when the end of the menu is reached. Defaults to True.
quit_keys: An iterable of keys which quit the terminal menu. Defaults to ("escape", "q").
raise_error_on_interrupt: Set this to True to reraise Keyboard interrupts (by pressing <Ctrl-c>). Defaults to False.
status_bar_below_preview: Position the status bar below the preview window (default positioning is above).
Command line program
simple-term-menu can be used as a terminal program in shell scripts. The exit code of the script is the 1-based index of the selected menu entry. The exit code 0 reports the cancel action. The following command line arguments are supported:

usage: simple-term-menu [-h] [-s] [-X] [-l] [--cursor CURSOR]
                        [-i CURSOR_INDEX] [--cursor-style CURSOR_STYLE] [-C]
                        [-E] [--highlight-style HIGHLIGHT_STYLE] [-m]
                        [--multi-select-cursor MULTI_SELECT_CURSOR]
                        [--multi-select-cursor-brackets-style MULTI_SELECT_CURSOR_BRACKETS_STYLE]
                        [--multi-select-cursor-style MULTI_SELECT_CURSOR_STYLE]
                        [--multi-select-keys MULTI_SELECT_KEYS]
                        [--multi-select-no-select-on-accept]
                        [--multi-select-empty-ok] [-p PREVIEW_COMMAND]
                        [--no-preview-border] [--preview-size PREVIEW_SIZE]
                        [--preview-title PREVIEW_TITLE]
                        [--search-highlight-style SEARCH_HIGHLIGHT_STYLE]
                        [--search-key SEARCH_KEY]
                        [--shortcut-brackets-highlight-style SHORTCUT_BRACKETS_HIGHLIGHT_STYLE]
                        [--shortcut-key-highlight-style SHORTCUT_KEY_HIGHLIGHT_STYLE]
                        [--show-multi-select-hint]
                        [--show-multi-select-hint-text SHOW_MULTI_SELECT_HINT_TEXT]
                        [--show-search-hint]
                        [--show-search-hint-text SHOW_SEARCH_HINT_TEXT]
                        [--show-shortcut-hints]
                        [--show-shortcut-hints-in-title]
                        [--skip-empty-entries] [-b STATUS_BAR] [-d]
                        [--status-bar-style STATUS_BAR_STYLE] [--stdout]
                        [-t TITLE] [-V]
                        [-r PRESELECTED_ENTRIES | -R PRESELECTED_INDICES]
                        [entries ...]

simple-term-menu creates simple interactive menus in the terminal and returns the selected entry as exit code.

positional arguments:
  entries               the menu entries to show

options:
  -h, --help            show this help message and exit
  -s, --case-sensitive  searches are case sensitive
  -X, --no-clear-menu-on-exit
                        do not clear the menu on exit
  -l, --clear-screen    clear the screen before the menu is shown
  --cursor CURSOR       menu cursor (default: "> ")
  -i CURSOR_INDEX, --cursor-index CURSOR_INDEX
                        initially selected item index
  --cursor-style CURSOR_STYLE
                        style for the menu cursor as comma separated list
                        (default: "fg_red,bold")
  -C, --no-cycle        do not cycle the menu selection
  -E, --no-exit-on-shortcut
                        do not exit on shortcut keys
  --highlight-style HIGHLIGHT_STYLE
                        style for the selected menu entry as comma separated
                        list (default: "standout")
  -m, --multi-select    Allow the selection of multiple entries (implies
                        `--stdout`)
  --multi-select-cursor MULTI_SELECT_CURSOR
                        multi-select menu cursor (default: "[*] ")
  --multi-select-cursor-brackets-style MULTI_SELECT_CURSOR_BRACKETS_STYLE
                        style for brackets of the multi-select menu cursor as
                        comma separated list (default: "fg_gray")
  --multi-select-cursor-style MULTI_SELECT_CURSOR_STYLE
                        style for the multi-select menu cursor as comma
                        separated list (default: "fg_yellow,bold")
  --multi-select-keys MULTI_SELECT_KEYS
                        key for toggling a selected item in a multi-selection
                        (default: " ,tab",
  --multi-select-no-select-on-accept
                        do not select the currently highlighted menu item when
                        the accept key is pressed (it is still selected if no
                        other item was selected before)
  --multi-select-empty-ok
                        when used together with --multi-select-no-select-on-
                        accept allows returning no selection at all
  -p PREVIEW_COMMAND, --preview PREVIEW_COMMAND
                        Command to generate a preview for the selected menu
                        entry. "{}" can be used as placeholder for the menu
                        text. If the menu entry has a data component
                        (separated by "|"), this is used instead.
  --no-preview-border   do not draw a border around the preview window
  --preview-size PREVIEW_SIZE
                        maximum height of the preview window in fractions of
                        the terminal height (default: "0.25")
  --preview-title PREVIEW_TITLE
                        title of the preview window (default: "preview")
  --search-highlight-style SEARCH_HIGHLIGHT_STYLE
                        style of matched search patterns (default:
                        "fg_black,bg_yellow,bold")
  --search-key SEARCH_KEY
                        key to start a search (default: "/", "none" is treated
                        a special value which activates the search on any
                        letter key)
  --shortcut-brackets-highlight-style SHORTCUT_BRACKETS_HIGHLIGHT_STYLE
                        style of brackets enclosing shortcut keys (default:
                        "fg_gray")
  --shortcut-key-highlight-style SHORTCUT_KEY_HIGHLIGHT_STYLE
                        style of shortcut keys (default: "fg_blue")
  --show-multi-select-hint
                        show a multi-select hint in the status bar
  --show-multi-select-hint-text SHOW_MULTI_SELECT_HINT_TEXT
                        Custom text which will be shown as multi-select hint.
                        Use the placeholders {multi_select_keys} and
                        {accept_keys} if appropriately.
  --show-search-hint    show a search hint in the search line
  --show-search-hint-text SHOW_SEARCH_HINT_TEXT
                        Custom text which will be shown as search hint. Use
                        the placeholders {key} for the search key if
                        appropriately.
  --show-shortcut-hints
                        show shortcut hints in the status bar
  --show-shortcut-hints-in-title
                        show shortcut hints in the menu title
  --skip-empty-entries  Interpret an empty string in menu entries as an empty
                        menu entry
  -b STATUS_BAR, --status-bar STATUS_BAR
                        status bar text
  -d, --status-bar-below-preview
                        show the status bar below the preview window if any
  --status-bar-style STATUS_BAR_STYLE
                        style of the status bar lines (default:
                        "fg_yellow,bg_black")
  --stdout              Print the selected menu index or indices to stdout (in
                        addition to the exit status). Multiple indices are
                        separated by ";".
  -t TITLE, --title TITLE
                        menu title
  -V, --version         print the version number and exit
  -r PRESELECTED_ENTRIES, --preselected_entries PRESELECTED_ENTRIES
                        Comma separated list of strings matching menu items to
                        start pre-selected in a multi-select menu.
  -R PRESELECTED_INDICES, --preselected_indices PRESELECTED_INDICES
                        Comma separated list of numeric indexes of menu items
                        to start pre-selected in a multi-select menu.
Example with preview option
Instead of using the Python api as in the previous examples, a file menu with bat preview can also be created from the command line:

simple-term-menu -p "bat --color=always {}" \
                 --preview-size 0.75 \
                 $(find . -maxdepth 1  -type f | awk '{ print substr($0, 3) }')

        5.3.11 More advanced example
A more advanced example with sub menus (thanks to pageauc):

#!/usr/bin/env python3
"""
Demonstration example for GitHub Project at
https://github.com/IngoMeyer441/simple-term-menu

This code only works in python3. Install per

    sudo pip3 install simple-term-menu

"""
import time

from simple_term_menu import TerminalMenu


def main():
    main_menu_title = "  Main Menu\n"
    main_menu_items = ["Edit Menu", "Second Item", "Third Item", "Quit"]
    main_menu_cursor = "> "
    main_menu_cursor_style = ("fg_red", "bold")
    main_menu_style = ("bg_red", "fg_yellow")
    main_menu_exit = False

    main_menu = TerminalMenu(
        menu_entries=main_menu_items,
        title=main_menu_title,
        menu_cursor=main_menu_cursor,
        menu_cursor_style=main_menu_cursor_style,
        menu_highlight_style=main_menu_style,
        cycle_cursor=True,
        clear_screen=True,
    )

    edit_menu_title = "  Edit Menu\n"
    edit_menu_items = ["Edit Config", "Save Settings", "Back to Main Menu"]
    edit_menu_back = False
    edit_menu = TerminalMenu(
        edit_menu_items,
        title=edit_menu_title,
        menu_cursor=main_menu_cursor,
        menu_cursor_style=main_menu_cursor_style,
        menu_highlight_style=main_menu_style,
        cycle_cursor=True,
        clear_screen=True,
    )

    while not main_menu_exit:
        main_sel = main_menu.show()

        if main_sel == 0:
            while not edit_menu_back:
                edit_sel = edit_menu.show()
                if edit_sel == 0:
                    print("Edit Config Selected")
                    time.sleep(5)
                elif edit_sel == 1:
                    print("Save Selected")
                    time.sleep(5)
                elif edit_sel == 2:
                    edit_menu_back = True
                    print("Back Selected")
            edit_menu_back = False
        elif main_sel == 1:
            print("option 2 selected")
            time.sleep(5)
        elif main_sel == 2:
            print("option 3 selected")
            time.sleep(5)
        elif main_sel == 3:
            main_menu_exit = True
            print("Quit Selected")


if __name__ == "__main__":
    main()

        5.3.12 Similar projects
bullet: Creates bullet-lists with multi-selection support.
Contributing
Please open an issue on GitHub if you experience bugs or miss features. Please consider to send a pull request if you can spend time on fixing the issue yourself. This project uses pre-commit to ensure code quality and a consistent code style. Run

make git-hooks-install
to install all linters as Git hooks in your local clone of simple-term-menu.



id=__How_to_implement_a_dynamic_command_line_menu_in_Python
    5.4 How to implement a dynamic command line menu in Python
https://chunkofcode.net/how-to-implement-a-dynamic-command-line-menu-in-python/

In this post I will show you how to implement a dynamic command line menu in Python. In most tutorials and posts that I have seen online the usual way to implement a command line menu is a while loop with a bunch of if-elif statements. This implementation is okay for small menu’s, but it can quickly get very tedious if you need to modify your menu afterwards. In this implementation it is not possible to create dynamic menu items, meaning that we need to hard code the menu structure in a separate function.

First I will show you the usual implementation so that the advantages of a dynamic controller become more obvious. For the sake of this example I won’t worry about error handling or other things in order to keep the code as concise as possible. After the first example, I will also show what is the better way to do the menu dynamically.

if/elif menu
The first thing we need is a function that prints out the menu, so that the user knows which commands are available.

def print_menu():

    print((================================\n
           MENU\n
               ================================\n
               1 - First menu item\n
               2 - Second menu item\n
               3 - Third menu item\n
               5 - Fourth menu item\n
               6 - Exit\n
               ================================\n
           Enter a choice and press enter:), end= )
I prefer this kind of multiline syntax, because it enables you to write clean and aligned rows and it is surely more beautiful than the following version:

def print_menu():

    print(================================
MENU
    ================================
    1 - First menu item
    2 - Second menu item
    3 - Third menu item
    5 - Fourth menu item
    6 - Exit
    ================================
Enter a choice and press enter:)
My preferred version needs more typing, but at least that way you don’t have some strange unaligned string rows on the left. Then we create the actual menu functionality.

def menu():

    print_menu()
    user_input = 0

    while user_input != 6:

        user_input = int(input())

        if user_input == 1:
            print(Executing menu item 1)

        elif user_input == 2:
            print(Executing menu item 2)

        elif user_input == 3:
            print(Executing menu item 3)

        elif user_input == 4:
            print(Executing menu item 4)

        elif user_input == 5:
            print(Executing menu item 5)

        elif user_input == 6:
            print(Exiting...)
First we call the previously created print_menu() function and create a user_input variable to store the inputs typed in by the user. Then we enter the while loop and we run the while loop over and over again until the user types in the number 6, which exits the program.

Now we have our print_menu() function and the basic functionality in place. The next step is to run the program.

if __name__ == __main__:
    menu()
We could start developing the progam further by elaborating the if statements. We could create additional functions or modules to be called after a specific key press. We could add more elif statements to run more function. Of course, every time we add a new elif statement, we need to modify also our print_menu() function. This is where the dynamic menu controller starts making sense. Lets see next how it can be implemented.

Dynamic controller menu
We start by creating a new class for the controller with some methods in it. Let’s call the class Controller and the methods do_X.

class Controller:

    @staticmethod
    def do_1():
        print(Doing 1)

    @staticmethod
    def do_2():
        print(Doing 2)

    @staticmethod
    def do_3():
        print(Doing 3)

    @staticmethod
    def do_4():
        print(Doing 4)

    @staticmethod
    def do_5():
        print(Doing 5)

    @staticmethod
    def do_6():
        print(Exiting...)
This is the basic structure of the class. We simulate the if/elif statements of the previous example with separate methods that are called do_X methods. The do-methods in this example must be named with a do prefix. You will understand why later. So these are the methods that the user will be able to run from the command line. In this example we do not need to create an instance of the Controller class and that is why I use static methods here.

But how can we run all these methods? How do we bind the user input to a specific do-method?

The answer is rather simple once you’ve learned it once. We will need a separate helper method that takes the user input as an argument.

    @staticmethod
    def execute(user_input):
        controller_name = fdo_{user_input}
        try:
            controller = getattr(Controller, controller_name)
        except AttributeError:
            print(Method not found)
        else:
            controller()
In the snippet you can see the helper method. I still need to show you where to call this method from, but let’s go through the execute() method first.

First we create a string and store it to controller_name. The stored string will be formatted like do_{user_input}. Ideally the user input is a number that correnponds to one of our do-methods. If it doesn’t, an AttibureError will be raised.

Then we create a new variable, controller. We use the getattr() function to look for the requested do-method from within the Controller class. The first argument of the getattr() function is the class and the second one is the method name to be searched in a string format.

If no error is raised, we run the controller method created by the getattr() function. Now we need something to tie all this together. In the if/elif example we used a while loop, and so we do also in this one. Let’s create the run() method.

    @staticmethod
    def run():
        user_input = 0
        while(user_input != 6):
            user_input = int(input())
            Controller.execute(user_input)
        print(Program stopped.)
Perfect. Now we’ve got ourselves a working program. While the user inserts something that is not 6, the while loop will keep calling the execute() method with the wanted user input. If the user gives 6 as an input. The program stops and prints out “Program stopped.” notification.

Note that in this example we suppose that the exit method is the do_6 method. It may be necessary to write some code to find the last do-method from the class and use it as the exit method, so that you can add more do-methods without having to modify the run() method.

But there is no menu. We will need a generate_menu() method. But before writing the generate_menu() method. Add a docstring to all your do-methods. The docstring will be used for generating the menu dynamically.

Modify your do-methods as you please. I leave a one-method example here just for clarification.

    @staticmethod
    def do_1():
        First menu item
        print(Doing 1)
Then create the generate_menu() method.

    @staticmethod
    def generate_menu():
        print(================================)
        do_methods = [m for m in dir(Controller) if m.startswith('do_')]
        menu_string = "\n".join(
            [f{method[-1]}.  {getattr(Controller, method).__doc__}
             for method in do_methods])
        print(menu_string)
        print(================================)
        print(Insert a number:, end= )
So what is happening there. After the first line of equals signs we create a list of all methods (do_methods) that start with do_. These are the famous do-methods that will be used as our menu items.

After that we create a single string that contains the whole menu by using list comprehension and join method. This part is a bit hard to read so I will go through the list comprehension to make it more clear.

method[-1] is the number in the do-method name. For example, do_1.
getattr(Controller, method).__doc__ fetches the docstring of the method.
These steps are repeated for all the methods in the do_methods list. As a result we get a string that in this case would be “1. First menu item”. You may notice that done like this the menu work only when there are less than 10 do-methods, as method[-1] slices only the last character of the method name. This could be easily fixed, for example, by using a simple regular expression. Or maybe in some cases you could just grab everything that comes after the underscore.

Come to this point, we are ready to run our working controller.

def main():
    Controller.run()


if __name__ == __main__:
    main()
Conclusion
I showed you two ways to create menu structure for a command line application.

The first way was the simple way by using if/elif statements. This menu structure is fine for small menus that are not going to be modified afterwards. It is really easy to understand even for a beginner programmer. The downside of it is that if some revisions are needed, the modifying process can get tedious and error prone.

The second way was a dynamic controller. This is the way that I recommend for larger menus. It is a bit harder to understand, but it is important to get good grasp of it if you are planning on designing bigger applications. The advantage is that by adding more do-methods, the menu updates dynamically without having to revise it manually. This means that the menu is always up-to-date and the methods are accessible.

With the dynamic menu you can easily use other method ID’s than numbers. Nothing prevents you from using something like dojump_ or doshoot_ as your method name. You just need to adapt the generate_menu() to your method names.

That’s all for now. :)

6. Compile python
	6.1 for windows
	Creating a Python Executable File Using makeExe.py
This file creates an executable file from a Python file specified by the user and writes the new executable to disk.

	First, create a setup file with this content:
	from distutils.core import setup
	import py2exe
	
	setup(console=['name.py'])

	Copy name.py to python install dir
	Then, run (from a dos shell in python install path):
	python setup.py py2exe

	two dirs are created, build - is workspace and can be deleted, dist - contains the exe and its denpendencies.

		6.1.1 get help on configuration options of py2exe
		>>> import py2exe
		>>> help(py2exe)

		6.1.2
"""
makeExe.py
- Simple Python script to automate the creation
  of Python executables using py2exe.

(c) 2004 Premshree Pillai (24/01/04) http://www.qiksearch.com/
"""
http://www.py2exe.org/

7. Run system shell commands.
	7.1 du example:
	To run a command and analyse its output like the backquotes do in other languages, is two things to be done, so it's two functions.

	popen opens a pipe to another command (varients allow you to decide whether you want to access input, output and error channels) then you can readlines (or use another file read function) to go through the output.

	Here's an example that runs a du command ... clearly it will only work on operating systems that support the command.  popen4 allows us to handle both stdout and stderr through the same input stream.

	import os
	import re

	put, get = os.popen4("du -s /Users/*")

	for user in get.readlines():
		       if re.match(r'^\d',user): print user,

8. Bit manipulation:

	8.1 http://wiki.python.org/moin/BitManipulation
	Manipulations

To integer.
Toggle line numbers

    >>> print int('00100001', 2)
    33

To hex string. Note that you don't need to use x8 bits.
Toggle line numbers

   >>> print "0x%x" % int('11111111', 2)
   0xff
   >>> print "0x%x" % int('0110110110', 2)
   0x1b6
   >>> print "0x%x" % int('0010101110101100111010101101010111110101010101', 2)
   0xaeb3ab57d55

To character. 8 bits max.
Toggle line numbers

 >>> chr(int('111011', 2))
 ';'
 >>> chr(int('1110110', 2))
 'v'
 >>> chr(int('11101101', 2))
 '\xed'

Characters to integers, but not to strings of 1's and 0's.
Toggle line numbers

 >>> int('01110101', 2)
 117
 >>> chr(int('01110101', 2))
 'u'
 >>> ord('u')
 117

Individual bits.
Toggle line numbers

   >>> 1 << 0
   1
   >>> 1 << 1
   2
   >>> 1 << 2
   4
   >>> 1 << 3
   8
   >>> 1 << 4
   16
   >>> 1 << 5
   32
   >>> 1 << 6
   64
   >>> 1 << 7
   128

Transformations Summary

Strings to Integers:

    *

      "1011101101": int(str, 2)
    *

      "m": ord(str)
    *

      "0xdecafbad": int(str, 16) (known to work in Python 2.4)
    *

      "decafbad": int(str, 16) (known to work in Python 2.4)

Integers to Strings:

    *

      "1011101101": no built-in technique (see below)
    *

      "m": chr(str)
    *

      "0xdecafbad": hex(val)
    *

      "decafbad": "%x" % val

We are still left without a technique for producing binary strings, and decyphering hex strings.
Hex String to Integer

(not needed anymore)

One approach to decyphering "0xdecafbad" style hex strings, is to use eval:
Toggle line numbers

    >>> eval("0xdecafbad ")
    3737844653L

However, this could be dangerous, depending on where you're getting your data from.

Here's a function that is safer:
Toggle line numbers

   def hex_to_integer(h):
       """Convert a hex string to an integer.
   
       The hex string can be any length. It can start with an 0x, or not.
       Unrecognized characters will raise a ValueError.
   
       This function released into the public domain by it's author, Lion
       Kimbro.
       """
       num = 0  # Resulting integer
       h = h.lower()  # Hex string
       if h[:2] == "0x":
           h = h[2:]
       for c in h:  # Hex character
           num = num * 16
           if "0" <= c <= "9":
               num = num + (ord(c) - ord("0"))
           elif "a" <= c <= "f":
               num = num + (ord(c) - ord("a"))
               num = num + 10
           else:
               raise ValueError(c)
       return num

Integer to Bin String

It's high time Python supported binary literals (e.g. 0b10011000) and a bin() function!
Toggle line numbers

    >>> def bin(a):
            s=''
            t={'0':'000','1':'001','2':'010','3':'011',
               '4':'100','5':'101','6':'110','7':'111'}
            for c in oct(a)[1:]:
                    s+=t[c]
            return s

Research Links

this is the sort of thing we're looking for:

    *

      [WWW] ASPN: bit-field manipulation

related modules:

    *

      [WWW] array module -- (issued with Python)
    *

      [WWW] struct module -- (issued with Python)
    *

      [WWW] binascii module -- (issued with Python)
    *

      [WWW] pySerial module -- access the serial port

see also: BitwiseOperators


	8.2 http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/113799

	#
# bitfield manipulation
#

class bf(object):
    def __init__(self,value=0):
        self._d = value

    def __getitem__(self, index):
        return (self._d >> index) & 1 

    def __setitem__(self,index,value):
        value    = (value&1L)<<index
        mask     = (1L)<<index
        self._d  = (self._d & ~mask) | value

    def __getslice__(self, start, end):
        mask = 2L**(end - start) -1
        return (self._d >> start) & mask

    def __setslice__(self, start, end, value):
        mask = 2L**(end - start) -1
        value = (value & mask) << start
        mask = mask << start
        self._d = (self._d & ~mask) | value
        return (self._d >> start) & mask

    def __int__(self):
        return self._d

k = bf()
k[3:7]=5
print k[3]
print k[5]
k[7]=1
print k[4:8]
print int(k)

	8.3 Binary buffers.
		8.3.1	Strings in python can contain 0 and control chars, Unlike C, C++ and Jave!
		So a python string can perfectly well serve as a binary buffer (also a list can serve well), 
		but its content won't show in print unless the list of individual chars is printed via:
		>>> print list(buffer)

		8.3.2 struct
			use struct to pack and unpack binary data in python string format.
			see example from tutorial:
			>>> from struct import *
			>>> pack('hhl', 1, 2, 3)
			'\x00\x01\x00\x02\x00\x00\x00\x03'
			>>> unpack('hhl', '\x00\x01\x00\x02\x00\x00\x00\x03')
			(1, 2, 3)
			>>> calcsize('hhl')
			8


9. Network programming
	9.1 simple TCP server and client
		9.1.1 simple TCP server
# This is a simple TCP server, example taken from http://www.devshed.com/c/a/Python/Sockets-in-Python-Into-the-World-of-Python-Network-Programming/2/

from socket import * 

#Then the constants that defines the host, port, buffer size and the address tuple to be used with bind().

HOST = 'localhost'
PORT = 21567
BUFSIZ = 1024
ADDR = (HOST, PORT)

#Then we create the server side socket and bind it to the host and the port. Then comes the max queue size to 2:

serversock = socket(AF_INET, SOCK_STREAM)
serversock.bind(ADDR)
serversock.listen(2)

#Now, to make it listen for incoming requests continuously, place the accept() method in a while loop. This is not the most preferred mode. The preferred way will be discussed in the next section:
#Next, receive the data from the client and echo it back. This has to continue until the client doesnt send the null data or ctrl+c. To achieve this, use a while loop again and then close the connection when done.

while 1:
        print 'waiting for connection'
        clientsock, addr = serversock.accept()
        print 'connected from:', addr

        while 1:
                data = clientsock.recv(BUFSIZ)
                if not data: break
                clientsock.send('>>> '+data)

        clientsock.close()
serversock.close()

		9.1.2 simple TCP client
# This is a simple TCP client, example taken from http://www.devshed.com/c/a/Python/Sockets-in-Python-Into-the-World-of-Python-Network-Programming/2/
from socket import *

HOST = 'localhost'
PORT = 21567
BUFSIZ = 1024
ADDR = (HOST, PORT)

tcpCliSock = socket(AF_INET, SOCK_STREAM)
tcpCliSock.connect(ADDR)

while 1:
        data = raw_input('> ')
        if not data: 
                break
        tcpCliSock.send(data)
        data = tcpCliSock.recv(1024)
        if not data: 
                break 
        print data

tcpCliSock.close()

	9.2 byte buffers.
	use list of chars as byte buffer
	>>> buffer = []

	use append to add data.
	use join to convert to string for sending

	>>> "".join(buffer)

	9.3 Data structures, common tyes and idioms 
	 
		9.3.1 hexa strings


- # Hex literals
>>> 0x01, 0x10, 0xFF  
(1, 16, 255)

		9.3.2 Struct
- struct <URL:#r=__python_struct__>
struct  â Interpret strings as packed binary dataÂ¶

This module performs conversions between Python values and C structs represented as Python strings. This can be used in handling binary data stored in files or from network connections, among other sources. It uses Format Strings as compact descriptions of the layout of the C structs and the intended conversion to/from Python values.

Note

By default, the result of packing a given C struct includes pad bytes in order to maintain proper alignment for the C types involved; similarly, alignment is taken into account when unpacking. This behavior is chosen so that the bytes of a packed struct correspond exactly to the layout in memory of the corresponding C struct. To handle platform-independent data formats or omit implicit pad bytes, use standard size and alignment instead of native size and alignment: see Byte Order, Size, and Alignment for details.

			9.3.2.1 Functions and ExceptionsÂ¶

The module defines the following exception and functions:

exception struct.errorÂ¶
    Exception raised on various occasions; argument is a string describing what is wrong.

struct.pack(fmt, v1, v2, ...)Â¶
    Return a string containing the values v1, v2, ... packed according to the given format. The arguments must match the values required by the format exactly.

struct.pack_into(fmt, buffer, offset, v1, v2, ...)Â¶

    Pack the values v1, v2, ... according to the given format, write the packed bytes into the writable buffer starting at offset. Note that the offset is a required argument.

    New in version 2.5.

struct.unpack(fmt, string)Â¶
    Unpack the string (presumably packed by pack(fmt, ...)) according to the given format. The result is a tuple even if it contains exactly one item. The string must contain exactly the amount of data required by the format (len(string) must equal calcsize(fmt)).

struct.unpack_from(fmt, buffer[, offset=0])Â¶

    Unpack the buffer according to the given format. The result is a tuple even if it contains exactly one item. The buffer must contain at least the amount of data required by the format (len(buffer[offset:]) must be at least calcsize(fmt)).

    New in version 2.5.

struct.calcsize(fmt)Â¶
    Return the size of the struct (and hence of the string) corresponding to the given format.

			9.3.2.2 Format Strings

Format strings are the mechanism used to specify the expected layout when packing and unpacking data. They are built up from Format Characters, which specify the type of data being packed/unpacked. In addition, there are special characters for controlling the Byte Order, Size, and Alignment.

				9.3.2.2.1 Byte Order, Size, and AlignmentÂ¶

By default, C types are represented in the machineâs native format and byte order, and properly aligned by skipping pad bytes if necessary (according to the rules used by the C compiler).

Alternatively, the first character of the format string can be used to indicate the byte order, size and alignment of the packed data, according to the following table:
Character 	Byte order 	Size 	Alignment
@ 	native 	native 	native
= 	native 	standard 	none
< 	little-endian 	standard 	none
> 	big-endian 	standard 	none
! 	network (= big-endian) 	standard 	none

If the first character is not one of these, '@' is assumed.

Native byte order is big-endian or little-endian, depending on the host system. For example, Intel x86 and AMD64 (x86-64) are little-endian; Motorola 68000 and PowerPC G5 are big-endian; ARM and Intel Itanium feature switchable endianness (bi-endian). Use sys.byteorder to check the endianness of your system.

Native size and alignment are determined using the C compilerâs sizeof expression. This is always combined with native byte order.

Standard size depends only on the format character; see the table in the Format Characters section.

Note the difference between '@' and '=': both use native byte order, but the size and alignment of the latter is standardized.

The form '!' is available for those poor souls who claim they canât remember whether network byte order is big-endian or little-endian.

There is no way to indicate non-native byte order (force byte-swapping); use the appropriate choice of '<' or '>'.

Notes:

|   1. Padding is only automatically added between successive structure members. No padding is added at the beginning or the end of the encoded struct.
|   2. No padding is added when using non-native size and alignment, e.g. with â<â, â>â, â=â, and â!â.
|   3. To align the end of a structure to the alignment requirement of a particular type, end the format with the code for that type with a repeat count of zero. See Examples.

				9.3.2.2.2 Format CharactersÂ¶

Format characters have the following meaning; the conversion between C and Python values should be obvious given their types:
Format 	C Type 	Python type 	Standard size 	Notes
x 	pad byte 	no value 	  	 
c 	char 	string of length 1 	1 	 
b 	signed char 	integer 	1 	 
B 	unsigned char 	integer 	1 	 
? 	_Bool 	bool 	1 	(1)
h 	short 	integer 	2 	 
H 	unsigned short 	integer 	2 	 
i 	int 	integer 	4 	 
I 	unsigned int 	integer 	4 	 
l 	long 	integer 	4 	 
L 	unsigned long 	integer 	4 	 
q 	long long 	integer 	8 	(2)
Q 	unsigned long long 	integer 	8 	(2)
f 	float 	float 	4 	(3)
d 	double 	float 	8 	(3)
s 	char[] 	string 	  	 
p 	char[] 	string 	  	 
P 	void * 	integer 	  	(4)

Notes:

|   1.  The '?' conversion code corresponds to the _Bool type defined by C99. If this type is not available, it is simulated using a char. In standard mode, it is always represented by one byte.
|
|      New in version 2.6.
|   2.  The 'q' and 'Q' conversion codes are available in native mode only if the platform C compiler supports C long long, or, on Windows, __int64. They are always available in standard modes.
|
|      New in version 2.2.
|   3.  For the 'f' and 'd' conversion codes, the packed representation uses the IEEE 754 binary32 (for 'f') or binary64 (for 'd') format, regardless of the floating-point format used by the platform.
|   4.  The 'P' format character is only available for the native byte ordering (selected as the default or with the '@' byte order character). The byte order character '=' chooses to use little- or big-endian ordering based on the host system. The struct module does not interpret this as native ordering, so the 'P' format is not available.

A format character may be preceded by an integral repeat count. For example, the format string '4h' means exactly the same as 'hhhh'.

Whitespace characters between formats are ignored; a count and its format must not contain whitespace though.

For the 's' format character, the count is interpreted as the size of the string, not a repeat count like for the other format characters; for example, '10s' means a single 10-byte string, while '10c' means 10 characters. For packing, the string is truncated or padded with null bytes as appropriate to make it fit. For unpacking, the resulting string always has exactly the specified number of bytes. As a special case, '0s' means a single, empty string (while '0c' means 0 characters).

The 'p' format character encodes a âPascal stringâ, meaning a short variable-length string stored in a fixed number of bytes. The count is the total number of bytes stored. The first byte stored is the length of the string, or 255, whichever is smaller. The bytes of the string follow. If the string passed in to pack() is too long (longer than the count minus 1), only the leading count-1 bytes of the string are stored. If the string is shorter than count-1, it is padded with null bytes so that exactly count bytes in all are used. Note that for unpack(), the 'p' format character consumes count bytes, but that the string returned can never contain more than 255 characters.

For the 'P' format character, the return value is a Python integer or long integer, depending on the size needed to hold a pointer when it has been cast to an integer type. A NULL pointer will always be returned as the Python integer 0. When packing pointer-sized values, Python integer or long integer objects may be used. For example, the Alpha and Merced processors use 64-bit pointer values, meaning a Python long integer will be used to hold the pointer; other platforms use 32-bit pointers and will use a Python integer.

For the '?' format character, the return value is either True or False. When packing, the truth value of the argument object is used. Either 0 or 1 in the native or standard bool representation will be packed, and any non-zero value will be True when unpacking.

				9.3.2.2.3 ExamplesÂ¶

Note

All examples assume a native byte order, size, and alignment with a big-endian machine.

A basic example of packing/unpacking three integers:

>>> from struct import *
>>> pack('hhl', 1, 2, 3)
'\x00\x01\x00\x02\x00\x00\x00\x03'
>>> unpack('hhl', '\x00\x01\x00\x02\x00\x00\x00\x03')
(1, 2, 3)
>>> calcsize('hhl')
8

Unpacked fields can be named by assigning them to variables or by wrapping the result in a named tuple:

>>> record = 'raymond   \x32\x12\x08\x01\x08'
>>> name, serialnum, school, gradelevel = unpack('<10sHHb', record)

>>> from collections import namedtuple
>>> Student = namedtuple('Student', 'name serialnum school gradelevel')
>>> Student._make(unpack('<10sHHb', s))
Student(name='raymond   ', serialnum=4658, school=264, gradelevel=8)

The ordering of format characters may have an impact on size since the padding needed to satisfy alignment requirements is different:

>>> pack('ci', '*', 0x12131415)
'*\x00\x00\x00\x12\x13\x14\x15'
>>> pack('ic', 0x12131415, '*')
'\x12\x13\x14\x15*'
>>> calcsize('ci')
8
>>> calcsize('ic')
5

The following format 'llh0l' specifies two pad bytes at the end, assuming longs are aligned on 4-byte boundaries:

>>> pack('llh0l', 1, 2, 3)
'\x00\x00\x00\x01\x00\x00\x00\x02\x00\x03\x00\x00'

This only works when native size and alignment are in effect; standard size and alignment does not enforce any alignment.

See also

Module array
    Packed binary storage of homogeneous data.
Module xdrlib
    Packing and unpacking of XDR data.

			9.3.2.3

		9.3.3 deque

		    9.3.3.0  basics and API

 help(deque)
 |  append(...)
 |      Add an element to the right side of the deque.
 |  
 |  appendleft(...)
 |      Add an element to the left side of the deque.
 |  
 |  clear(...)
 |      Remove all elements from the deque.
 |  
 |  copy(...)
 |      Return a shallow copy of a deque.
 |  
 |  count(...)
 |      D.count(value) -> integer -- return number of occurrences of value
 |  
 |  extend(...)
 |      Extend the right side of the deque with elements from the iterable
 |  
 |  extendleft(...)
 |      Extend the left side of the deque with elements from the iterable
 |  
 |  index(...)
 |      D.index(value, [start, [stop]]) -> integer -- return first index of value.
 |      Raises ValueError if the value is not present.
 |  
 |  insert(...)
 |      D.insert(index, object) -- insert object before index
 |  
 |  pop(...)
 |      Remove and return the rightmost element.
 |  
 |  popleft(...)
 |      Remove and return the leftmost element.
 |  
 |  remove(...)
 |      D.remove(value) -- remove first occurrence of value.
 |  
 |  reverse(...)
 |      D.reverse() -- reverse *IN PLACE*
 |  
 |  rotate(...)
 |      Rotate the deque n steps to the right (default n=1).  If n is negative, rotates left.


		    9.3.3.1 deque bfs usage
[i500695@C02X632CJGH6:2021-04-07 15:45:23:~/work/code/interviewQs/algos:]2012$ py3 bfs_dfs.py
cat bfs_dfs.py
#from collections import defaultdict
from collections import deque

class Graph:
    def __init__(self):
        self.graph = dict()
        #self.graph = defaultdict(set)
    
    def addEdge(self, v, u):
        if v in self.graph:
            if not (u in self.graph[v]):
                self.graph[v].add(u)
        else:
            self.graph[v] = {u}
        if u in self.graph:
            if not (v in self.graph[u]):
                self.graph[u] = v
        else:
            self.graph[u] = {v}

# Note that BFS by nature will always find the shortest path from root to any node by edge count
# if edges were weighted then for min path we would have needed to keep tabs of path weithts 
    def bfspath(self, r, cb):
        q = deque()
        q.append((r, [r]))
        visited = {r}

        while len(q)>0:
            (c, path) = q.popleft()
            cb((c,path))
            #print('node: {}. path: {}'.format(c, path))

            for neigh in self.graph[c]:
                if not neigh in visited:
                    visited.add(neigh)
                    npath = path[:]
                    npath.append(neigh)
                    q.append((neigh, npath ))

    def bfs(self, r, cb):
        q = deque()
        q.append(r)
        visited = {r}

        while len(q)>0:
            c = q.popleft()
            cb(c)
            for neigh in self.graph[c]:
                if not neigh in visited:
                    visited.add(neigh)
                    q.append(neigh)

    def dfs(self, r, cb):
        s = [(r, [r])]  #use list for stack. node, path tuple
        visited = {r}

        while len(s) > 0:
            c,path = s.pop()
            cb((c, path))
            for neigh in self.graph[c]:
                if not neigh in visited:
                    visited.add(neigh)
                    npath = path[:]
                    npath.append(neigh) 
                    s.append((neigh, npath))

    def dfsR(self, r, cb):
        visited = {r}
        self.dfsRInner(r, visited, cb)

    def dfsRInner(self, r, visited , cb):
        cb(r)
        for neigh in self.graph[r]:
            if not neigh in visited:
                visited.add(neigh)
                self.dfsRInner(neigh, visited, cb)


# practice section
# practice section
    def bfs1(self, r, cb):
        q = deque()
        visited = {r}
        q.append(r)
        while q:
            c = q.popleft()
            cb(c)
            for n in self.graph[c]:
                if not (n in visited):
                    visited.add(n)
                    q.append(n)


        


    def printme(self):
        nodes = list(self.graph.keys())

        print('non recursive bfs:')
        self.bfs(nodes[0], lambda x: print(x, ' '))
        print('non recursive bfs, plus path:')
        self.bfspath(nodes[0], lambda x: print(x, ' '))
        print('recursive dfs:')
        self.dfsR(nodes[0],  lambda x: print(x, ' '))
        print('non recursive dfs:')
        self.dfs(nodes[0],  lambda x: print(x, ' '))
        print('practice ---')
        print('simple bfs')
        self.bfs1(nodes[0],  lambda x: print(x, ' '))
        print('practice ---')

def test_graph():
    g = Graph()
    g.addEdge('a', 'b')
    g.addEdge('a', 'c')
    g.addEdge('a', 'd')
    g.addEdge('b', 'd')
    g.addEdge('b', 'e')
    g.addEdge('e', 'f')
    g.printme()

def shortest_path(mtx):
    #0-clear. 1 - blocked
    def neighbors(i,j):
        possible = [(1,0), (0,1), (-1,0), (0,-1)] 
        neighbors = []
        for l,m in possible:
            if (i+l>=0) and  (i+l <len(mtx)) and (j+m>=0) and  (j+m<len(mtx[0])):
                if mtx[i+l][j+m] == 0:
                    neighbors.append((i+l, j+m))
        print(neighbors)
        return neighbors
            
    if (len(mtx) == 0) or len(mtx[0]) ==0 or (mtx[0][0] == 1) or (mtx[len(mtx)-1][len(mtx[0])-1] == 1):
        return -1
    visited = {(0,0)}
    q = deque()
    pathLen =  0
    q.append(((0,0), pathLen))
    m = len(mtx)
    n = len(mtx[0])
    while q:
        c,pathLen = q.popleft()
        if c == (m-1, n-1):
            return pathLen
        else:
            for neighbor in neighbors(*c):
                if neighbor not in visited :
                    q.append((neighbor, pathLen+1)) 
                    visited.add(neighbor)
    return -1



def test_matrix():
    matrix = [
            [0,0,0,0],
            [0,1,0,1],
            [0,1,0,0],
            [1,0,0,0]
            ]
    pathLen = shortest_path(matrix)
    print("calculate shortest path len of matrix", matrix)
    print("path len", pathLen)

if __name__ == "__main__":
    test_graph()
    test_matrix()

		    9.3.3.2 deque vs list performance
Python Duque vs list performance

In [18]: s = list(range(10000)
    ...: )

In [19]: %timeit s.pop(); s.append(None)
154 ns ± 4 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)

In [20]: %timeit d.pop(); d.append(None)
145 ns ± 2.49 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)


https://stackoverflow.com/questions/23487307/python-deque-vs-list-performance-comparison#:~:text=That%20said%2C%20the%20real%20differences,realloc()%20under%20the%20hood.

Yes, your timing is dominated by the time to create the list or deque. The time to do the pop is insignificant in comparison.
Instead you should isolate the thing you're trying to test (the pop speed) from the setup time:
In [1]: from collections import deque

In [2]: s = list(range(1000))

In [3]: d = deque(s)

In [4]: s_append, s_pop = s.append, s.pop

In [5]: d_append, d_pop = d.append, d.pop

In [6]: %timeit s_pop(); s_append(None)
10000000 loops, best of 3: 115 ns per loop

In [7]: %timeit d_pop(); d_append(None)
10000000 loops, best of 3: 70.5 ns per loop
That said, the real differences between deques and list in terms of performance are:
* Deques have O(1) speed for appendleft() and popleft() while lists have O(n) performance for insert(0, value) and pop(0). 
* List append performance is hit and miss because it uses realloc() under the hood. As a result, it tends to have over-optimistic timings in simple code (because the realloc doesn't have to move data) and really slow timings in real code (because fragmentation forces realloc to move all the data). In contrast, deque append performance is consistent because it never reallocs and never moves data. 

		    9.3.3.3 more on deque from Chatgpt
izaqyos@me.com
breakdown python deque api

deque is a class in Python's built-in collections module that provides a double-ended queue data structure. Here's a breakdown of its API:

Creating a deque
deque([iterable[, maxlen]]): Creates a new deque instance. If an iterable is provided, its elements are added to the deque. The maxlen argument sets the maximum size of the deque. If the deque grows beyond this size, the oldest items are discarded.
Adding and removing elements
append(x): Add an element to the right end of the deque.
appendleft(x): Add an element to the left end of the deque.
extend(iterable): Extend the deque by appending all elements from the iterable to the right end.
extendleft(iterable): Extend the deque by appending all elements from the iterable to the left end. Note that the elements are added in reverse order, so they will appear in the deque in the opposite order to which they were appended.
pop(): Remove and return the rightmost element from the deque. Raises IndexError if the deque is empty.
popleft(): Remove and return the leftmost element from the deque. Raises IndexError if the deque is empty.
remove(value): Remove the first occurrence of value from the deque. Raises ValueError if value is not found.
Accessing elements
deque[index]: Access an element in the deque at the specified index. The index can be negative to count from the right end of the deque.
deque[-1]: Access the rightmost element in the deque.
deque[0]: Access the leftmost element in the deque.
Miscellaneous
len(deque): Return the number of elements in the deque.
deque.rotate(n=1): Rotate the deque n steps to the right. If n is negative, rotate to the left. Items that would be rotated off the end are appended to the other end.
deque.clear(): Remove all elements from the deque.
deque.copy(): Create a shallow copy of the deque.
deque.count(value): Return the number of occurrences of value in the deque.
deque.reverse(): Reverse the elements of the deque in place.



izaqyos@me.com
compare performance of python list vs deque when used as a stack data structure
list and deque are both commonly used as a stack data structure in Python, but they have different performance characteristics depending on the specific use case.

Generally, for small to medium-sized stacks, list tends to be faster than deque. This is because list is implemented as an array, which has constant-time access to elements by index, while deque is implemented as a doubly-linked list, which requires following pointers to access elements.

However, as the size of the stack grows, deque may become faster than list because deque is optimized for adding and removing elements from both ends of the queue, while list is optimized for random access. Specifically, deque has O(1) time complexity for adding and removing elements from both ends, while list has O(1) time complexity for adding and removing elements from the end, but O(n) time complexity for adding and removing elements from the beginning.

To demonstrate this performance difference, let's compare the time it takes to push and pop elements from a stack of varying sizes using list and deque:

from collections import deque
import timeit

def test_list_stack(n):
    stack = []
    for i in range(n):
        stack.append(i)
    for i in range(n):
        stack.pop()

def test_deque_stack(n):
    stack = deque()
    for i in range(n):
        stack.append(i)
    for i in range(n):
        stack.pop()

n = 1000000
print('list push/pop:', timeit.timeit(lambda: test_list_stack(n), number=100))
print('deque push/pop:', timeit.timeit(lambda: test_deque_stack(n), number=100))
This code creates a stack of n elements using list and deque, and then times how long it takes to push and pop all the elements from the stack 100 times. When I run this code on my machine, I get the following results:

list push/pop: 3.1149103
deque push/pop: 3.2102326
As expected, list is slightly faster than deque for a stack of 1 million elements. However, as the size of the stack grows, deque may become faster than list due to its optimized performance for adding and removing elements from both ends of the queue.

It's important to note that the actual performance of list and deque may vary depending on the specific use case, so it's always a good idea to test both implementations with realistic data and operations to see which one performs better for your specific scenario.




izaqyos@me.com
compare performance of python list vs deque when used as a queue data structure
When used as a queue data structure, deque is generally faster than list for both small and large queues. This is because deque is optimized for adding and removing elements from both ends of the queue, while list is optimized for random access.

Let's compare the performance of list and deque when used as a queue by adding and removing elements from both ends of the queue:

from collections import deque
import timeit

def test_list_queue(n):
    queue = []
    for i in range(n):
        queue.append(i)
    for i in range(n):
        queue.pop(0)
    for i in range(n):
        queue.insert(0, i)
    for i in range(n):
        queue.pop()

def test_deque_queue(n):
    queue = deque()
    for i in range(n):
        queue.append(i)
    for i in range(n):
        queue.popleft()
    for i in range(n):
        queue.appendleft(i)
    for i in range(n):
        queue.pop()

n = 100000
print('list queue:', timeit.timeit(lambda: test_list_queue(n), number=10))
print('deque queue:', timeit.timeit(lambda: test_deque_queue(n), number=10))
This code creates a queue of n elements using list and deque, and then times how long it takes to add and remove elements from both ends of the queue 10 times. When I run this code on my machine, I get the following results:

list queue: 52.372947
deque queue: 5.2245636
As we can see, deque is significantly faster than list for a queue of 100,000 elements. This is because deque is optimized for adding and removing elements from both ends of the queue, while list has O(n) time complexity for adding and removing elements from the beginning.

It's worth noting that the difference in performance between list and deque may vary depending on the specific use case, but in general, deque is the preferred choice when implementing a queue in Python.


		    9.3.3.4
		9.3.4
	9.4 Examples, Recipes

		9.4.1 TLV parser
import struct

def tlvs(data):
    '''TLVs parser generator'''
    while data:
        try:
            type, length = struct.unpack('!HH', data[:4])
            value = struct.unpack('!%is'%length, data[4:4+length])[0]
        except: 
            print "Unproper TLV structure found: ", (data,)
            break
        yield type, value
        data = data[4+length:]

########### example ####################################

# building network ordered data as TLVs list
a = struct.pack('!HHI', 1,4,2)     # first tlv
a+= struct.pack('!HHI', 3,4,4)     # second tlv
a+= struct.pack('!HHII', 5,8,6,6)  # third tlv
a+= struct.pack('!HH', 2,3)        # unproper tlv

# using TLV parser generator
for type, data in tlvs(a):
    print type, (data,) #trick for print binary data

'''Produced output:
1 ('\x00\x00\x00\x02',)
3 ('\x00\x00\x00\x04',)
5 ('\x00\x00\x00\x06\x00\x00\x00\x06',)
Unproper TLV structure found:  ('\x00\x02\x00\x03',)
'''


		9.4.2 PEZ RADIUS Request composing
from radcodes import RadCodes
import os
from ctypes import create_string_buffer
import struct
import hashlib
import re
import socket
import helpers
import decocred
import hmac
from hexdump import hexdump
import pacparser
from eapfasttlv import encode_pac_opaque
from eapfasthelper import T_PRF

radius_codes = RadCodes()

class RadRequestBuilder:
    def __init__(
            self,
            rad_dict,
            code,
            identifier,
            attributes,
            secret
        ):
        self.rad_dict = rad_dict
        self.code = radius_codes.get(code)
        self.identifier = identifier
        self.authenticator = os.urandom(16)
        self.attributes = attributes
        self.secret = secret 
        self.request = create_string_buffer(4096)
        self.length = 0
        self.offset = 0
        
    
        match = re.match('^(.*)=(.*)$', secret)
        if match and match.group(1) == 'cts-pac-opaque':
            pac_file_name = match.group(2)
            pac_file_name = decocred.decorate(pac_file_name)
            pac = open(pac_file_name, 'rb').read()
            pac_attributes = pacparser.parse_pac(pac)
            
            pac_key = pac_attributes['PAC-Key'][0].value
            iid = pac_attributes['PAC-Info'][0].attributes['I-ID'][0].value
            self.secret = T_PRF(pac_key, None, iid, 20)

    def build(self):
        if self.code == radius_codes.get('Accounting-Request'):
            self.authenticator = '\0' * 16
        struct.pack_into('BB', self.request, 0, self.code, self.identifier)
        struct.pack_into('16s', self.request, 4, self.authenticator)
        self.add_attributes()
        has_message_authenticator = 'Message-Authenticator' in self.attributes

        if has_message_authenticator:
            self.add_attribute('Message-Authenticator', ['\0' * 16])
            
        self.length = self.offset
        struct.pack_into('!H', self.request, 2, self.length)
        if has_message_authenticator:
            message_authenticator = hmac.new(self.secret, self.get_result()).digest()
            self.offset -= 18
            self.add_attribute('Message-Authenticator', [message_authenticator])

        if self.code == radius_codes.get('Accounting-Request'):
            d = hashlib.md5()
            d.update(self.get_result())
            d.update(self.secret)
            self.authenticator = d.digest()
            struct.pack_into('16s', self.request, 4, self.authenticator)
            
    
    def get_result(self):
        return self.request.raw[:self.length]

    def add_encrypted_attribute(self, attribute, attribute_info):
        if attribute_info.attribute_name == 'User-Password':
            attribute = decocred.decorate(attribute)

        attribute_length = len(attribute)
        assert attribute_length <= 128, 'attribute %s is too long for encryption' % attribute_info.attribute_name
        S = self.secret
        RA = self.authenticator
        d = hashlib.md5()
        d.update(self.secret)
        d.update(self.authenticator)
        bi = d.digest()
        padded = '%s%s' % (attribute, (16 - attribute_length % 16) * '\0')
        encrypted_list = []
        for pi in helpers.chunkify(16, padded):
            ci_list = []
            for i in xrange(16):
                ci_list.append(chr(ord(pi[i]) ^ ord(bi[i])))
            ci = ''.join(ci_list)
            encrypted_list.append(ci)
            
            d = hashlib.md5()
            d.update(S)
            d.update(ci)
            bi = d.digest()
        
        encrypted = ''.join(encrypted_list)
        encrypted_len = len(encrypted)
        struct.pack_into(
            '!BB%ds' % encrypted_len,
            self.request,
            self.offset,
            attribute_info.attribute_number,
            encrypted_len + 2,
            encrypted)
            
        self.offset = self.offset + encrypted_len + 2
        
    def add_octet_attribute(self, attribute, attribute_info):
        if attribute_info.attribute_name == 'User-Name':
            attribute = decocred.decorate(attribute)

        if attribute_info.attribute_name == 'Calling-Station-Id':
            attribute = decocred.hdecorate(attribute)

        attribute_length = len(attribute) + 2
        assert attribute_length <= 255, 'attribute %s is too long' % attribute_info.attribute_name
        if attribute_info.has_tag:
            tagged = re.match('^\[tag:([0-9]{1,2})](.*)$', attribute)
            if tagged:
                tag = int(tagged.groups()[0])
                attribute = tagged.groups()[1].strip()
                attribute_length = len(attribute) + 3
            struct.pack_into(
                    'BBB%ds' % (attribute_length - 3), 
                    self.request, 
                    self.offset, 
                    attribute_info.attribute_number, 
                    attribute_length,
                    tag,
                    attribute)

        else:
            struct.pack_into(
                    'BB%ds' % (attribute_length - 2), 
                    self.request, 
                    self.offset, 
                    attribute_info.attribute_number, 
                    attribute_length, 
                    attribute)
        self.offset = self.offset + attribute_length

    def add_int_attribute(self, attribute, attribute_info):
        if isinstance(attribute, str):
            tag = 0
            if attribute_info.has_tag:
                tagged = re.match('^\[tag:([0-9]{1,2})](.*)$', attribute)
                if tagged:
                    tag = int(tagged.groups()[0])
                    attribute = tagged.groups()[1].strip()
            if attribute_info.integer_translations:
                attribute = attribute_info.integer_translations[attribute]
            else:
                attribute = int(attribute)
            attribute = attribute | tag << 24
            
        struct.pack_into(
            '!BBi',
            self.request,
            self.offset,
            attribute_info.attribute_number,
            6,
            attribute)
        self.offset = self.offset + 6
        
    def add_attribute(self, attribute_name, attribute_list):
        attribute_info = self.rad_dict.get_attribute_info(attribute_name)
        attribute_list_length = 0
        for attribute in attribute_list:
            if attribute_info.encrypt == True:
                self.add_encrypted_attribute(attribute, attribute_info)
            elif attribute_info.attribute_type == 'octets':
                self.add_octet_attribute(attribute, attribute_info)
            elif attribute_info.attribute_type == 'string':
                self.add_octet_attribute(attribute, attribute_info)
            elif attribute_info.attribute_type == 'integer':
                self.add_int_attribute(attribute, attribute_info)
            elif attribute_info.attribute_type == 'ipaddr':
                attribute = socket.inet_aton(attribute)
                self.add_octet_attribute(attribute, attribute_info)
            else:
                raise Exception(
                        'unknown attribute type [%s] for attribute [%s]' % 
                        (attribute_info.attribute_type, attribute_name))

    def add_vsa_int_attribute(self, attribute, attribute_info, vendor_id):
        attribute_length = 6
        vendor_length = 12
        struct.pack_into(
                '!BBiBBi',
                self.request, 
                self.offset,
                26,
                vendor_length,
                vendor_id,
                attribute_info.attribute_number, 
                attribute_length, 
                attribute)
        self.offset = self.offset + vendor_length

    def add_vsa_string_attribute(self, attribute, attribute_info, vendor_id):
        if attribute_info.attribute_name == 'Cisco-AVPair':
            match = re.match('^(.*)=(.*)$', attribute)
            if match and match.group(1) == 'cts-pac-opaque':
                pac_file_name = match.group(2)
                pac_file_name = decocred.decorate(pac_file_name)
                pac = open(pac_file_name, 'rb').read()
                pac_attributes = pacparser.parse_pac(pac)
                pac_opaque = encode_pac_opaque(pac_attributes['PAC-Opaque'][0].value)
                attribute = 'cts-pac-opaque=%s' % pac_opaque
            

        attribute_length = len(attribute) + 2
        vendor_length = attribute_length + 6
        assert vendor_length <= 255, 'attribute %s is too long' % attribute_info.attribute_name
        struct.pack_into(
                '!BBiBB%ds' % (attribute_length - 2),
                self.request, 
                self.offset,
                26,
                vendor_length,
                vendor_id,
                attribute_info.attribute_number, 
                attribute_length, 
                attribute)
        self.offset = self.offset + vendor_length
    
    def add_vsa_attribute_for_vendor(self, vendor):
        vsa_length = 0
        vendor_info = self.rad_dict.get_vendor(vendor)
        vendor_id = vendor_info.vendor_id
        for attributes in self.attributes['VSA'][vendor].iteritems():
            attribute_name = attributes[0]
            attribute_list = attributes[1]
            attribute_info = self.rad_dict.get_vendor_attribute_info(
                vendor_id,
                attribute_name)
    
            for attribute in attribute_list:
                if attribute_info.encrypt == True:
                    raise Exception("Encrypted vendor attributes not implemented")
                elif attribute_info.attribute_type == 'integer':
                    self.add_vsa_int_attribute(attribute, attribute_info, vendor_id)
                elif attribute_info.attribute_type == 'string':
                    self.add_vsa_string_attribute(attribute, attribute_info, vendor_id)
                elif attribute_info.attribute_type == 'octets':
                    self.add_vsa_string_attribute(attribute, attribute_info, vendor_id)
                else:
                    raise Exception(
                            'unknown attribute type [%s] for attribute [%s]' % 
                            (attribute_info.attribute_type, attribute_name))
        self.offset = self.offset + vsa_length
    
    def add_vsa_attributes(self):
        for vendor in self.attributes['VSA']:
            self.add_vsa_attribute_for_vendor(vendor)
        
    def add_attributes(self):
        self.offset = 20
        for attributes in self.attributes.iteritems():
            attribute_name = attributes[0]
            attribute_list = attributes[1]
            if attribute_name == 'VSA':
                self.add_vsa_attributes()
            else:
                if attribute_name != 'Message-Authenticator':
                    self.add_attribute(attribute_name, attribute_list)


if __name__ == '__main__':
    pass

		9.4.3 Copy hex stream of packet from wireshark and parsing, verifying it
So, example of RADIUS reject packet:
>>> print packet
0301002655bfaa06b26a419f9239b7c7601e2ff55012897ddbaac04781074557e17e44772e1c

Convert it to hexa buffer:
>>> fake_msg = ""
>>> for i,c in enumerate(packet):
...     if (i%2 == 0):
...             fake_msg += struct.pack('!B',int(packet[i:i+2],16))
... 
>>> fake_msg.encode("hex")
'0301002655bfaa06b26a419f9239b7c7601e2ff55012897ddbaac04781074557e17e44772e1c'


Now use struct.unpack to verify various fields


		9.4.4 RADIUS CoA server

import socket
import struct
import md5

# CHANGE THESE PARAMETERS TO CONFIGURE THINGS THE WAY YOU NEED IT
# THE HOST IS THE IP OF _THIS_ SERVER...NOW THAT I THINK OF IT, I SHOULDA' USED ANY...
# RESPONSE TYPE IS THE PACKET TYPE FOR THE RESPONSE. 41 IS DISCONNECT ACK. CHANGE THIS IF YOU WANT
# A DIFFERENT KING OF RESPONSE
host = '10.56.232.154'
secret = 'faham'
responseType = 41

#THESE ARE THE ATTRIBUTES FOR THE RESPONSE. IF YOU NEED TO ADD ONE, DO IT LIKE THE COMMENTED OUT EXAMPLE
#THERE IS NO NEED TO ADD MESSAGE-AUTHENTICATOR, THAT GETS ADDED AUTOMATICALLY
d = dict()
#d['101'] = struct.pack('>I', 504)

port = 1700
buf = 1024

def xor(s1, s2):
	res = ''
	for i in range(0, len(s1)):
		res = res + chr( ord(s1[i]) ^ ord(s2[i]))
	return res

def insert_msg_auth(pack):
    md5BlockLen = 64
    md5OutLen = 16
    ipad = '\x36' * md5BlockLen
    opad = '\x5c' * md5BlockLen
    key = secret

    key = key + ('\x00' * (md5BlockLen - len(key)))

    innerHash = md5.new(xor(key, ipad) + pack).digest()
    msgAuth = md5.new(xor(key, opad) + innerHash).digest()

    return pack[0:-16] + struct.pack('>16s', msgAuth)

def build_zeroed_response(id, code, reqAuthenticator, attrs):
    body = ''
    for k, v in attrs.items():
        body += struct.pack('>BB%is' % len(v), int(k), len(v) + 2, v)
    body += struct.pack('>BB16s', 80, 18, 16 * chr(0))

    header = struct.pack('>BBH16s', code, id, len(body) + 20, reqAuthenticator) 
    return header + body

    

sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
sock.bind((host, port))


data, addr = sock.recvfrom(buf)

print type(data)

(code, id, length, authenticator) = struct.unpack('>BBH16s', data[0:20])
print code
print id
print length
for c in authenticator:
    print hex(ord(c)), 


resp = build_zeroed_response(id, responseType, authenticator, d)
resp = insert_msg_auth(resp)
respAuthenticator = md5.new(resp + secret).digest()
resp = resp[0:4] + respAuthenticator + resp[20:]
sock.sendto(resp, addr)

sock.close()

		9.4.5
	9.5 libs

		9.5.1 dpkt

		9.5.2

	9.6
10. Python CLI
	10.1 CMD module, http://effbot.org/librarybook/cmd.htm
	Example:
# File: cmd-example-1.py

import cmd
import string, sys

class CLI(cmd.Cmd):

    def __init__(self):
        cmd.Cmd.__init__(self)
        self.prompt = '> '

    def do_hello(self, arg):
        print "hello again", arg, "!"

    def help_hello(self):
        print "syntax: hello [message]",
        print "-- prints a hello message"

    def do_quit(self, arg):
        sys.exit(1)

    def help_quit(self):
        print "syntax: quit",
        print "-- terminates the application"

    # shortcuts
    do_q = do_quit

#
# try it out

cli = CLI()
cli.cmdloop()

	output:
$ python cmd-example-1.py
> help

Documented commands (type help <topic>):
========================================
hello           quit

Undocumented commands:
======================
help            q

> hello world
hello again world !
> q

11. Doc strings,
		for each method or class you can wrap a string with """ string """ to make a doc string.
		It has to be the first thing in the class/function and can be accessed in run time and used for 
		document creating utilities and IDEs.
		To access method xxx doc string in run time do:
		xxx.__doc__

12. Logging, tags: Logging


    12.1 Chatgpt: describe common ways to add logging capabilities to python code, tags Chatgpt: describe common ways to add logging capabilities to python code

Adding logging capabilities to Python code is crucial for monitoring and debugging applications. Here are some common ways to implement logging in Python:

|1. **Using the `logging` Module:**
|   Python's built-in `logging` module provides a flexible and powerful framework for adding logging to your code. It supports different log levels, log formatting, and output destinations. Basic usage involves importing the module, configuring the logger, and using logging functions to emit log messages.
|
|   ```python
|   import logging
|
|   # Configure the logger
|   logging.basicConfig(level=logging.DEBUG, filename='app.log', format='%(asctime)s - %(levelname)s - %(message)s')
|
|   # Emit log messages
|   logging.debug('Debug message')
|   logging.info('Info message')
|   logging.warning('Warning message')
|   logging.error('Error message')
|   logging.critical('Critical message')
|   ```
|
|2. **Configuring Logging via Configuration Files:**
|   The `logging` module allows you to configure logging using configuration files (e.g., JSON or INI format). This provides more flexibility and avoids hardcoding configuration in your code. You can use the `fileConfig` or `dictConfig` functions to load the configuration from a file.
|
|   ```python
|   import logging.config
|
|   logging.config.fileConfig('logging_config.ini')
|   logger = logging.getLogger('my_logger')
|   ```
|
|3. **Customizing Log Handlers:**
|   You can customize logging behavior by adding different handlers to the logger. Handlers define where log messages are output. Common handlers include `StreamHandler` (outputs to console), `FileHandler` (outputs to a file), and `RotatingFileHandler` (rotates log files when they reach a certain size).
|
|   ```python
|   import logging
|
|   logger = logging.getLogger('my_logger')
|   handler = logging.StreamHandler()  # or other handler types
|   logger.addHandler(handler)
|   ```
|
|4. **Logging to Different Destinations:**
|   You can configure log messages to be sent to various destinations, such as the console, files, and remote servers. This is useful for separating application logs from error logs or sending logs to a centralized logging system.
|
|5. **Custom Log Formats:**
|   The `logging` module allows you to define custom log formats using placeholders. This lets you include various pieces of information in your log messages, such as timestamp, log level, module name, etc.
|
|6. **Logging Exceptions and Stack Traces:**
|   You can log exceptions and stack traces using the `exception` method of a logger. This is helpful for capturing detailed error information.
|
|   ```python
|   try:
|       # Code that may raise an exception
|   except Exception as e:
|       logger.error('An error occurred: %s', e, exc_info=True)
|   ```
|
|7. **Logging in Libraries and Modules:**
|   When developing libraries or modules, it's a good practice to use the `logging` module for logging instead of printing to the console. This allows consumers of your library to configure logging according to their needs.
|
|8. **Third-Party Logging Libraries:**
   Besides the built-in `logging` module, there are third-party logging libraries like `loguru` and `structlog` that offer additional features and customization options.

Remember to carefully choose log levels, capture relevant information, and ensure that log messages are meaningful and informative for effective debugging and monitoring.


    12.2 The logging package, tags: The logging package
    provides a lot of flexibility, and its configuration can appear daunting. This section demonstrates that simple use of the logging package is possible.

The simplest example shows logging to the console:

import logging

logging.debug('A debug message')
logging.info('Some information')
logging.warning('A shot across the bows')

If you run the above script, you'll see this:

WARNING:root:A shot across the bows

Because no particular logger was specified, the system used the root logger. The debug and info messages didn't appear because by default, the root logger is configured to only handle messages with a severity of WARNING or above. The message format is also a configuration default, as is the output destination of the messages - sys.stderr. The severity level, the message format and destination can be easily changed, as shown in the example below:

import logging

logging.basicConfig(level=logging.DEBUG,
                    format='%(asctime)s %(levelname)s %(message)s',
                    filename='/tmp/myapp.log',
                    filemode='w')
logging.debug('A debug message')
logging.info('Some information')
logging.warning('A shot across the bows')

The basicConfig() method is used to change the configuration defaults, which results in output (written to /tmp/myapp.log) which should look something like the following:

2004-07-02 13:00:08,743 DEBUG A debug message
2004-07-02 13:00:08,743 INFO Some information
2004-07-02 13:00:08,743 WARNING A shot across the bows

This time, all messages with a severity of DEBUG or above were handled, and the format of the messages was also changed, and output went to the specified file rather than the console.

Formatting uses standard Python string formatting - see section 3.6.2. The format string takes the following common specifiers. For a complete list of specifiers, consult the Formatter documentation.

13. Debug Python
	13.1 PDB

	The Python Debugger

The module pdb defines an interactive source code debugger for Python programs. It supports setting (conditional) breakpoints and single stepping at the source line level, inspection of stack frames, source code listing, and evaluation of arbitrary Python code in the context of any stack frame. It also supports post-mortem debugging and can be called under program control.

The debugger is extensible -- it is actually defined as the class Pdb. This is currently undocumented but easily understood by reading the source. The extension interface uses the modules bdb (undocumented) and cmd.

The debugger's prompt is "(Pdb) ". Typical usage to run a program under control of the debugger is:

>>> import pdb
>>> import mymodule
>>> pdb.run('mymodule.test()')
> <string>(0)?()
(Pdb) continue
> <string>(1)?()
(Pdb) continue
NameError: 'spam'
> <string>(1)?()
(Pdb)

pdb.py can also be invoked as a script to debug other scripts. For example:

python -m pdb myscript.py

When invoked as a script, pdb will automatically enter post-mortem debugging if the program being debugged exits abnormally. After post-mortem debugging (or after normal exit of the program), pdb will restart the program. Automatic restarting preserves pdb's state (such as breakpoints) and in most cases is more useful than quitting the debugger upon program's exit. New in version 2.4: Restarting post-mortem behavior added.

Typical usage to inspect a crashed program is:

>>> import pdb
>>> import mymodule
>>> mymodule.test()
Traceback (most recent call last):
  File "<stdin>", line 1, in ?
  File "./mymodule.py", line 4, in test
    test2()
  File "./mymodule.py", line 3, in test2
    print spam
NameError: spam
>>> pdb.pm()
> ./mymodule.py(3)test2()
-> print spam
(Pdb)


	MORE ON PDB:

    * For information on the IDLE interactive debugger, go to http://www.python.org/idle/doc/idle2.html#Debugger
    * For information on the Wing IDE debugger, go to http://wingide.com/psupport/wingide-1.1/node7.html

Getting started -- pdb.set_trace()

To start, I'll show you the very simplest way to use the Python debugger.

   13.1.1. Let's start with a simple program, epdb1.py.

              # epdb1.py -- experiment with the Python debugger, pdb
              a = "aaa"
              b = "bbb"
              c = "ccc"
              final = a + b + c
              print final
              

   13.1.2. Insert the following statement at the beginning of your Python program. This statement imports the Python debugger module, pdb.

          import pdb

   13.1.3. Now find a spot where you would like tracing to begin, and insert the following code:

          pdb.set_trace()

      So now your program looks like this. (Click HERE for the source code.)

              # epdb1.py -- experiment with the Python debugger, pdb
              import pdb
              a = "aaa"
              pdb.set_trace()
              b = "bbb"
              c = "ccc"
              final = a + b + c
              print final
              

   13.1.4. Now run your program from the command line as you usually do, which will probably look something like this:

              PROMPT> python epdb1.py

When your program encounters the line with pdb.set_trace() it will start tracing. That is, it will (1) stop, (2) display the "current statement" (that is, the line that will execute next) and (3) wait for your input. You will see the pdb prompt, which looks like this:

           (Pdb)

Execute the next statement... with "n" (next)

At the (Pdb) prompt, press the lower-case letter "n" (for "next") on your keyboard, and then press the ENTER key. This will tell pdb to execute the current statement. Keep doing this -- pressing "n", then ENTER.

Eventually you will cometo the end of your program, and it will terminate and return you to the normal command prompt.

Congratulations! You've just done your first debugging run!

Repeating the last debugging command... with ENTER

This time, do the same thing as you did before. Start your program running. At the (Pdb) prompt, press the lower-case letter "n" (for "next") on your keyboard, and then press the ENTER key.

But this time, after the first time that you press "n" and then ENTER, don't do it any more. Instead, when you see the (Pdb) prompt, just press ENTER. You will notice that pdb continues, just as if you had pressed "n".   So this is Handy Tip #1:

    If you press ENTER without entering anything, pdb will re-execute the last command that you gave it. 

In this case, the command was "n", so you could just keep stepping through the program by pressing ENTER.

Notice that as you passed the last line (the line with the "print" statement), it was executed and you saw the output of the print statement ("aaabbbccc") displayed on your screen.

Quitting it all... with "q" (quit)

The debugger can do all sorts of things, some of which you may find totally mystifying. So the most important thing to learn now -- before you learn anything else -- is how to quit debugging!

It is easy. When you see the (Pdb) prompt, just press "q" (for "quit") and the ENTER key. Pdb will quit and you will be back at your command prompt. Try it, and see how it works.

Printing the value of variables... with "p" (print)

The most useful thing you can do at the (Pdb) prompt is to print the value of a variable. Here's how to do it.

When you see the (Pdb) prompt, enter "p" (for "print") followed by the name of the variable you want to print. And of course, you end by pressing the ENTER key.

Note that you can print multiple variables, by separating their names with commas (just as in a regular Python "print" statement). For example, you can print the value of the variables a, b, and c this way:

          p a, b, c

When does pdb display a line?

Suppose you have progressed through the program until you see the line

          final = a + b + c

and you give pdb the command

          p final

You will get a NameError exception. This is because, although you are seeing the line, it has not yet executed. So the final variable has not yet been created.

Now press "n" and ENTER to continue and execute the line. Then try the "p final" command again. This time, when you give the command "p final", pdb will print the value of final, which is "aaabbbccc".

Turning off the (Pdb) prompt... with "c" (continue)

You probably noticed that the "q" command got you out of pdb in a very crude way -- basically, by crashing the program.

If you wish simply to stop debugging, but to let the program continue running, then you want to use the "c" (for "continue") command at the (Pdb) prompt. This will cause your program to continue running normally, without pausing for debugging. It may run to completion. Or, if the pdb.set_trace() statement was inside a loop, you may encounter it again, and the (Pdb) debugging prompt will appear once more.

Seeing where you are... with "l" (list)

As you are debugging, there is a lot of stuff being written to the screen, and it gets really hard to get a feeling for where you are in your program. That's where the "l" (for "list") command comes in. (Note that it is a lower-case "L", not the numeral "one" or the capital letter "I".)

"l" shows you, on the screen, the general area of your program's souce code that you are executing. By default, it lists 11 (eleven) lines of code. The line of code that you are about to execute (the "current line") is right in the middle, and there is a little arrow "-->" that points to it.

So a typical interaction with pdb might go like this

    * The pdb.set_trace() statement is encountered, and you start tracing with the (Pdb) prompt
    * You press "n" and then ENTER, to start stepping through your code.
    * You just press ENTER to step again.
    * You just press ENTER to step again.
    * You just press ENTER to step again. etc. etc. etc.
    * Eventually, you realize that you are a bit lost. You're not exactly sure where you are in your program any more. So...

    * You press "l" and then ENTER. This lists the area of your program that is currently being executed.
    * You inspect the display, get your bearings, and are ready to start again. So....
    * You press "n" and then ENTER, to start stepping through your code.
    * You just press ENTER to step again.
    * You just press ENTER to step again. etc. etc. ec.

Stepping into subroutines... with "s" (step into)

Eventually, you will need to debug larger programs -- programs that use subroutines. And sometimes, the problem that you're trying to find will lie buried in a subroutine. Consider the following program. (Click HERE for the source code.)

        # epdb2.py -- experiment with the Python debugger, pdb
        import pdb

        def combine(s1,s2):      # define subroutine combine, which...
            s3 = s1 + s2 + s1    # sandwiches s2 between copies of s1, ...
            s3 = '"' + s3 +'"'   # encloses it in double quotes,...
            return s3            # and returns it.

        a = "aaa"
        pdb.set_trace()
        b = "bbb"
        c = "ccc"
        final = combine(a,b)
        print final
        

As you move through your programs by using the "n" command at the (Pdb) prompt, you will find that when you encounter a statement that invokes a subroutine -- the final = combine(a,b) statement, for example -- pdb treats it no differently than any other statement. That is, the statement is executed and you move on to the next statement -- in this case, to print final.

But suppose you suspect that there is a problem in a subroutine. In our case, suppose you suspect that there is a problem in the combine subroutine. What you want -- when you encounter the final = combine(a,b) statement -- is some way to step into the combine subroutine, and to continue your debugging inside it.

Well, you can do that too. Do it with the "s" (for "step into") command.

When you execute statements that do not involve function calls, "n" and "s" do the same thing -- move on to the next statement. But when you execute statements that invoke functions, "s", unlike "n", will step into the subroutine. In our case, if you executed the final = combine(a,b) statement using "s", then next statement that pdb would show you would be the first statement in the combine subroutine:

               def combine(s1,s2): 

and you will continue debugging from there.

Continuing... but just to the end of the current subroutine... with "r" (return)

When you use "s" to step into subroutines, you will often find yourself trapped in a subroutine. You have examined the code that you're interested in, but now you have to step through a lot of uninteresting code in the subroutine.

In this situation, what you'd like to be able to do is just to skip ahead to the end of the subroutine. That is, you want to do something like the "c" ("continue") command does, but you want just to continue to the end of the subroutine, and then resume your stepping through the code.

You can do it. The command to do it is "r" (for "return" or, better, "continue until return"). If you are in a subroutine and you enter the "r" command at the (Pdb) prompt, pdb will continue executing until the end of the subroutine. At that point -- the point when it is ready to return to the calling routine -- it will stop and show the (Pdb) prompt again, and you can resume stepping through your code.

You can do anything at all at the (Pdb) prompt ...

Sometimes you will be in the following situation -- You think you've discovered the problem. The statement that was assigning a value of, say, "aaa" to variable var1 was wrong, and was causing your program to blow up. It should have been assigning the value "bbb" to var1.

... at least, you're pretty sure that was the problem...

What you'd really like to be able to do, now that you've located the problem, is to assign "bbb" to var1, and see if your program now runs to completion without bombing.

It can be done!

One of the nice things about the (Pdb) prompt is that you can do anything at it -- you can enter any command that you like at the (Pdb) prompt. So you can, for instance, enter this command at the (Pdb) prompt.

               (Pdb) var1 = "bbb"

You can then continue to step through the program. Or you could be adventurous -- use "c" to turn off debugging, and see if your program will end without bombing!

... but be a little careful! 1

Since you can do anything at all at the (Pdb) prompt, you might decide to try setting the variable b to a new value, say "BBB", this way:

               (Pdb) b = "BBB"

If you do, pdb produces a strange error message about being unable to find an object named '= "BBB" '. Why???

What happens is that pdb attempts to execute the pdb b command for setting and listing breakpoints (a command that we haven't discussed). It interprets the rest of the line as an argument to the b command, and can't find the object that (it thinks) is being referred to. So it produces an error message.

So how can we assign a new value to b? The trick is to start the command with an exclamation point (!).

               (Pdb)!b = "BBB"

An exclamation point tells pdb that what follows is a Python statement, not a pdb command.

	13.2

14. Reading, Handling of command line arguments, getopt: options, arguments, file(s) content iterator

	14.1 Example from http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/302262

# -*- coding: iso-8859-1 -*-
"""
Handling of arguments: options, arguments, file(s) content iterator

For small scripts that:
- read some command line options
- read some command line positional arguments
- iterate over all lines of some files given on the command line, or stdin if none given
- give usage message if positional arguments are missing
- give usage message if input files are missing and stdin is not redirected
"""

__author__ = 'Peter Kleiweg'
__version__ = '0.2'
__date__ = '2004/08/28'

import os, sys, getopt

class Args:
    """
    Perform common tasks on command line arguments
    
    Instance data:
    progname (string) -- name of program
    opt (dictionary) -- options with values
    infile (string) -- name of current file being processed
    lineno (int) -- line number of last line read in current file
    linesum (int) -- total of lines read
    """

    def __init__(self, usage='Usage: %(progname)s [opt...] [file...]'):
        "init, usage string: embed program name as %(progname)s"
        self.progname = os.path.basename(sys.argv[0])
        self.opt = {}
        self.infile = None
        self.lineno = 0
        self.linesum = 0
        self._argv = sys.argv[1:]
        self._usage = usage

    def __iter__(self):
        "iterator: set-up"
        if self._argv:
            self.infile = self._argv.pop(0)
            self._in = open(self.infile, 'r')
            self._stdin = False
        else:
            if sys.stdin.isatty():
                self.usage()  # Doesn't return
            self.infile = '<stdin>'
            self._in = sys.stdin
            self._stdin = True
        return self

    def next(self):
        "iterator: get next line, possibly from next file"
        while True:
            line = self._in.readline()
            if line:
                self.lineno += 1
                self.linesum += 1
                return line

            if self._stdin:
                break

            self._in.close()
            try:
                self.infile = self._argv.pop(0)
            except IndexError:
                break
            self.lineno = 0
            self._in = open(self.infile, 'r')

        self.lineno = -1
        self.infile = None
        raise StopIteration

    def getopt(self, shortopts, longopts=[]):
        "get options and merge into dict 'opt'"
        try:
            options, self._argv = getopt.getopt(self._argv, shortopts, longopts)
        except getopt.GetoptError:
            self.usage()
        self.opt.update(dict(options))

    def shift(self):
        "pop first of remaining arguments (shift)"
        try:
            return self._argv.pop(0)
        except IndexError:
            self.usage()

    def pop(self):
        "pop last of remaining arguments"
        try:
            return self._argv.pop()
        except IndexError:
            self.usage()

    def warning(self, text):
        "print warning message to stderr, possibly with filename and lineno"
        if self.lineno > 0:
            print >> sys.stderr, '%s:%i: warning: %s' % (self.infile, self.lineno, text)
        else:
            print >> sys.stderr, '\nWarning %s: %s\n' % (self.progname, text)

    def error(self, text):
        "print error message to stderr, possibly with filename and lineno, and exit"
        if self.lineno > 0:
            print >> sys.stderr, '%s:%i: %s' % (self.infile, self.lineno, text)
        else:
            print >> sys.stderr, '\nError %s: %s\n' % (self.progname, text)
        sys.exit(1)

    def usage(self):
        "print usage message, and exit"
        print >> sys.stderr
        print >> sys.stderr, self._usage % {'progname': self.progname}
        print >> sys.stderr        
        sys.exit(1)


if __name__ == '__main__':

    a = Args('Usage: %(progname)s [-a value] [-b value] [-c] word [file...]')

    a.opt['-a'] = 'option a'    # set some default option values
    a.opt['-b'] = 'option b'    #
    a.getopt('a:b:c')           # get user supplied option values

    word = a.shift()            # get the first of the remaining arguments
                                # use a.pop() to get the last instead

    for line in a:              # iterate over the contents of all remaining arguments (file names)
        if a.lineno == 1:
            print 'starting new file:', a.infile
        a.warning(line.rstrip())

    print 'Options:', a.opt
    print 'Word:', word
    print 'Total number of lines:', a.linesum

    print 'Command line:', sys.argv     # unchanged

    a.warning('warn 1')         # print a warning
    a.error('error')            # print an error message and exit
    a.warning('warn 2')         # this won't show

    14.2 Example from http://docs.python.org/lib/module-optparse.html
    optparse is a more convenient, flexible, and powerful library for parsing command-line options than getopt. optparse uses a more declarative style of command-line parsing: you create an instance of OptionParser, populate it with options, and parse the command line. optparse allows users to specify options in the conventional GNU/POSIX syntax, and additionally generates usage and help messages for you.

Here's an example of using optparse in a simple script:

from optparse import OptionParser
[...]
parser = OptionParser()
parser.add_option("-f", "--file", dest="filename",
                  help="write report to FILE", metavar="FILE")
parser.add_option("-q", "--quiet",
                  action="store_false", dest="verbose", default=True,
                  help="don't print status messages to stdout")

(options, args) = parser.parse_args()

With these few lines of code, users of your script can now do the ``usual thing'' on the command-line, for example:

<yourscript> --file=outfile -q

As it parses the command line, optparse sets attributes of the options object returned by parse_args() based on user-supplied command-line values. When parse_args() returns from parsing this command line, options.filename will be "outfile" and options.verbose will be False. optparse supports both long and short options, allows short options to be merged together, and allows options to be associated with their arguments in a variety of ways. Thus, the following command lines are all equivalent to the above example:

<yourscript> -f outfile --quiet
<yourscript> --quiet --file outfile
<yourscript> -q -foutfile
<yourscript> -qfoutfile

Additionally, users can run one of

<yourscript> -h
<yourscript> --help

and optparse will print out a brief summary of your script's options:

usage: <yourscript> [options]

options:
  -h, --help            show this help message and exit
  -f FILE, --file=FILE  write report to FILE
  -q, --quiet           don't print status messages to stdout

where the value of yourscript is determined at runtime (normally from sys.argv[0]).

	14.3 Example from http://wiki.python.org/moin/OptParse
	if __name__=="__main__":
    parser = optparse.OptionParser("usage: %prog [options] arg1 arg2")
    parser.add_option("-H", "--host", dest="hostname",
                      default="127.0.0.1", type="string",
                      help="specify hostname to run on")
    parser.add_option("-p", "--port", dest="portnum", default=80,
                      type="int", help="port number to run on")

    (options, args) = parser.parse_args()
    if len(args) != 2:
        parser.error("incorrect number of arguments")
    hostname = options.hostname
    portnum = options.portnum

args contains your fixed arguments, options contains your values.

For example, options.portnum would contain the integer 80, in the example above.

	14.4 argparse

		14.4.1 https://blog.oyam.dev/python-argparse-cheatsheet/ 

		    14.4.1.1   Basic argparse usage
[I500695@C02ZR8BSMD6N:2021-12-30 13:26:31:~/work/code/python/argparse:]2014$ cat basic.py 
import argparse

parser = argparse.ArgumentParser(
    description='Performs some useful work.',
)

# Put your add_argument calls here
# parser.add_argument(...)

args = parser.parse_args()
print(args)
#print(args.some_param)
[I500695@C02ZR8BSMD6N:2021-12-30 13:26:33:~/work/code/python/argparse:]2015$ py3 basic.py a
usage: basic.py [-h]
basic.py: error: unrecognized arguments: a
[I500695@C02ZR8BSMD6N:2021-12-30 13:26:44:~/work/code/python/argparse:]2016$ py3 basic.py 
Namespace()
		    14.4.1.2  cmd arg - a single positional argument

		    14.4.1.3  
		    14.4.1.4  
		    14.4.1.5  
		    14.4.1.6  
		    14.4.1.7  
		    14.4.1.8  
		    14.4.1.9  
		    14.4.1.10 



		14.4.2 https://docs.python.org/3/howto/argparse.html

		    14.4.2.1 positional arguments
[I500695@C02ZR8BSMD6N:2021-12-30 16:10:10:~/work/code/python/argparse:]2010$ cat !!:1
cat singlePosArg.py
import argparse

parser = argparse.ArgumentParser(
    description='Performs some useful work.',
)

parser.add_argument(
    'name',
    type=str,
    default='yosi',
    nargs='?',
    help='name, a positional argument',
    #metavar='ARG',
)
parser.add_argument(
    'age',
    type=int,
    default='35',
    nargs='?',
    help='age, a positional argument',
)

#args = parser.parse_args([])  # Namespace(arg='arg_default')
#print(f"args from default Namespace: {args}")
#args = parser.parse_args(['name'])  # Namespace(arg='value')
#print(f"args from name arg Namespace: {args.name}")
args = parser.parse_args()
print(f"single arg: {args.name}")
print(f"second arg: {args.age}")
[I500695@C02ZR8BSMD6N:2021-12-30 16:10:33:~/work/code/python/argparse:]2011$ py3 singlePosArg.py aaa 10
single arg: aaa
second arg: 10

		    14.4.2.2 add optional argument

[I500695@C02ZR8BSMD6N:2021-12-30 16:53:50:~/work/code/python/argparse:]2016$ cat optionalPosArg.py | pbcopy 
import argparse

parser = argparse.ArgumentParser(
    description='Performs some useful work.',
)

parser.add_argument(
    '--age',
    type=int,
    default='35',
    nargs='?',
    help='age, an optional positional argument',
)

args = parser.parse_args()
print(f"second arg: {args.age}")

run it:
[I500695@C02ZR8BSMD6N:2021-12-30 16:10:36:~/work/code/python/argparse:]2012$ py3 optionalPosArg.py 
second arg: 35
[I500695@C02ZR8BSMD6N:2021-12-30 16:53:24:~/work/code/python/argparse:]2013$ py3 optionalPosArg.py -h
usage: optionalPosArg.py [-h] [--age [AGE]]

Performs some useful work.

optional arguments:
  -h, --help   show this help message and exit
  --age [AGE]  age, an optional positional argument
[I500695@C02ZR8BSMD6N:2021-12-30 16:53:29:~/work/code/python/argparse:]2014$ py3 optionalPosArg.py 15
usage: optionalPosArg.py [-h] [--age [AGE]]
optionalPosArg.py: error: unrecognized arguments: 15
[I500695@C02ZR8BSMD6N:2021-12-30 16:53:37:~/work/code/python/argparse:]2015$ py3 optionalPosArg.py --age 15
second arg: 15

- optional boolean toggle
[I500695@C02ZR8BSMD6N:2021-12-30 16:59:37:~/work/code/python/argparse:]2019$ py3 optionalPosArg2.py -t
toggle arg: True
[I500695@C02ZR8BSMD6N:2021-12-30 16:59:46:~/work/code/python/argparse:]2019$ py3 optionalPosArg2.py 
toggle arg: False
[I500695@C02ZR8BSMD6N:2021-12-30 16:59:48:~/work/code/python/argparse:]2020$ cat !$
cat optionalPosArg2.py
import argparse

parser = argparse.ArgumentParser(
    description='Performs some useful work.',
)

parser.add_argument(
    '--toggle', '-t',
    #type=bool,
    default=False,
    #nargs='?',
    action='store_true', #means that if -t is passed set to True
    help='a boolean toggle',
)

args = parser.parse_args()
print(f"toggle arg: {args.toggle}")
-
		14.4.3 args restrict values 
[I500695@C02ZR8BSMD6N:2021-12-30 19:20:33:~/work/code/python/argparse:]2037$ py3 restrictArgValues.py -l 2
toggle arg: 2
[I500695@C02ZR8BSMD6N:2021-12-30 19:20:44:~/work/code/python/argparse:]2037$ py3 restrictArgValues.py -l 5
usage: restrictArgValues.py [-h] [-l {0,1,2,3}]
restrictArgValues.py: error: argument -l/--levels: invalid choice: 5 (choose from 0, 1, 2, 3)
[I500695@C02ZR8BSMD6N:2021-12-30 19:20:46:~/work/code/python/argparse:]2038$ py3 restrictArgValues.py -l 
usage: restrictArgValues.py [-h] [-l {0,1,2,3}]
restrictArgValues.py: error: argument -l/--levels: expected one argument
[I500695@C02ZR8BSMD6N:2021-12-30 19:20:47:~/work/code/python/argparse:]2039$ cat countArgRepetition.py 
import argparse

parser = argparse.ArgumentParser(
    description='restict argument values',
)

parser.add_argument(
    "-v", "--verbosity",
    action="count",
    help='increase verbosity',
)

args = parser.parse_args()
print(f"verbosity count: {args.verbosity}")

		14.4.4 count arg repetition
[I500695@C02ZR8BSMD6N:2021-12-30 19:18:35:~/work/code/python/argparse:]2026$ py3 countArgRepetition.py 
verbosity count: None
[I500695@C02ZR8BSMD6N:2021-12-30 19:18:46:~/work/code/python/argparse:]2027$ py3 countArgRepetition.py -v
verbosity count: 1
[I500695@C02ZR8BSMD6N:2021-12-30 19:18:48:~/work/code/python/argparse:]2028$ py3 countArgRepetition.py -vv
verbosity count: 2
[I500695@C02ZR8BSMD6N:2021-12-30 19:18:50:~/work/code/python/argparse:]2029$ py3 countArgRepetition.py -vvv
verbosity count: 3
[I500695@C02ZR8BSMD6N:2021-12-30 19:18:51:~/work/code/python/argparse:]2030$ cat countArgRepetition.py 
import argparse

parser = argparse.ArgumentParser(
    description='restict argument values',
)

parser.add_argument(
    "-v", "--verbosity",
    action="count",
    help='increase verbosity',
)

args = parser.parse_args()
print(f"verbosity count: {args.verbosity}")

		14.4.5 Python argparse mutual exclusive Groups
https://stackoverflow.com/questions/17909294/python-argparse-mutual-exclusive-group

Q. What I need is:

pro [-a xxx | [-b yyy -c zzz]]

A. 
add_mutually_exclusive_group doesn't make an entire group mutually exclusive. It makes options within the group mutually exclusive.

What you're looking for is subcommands. Instead of prog [ -a xxxx | [-b yyy -c zzz]], you'd have:

prog 
  command 1 
    -a: ...
  command 2
    -b: ...
    -c: ...
To invoke with the first set of arguments:

prog command_1 -a xxxx
To invoke with the second set of arguments:

prog command_2 -b yyyy -c zzzz
You can also set the sub command arguments as positional.

prog command_1 xxxx
Kind of like git or svn:

git commit -am
git merge develop
Working Example
# create the top-level parser
parser = argparse.ArgumentParser(prog='PROG')
parser.add_argument('--foo', action='store_true', help='help for foo arg.')
subparsers = parser.add_subparsers(help='help for subcommand')

# create the parser for the "command_1" command
parser_a = subparsers.add_parser('command_1', help='command_1 help')
parser_a.add_argument('a', type=str, help='help for bar, positional')

# create the parser for the "command_2" command
parser_b = subparsers.add_parser('command_2', help='help for command_2')
parser_b.add_argument('-b', type=str, help='help for b')
parser_b.add_argument('-c', type=str, action='store', default='', help='test')
Test it
>>> parser.print_help()
usage: PROG [-h] [--foo] {command_1,command_2} ...

positional arguments:
  {command_1,command_2}
                        help for subcommand
    command_1           command_1 help
    command_2           help for command_2

optional arguments:
  -h, --help            show this help message and exit
  --foo                 help for foo arg.
>>>

>>> parser.parse_args(['command_1', 'working'])
Namespace(a='working', foo=False)
>>> parser.parse_args(['command_1', 'wellness', '-b x'])
usage: PROG [-h] [--foo] {command_1,command_2} ...
PROG: error: unrecognized arguments: -b x


		14.4.6
	14.5
15. Lists

    15.1 An Introduction to Python Lists

Fredrik Lundh | August 2006
Overview #

The list type is a container that holds a number of other objects, in a given order. The list type implements the sequence protocol, and also allows you to add and remove objects from the sequence.
Creating Lists #

To create a list, put a number of expressions in square brackets:

    L = []
    L = [expression, ...]

This construct is known as a Â¿list displayÂ¿. Python also supports computed lists, called Â¿list comprehensionsÂ¿. In its simplest form, a list comprehension has the following syntax:

    L = [expression for variable in sequence]

where the expression is evaluated once, for every item in the sequence.

The expressions can be anything; you can put all kinds of objects in lists, including other lists, and multiple references to a single object.

You can also use the built-in list type object to create lists:

    L = list() # empty list
    L = list(sequence)
    L = list(expression for variable in sequence)

The sequence can be any kind of sequence object or iterable, including tuples and generators. If you pass in another list, the list function makes a copy.

Note that Python creates a single new list every time you execute the [] expression. No more, no less. And Python never creates a new list if you assign a list to a variable.

A = B = [] # both names will point to the same list

A = []
B = A # both names will point to the same list

A = []; B = [] # independent lists

For information on how to add items to a list once youÂ¿ve created it, see Modifying Lists below.
Accessing Lists #

size of list, len(list)

Lists implement the standard sequence interface; len(L) returns the number of items in the list, L[i] returns the item at index i (the first item has index 0), and L[i:j] returns a new list, containing the objects between i and j.

    n = len(L)

    item = L[index]

    seq = L[start:stop]

If you pass in a negative index, Python adds the length of the list to the index. L[-1] can be used to access the last item in a list.

For normal indexing, if the resulting index is outside the list, Python raises an IndexError exception. Slices are treated as boundaries instead, and the result will simply contain all items between the boundaries.

Lists also support slice steps:
 

    seq = L[start:stop:step]

    seq = L[::2] # get every other item, starting with the first
    seq = L[1::2] # get every other item, starting with the second

Looping Over Lists #

The for-in statement makes it easy to loop over the items in a list:

    for item in L:
        print item

If you need both the index and the item, use the enumerate function:

    for index, item in enumerate(L):
        print index, item

If you need only the index, use range and len:

    for index in range(len(L)):
        print index

The list object supports the iterator protocol. To explicitly create an iterator, use the built-in iter function:

    i = iter(L)
    item = i.next() # fetch first value
    item = i.next() # fetch second value

Python provides various shortcuts for common list operations. For example, if a list contains numbers, the built-in sum function gives you the sum:

    v = sum(L)

    total = sum(L, subtotal)

    average = float(sum(L)) / len(L)

If a list contains strings, you can combine the string into a single long string using the join string method:

    s = ''.join(L)

Python also provides built-in operations to search for items, and to sort the list. These operations are described later.
Modifying Lists #

The list type also allows you to assign to individual items or slices, and to delete them.

    L[i] = obj
    L[i:j] = sequence

Note that operations that modify the list will modify it in place. This means that if you have multiple variables that point to the same list, all variables will be updated at the same time.

    L = []
    M = L

    # modify both lists
    L.append(obj)

To create a separate list, you can use slicing or the list function to quickly create a copy:

    L = []
    M = L[:] # create a copy

    # modify L only
    L.append(obj)

You can also add items to an existing sequence. The append method adds a single item to the end of the list, the extend method adds items from another list (or any sequence) to the end, and insert inserts an item at a given index, and move the remaining items to the right.

    L.append(item)
    L.extend(sequence)
    L.insert(index, item)

To insert items from another list or sequence at some other location, use slicing syntax:

    L[index:index] = sequence

    15.2 Delete from list
You can also remove items. The del statement can be used to remove an individual item, or to remove all items identified by a slice. The pop method removes an individual item and returns it, while remove searches for an item, and removes the first matching item from the list.

    del L[i]

        15.2.1 delete range from list
    del L[i:j]

    can also copy to new list
    newlist = L[:i].extend(L[j+1:])

            15.2.1.1 delete arbitrary indices in one go from list
https://stackoverflow.com/questions/497426/deleting-multiple-elements-from-a-list

You can use enumerate and remove the values whose index matches the indices you want to remove:

indices = 0, 2
somelist = [i for j, i in enumerate(somelist) if j not in indices]

For some reason I don't like any of the answers here. Yes, they work, but strictly speaking most of them aren't deleting elements in a list, are they? (But making a copy and then replacing the original one with the edited copy).

Why not just delete the higher index first?

Is there a reason for this? I would just do:

for i in sorted(indices, reverse=True):
    del somelist[i]
If you really don't want to delete items backwards, then I guess you should just deincrement the indices values which are greater than the last deleted index (can't really use the same index since you're having a different list) or use a copy of the list (which wouldn't be 'deleting' but replacing the original with an edited copy).

Am I missing something here, any reason to NOT delete in the reverse order?

            15.2.1.2

        15.2.2
    item = L.pop() # last item
    item = L.pop(0) # first item
    item = L.pop(index)
    L.remove(item)

The del statement and the pop method does pretty much the same thing, except that pop returns the removed item.

    15.3
Finally, the list type allows you to quickly reverse the order of the list.

    L.reverse()

Reversing is fast, so temporarily reversing the list can often speed things up if you need to remove and insert a bunch of items at the beginning of the list:

    L.reverse()
    # append/insert/pop/delete at far end
    L.reverse()

Note that the for-in statement maintains an internal index, which is incremented for each loop iteration. This means that if you modify the list youÂ¿re looping over, the indexes will get out of sync, and you may end up skipping over items, or process the same item multiple times. To work around this, you can loop over a copy of the list:

    for object in L[:]:
        if not condition:
            del L[index]

Alternatively, you can use create a new list, and append to it:

    out = []
    for object in L:
        if condition:
            out.append(object)

A common pattern is to apply a function to every item in a list, and replace the item with the return value from the function:

    for index, object in enumerate(L):
        L[index] = function(object)

    out = []
    for object in L:
        out.append(function(object))

The above can be better written using either the built-in map function, or as a list comprehension:

    out = map(function, L)

    out = [function(object) for object in L]

For straightforward function calls, the map solution is more efficient, since the function object only needs to be fetched once. For other constructs (e.g. expressions or calls to object methods), you have to use a callback or a lambda to wrap the operation; in such cases, the list comprehension is more efficient, and usually also easier to read.
 

Again, if you need both the item and the index, use enumerate:

    out = [function(index, object) for index, object in enumerate(L)]

You can use the list type to implement simple data structures, such as stacks and queues.

    stack = []
    stack.append(object) # push
    object = stack.pop() # pop from end

    queue = []
    queue.append(object) # push
    object = queue.pop(0) # pop from beginning

The list type isnÂ¿t optimized for this, so this works best when the structures are small (typically a few hundred items or smaller). For larger structures, you may need a specialized data structure, such as collections.deque.

Another data structure for which a list works well in practice, as long as the structure is reasonably small, is a LRU (least-recently-used) container. The following statements moves an object to the end of the list:

    lru.remove(item)
    lru.append(item)

If you do the above every time you access an item in the LRU list, the least recently used items will move towards the beginning of the list. (for a simple cache implementation using this approach, see Caching.)
Searching Lists #

The in operator can be used to check if an item is present in the list:

    if value in L:
        print "list contains", value

To get the index of the first matching item, use index:

    i = L.index(value)

The index method does a linear search, and stops at the first matching item. If no matching item is found, it raises a ValueError exception.

    try:
        i = L.index(value)
    except ValueError:
        i = -1 # no match

To get the index for all matching items, you can use a loop, and pass in a start index:

    i = -1
    try:
        while 1:
            i = L.index(value, i+1)
            print "match at", i
    except ValueError:
        pass

    def findall(L, value, start=0):
        # generator version
        i = start - 1
        try:
            i = L.index(value, i+i)
            yield i
        except ValueError:
            pass

    for index in findall(L, value):
        print "match at", i

To count matching items, use the count method:

    n = L.count(value)

Note that count loops over the entire list, so if you just want to check if a value is present in the list, you should use in or, where applicable, index.

To get the smallest or largest item in a list, use the built-in min and max functions:

    lo = min(L)
    hi = max(L)

As with sort (see below), you can pass in a key function that is used to map the list items before they are compared:

    lo = min(L, key=int)
    hi = max(L, key=int)

Sorting Lists #

The sort method sorts a list in place.

    L.sort()

To get a sorted copy, use the built-in sorted function:

    out = sorted(L)

An in-place sort is slightly more efficient, since Python does not have to allocate a new list to hold the result.

By default, PythonÂ¿s sort algorithm determines the order by comparing the objects in the list against each other. You can override this by passing in a callable object that takes two items, and returns -1 for Â¿less thanÂ¿, 0 for Â¿equalÂ¿, and 1 for Â¿greater thanÂ¿. The built-in cmp function is often useful for this:

    def compare(a, b):
        return cmp(int(a), int(b)) # compare as integers

    L.sort(compare)

    def compare_columns(a, b):
        # sort on ascending index 0, descending index 2
        return cmp(a[0], b[0]) or cmp(b[2], a[2])

    out = sorted(L, compare_columns)

Alternatively, you can specify a mapping between list items and search keys. If you do this, the sort algorithm will make one pass over the data to build a key array, and then sort both the key array and the list based on the keys.

    L.sort(key=int)

    out = sorted(L, key=int)

If the transform is complex, or the list is large, this can be a lot faster than using a compare function, since the items only have to be transformed once.

PythonÂ¿s sort is stable; the order of items that compare equal will be preserved.
Printing Lists #

By default, the list type does a repr on all items, and adds brackets and commas as necessary.

To control formatting, use the string join method, combined with either map or a list comprehension or generator expression.

    print "".join(L) # if all items are strings

    print ", ".join(map(str, L))

    print "|".join(str(v) for v in L if v > 0)

To print a list of string fragments to a file, you can use writelines instead of write:

    sys.stdout.writelines(L) # if all items are strings

Performance Notes #

The list object consists of two internal parts; one object header, and one separately allocated array of object references. The latter is reallocated as necessary.

The list has the following performance characteristics:

    * The list object stores pointers to objects, not the actual objects themselves. The size of a list in memory depends on the number of objects in the list, not the size of the objects.
    * The time needed to get or set an individual item is constant, no matter what the size of the list is (also known as Â¿O(1)Â¿ behaviour).
    * The time needed to append an item to the list is Â¿amortized constantÂ¿; whenever the list needs to allocate more memory, it allocates room for a few items more than it actually needs (this assumes that the memory allocator is fast; for huge lists, the allocation overhead may push the behaviour towards O(n*n)).
    * The time needed to insert an item depends on the size of the list, or more exactly, how many items that are to the right of the inserted item (O(n)). In other words, inserting items at the end is fast, but inserting items at the beginning can be relatively slow, if the list is large.
    * The time needed to remove an item is about the same as the time needed to insert an item at the same location; removing items at the end is fast, removing items at the beginning is slow.
    * The time needed to reverse a list is proportional to the list size (O(n)).
    * The time needed to sort a list varies; the worst case is O(n log n), but typical cases are often a lot better than that.

16. Validate input
You probably won't be using Python long before writing a program which needs user input. As a wide-eyed, innocent new Python programmer, you may naively expect that you can simply ask users for input and they will just give it to you....

    WARNING: Showing the preceding sentence to veteran programmers may cause them to collapse on the floor giggling helplessly. 

Users don't work that way.

For example, if you ask for a simple 'y or n' response, your user may cheerfully type in their name - or their lunch order, or nothing at all - and your program will break. They don't do this on purpose (well, mostly). It's just that the poor dears are easily distracted, totally ignore your carefully worded input prompts and often type complete gibberish as far as your program is concerned. Next, oddly enough, they will blame you, the programmer. Then you will look foolish and feel Unhappy.

To avoid this misery, the very first thing you need to do is make sure that whatever comes back from the user is checked to see if it's even vaguely close to what you expected. Python has heaps of functions to help you with this and we'll begin by going through some of them together below.

Another thing you can do is use validators on your input widgets. The way these work is they simply throw away any keystrokes that are not what you are after. As an example, if you set a numeric validator on a string widget, users can press 'ABC' etc. as much as they like and nothing will even show up in the widget. The only keys they can press that will have any effect are 0-9 and, perhaps, a decimal or dollar symbol. We'll play with these too later on.

Finally, even if you are lucky enough to find yourself in possession of a particularly well-trained and obedient user who always types what you ask, the input is unlikely to be formatted exactly the way you want it. Careless typing often produces strings like 'jOHN sMith' (caps lock) or phone numbers resembling '604555-1212'.

All kidding aside, it's actually your job as a programmer to make it as easy and fast as possible for the user to input data and that it be presented and stored in a consistent format. Plus, you can get a great deal of personal satisfaction and even, dare I say it? gratitude from users if you can save them from the hell of properly typing something like a Canadian postal code.
Acquiring Input

First your program will need to acquire some user input. From the console Python offers two methods for this 'raw_input("Prompt")' and 'input("Prompt")'. (Don't use 'input', see below.) You can also get input from good ol' command line arguments or environment variables.

Other, more graphical methods are available, without getting too carried away, such as Xdialog, Gdialog (part of gnome-utils) or Kaptain.

Access to full-blown GUI toolkits is available from Python using PyQT , TKinter, WxPython and PyGTK among others.

This is probably a good time to provide a few words of caution. Most users are contented, docile creatures who like to have their belly rubbed, but you will encounter rogue types bent on destruction.

For this reason you must never allow user input to leak into your command space:

    * Use 'raw_input()' instead of 'input()'. 'input()' is fed to 'eval()' before your program gets it. This automatically converts types, which is convenient if you want an integer and the user accidentally gives you one. But the user would have to quote strings. Worse, a rogue user might type 'os.system("rm -r *")', which would give you a bad day. 'raw_input()' returns whatever the user entered as a string. This makes validation easier, because you know what type it will be and thus which operations you can apply to it.

    * Always check input to os.system(), os.popen() or os.exec*() calls.

    * Always escape user input before printing it to a web page or using it in a SQL query. This is in addition to input checking. Unescaped special characters can cause invalid HTML, screw up your page formatting, and allow the user to exploit Javascript "features" against another user. Unescaped special characters in a SQL query may cause a SQL syntax error or cause the query to do more than intended. Use 'cgi.escape()' to escape HTML. See your database documentation to escape SQL queries.

O.K. Relax. The spooky part is over.

Open an xterm and type 'python' to enter the interpreter. Note: Many of these examples require that you be using a version of Python greater than or equal to version '2'. Redhat still ships with version 1.5x as default, so if you are a Redhat user you will need to type 'python2' instead (and possibly install the rpm first from 'add-ons'). For the record, version '1.5' was released in a year which began with the digits '1' and '9'.
Checking the Content of String Objects

Programming languages usually include methods for checking of this kind and Python is no exception. Consider one of our first challenges as stated above: making sure the user gives us a valid number when we ask for one.

It happens that all string objects in Python have built-in methods which make this quite painless. Type these lines in at the '>>>' prompt:

>>>input = '9'
>>>input.isdigit()
1

This will return a '1' (true), so you can easily use it in an 'if' statement as a condition. Some other handy attributes of this kind are:

s.isalnum() returns true if all characters in s are alphanumeric, false otherwise.
s.isalpha() returns true if all characters in s are alphabetic, false otherwise.

For a complete list of these and much more, I highly recommend the Python 2.1 Quick Reference. I use this all the time and even have an older text version stuffed into HNB for speed.

This will get us through simple cases like menu choices, but what if we wanted a float or a real number?

Consider:

input = '9.9' or
input = '-9'

Both of these are valid numbers, but input.isdigit() will return '0' (false), because the negative sign and the decimal point are not 'digits'. Our poor user will be very confused when we spit back an error message if these entries are valid.

So, let's assume that they are what we want and try to convert them explicitly. For this we'll use the Python try/except construction. Python raises exceptions of different kinds on errors and we can trap these errors individually by name.

Say we wanted an integer like '-9', we can use the numeric operator 'int()' to explicitly attempt the conversion for us.

try:
    someVar = int(input)
    print 'Is an integer'
except (TypeError, ValueError):
    print 'Not an integer'

Two things to notice here. The first is that we are checking for two different exceptions, Type and Value. This way we not only handle the user entering a float (like '9.9'), but we also allow for the possibility that they didn't even enter a number of any kind - perhaps they entered 'Ham on rye'. The second thing to notice is that we actually entered the kinds of exceptions we were interested in trapping. It's very easy to just type in open ended exceptions without bothering to look up which errors you are trapping like this:

try:
    someVar = int(input)
    print 'Is an integer'
except:
    print 'Not an integer'

DO NOT DO THIS. Python will let you, but since you are now trapping all exceptions debugging will be a nightmare for you if anything breaks. Just trust me on this one; look up the errors you mean to trap and you'll save time in the long run.

Other operators you'll find useful are long() and float(). On the flip side, str() can convert anything to a string.

Don't forget to range check - it's no good congratulating yourself on ensuring your program always gets an integer from a user if it blithely accepts the integer '42' as a valid month day... Make sure the number falls into the expected range using the comparison operators '>, <, >=' etc.
Validating Input

As we've seen, we can validate input after we get it, but wouldn't it be nice if we could prevent the user from entering mistakes in the first place?

Enter widget validators.

These are things built into graphical user interface toolkits that prevent unwanted keystrokes from even appearing in the string widget. Toolkits usually come with some built-in validators for numeric, alpha, and alphanumeric etc. and are quite easy to use. I'm currently using mostly PyQT for gui's, but TKinter, WxPython and even Kaptain all have validators. I could be wrong, but PyGTK seems not to have them - yet. Perhaps you could hook up a signal and roll your own if you happen to use a toolkit that doesn't have them.

If the built-in validators don't suit you then PyQt, for example, allows you to specify your own, custom validators.

Clearly, I can't go into detail for every toolkit out there, but here's an example of how to attach a numeric validator to a widget in PyQT. The widget's name is 'self.rate', we're attaching the 'QDoubleValidator' and telling it to accept numbers between 0.0 and 999.0 up to 2 decimal places:

self.rate.setValidator(QDoubleValidator(0.0, 999.0, 2, self.rate) )

Nice eh? Notice it took care of range checking for us too!

Other ways to help users enter information include spinners, pick-lists and combo-boxes, but you already knew that.
Formatting Input

Remember the 'jOHN sMith' example from the introduction? Here's the fix:

>>>'jOHN sMith'.title()
'John Smith'

Yes, yet another attribute of all string objects in Python is the 'title()' attribute which will helpfully capitalize each word for you. 'capitalize()' is similar, but only does the first character:

>>> 'jOHN sMith'.capitalize()
'John smith'

Go ahead and try 'upper()', 'lower()' and 'swapcase()' on your own if you like. I think you can guess their behaviour.

But how about 'rjust(n)'? This is only one of some really handy attributes you can use to layout reports. Watch:

>>> 'John Smith'.rjust(15)
'     John Smith'

Our string has been right justified for us in a string 15 characters long. Sweet. As you've probably guessed, there are also 'center(n)' and 'ljust(n)'. Again, have a look at the Python 2.1 Quick Reference to see them all.

Another, very important operator in Python is the '%' (per cent) operator. The description of this in combination with list objects and printf-style formatting codes could easily consume several pages, so I'm just going to gloss over it with a few examples to pique your interest today.

In it's simplest form, the '%' operator lets you write, say, a proper sentence that includes variables which can change at runtime:

>>> 'This is a %s example of its %s.' % ('good', 'use')
'This is a good example of its use.'

At least, I hope it is. This is only the beginning of its power. In addition to just string object substitution with '%s' there is also '%r' and the printf friends from the 'C' language: c, d, i, u, o, x, X, e, E, f, g, G.

Here's an example from Python 2.1 Quick Reference:

>>> '%s has %03d quote types.' % ('Python', 2)
'Python has 002 quote types.'

The right hand side may also be a mapping, which allows you to refer to fields by name.

Let's move on to something a little more challenging, but common enough.
Phone Numbers

Phone numbers are variable in length. Sometimes they are only 2 or 3 digits long if you are behind a corporate PBX system. Other times they might stretch out to 15 digits or more for international calling. They might even contain '#' symbols or asterisks. Maybe even commas. Worse, the user may attempt to impose a format on it as they enter it. Or a partial format. Or not.

Now, it will only frustrate your user if you don't let them at least try to enter it properly, so your validator had better accept all of #, *, 'comma', -, ), ( as well as the digits 0-9. Of course, you could still end up with:

'250-(555)-12-12'

instead of the string:

'(250) 555-1212'

that we actually want (for a North American phone number anyhow). Don't worry, we'll make the solution generic enough to handle just about anything.

My first instinct when I need something like this is to copy someone else's work by mining Google - especially Google Groups. This turns out to be a good instinct for me to have since the code snippet I usually find will be far better than I could do on my own. Unfortunately, this time I turned up an email from Guido van Rossum (the inventor of Python) explaining to someone that Python did not have such a thing and perhaps they could use something like:

import string
def fmtstr(fmt, str):
    res = [] i = 0
    for c in fmt:
        if c == '#':
            res.append(str[i:i+1]) i = i+1
        else:
            res.append(c)
    res.append(str[i:])
    return string.join(res)

This is a darn good start of course and you can't argue with the credentials of its author, but it doesn't handle all the cases without a lot of 'if/then' constructs to count how many digits you were given in order to choose a format string of the correct length. Go ahead and paste it into your xterm and then call it like this:

>>> fmtstr('###-####', '5551212')
'5 5 5 - 1 2 1 2 '

In fact, I did copy and paste it into my editor and then constructed a long sequence of 'if/thens' for phone numbers, dates and other types of entries, but I still wasn't handling everything. Plus, I had dozens and dozens of lines doing self-similar things. They have since passed on to their reward.

O.K., here we go... First, let's filter any "extra" formatting characters we let the user type in:

def filter(inStr, allowed):
    outStr = ''
    for c in inStr:
        if c in allowed:
            outStr += c
    return outStr

We could call it like this:

>>>filter('250-(555)-12-12', string.digits)
'2505551212'

Or we could define the second argument ourselves as '0123456789#*,' to include all the allowable characters possible.

Now we just take Guido's code snippet and (this is the good bit) reverse both the input arguments. This way we can specify just one long format string and it will be matched until we run out of input. Any extra input will just get tacked on, so we will never lose any characters.

# import the regular expression module
import re

def formatStr(inStr, fmtStr, p = '^'):
    inList = [x for x in inStr] #list from strings..
    fmtList = [x for x in fmtStr]
    # the good bit
    inList.reverse(); fmtList.reverse()
    outList = []
    i = 0
    for c in fmtList:
        if c == p:
            try:
                outList.append(inList[i])
                i += 1
            # break if fmtStr longer than inStr
            except IndexError:
                break
        else:
            outList.append(c)
    # handle inStr longer than fmtStr
    while i < len(inList):
        outList.append(inList[i])
        i += 1
    # put it back the way we found it
    outList.reverse()
    outStr = ''.join(outList)
    # remove stray parens/- etc
    while re.match('[)|-| ]', outStr[0]):
        outStr = outStr[1:]
    # close any legit parens
    while outStr.count(')') > outStr.count('('):
        outStr = '(' + outStr
    return outStr

[Text version of this listing.]

It's basically the same as Guido's except the default placeholder character is now a '^' (caret), because we may need to use the '#'. Alternatively, this may be specified as an, optional, third argument if we ever need real carets in the output.

Here's some sample output:

>>> formatStr('51212', ' ^^^ ^^ (^^^) ^^^-^^^^')
'5-1212'
>>> formatStr('045551212', ' ^^^ ^^ (^^^) ^^^-^^^^')
'(04) 555-1212'
>>> formatStr('16045551212', ' ^^^ ^^ (^^^) ^^^-^^^^')
'1 (604) 555-1212'
>>> formatStr('1011446045551212', ' ^^^ ^^ (^^^) ^^^-^^^^')
'1 011 44 (604) 555-1212'

In practice, you'll probably want to simply define your phone formatting string early on e.g.:

phone_format_str = ' ^^^ ^^ (^^^) ^^^-^^^^'

There's a space at the beginning of the string so that any additional characters won't get smooshed onto it. You'd likely call it thus:

formatStr(input, phone_format_str)

... after you clean up your 'input' with something like the 'filter()' function.
Postal Codes

In case you are (blessedly) unfamiliar with Canadian postal codes, they look like this:

'V8G 4L2'

Which appears innocuous enough until you attempt to type it. Especially for non-typists (like me). You can turn on the caps lock - and then forget to turn it off - or you have to type [shift]+alpha, number, [shift]+alpha etc. and quite often end up with: 'v*g $l@' when you get out of sequence. Needless to say, users hate typing them in and they hardly ever look right. Mostly your application won't even capture postal codes, because users simply won't bother. Some other countries have similar post codes. Shame.

Now, with our new formatting function, they're a piece of cake. First, we either validate or filter whatever they give us, then we simply use Python's built-in string attribute 'upper()' to set the case of the alpha characters properly, finally:

>>>formatStr('V8G4L2', ' ^^^ ^^^')
'V8G 4L2'

If accurate postal codes are critical to your application, you will need to do more verification by way of counting the characters and verifying the pattern. For general use though, you need to allow for the postal codes of other countries. I think I normally format only if the number of characters == 6 after clean up.

How about Social Insurance Numbers? Same deal:

>>> formatStr('716555123', '^^^-^^^-^^^')
'716-555-123'

You should run a check digit routine over Social Insurance Numbers first to ensure they are valid. Ditto for credit cards.

I hope these examples will save you some time in coding user interfaces. I'd very much like to hear back with examples or improvements of your own. Particularly ways of dealing with dates1 with users. They're always fun.

By the way, it's very important that you not keep these formatting aids a secret from your users. Put it in the 'help', use 'tooltips' or 'whatis' to let them know the facility is there for them. If they find out after months of typing things the long way, they are liable to pout and you'll end up wasting afternoon coffee scratching them behind the ears (morning coffee is a given).

Have fun with it!
1 That's calendar dates...

	16.1 Tkinter widget validators

	
A Validating Entry Widget

June 30, 1998 | Fredrik Lundh

This module implements a validating version of the Tkinter Entry widget. It uses the textvariable option to attach a StringVar variable to the widget, and uses the variable trace function to keep track of whatÂ¿s going on. To specify how validation is to be done, override the validate method.

Note that the constructor takes a parent widget, and also allows you to use the value option to specify the initial contents. All other options are passed on to the Entry widget itself.
File: tkValidatingEntry.py

from Tkinter import *

class ValidatingEntry(Entry):
    # base class for validating entry widgets

    def __init__(self, master, value="", **kw):
        apply(Entry.__init__, (self, master), kw)
        self.__value = value
        self.__variable = StringVar()
        self.__variable.set(value)
        self.__variable.trace("w", self.__callback)
        self.config(textvariable=self.__variable)

    def __callback(self, *dummy):
        value = self.__variable.get()
        newvalue = self.validate(value)
        if newvalue is None:
            self.__variable.set(self.__value)
        elif newvalue != value:
            self.__value = newvalue
            self.__variable.set(self.newvalue)
        else:
            self.__value = value

    def validate(self, value):
        # override: return value, new value, or None if invalid
        return value

Note: Newer versions of Tkinter support validate and validatecommand options that can be used instead of the extra variable. The above code still works, though.
Examples #

The first two examples are subclasses that check that the input is a valid Python integer or float, respectively. The validate method simply tries to convert the value to an object of the right kind, and returns None (reject) if that fails.

class IntegerEntry(ValidatingEntry):

    def validate(self, value):
        try:
            if value:
                v = int(value)
            return value
        except ValueError:
            return None

class FloatEntry(ValidatingEntry):

    def validate(self, value):
        try:
            if value:
                v = float(value)
            return value
        except ValueError:
            return None

The third example is a subclass that restricts the length of the input to a given max length.
 

class MaxLengthEntry(ValidatingEntry):

    def __init__(self, master, value="", maxlength=None, **kw):
        self.maxlength = maxlength
        apply(ValidatingEntry.__init__, (self, master), kw)

    def validate(self, value):
        if self.maxlength is None or len(value) <= self.maxlength:
            return value
        return None # new value too long

HereÂ¿s how to use this class:

root = Tk()

entry = MaxLengthEntry(root, maxlength=20)
entry.pack()

mainloop()

Note that if the user pastes a long string into the entry box, it will be rejected by this implementation. A better solution might be to change the validate method to:

    def validate(self, value):
        if self.maxlength:
            value = value[:self.maxlength]
        return value

17. Files

	17.1 delete lines from file

      import os
      def CutOut(filename, start, end):
          """ Cut out lines from start to end from file filename """
       
          # check it's worth even reading the file (sanity check)
          if start >= end:
              return
       
          # read the entire file in as separate lines
          filelines = file(filename).readlines()
       
          # make sure start is a sane value
          if start > len(filelines):
             return
       
          # make sure end is a sane value
          if end > len(filelines):
             end = len(filelines)
       
          # chop out the bit we don't want
          filelines = filelines[:start] + filelines[end:]
       
          # could do this instead (not sure which is quicker):
          # del filelines[start:end]
       
          # write out the new set of lines
          file(filename,"w").writelines(filelines)

	17.2 Get list of files in dir

		17.2.1  os way
>>> import os
>>> dirList = os.listdir(os.curdir)
	>>> for file in dirList:
	...     print file
	... 


		17.2.2 Using glob ( > 2.6)
From http://docs.python.org/library/glob.html

The glob module finds all the pathnames matching a specified pattern according to the rules used by the Unix shell. No tilde expansion is done, but *, ?, and character ranges expressed with [] will be correctly matched. This is done by using the os.listdir() and fnmatch.fnmatch() functions in concert, and not by actually invoking a subshell. (For tilde and shell variable expansion, use os.path.expanduser() and os.path.expandvars().)

glob.glob(pathname)Â¶
    Return a possibly-empty list of path names that match pathname, which must be a string containing a path specification. pathname can be either absolute (like /usr/src/Python-1.5/Makefile) or relative (like ../../Tools/*/*.gif), and can contain shell-style wildcards. Broken symlinks are included in the results (as in the shell).

glob.iglob(pathname)Â¶

    Return an iterator which yields the same values as glob() without actually storing them all simultaneously.

    New in version 2.5.

For example, consider a directory containing only the following files: 1.gif, 2.txt, and card.gif. glob() will produce the following results. Notice how any leading components of the path are preserved.

>>> import glob
>>> glob.glob('./[0-9].*')
['./1.gif', './2.txt']
>>> glob.glob('*.gif')
['1.gif', 'card.gif']
>>> glob.glob('?.gif')
['1.gif']

See also

Module fnmatch
    Shell-style filename (not path) expansion



	17.3 Read files

	    17.3.1  read file character by character
Use open(file, mode) with a filename as file and "r" as mode to open the file. Use a for-loop to iterate through each line of the file. Use a nested for-loop to iterate through each character of each line. Use file.close() with the filename as file to close the file.

file = open("sample.txt", "r")

for line in file:
    for character in line:
        print(character)
OUTPUT
a
b
c
d
file.close()

my example:
$ cat st
hand
In [1]: with open('st', 'r') as f:
   ...:     for l in f:
   ...:         for c in l:
   ...:             print(c)
   ...: 
h
a
n
d
	    17.3.2  read file byte by byte
my example, read byte by byte until eof
with open('st', 'r') as f:
    while True:
        c = f.read(1)
        if not c: #eof
            break
        print(c)


h
a
n
d

Use file.read(size) with 1 as size to read the next character in file.

file = open("sample.txt", "r")

first_character = file.read(1)
print(first_character)
OUTPUT
a

second_character = file.read(1)
print(second_character)
OUTPUT
b
file.close()


To read in chunks.
In [2]: with open('st', 'r') as f:
   ...:     while True:
   ...:         c = f.read(3)
   ...:         if not c: #eof
   ...:             break
   ...:         print(c) #last chunk size can be <3 when size%3!=0
   ...: 
han
d

	    17.3.3 3 Ways to Read A Text File Line by Line in Python

https://cmdlinetips.com/2011/08/three-ways-to-read-a-text-file-line-by-line-in-python/

Opening a file and reading the content of a file is pretty easy in Python. One easy way to read a text file and parse each line is to use the python statement “readlines” on a file object.
readlines(): Read all lines in a file at once
There are at least two ways read all lines in a file. First, we can use readlines function. Python’s readlines function reads everything in the text file and has them in a list of lines. Here is how to use Python’s readlines after the opening file.
	# Open the file with read only permit
f = open('my_text_file.txt', "r")
# use readlines to read all lines in the file
# The variable "lines" is a list containing all lines in the file
lines = f.readlines()
# close the file after reading the lines.
f.close()
Another way to read lines at once is to simply use

	# read all lines at once
lines = list(f)
Note that the last character of each line is newline character.
Then you can go over the list of “lines” to parse each line. As you can immediately notice, “readlines” or “list(f) works great for a small text file. However, it is not memory efficient to use if your text files are really big. A better way to read a text file that is memory-friendly is to read the file line by line, that is one line at a time.


Read a Text File Line by Line Using While Statement in Python
Here is the way to read text file one line at a time using “While” statement and python’s readline function. Since we read one line at a time with readline, we can easily handle big files without worrying about memory problems.
	# Open the file with read only permit
f = open('my_text_file.txt')
# use readline() to read the first line 
line = f.readline()
# use the read line to read further.
# If the file is not empty keep reading one line
# at a time, till the file is empty
while line:
    # in python 2+
    # print line
    # in python 3 print is a builtin function, so
    print(line)
    # use realine() to read next line
    line = f.readline()
f.close()
Another variation of reading a file with while statement and readline statement is as follows. Here the while tests for boolean and read line by line until we reach the end of file and line will be empty.
1
2
3
4
5
6
7
8
9
10
11
12	# file handle fh
fh = open('my_text_file.txt')
while True:
    # read line
    line = fh.readline()
    # in python 2, print line
    # in python 3
    print(line)
    # check if line is not empty
    if not line:
        break
fh.close()

Read a Text File Line by Line Using an Iterator in Python
One can also use an iterator to read a text file one line at time. Here is how to do it.
1
2
3
4
5
6
7	fh = open('my_text_file.txt')
for line in fh:
    # in python 2
    # print line
    # in python 3
    print(line)
fh.close()
Remembering to close the file handler (“fh”) with the statement “fh.close()” can be difficult initially. One can check if a file handler is closed with
1
2
3
4	# check if the file file handler is closed or not
>fh.closed
# true if the file handler is closed 
True

	    17.3.3

	17.4
18. String represntation of classes
	
	18.1 use or override built in method __str__
	__str__(self)

Called by the str built-in function and by the print statement to compute the informal string representation of an object. This differs from __repr__ in that it does not have to be a valid Python expression: a more convenient or concise representation may be used instead. The return value must be a string object.

19. tags: Create installation, distribution, build, packages

	19.1 Tutorial

		19.1.1 Python 2.3 documentation

		 Concepts & Terminology

Using the Distutils is quite simple, both for module developers and for users/administrators installing third-party modules. As a developer, your responsibilities (apart from writing solid, well-documented and well-tested code, of course!) are:

    * write a setup script (setup.py by convention)
    * (optional) write a setup configuration file
    * create a source distribution
    * (optional) create one or more built (binary) distributions

Each of these tasks is covered in this document.

Not all module developers have access to a multitude of platforms, so it's not always feasible to expect them to create a multitude of built distributions. It is hoped that a class of intermediaries, called packagers, will arise to address this need. Packagers will take source distributions released by module developers, build them on one or more platforms, and release the resulting built distributions. Thus, users on the most popular platforms will be able to install most popular Python module distributions in the most natural way for their platform, without having to run a single setup script or compile a line of code.

		19.1.2 A Simple Example

The setup script is usually quite simple, although since it's written in Python, there are no arbitrary limits to what you can do with it, though you should be careful about putting arbitrarily expensive operations in your setup script. Unlike, say, Autoconf-style configure scripts, the setup script may be run multiple times in the course of building and installing your module distribution.

If all you want to do is distribute a module called foo, contained in a file foo.py, then your setup script can be as simple as this:

from distutils.core import setup
setup(name="foo",
      version="1.0",
      py_modules=["foo"])

Some observations:

    * most information that you supply to the Distutils is supplied as keyword arguments to the setup() function
    * those keyword arguments fall into two categories: package metadata (name, version number) and information about what's in the package (a list of pure Python modules, in this case)
    * modules are specified by module name, not filename (the same will hold true for packages and extensions)
    * it's recommended that you supply a little more metadata, in particular your name, email address and a URL for the project (see section 3 for an example)

To create a source distribution for this module, you would create a setup script, setup.py, containing the above code, and run:

python setup.py sdist

which will create an archive file (e.g., tarball on Unix, ZIP file on Windows) containing your setup script setup.py, and your module foo.py. The archive file will be named foo-1.0.tar.gz (or .zip), and will unpack into a directory foo-1.0.

If an end-user wishes to install your foo module, all she has to do is download foo-1.0.tar.gz (or .zip), unpack it, and--from the foo-1.0 directory--run

python setup.py install

which will ultimately copy foo.py to the appropriate directory for third-party modules in their Python installation.

This simple example demonstrates some fundamental concepts of the Distutils. First, both developers and installers have the same basic user interface, i.e. the setup script. The difference is which Distutils commands they use: the sdist command is almost exclusively for module developers, while install is more often for installers (although most developers will want to install their own code occasionally).

If you want to make things really easy for your users, you can create one or more built distributions for them. For instance, if you are running on a Windows machine, and want to make things easy for other Windows users, you can create an executable installer (the most appropriate type of built distribution for this platform) with the bdist_wininst command. For example:

		19.1.3 Determine the package installation path.
		This is sometimes needed in cases where there are interdependencies in the package 
		and this path is needed to be resolved in runtime.

		Packages in Multiple Directories

Packages support one more special attribute, __path__. This is initialized to be a list containing the name of the directory holding the package's __init__.py before the code in that file is executed. This variable can be modified; doing so affects future searches for modules and subpackages contained in the package.

While this feature is not often needed, it can be used to extend the set of modules found in a package.

python setup.py bdist_wininst

will create an executable installer, foo-1.0.win32.exe, in the current directory.

Other useful built distribution formats are RPM, implemented by the bdist_rpm command, Solaris pkgtool (bdist_pkgtool), and HP-UX swinstall (bdist_sdux). For example, the following command will create an RPM file called foo-1.0.noarch.rpm:

python setup.py bdist_rpm

(The bdist_rpm command uses the rpm executable, therefore this has to be run on an RPM-based system such as Red Hat Linux, SuSE Linux, or Mandrake Linux.)

You can find out what distribution formats are available at any time by running

python setup.py bdist --help-formats

-> General Python terminology

If you're reading this document, you probably have a good idea of what modules, extensions, and so forth are. Nevertheless, just to be sure that everyone is operating from a common starting point, we offer the following glossary of common Python terms:

module
    the basic unit of code reusability in Python: a block of code imported by some other code. Three types of modules concern us here: pure Python modules, extension modules, and packages.

      
pure Python module
    a module written in Python and contained in a single .py file (and possibly associated .pyc and/or .pyo files). Sometimes referred to as a ``pure module.''

      
extension module
    a module written in the low-level language of the Python implementation: C/C++ for Python, Java for Jython. Typically contained in a single dynamically loadable pre-compiled file, e.g. a shared object (.so) file for Python extensions on Unix, a DLL (given the .pyd extension) for Python extensions on Windows, or a Java class file for Jython extensions. (Note that currently, the Distutils only handles C/C++ extensions for Python.)

      
package
    a module that contains other modules; typically contained in a directory in the filesystem and distinguished from other directories by the presence of a file __init__.py.

      
root package
    the root of the hierarchy of packages. (This isn't really a package, since it doesn't have an __init__.py file. But we have to call it something.) The vast majority of the standard library is in the root package, as are many small, standalone third-party modules that don't belong to a larger module collection. Unlike regular packages, modules in the root package can be found in many directories: in fact, every directory listed in sys.path contributes modules to the root package.

-> Distutils-specific terminology

The following terms apply more specifically to the domain of distributing Python modules using the Distutils:

module distribution
    a collection of Python modules distributed together as a single downloadable resource and meant to be installed en masse. Examples of some well-known module distributions are Numeric Python, PyXML, PIL (the Python Imaging Library), or mxBase. (This would be called a package, except that term is already taken in the Python context: a single module distribution may contain zero, one, or many Python packages.)

      
pure module distribution
    a module distribution that contains only pure Python modules and packages. Sometimes referred to as a ``pure distribution.''

      
non-pure module distribution
    a module distribution that contains at least one extension module. Sometimes referred to as a ``non-pure distribution.''

      
distribution root
    the top-level directory of your source tree (or source distribution); the directory where setup.py exists. Generally setup.py will be run from this directory.

		19.1.4 Package lore
		Packages are a way of structuring Python's module namespace by using "dotted module names". For example, the module name Â¿A.BÂ¿ designates a submodule named Â¿BÂ¿ in a package named Â¿AÂ¿. Just like the use of modules saves the authors of different modules from having to worry about each other's global variable names, the use of dotted module names saves the authors of multi-module packages like NumPy or the Python Imaging Library from having to worry about each other's module names.

Suppose you want to design a collection of modules (a "package") for the uniform handling of sound files and sound data. There are many different sound file formats (usually recognized by their extension, for example: Â¿.wavÂ¿, Â¿.aiffÂ¿, Â¿.auÂ¿), so you may need to create and maintain a growing collection of modules for the conversion between the various file formats. There are also many different operations you might want to perform on sound data (such as mixing, adding echo, applying an equalizer function, creating an artificial stereo effect), so in addition you will be writing a never-ending stream of modules to perform these operations. Here's a possible structure for your package (expressed in terms of a hierarchical filesystem):

    Sound/                             Top-level package
          __init__.py       Initialize the sound package
          Formats/           Subpackage for file formats
                  __init__.py
                  wavread.py
                  wavwrite.py
                  aiffread.py
                  aiffwrite.py
                  auread.py
                  auwrite.py
                  ...
          Effects/          Subpackage for sound effects
                  __init__.py
                  echo.py
                  surround.py
                  reverse.py
                  ...
          Filters/                Subpackage for filters
                  __init__.py
                  equalizer.py
                  vocoder.py
                  karaoke.py
                  ...

When importing the package, Python searches through the directories on sys.path looking for the package subdirectory.

The Â¿__init__.pyÂ¿ files are required to make Python treat the directories as containing packages; this is done to prevent directories with a common name, such as Â¿stringÂ¿, from unintentionally hiding valid modules that occur later on the module search path. In the simplest case, Â¿__init__.pyÂ¿ can just be an empty file, but it can also execute initialization code for the package or set the __all__ variable, described later.

Users of the package can import individual modules from the package, for example:

    import Sound.Effects.echo

This loads the submodule Â¿Sound.Effects.echoÂ¿. It must be referenced with its full name.

    Sound.Effects.echo.echofilter(input, output, delay=0.7,
                                  atten=4)

An alternative way of importing the submodule is:

    from Sound.Effects import echo

This also loads the submodule Â¿echoÂ¿, and makes it available without its package prefix, so it can be used as follows:

    echo.echofilter(input, output, delay=0.7, atten=4)

Yet another variation is to import the desired function or variable directly:

    from Sound.Effects.echo import echofilter

Again, this loads the submodule Â¿echoÂ¿, but this makes its function echofilter() directly available:

    echofilter(input, output, delay=0.7, atten=4)

Note that when using from package import item, the item can be either a submodule (or subpackage) of the package, or some other name defined in the package, like a function, class or variable. The import statement first tests whether the item is defined in the package; if not, it assumes it is a module and attempts to load it. If it fails to find it, an ImportError exception is raised.

Contrarily, when using syntax like import item.subitem.subsubitem, each item except for the last must be a package; the last item can be a module or a package but can't be a class or function or variable defined in the previous item.

	-> Importing From a Package

Now what happens when the user writes from Sound.Effects import *? Ideally, one would hope that this somehow goes out to the filesystem, finds which submodules are present in the package, and imports them all. Unfortunately, this operation does not work very well on Mac and Windows platforms, where the filesystem does not always have accurate information about the case of a filename! On these platforms, there is no guaranteed way to know whether a file Â¿ECHO.PYÂ¿ should be imported as a module Â¿echoÂ¿, Â¿EchoÂ¿ or Â¿ECHOÂ¿. (For example, Windows 95 has the annoying practice of showing all file names with a capitalized first letter.) The DOS 8+3 filename restriction adds another interesting problem for long module names.

The only solution is for the package author to provide an explicit index of the package. The import statement uses the following convention: if a package's Â¿__init__.pyÂ¿ code defines a list named __all__, it is taken to be the list of module names that should be imported when from package import * is encountered. It is up to the package author to keep this list up-to-date when a new version of the package is released. Package authors may also decide not to support it, if they don't see a use for importing * from their package. For example, the file Â¿Sounds/Effects/__init__.pyÂ¿ could contain the following code:

    __all__ = ["echo", "surround", "reverse"]

This would mean that from Sound.Effects import * would import the three named submodules of the Â¿SoundÂ¿ package.

If __all__ is not defined, the statement from Sound.Effects import * does not import all submodules from the package Â¿Sound.EffectsÂ¿ into the current namespace; it only ensures that the package Â¿Sound.EffectsÂ¿ has been imported (possibly running any initialization code in Â¿__init__.pyÂ¿) and then imports whatever names are defined in the package. This includes any names defined (and submodules explicitly loaded) by Â¿__init__.pyÂ¿. It also includes any submodules of the package that were explicitly loaded by previous import statements. Consider this code:

    import Sound.Effects.echo
    import Sound.Effects.surround
    from Sound.Effects import *

In this example, the echo and surround modules are imported in the current namespace because they are defined in the Â¿Sound.EffectsÂ¿ package when the from...import statement is executed. (This also works when __all__ is defined.)

Note that in general the practice of importing * from a module or package is frowned upon, since it often causes poorly readable code. However, it is okay to use it to save typing in interactive sessions, and certain modules are designed to export only names that follow certain patterns.

Remember, there is nothing wrong with using from Package import specific_submodule! In fact, this is the recommended notation unless the importing module needs to use submodules with the same name from different packages. 

		19.1.5 Manual, http://docs.python.org/dist/intro.html

		19.1.6 tags: creating package python3

		    19.1.6.1 https://packaging.python.org/en/latest/tutorials/packaging-projects/

		    19.1.6.2
		19.1.7
    19.2 PIP

        19.2.1  installation guide 
https://packaging.python.org/tutorials/installing-packages/
http://dcjtech.info/wp-content/uploads/2015/10/Pip-Cheatsheet.pdf

General Parameters Description
-v,--verbose Verbose mode (More output)
-q,--quiet Quiet mode (Less output)
-h,--help Help/Options
-V,--version Display version info
--isolated Ignore environment variables and

user configuration
--log PATH Log file
--proxy PROXY USER:PSWD@SERVER:PORT
--retries X Retry connection X times
--timeout SECONDS Try for X seconds before retry
--cache-dir DIR Cache directory
--no-cache-dir Disable cache
--disable-pip-version-check Do not check Pip version
--cert PATH Path to secondary CA bundle
--client-cert CERT Path to SSL certificate
--trusted-host HOSTNAME Consider the host trusted

List Description
pip list List packages
-o,--outdated List outdated packages
-u,--uptodate List current packages
-e,--editable List editable items
-l,--local List local virtualenv packages
--user List user-site packages
--pre Include developmental packages
-i URL,--index-url URL PyPI URL
--extra-index-url URL Additional package repos
--no-index Ignore package index
-f URL,--find-links URL Search for archives at the URL
--allow-external PKG Allow package installation
--allow-all-external Allow externally hosted packages to
be installed
--allow-unverified PKG Install insecure package
--process-dependency-links Process links for dependencies
Show Description
pip show PKG Display package info
-f, --files List package's files
Search Description
pip search KEYWORD Search PyPI for keyword
--index URL Repo to search

Uninstall Description
pip uninstall PKG Uninstall/remove package
-r FILE,--requirement FILE Uninstall packages listed in
requirements file
-y,--yes Assume “yes” for questions

Freeze Description
pip freeze Generate requirements file
-r FILE,--requirement FILE Use the order given in the file
-f URL,--find-links URL URL for finding packages
-l,--local Only list virtualenv packages
--user Only list user-site packages

Install Description
pip install PKG Install package
pip install PKG==1.0 Install specific version
pip install 'PKG>=1.0' At least, install version X
-r FILE,--requirement FILE Install listed packages in the

requirements file
-b DIR,--build DIR Directory for building packages
-t DIR,--target DIR Install in directory
-d DIR, --download DIR Download only
-U,--upgrade Update listed packages
--force-reinstall Re-install packages when updating
-I,--ignore-installed Re-install
--no-deps Do not install dependencies
--egg Install as an Egg
--compile Compile *.py to *.pyc
--no-compile Do not compile
--no-use-wheel Do not use wheels
--pre Include developmental versions
--no-clean Do not clean build directories
-i URL,--index-url URL PyPI URL
--extra-index-url URL Additional URLs
--no-index Only use --find-links URLs
-f URL,--find-links URL Parse links for archives
--allow-external PKG Install 3rd-party package
--allow-all-external Install 3rd-party packages
--allow-unverified PKG Install unverified package
--process-dependency-links Process links for dependencies

        19.2.2 creating a pip package
https://packaging.python.org/tutorials/packaging-projects/

Packaging Python Projects¶
This tutorial walks you through how to package a simple Python project. It will show you how to add the necessary files and structure to create the package, how to build the package, and how to upload it to the Python Package Index.

A simple project
This tutorial uses a simple project named example_pkg. If you are unfamiliar with Python’s modules and import packages, take a few minutes to read over the Python documentation for packages and modules. Even if you already have a project that you want to package up, we recommend following this tutorial as-is using this example package and then trying with your own package.

To create this project locally, create the following file structure:

/packaging_tutorial
  /example_pkg
    __init__.py
Once you create this structure, you’ll want to run all of the commands in this tutorial within the top-level folder - so be sure to cd packaging_tutorial.

You should also edit example_pkg/__init__.py and put the following code in there:

name = "example_pkg"
This is just so that you can verify that it installed correctly later in this tutorial and is not used by PyPI.

Creating the package files
You will now create a handful of files to package up this project and prepare it for distribution. Create the new files listed below - you will add content to them in the following steps.

/packaging_tutorial
  /example_pkg
    __init__.py
  setup.py
  LICENSE
  README.md
Creating setup.py
setup.py is the build script for setuptools. It tells setuptools about your package (such as the name and version) as well as which code files to include.

Open setup.py and enter the following content. Update the package name to include your username (for example, example-pkg-theacodes), this ensures that you have a unique package name and that your package doesn’t conflict with packages uploaded by other people following this tutorial.

import setuptools

with open("README.md", "r") as fh:
    long_description = fh.read()

setuptools.setup(
    name="example-pkg-your-username",
    version="0.0.1",
    author="Example Author",
    author_email="author@example.com",
    description="A small example package",
    long_description=long_description,
    long_description_content_type="text/markdown",
    url="https://github.com/pypa/sampleproject",
    packages=setuptools.find_packages(),
    classifiers=[
        "Programming Language :: Python :: 3",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
    ],
)
setup() takes several arguments. This example package uses a relatively minimal set:

name is the distribution name of your package. This can be any name as long as only contains letters, numbers, _ , and -. It also must not already taken on pypi.org. Be sure to update this with your username, as this ensures you won’t try to upload a package with the same name as one which already exists when you upload the package.
version is the package version see PEP 440 for more details on versions.
author and author_email are used to identify the author of the package.
description is a short, one-sentence summary of the package.
long_description is a detailed description of the package. This is shown on the package detail package on the Python Package Index. In this case, the long description is loaded from README.md which is a common pattern.
long_description_content_type tells the index what type of markup is used for the long description. In this case, it’s Markdown.
url is the URL for the homepage of the project. For many projects, this will just be a link to GitHub, GitLab, Bitbucket, or similar code hosting service.
packages is a list of all Python import packages that should be included in the distribution package. Instead of listing each package manually, we can use find_packages() to automatically discover all packages and subpackages. In this case, the list of packages will be example_pkg as that’s the only package present.
classifiers gives the index and pip some additional metadata about your package. In this case, the package is only compatible with Python 3, is licensed under the MIT license, and is OS-independent. You should always include at least which version(s) of Python your package works on, which license your package is available under, and which operating systems your package will work on. For a complete list of classifiers, see https://pypi.org/classifiers/.
There are many more than the ones mentioned here. See Packaging and distributing projects for more details.

Creating README.md
Open README.md and enter the following content. You can customize this if you’d like.

# Example Package

This is a simple example package. You can use
[Github-flavored Markdown](https://guides.github.com/features/mastering-markdown/)
to write your content.
Creating a LICENSE
It’s important for every package uploaded to the Python Package Index to include a license. This tells users who install your package the terms under which they can use your package. For help picking a license, see https://choosealicense.com/. Once you have chosen a license, open LICENSE and enter the license text. For example, if you had chosen the MIT license:

Copyright (c) 2018 The Python Packaging Authority

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
Generating distribution archives
The next step is to generate distribution packages for the package. These are archives that are uploaded to the Package Index and can be installed by pip.

Make sure you have the latest versions of setuptools and wheel installed:

python3 -m pip install --user --upgrade setuptools wheel
Tip IF you have trouble installing these, see the Installing Packages tutorial.
Now run this command from the same directory where setup.py is located:

python3 setup.py sdist bdist_wheel
This command should output a lot of text and once completed should generate two files in the dist directory:

dist/
  example_pkg_your_username-0.0.1-py3-none-any.whl
  example_pkg_your_username-0.0.1.tar.gz
Note If you run into trouble here, please copy the output and file an issue over on packaging problems and we’ll do our best to help you!
The tar.gz file is a source archive whereas the .whl file is a built distribution. Newer pip versions preferentially install built distributions, but will fall back to source archives if needed. You should always upload a source archive and provide built archives for the platforms your project is compatible with. In this case, our example package is compatible with Python on any platform so only one built distribution is needed.

Uploading the distribution archives
Finally, it’s time to upload your package to the Python Package Index!

The first thing you’ll need to do is register an account on Test PyPI. Test PyPI is a separate instance of the package index intended for testing and experimentation. It’s great for things like this tutorial where we don’t necessarily want to upload to the real index. To register an account, go to https://test.pypi.org/account/register/ and complete the steps on that page. You will also need to verify your email address before you’re able to upload any packages. For more details on Test PyPI, see Using TestPyPI.

Now that you are registered, you can use twine to upload the distribution packages. You’ll need to install Twine:

python3 -m pip install --user --upgrade twine
Once installed, run Twine to upload all of the archives under dist:

python3 -m twine upload --repository-url https://test.pypi.org/legacy/ dist/*
You will be prompted for the username and password you registered with Test PyPI. After the command completes, you should see output similar to this:

Uploading distributions to https://test.pypi.org/legacy/
Enter your username: [your username]
Enter your password:
Uploading example_pkg_your_username-0.0.1-py3-none-any.whl
100%|█████████████████████| 4.65k/4.65k [00:01<00:00, 2.88kB/s]
Uploading example_pkg_your_username-0.0.1.tar.gz
100%|█████████████████████| 4.25k/4.25k [00:01<00:00, 3.05kB/s]
Once uploaded your package should be viewable on TestPyPI, for example, https://test.pypi.org/project/example-pkg-your-username

Installing your newly uploaded package
You can use pip to install your package and verify that it works. Create a new virtualenv (see Installing Packages for detailed instructions) and install your package from TestPyPI:

python3 -m pip install --index-url https://test.pypi.org/simple/ --no-deps example-pkg-your-username
Make sure to specify your username in the package name!

pip should install the package from Test PyPI and the output should look something like this:

Collecting example-pkg-your-username
  Downloading https://test-files.pythonhosted.org/packages/.../example-pkg-your-username-0.0.1-py3-none-any.whl
Installing collected packages: example-pkg-your-username
Successfully installed example-pkg-your-username-0.0.1
Note This example uses --index-url flag to specify TestPyPI instead of live PyPI. Additionally, it specifies --no-deps. Since TestPyPI doesn’t have the same packages as the live PyPI, it’s possible that attempting to install dependencies may fail or install something unexpected. While our example package doesn’t have any dependencies, it’s a good practice to avoid installing dependencies when using TestPyPI.
You can test that it was installed correctly by importing the module and referencing the name property you put in __init__.py earlier.

Run the Python interpreter (make sure you’re still in your virtualenv):

python
And then import the module and print out the name property. This should be the same regardless of what you name you gave your distribution package in setup.py (in this case, example-pkg-your-username) because your import package is example_pkg.

>>>
>>> import example_pkg
>>> example_pkg.name
'example_pkg'
Next steps
Congratulations, you’ve packaged and distributed a Python project! ✨ 🍰 ✨

Keep in mind that this tutorial showed you how to upload your package to Test PyPI, which isn’t a permanent storage. The Test system occasionally deletes packages and accounts. It is best to use Test PyPI for testing and experiments like this tutorial.

When you are ready to upload a real package to the Python Package Index you can do much the same as you did in this tutorial, but with these important differences:

Choose a memorable and unique name for your package. You don’t have to append your username as you did in the tutorial.
Register an account on https://pypi.org - note that these are two separate servers and the login details from the test server are not shared with the main server.
Use twine upload dist/* to upload your package and enter your credentials for the account you registered on the real PyPI.
Install your package from the real PyPI using pip install [your-package].
At this point if you want to read more on packaging Python libraries here are some things you can do:

Read more about using setuptools to package libraries in Packaging and distributing projects.
Read about Packaging binary extensions.
Consider alternatives to setuptools such as flit, hatch, and poetry.

        19.2.3

    19.3 pipenv
$ brew install pipenv

        19.3.1  Basic 
https://docs.pipenv.org/en/latest/basics/
Basic Usage of Pipenv
https://farm4.staticflickr.com/3931/33173826122_b7ee8f1a26_k_d.jpg
This document covers some of Pipenv’s more basic features.

☤ Example Pipfile & Pipfile.lock
Here is a simple example of a Pipfile and the resulting Pipfile.lock.

Example Pipfile
[[source]]
url = "https://pypi.python.org/simple"
verify_ssl = true
name = "pypi"

[packages]
requests = "*"


[dev-packages]
pytest = "*"
Example Pipfile.lock
{
    "_meta": {
        "hash": {
            "sha256": "8d14434df45e0ef884d6c3f6e8048ba72335637a8631cc44792f52fd20b6f97a"
        },
        "host-environment-markers": {
            "implementation_name": "cpython",
            "implementation_version": "3.6.1",
            "os_name": "posix",
            "platform_machine": "x86_64",
            "platform_python_implementation": "CPython",
            "platform_release": "16.7.0",
            "platform_system": "Darwin",
            "platform_version": "Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64",
            "python_full_version": "3.6.1",
            "python_version": "3.6",
            "sys_platform": "darwin"
        },
        "pipfile-spec": 5,
        "requires": {},
        "sources": [
            {
                "name": "pypi",
                "url": "https://pypi.python.org/simple",
                "verify_ssl": true
            }
        ]
    },
    "default": {
        "certifi": {
            "hashes": [
                "sha256:54a07c09c586b0e4c619f02a5e94e36619da8e2b053e20f594348c0611803704",
                "sha256:40523d2efb60523e113b44602298f0960e900388cf3bb6043f645cf57ea9e3f5"
            ],
            "version": "==2017.7.27.1"
        },
        "chardet": {
            "hashes": [
                "sha256:fc323ffcaeaed0e0a02bf4d117757b98aed530d9ed4531e3e15460124c106691",
                "sha256:84ab92ed1c4d4f16916e05906b6b75a6c0fb5db821cc65e70cbd64a3e2a5eaae"
            ],
            "version": "==3.0.4"
        },
        "idna": {
            "hashes": [
                "sha256:8c7309c718f94b3a625cb648ace320157ad16ff131ae0af362c9f21b80ef6ec4",
                "sha256:2c6a5de3089009e3da7c5dde64a141dbc8551d5b7f6cf4ed7c2568d0cc520a8f"
            ],
            "version": "==2.6"
        },
        "requests": {
            "hashes": [
                "sha256:6a1b267aa90cac58ac3a765d067950e7dbbf75b1da07e895d1f594193a40a38b",
                "sha256:9c443e7324ba5b85070c4a818ade28bfabedf16ea10206da1132edaa6dda237e"
            ],
            "version": "==2.18.4"
        },
        "urllib3": {
            "hashes": [
                "sha256:06330f386d6e4b195fbfc736b297f58c5a892e4440e54d294d7004e3a9bbea1b",
                "sha256:cc44da8e1145637334317feebd728bd869a35285b93cbb4cca2577da7e62db4f"
            ],
            "version": "==1.22"
        }
    },
    "develop": {
        "py": {
            "hashes": [
                "sha256:2ccb79b01769d99115aa600d7eed99f524bf752bba8f041dc1c184853514655a",
                "sha256:0f2d585d22050e90c7d293b6451c83db097df77871974d90efd5a30dc12fcde3"
            ],
            "version": "==1.4.34"
        },
        "pytest": {
            "hashes": [
                "sha256:b84f554f8ddc23add65c411bf112b2d88e2489fd45f753b1cae5936358bdf314",
                "sha256:f46e49e0340a532764991c498244a60e3a37d7424a532b3ff1a6a7653f1a403a"
            ],
            "version": "==3.2.2"
        }
    }
}
☤ General Recommendations & Version Control
Generally, keep both Pipfile and Pipfile.lock in version control.
Do not keep Pipfile.lock in version control if multiple versions of Python are being targeted.
Specify your target Python version in your Pipfile’s [requires] section. Ideally, you should only have one target Python version, as this is a deployment tool.
pipenv install is fully compatible with pip install syntax, for which the full documentation can be found here.
Note that the Pipfile uses the TOML Spec.
☤ Example Pipenv Workflow
Clone / create project repository:

$ cd myproject
Install from Pipfile, if there is one:

$ pipenv install
Or, add a package to your new project:

$ pipenv install <package>
This will create a Pipfile if one doesn’t exist. If one does exist, it will automatically be edited with the new package you provided.

Next, activate the Pipenv shell:

$ pipenv shell
$ python --version
This will spawn a new shell subprocess, which can be deactivated by using exit.

☤ Example Pipenv Upgrade Workflow
Find out what’s changed upstream: $ pipenv update --outdated.
Upgrade packages, two options:
Want to upgrade everything? Just do $ pipenv update.
Want to upgrade packages one-at-a-time? $ pipenv update <pkg> for each outdated package.
☤ Importing from requirements.txt
If you only have a requirements.txt file available when running pipenv install, pipenv will automatically import the contents of this file and create a Pipfile for you.

You can also specify $ pipenv install -r path/to/requirements.txt to import a requirements file.

If your requirements file has version numbers pinned, you’ll likely want to edit the new Pipfile to remove those, and let pipenv keep track of pinning. If you want to keep the pinned versions in your Pipfile.lock for now, run pipenv lock --keep-outdated. Make sure to upgrade soon!

☤ Specifying Versions of a Package
You can specify versions of a package using the Semantic Versioning scheme (i.e. major.minor.micro).

For example, to install requests you can use:

$ pipenv install requests~=1.2   # equivalent to requests~=1.2.0
Pipenv will install version 1.2 and any minor update, but not 2.0.

This will update your Pipfile to reflect this requirement, automatically.

In general, Pipenv uses the same specifier format as pip. However, note that according to PEP 440 , you can’t use versions containing a hyphen or a plus sign.

To make inclusive or exclusive version comparisons you can use:

$ pipenv install "requests>=1.4"   # will install a version equal or larger than 1.4.0
$ pipenv install "requests<=2.13"  # will install a version equal or lower than 2.13.0
$ pipenv install "requests>2.19"   # will install 2.19.1 but not 2.19.0
Note
The use of double quotes around the package and version specification (i.e. "requests>2.19") is highly recommended to avoid issues with Input and output redirection in Unix-based operating systems.

The use of ~= is preferred over the == identifier as the latter prevents pipenv from updating the packages:

$ pipenv install "requests~=2.2"  # locks the major version of the package (this is equivalent to using ==2.*)
To avoid installing a specific version you can use the != identifier.

For an in depth explanation of the valid identifiers and more complex use cases check the relevant section of PEP-440.

☤ Specifying Versions of Python
To create a new virtualenv, using a specific version of Python you have installed (and on your PATH), use the --python VERSION flag, like so:

Use Python 3:

$ pipenv --python 3
Use Python3.6:

$ pipenv --python 3.6
Use Python 2.7.14:

$ pipenv --python 2.7.14
When given a Python version, like this, Pipenv will automatically scan your system for a Python that matches that given version.

If a Pipfile hasn’t been created yet, one will be created for you, that looks like this:

[[source]]
url = "https://pypi.python.org/simple"
verify_ssl = true

[dev-packages]

[packages]

[requires]
python_version = "3.6"
Note
The inclusion of [requires] python_version = "3.6" specifies that your application requires this version of Python, and will be used automatically when running pipenv install against this Pipfile in the future (e.g. on other machines). If this is not true, feel free to simply remove this section.

If you don’t specify a Python version on the command–line, either the [requires] python_full_version or python_version will be selected automatically, falling back to whatever your system’s default python installation is, at time of execution.

☤ Editable Dependencies (e.g. -e . )
You can tell Pipenv to install a path as editable — often this is useful for the current working directory when working on packages:

$ pipenv install --dev -e .

$ cat Pipfile
...
[dev-packages]
"e1839a8" = {path = ".", editable = true}
...
Note
All sub-dependencies will get added to the Pipfile.lock as well. Sub-dependencies are not added to the Pipfile.lock if you leave the -e option out.

☤ Environment Management with Pipenv
The three primary commands you’ll use in managing your pipenv environment are $ pipenv install, $ pipenv uninstall, and $ pipenv lock.

$ pipenv install
$ pipenv install is used for installing packages into the pipenv virtual environment and updating your Pipfile.

Along with the basic install command, which takes the form:

$ pipenv install [package names]
The user can provide these additional parameters:

--two — Performs the installation in a virtualenv using the system python2 link.
--three — Performs the installation in a virtualenv using the system python3 link.
--python — Performs the installation in a virtualenv using the provided Python interpreter.
Warning
None of the above commands should be used together. They are also destructive and will delete your current virtualenv before replacing it with an appropriately versioned one.

Note
The virtualenv created by Pipenv may be different from what you were expecting. Dangerous characters (i.e. $`!*@" as well as space, line feed, carriage return, and tab) are converted to underscores. Additionally, the full path to the current folder is encoded into a “slug value” and appended to ensure the virtualenv name is unique.

--dev — Install both develop and default packages from Pipfile.
--system — Use the system pip command rather than the one from your virtualenv.
--ignore-pipfile — Ignore the Pipfile and install from the Pipfile.lock.
--skip-lock — Ignore the Pipfile.lock and install from the Pipfile. In addition, do not write out a Pipfile.lock reflecting changes to the Pipfile.
$ pipenv uninstall
$ pipenv uninstall supports all of the parameters in pipenv install, as well as two additional options, --all and --all-dev.

--all — This parameter will purge all files from the virtual environment, but leave the Pipfile untouched.
--all-dev — This parameter will remove all of the development packages from the virtual environment, and remove them from the Pipfile.
$ pipenv lock
$ pipenv lock is used to create a Pipfile.lock, which declares all dependencies (and sub-dependencies) of your project, their latest available versions, and the current hashes for the downloaded files. This ensures repeatable, and most importantly deterministic, builds.

☤ About Shell Configuration
Shells are typically misconfigured for subshell use, so $ pipenv shell --fancy may produce unexpected results. If this is the case, try $ pipenv shell, which uses “compatibility mode”, and will attempt to spawn a subshell despite misconfiguration.

A proper shell configuration only sets environment variables like PATH during a login session, not during every subshell spawn (as they are typically configured to do). In fish, this looks like this:

if status --is-login
    set -gx PATH /usr/local/bin $PATH
end
You should do this for your shell too, in your ~/.profile or ~/.bashrc or wherever appropriate.

Note
The shell launched in interactive mode. This means that if your shell reads its configuration from a specific file for interactive mode (e.g. bash by default looks for a ~/.bashrc configuration file for interactive mode), then you’ll need to modify (or create) this file.

If you experience issues with $ pipenv shell, just check the PIPENV_SHELL environment variable, which $ pipenv shell will use if available. For detail, see configuration-with-environment-variables.

☤ A Note about VCS Dependencies
You can install packages with pipenv from git and other version control systems using URLs formatted according to the following rule:

<vcs_type>+<scheme>://<location>/<user_or_organization>/<repository>@<branch_or_tag>#egg=<package_name>
The only optional section is the @<branch_or_tag> section. When using git over SSH, you may use the shorthand vcs and scheme alias git+git@<location>:<user_or_organization>/<repository>@<branch_or_tag>#<package_name>. Note that this is translated to git+ssh://git@<location> when parsed.

Note that it is strongly recommended that you install any version-controlled dependencies in editable mode, using pipenv install -e, in order to ensure that dependency resolution can be performed with an up to date copy of the repository each time it is performed, and that it includes all known dependencies.

Below is an example usage which installs the git repository located at https://github.com/requests/requests.git from tag v2.20.1 as package name requests:

$ pipenv install -e git+https://github.com/requests/requests.git@v2.20.1#egg=requests
Creating a Pipfile for this project...
Installing -e git+https://github.com/requests/requests.git@v2.20.1#egg=requests...
[...snipped...]
Adding -e git+https://github.com/requests/requests.git@v2.20.1#egg=requests to Pipfile's [packages]...
[...]

$ cat Pipfile
[packages]
requests = {git = "https://github.com/requests/requests.git", editable = true, ref = "v2.20.1"}
Valid values for <vcs_type> include git, bzr, svn, and hg. Valid values for <scheme> include http, https, ssh, and file. In specific cases you also have access to other schemes: svn may be combined with svn as a scheme, and bzr can be combined with sftp and lp.

You can read more about pip’s implementation of VCS support here. For more information about other options available when specifying VCS dependencies, please check the Pipfile spec.

☤ Pipfile.lock Security Features
Pipfile.lock takes advantage of some great new security improvements in pip. By default, the Pipfile.lock will be generated with the sha256 hashes of each downloaded package. This will allow pip to guarantee you’re installing what you intend to when on a compromised network, or downloading dependencies from an untrusted PyPI endpoint.

We highly recommend approaching deployments with promoting projects from a development environment into production. You can use pipenv lock to compile your dependencies on your development environment and deploy the compiled Pipfile.lock to all of your production environments for reproducible builds.

        19.3.2
    19.4
20. Tips and Tricks

	20.1 Running in background instead of swapning thread
	There is an issue with running os.system inside a thread so on linux systems add "&" to os.system call (e.g. os.system("ls&") to run in bg.
			in windows add "start", e.g. os.system("start calc")
			example:
>>> if os.name == 'nt':
...     os.system("start calc")


	20.2 Printing without newline (print 'a',) prints a space, how to remove?

 33 down vote favorite
4
	

I have this code:

>>> for i in xrange(20):
...     print 'a',
... 
a a a a a a a a a a a a a a a a a a a a

I want to output 'a', without ' ' like this:

aaaaaaaaaaaaaaaaaaaa

Is it possible?



I'm surprised that no-one's yet mentioned "".join("a" for i in xrange(20)). (It's much more flexible than just doing "a" * 20, as I assume it's a simplfied example). 


There are a number of ways of achieving your result. If you're just wanting a solution for your case, use string multiplication as @Ant mentions. This is only going to work if each of your print statements prints the same string. Note that it works for multiplication of any length string (e.g. 'foo' * 20 works).

>>> print 'a' * 20
aaaaaaaaaaaaaaaaaaaa

If you want to do this in general, build up a string and then print it once. This will consume a bit of memory for the string, but only make a single call to print. Note that string concatenation using += is now linear in the size of the string you're concatenating so this will be fast.

>>> for i in xrange(20):
...     s += 'a'
... 
>>> print s
aaaaaaaaaaaaaaaaaaaa

Or you can do it more directly using sys.stdout.write(), which print is a wrapper around. This will write only the raw string you give it, without any formatting. Note that no newline is printed even at the end of the 20 as.

>>> for i in xrange(20):
...     sys.stdout.write('a')
... 
aaaaaaaaaaaaaaaaaaaa>>> 

Python 3 changes the print statement into a print() function, which allows you to set an end parameter. You can use it in >=2.6 by importing from __future__. I'd avoid this in any serious 2.x code though, as it will be a little confusing for those who have never used 3.x. However, it should give you a taste of some of the goodness 3.x brings.

>>> from __future__ import print_function
>>> for i in xrange(20):
...     print('a', end='')
... 
aaaaaaaaaaaaaaaaaaaa>>> 




	20.3 Print with colors, bold etc
	    20.3 Print with colors, bold etc

	        20.3.1 My example
$ ipython
In [1]: class color:
   ...:    PURPLE = '\033[95m'
   ...:    CYAN = '\033[96m'
   ...:    DARKCYAN = '\033[36m'
   ...:    BLUE = '\033[94m'
   ...:    GREEN = '\033[92m'
   ...:    YELLOW = '\033[93m'
   ...:    RED = '\033[91m'
   ...:    BOLD = '\033[1m'
   ...:    UNDERLINE = '\033[4m'
   ...:    END = '\033[0m'
   ...: 

In [2]: print(color.BOLD+'aaa'+color.END)
aaa

In [3]: print(color.PURPLE+'aaa'+color.END)
aaa

In [4]: print(color.DARKCYAN+'aaa'+color.END)
aaa

In [5]: print(color.UNDERLINE+color.BOLD+color.PURPLE+'aaa'+color.END)
aaa

Just run to see it take effect

	        20.3.2

	20.4 
21. Information from Learning Python book (3rd edition).
Note, This information is extracted here for my personal usage only as a quick reference after having read the book on safari.

	21.1 System Programming

		21.1.1 overview

		-> Learn the number of attributes in system related packages. 
>>> import sys, os
>>> len(dir(sys))          # 56 attributes
56
>>> len(dir(os))           # 118 on Windows, more on Unix
118
>>> len(dir(os.path))      # a nested module within os
43

		Its possible to use dir to learn what API the package exposes.

		21.1.2 Python System Modules

Most system-level interfaces in Python are shipped in just two modules: sys and os. That's somewhat oversimplified; other standard modules belong to this domain too. Among them are the following:

			In principle at least, sys exports components related to the Python interpreter itself (e.g., the module search path), and os contains variables and functions that map to the operating system on which Python is run. In practice, this distinction may not always seem clear-cut (e.g., the standard input and output streams show up in sys, but they are arguably tied to operating system paradigms). The good news is that you'll soon use the tools in these modules so often that their locations will be permanently stamped on your memory.

The os module also attempts to provide a portable programming interface to the underlying operating system; its functions may be implemented differently on different platforms, but to Python scripts, they look the same everywhere. In addition, the os module exports a nested submodule, os.path, which provides a portable interface to file and directory processing tools.

glob For filename expansion

socket For network connections and Inter-Process Communication (IPC)

thread and queue For concurrent threads

time For accessing system time details

fcntl For low-level file control

			21.1.2.1 Module Documentation Sources

As you can probably deduce from the preceding paragraphs, learning to write system scripts in Python is mostly a matter of learning about Python's system modules. Luckily, there are a variety of information sources to make this task easierfrom module attributes to published references and books.

For instance, if you want to know everything that a built-in module exports, you can read its library manual entry, study its source code (Python is open source software, after all), or fetch its attribute list and documentation string interactively. Let's import sys and see what it has:

C:\...\PP3E\System> python
>>> import sys
>>> dir(sys)
['__displayhook__', '__doc__', '__excepthook__', '__name__', '__stderr__',
'__stdin__', '__stdout__', '_getframe', 'api_version', 'argv',
'builtin_module_names', 'byteorder', 'call_tracing', 'callstats', 'copyright',
'displayhook', 'dllhandle', 'exc_clear', 'exc_info', 'exc_traceback', 'exc_type',
'exc_value', 'excepthook', 'exec_prefix', 'executable', 'exit', 'exitfunc',
'getcheckinterval', 'getdefaultencoding', 'getfilesystemencoding',
'getrecursionlimit', 'getrefcount', 'getwindowsversion', 'hexversion', 'maxint',
'maxunicode', 'meta_path', 'modules', 'path', 'path_hooks', 'path_importer_cache',
'platform', 'prefix', 'ps1', 'ps2', 'setcheckinterval', 'setprofile',
'setrecursionlimit', 'settrace', 'stderr', 'stdin', 'stdout', 'version',
'version_info', 'warnoptions', 'winver']


					  


The dir function simply returns a list containing the string names of all the attributes in any object with attributes; it's a handy memory jogger for modules at the interactive prompt. For example, we know there is something called sys.version, because the name version came back in the dir result. If that's not enough, we can always consult the __doc__ string of built-in modules:

>>> sys.__doc__ 
"This module provides access to some objects used or maintained by the\ninterpreter
and to functions that interact strongly with the interpreter.\n\nDynamic
objects:\n\nargv -- command line arguments; argv[0] is the script pathname if
 known\npath -- module
search path; path[0] is the script directory, else ''\nmodules
...
 ...lots of text deleted here...
...
"


				21.1.2.2 Paging Documentation Strings

The __doc__ built-in attribute usually contains a string of documentation, but it may look a bit weird when displayed this wayit's one long string with embedded end-line characters that print as \n, not as a nice list of lines. To format these strings for a more humane display, you can simply use a print statement:

>>> print sys.__doc__ 
This module provides access to some objects used or maintained by the
interpreter and to functions that interact strongly with the interpreter.

Dynamic objects:

argv -- command line arguments; argv[0] is the script pathname if known
...
 ...lots of lines deleted here...
...


The print statement, unlike interactive displays, interprets end-line characters correctly. Unfortunately, print doesn't, by itself, do anything about scrolling or paging and so can still be unwieldy on some platforms. Tools such as the built-in help function can do better:

>>> help(sys) 
Help on built-in module sys:

NAME
    sys

FILE
    (built-in)

MODULE DOCS
    http://www.python.org/doc/current/lib/module-sys.html

DESCRIPTION
    This module provides access to some objects used or maintained by the
    interpreter and to functions that interact strongly with the interpreter.

    Dynamic objects:

    argv -- command line arguments; argv[0] is the script pathname if known
...
 ...lots of lines deleted here...
...


					  


The help function is one interface provided by the PyDoc systemcode that ships with Python and renders documentation (documentation strings, as well as structural details) related to an object in a formatted way. The format is either like a Unix manpage, which we get for help, or an HTML page, which is more grandiose. It's a handy way to get basic information when working interactively, and it's a last resort before falling back on manuals and books. It is also fairly fixed in the way it displays information; although it attempts to page the display in some contexts, its page size isn't quite right on some of the machines I use. When I want more control over the way help text is printed, I usually use a utility script of my own, like the one in more.py
more.py
#########################################################
# split and interactively page a string or file of text;
#########################################################

def more(text, numlines=15):
    lines = text.split('\n')
    while lines:
        chunk = lines[:numlines]
        lines = lines[numlines:]
        for line in chunk: print line
        if lines and raw_input('More?') not in ['y', 'Y']: break

if __name__ == '__main__':
    import sys                              # when run, not imported
    more(open(sys.argv[1]).read( ), 10)     # page contents of file on cmdline


					  


The meat of this file is its more function, and if you know any Python at all, it should be fairly straightforward. It simply splits up a string around end-line characters, and then slices off and displays a few lines at a time (15 by default) to avoid scrolling off the screen. A slice expression, lines[:15], gets the first 15 items in a list, and lines[15:] gets the rest; to show a different number of lines each time, pass a number to the numlines argument (e.g., the last line in Example 3-1 passes 10 to the numlines argument of the more function).

The split string object method call that this script employs returns a list of substrings (e.g., ["line", "line",...]). In recent Python releases, a new splitlines method does similar work:

>>> line = 'aaa\nbbb\nccc\n'
>>> line.split('\n')
['aaa', 'bbb', 'ccc', '']
>>> line.splitlines( )
['aaa', 'bbb', 'ccc']

As we'll see in the next chapter, the end-of-line character is always \n (which stands for a byte having a binary value of 10) within a Python script, no matter what platform it is run upon. (If you don't already know why this matters, DOS \r characters are dropped when read.)

				21.1.2.3 Introducing String Methods

Now, this is a simple Python program, but it already brings up three important topics that merit quick detours here: it uses string methods, reads from a file, and is set up to be run or imported. Python string methods are not a system-related tool per se, but they see action in most Python programs. In fact, they are going to show up throughout this chapter as well as those that follow, so here is a quick review of some of the more useful tools in this set. String methods include calls for searching and replacing:

>>> str = 'xxxSPAMxxx'
>>> str.find('SPAM')                             # return first offset
3
>>> str = 'xxaaxxaa'
>>> str.replace('aa', 'SPAM')                    # global replacement
'xxSPAMxxSPAM'

>>> str = '\t  Ni\n'
>>> str.strip( )                                  # remove whitespace
'Ni'


The find call returns the offset of the first occurrence of a substring, and replace does global search and replacement. Like all string operations, replace returns a new string instead of changing its subject in-place (recall that strings are immutable). With these methods, substrings are just strings; in Chapter 21, we'll also meet a module called re that allows regular expression patterns to show up in searches and replacements.

String methods also provide functions that are useful for things such as case conversions, and a standard library module named string defines some useful preset variables, among other things:

>>> str = 'SHRUBBERY'
>>> str.lower( )                            # case converters
'shrubbery'

>>> str.isalpha( )                          # content tests
True
>>> str.isdigit( )
False

>>> import string                          # case constants
>>> string.lowercase
'abcdefghijklmnopqrstuvwxyz'


There are also methods for splitting up strings around a substring delimiter and putting them back together with a substring in between. We'll explore these tools later in this book, but as an introduction, here they are at work:

>>> str = 'aaa,bbb,ccc'
>>> str.split(',')                          # split into substrings list
['aaa', 'bbb', 'ccc']

>>> str = 'a  b\nc\nd'
>>> str.split( )                             # default delimiter: whitespace
['a', 'b', 'c', 'd']

>>> delim = 'NI'
>>> delim.join(['aaa', 'bbb', 'ccc'])       # join substrings list
'aaaNIbbbNIccc'

>>> ' '.join(['A', 'dead', 'parrot'])       # add a space between
'A dead parrot'

>>> chars = list('Lorreta')                 # covert to characters list
>>> chars
['L', 'o', 'r', 'r', 'e', 't', 'a']
>>> chars.append('!')
>>> ''.join(chars)                          # to string: empty delimiter
'Lorreta!'


These calls turn out to be surprisingly powerful. For example, a line of data columns separated by tabs can be parsed into its columns with a single split call; the more.py script uses it to split a string into a list of line strings. In fact, we can emulate the replace call we saw earlier in this section with a split/join combination:

>>> str = 'xxaaxxaa'
>>> 'SPAM'.join(str.split('aa'))            # replace, the hard way
'xxSPAMxxSPAM'


For future reference, also keep in mind that Python doesn't automatically convert strings to numbers, or vice versa; if you want to use one as you would use the other, you must say so with manual conversions:

>>> int("42"), eval("42")                   # string to int conversions
(42, 42)

>>> str(42), repr(42), ("%d" % 42)          # int to string conversions
('42', '42', '42')

>>> "42" + str(1), int("42") + 1            # concatenation, addition
('421', 43)


In the last command here, the first expression triggers string concatenation (since both sides are strings), and the second invokes integer addition (because both objects are numbers). Python doesn't assume you meant one or the other and convert automatically; as a rule of thumb, Python tries to avoid magic whenever possible. String tools will be covered in more detail later in this book (in fact, they get a full chapter in Part V), but be sure to also see the library manual for additional string method tools.


				21.1.2.4  File Operation Basics

The more.py script also opens the external file whose name is listed on the command line using the built-in open function, and reads that file's text into memory all at once with the file object read method. Since file objects returned by open are part of the core Python language itself, I assume that you have at least a passing familiarity with them at this point in the text. But just in case you've flipped to this chapter early on in your Pythonhood, the calls:

open('file').read( )            # read entire file into string
open('file').read(N)           # read next N bytes into string
open('file').readlines( )       # read entire file into line strings list
open('file').readline( )        # read next line, through '\n'


load a file's contents into a string, load a fixed-size set of bytes into a string, load a file's contents into a list of line strings, and load the next line in the file into a string, respectively. As we'll see in a moment, these calls can also be applied to shell commands in Python to read their output. File objects also have write methods for sending strings to the associated file. File-related topics are covered in depth in the next chapter, but making an output file and reading it back is easy in Python:

>>> file = open('spam.txt', 'w')        # create file spam.txt
>>> file.write(('spam' * 5) + '\n')
>>> file.close( )

>>> file = open('spam.txt')             # or open('spam.txt').read( )
>>> text = file.read( )
>>> text
'spamspamspamspamspam\n'

				21.1.2.5  Using Programs in Two Ways

The last few lines in the more.py file also introduce one of the first big concepts in shell tool programming. They instrument the file to be used in either of two ways: as a script or as a library. Every Python module has a built-in __name__ variable that Python sets to the __main__ string only when the file is run as a program, not when it's imported as a library. Because of that, the more function in this file is executed automatically by the last line in the file when this script is run as a top-level program, not when it is imported elsewhere. This simple trick turns out to be one key to writing reusable script code: by coding program logic as functions rather than as top-level code, you can also import and reuse it in other scripts.

The upshot is that we can run more.py by itself or import and call its more function elsewhere. When running the file as a top-level program, we list on the command line the name of a file to be read and paged: as I'll describe in depth later in this chapter, words typed in the command that is used to start a program show up in the built-in sys.argv list in Python. For example, here is the script file in action, paging itself (be sure to type this command line in your PP3E\System directory, or it won't find the input file; more on command lines later):

C:\...\PP3E\System>python more.py more.py
#########################################################
# split and interactively page a string or file of text;
#########################################################

def more(text, numlines=15):
    lines = text.split('\n')
    while lines:
        chunk = lines[:numlines]
        lines = lines[numlines:]
        for line in chunk: print line
More?y
        if lines and raw_input('More?') not in ['y', 'Y']: break

if __name__ == '__main__':
    import sys                             # when run, not imported
    more(open(sys.argv[1]).read( ), 10)     # page contents of file on cmdline


When the more.py file is imported, we pass an explicit string to its more function, and this is exactly the sort of utility we need for documentation text. Running this utility on the sys module's documentation string gives us a bit more information in human-readable form about what's available to scripts:

C:\...\PP3E\System> python
>>> from more import more
>>> import sys
>>> more(sys.__doc__)
This module provides access to some objects used or maintained by the
interpreter and to functions that interact strongly with the interpreter.

Dynamic objects:

argv -- command line arguments; argv[0] is the script pathname if known
path -- module search path; path[0] is the script directory, else ''
modules -- dictionary of loaded modules

displayhook -- called to show results in an interactive session
excepthook -- called to handle any uncaught exception other than SystemExit
  To customize printing in an interactive session or to install a custom
  top-level exception handler, assign other functions to replace these.

exitfunc -- if sys.exitfunc exists, this routine is called when Python exits
More?

Pressing "y" or "Y" here makes the function display the next few lines of documentation, and then prompt again, unless you've run past the end of the lines list. Try this on your own machine to see what the rest of the module's documentation string looks like.

				21.1.2.6   Python Library Manuals

If that still isn't enough detail, your next step is to read the Python library manual's entry for sys to get the full story. All of Python's standard manuals ship as HTML pages, so you should be able to read them in any web browser you have on your computer. They are installed with Python on Windows, but here are a few simple pointers:

    *

      On Windows, click the Start button, pick Programs, select the Python entry there, and then choose the manuals item. The manuals should magically appear on your display within a browser like Internet Explorer. As of Python 2.4, the manuals are provided as a Windows help file and so support searching and navigation.
    *

      On Linux, you may be able to click on the manuals' entries in a file explorer, or start your browser from a shell command line and navigate to the library manual's HTML files on your machine.
    *

      If you can't find the manuals on your computer, you can always read them online. Go to Python's web site at http://www.python.org and follow the documentation links.

However you get started, be sure to pick the Library manual for things such as sys; Python's standard manual set also includes a short tutorial, language reference, extending references, and more.

			21.1.2.2 sys module

				21.1.2.2.1 Platforms and Versions

Like most modules, sys includes both informational names and functions that take action. For instance, its attributes give us the name of the underlying operating system on which the platform code is running, the largest possible integer on this machine, and the version number of the Python interpreter running our code:

C:\...\PP3E\System>python
>>> import sys
>>> sys.platform, sys.maxint, sys.version
('win32', 2147483647, '2.4 (#60, Nov 30 2004, 11:49:19) [MSC v.1310 32 bit (Intel)]')
>>>
>>> if sys.platform[:3] == 'win': print 'hello windows'
...
hello windows

If you have code that must act differently on different machines, simply test the sys.platform string as done here; although most of Python is cross-platform, nonportable tools are usually wrapped in if tests like the one here. For instance, we'll see later that today's program launch and low-level console interaction tools vary per platformsimply test sys.platform to pick the right tool for the machine on which your script is running.

				21.1.2.2.2 The Module Search Path

The sys module also lets us inspect the module search path both interactively and within a Python program. sys.path is a list of strings representing the true search path in a running Python interpreter. When a module is imported, Python scans this list from left to right, searching for the module's file on each directory named in the list. Because of that, this is the place to look to verify that your search path is really set as intended.[*]

    [*] It's not impossible that Python sees PYTHONPATH differently than you do. A syntax error in your system shell configuration files may botch the setting of PYTHONPATH, even if it looks fine to you. On Windows, for example, if a space appears around the = of a DOS set command in your autoexec.bat file (e.g., set NAME = VALUE), you will actually set NAME to an empty string, not to VALUE!

The sys.path list is simply initialized from your PYTHONPATH settingthe content of any .pth path files located in Python's directories on your machine plus system defaultswhen the interpreter is first started up. In fact, if you inspect sys.path interactively, you'll notice quite a few directories that are not on your PYTHONPATHsys.path also includes an indicator for the script's home directory (an empty stringsomething I'll explain in more detail after we meet os.getcwd) and a set of standard library directories that may vary per installation:

>>> sys.path 
['', 'C:\\PP3rdEd\\Examples',  ...plus standard paths deleted... ]


Surprisingly, sys.path can actually be changed by a program, too. A script can use list operations such as append, del, and the like to configure the search path at runtime to include all the source directories to which it needs access. Python always uses the current sys.path setting to import, no matter what you've changed it to:

>>> sys.path.append(r'C:\mydir') 
>>> sys.path 
['', 'C:\\PP3rdEd\\Examples',  ...more deleted..., 'C:\\mydir']


Changing sys.path directly like this is an alternative to setting your PYTHONPATH shell variable, but not a very good one. Changes to sys.path are retained only until the Python process ends, and they must be remade every time you start a new Python program or session. However, some types of programs (e.g., scripts that run on a web server) may not be able to depend on PYTHONPATH settings; such scripts can instead configure sys.path on startup to include all the directories from which they will need to import modules.

Note on Windows Directory Paths

Because backslashes normally introduce escape code sequences in Python strings, Windows users should be sure to either double up on backslashes when using them in DOS directory path strings (e.g., in "C:\\dir", \\ is an escape sequence that really means \), or use raw string constants to retain backslashes literally (e.g., r"C:\dir").

If you inspect directory paths on Windows (as in the sys.path interaction listing), Python prints double \\ to mean a single \. Technically, you can get away with a single \ in a string if it is followed by a character Python does not recognize as the rest of an escape sequence, but doubles and raw strings are usually easier than memorizing escape code tables.

Also note that most Python library calls accept either forward (/) or backward (\) slashes as directory path separators, regardless of the underlying platform. That is, / usually works on Windows too and aids in making scripts portable to Unix. Tools in the os and os.path modules, described later in this chapter, further aid in script path portability.


				21.1.2.2.3 The Loaded Modules Table

The sys module also contains hooks into the interpreter; sys.modules, for example, is a dictionary containing one name:module entry for every module imported in your Python session or program (really, in the calling Python process):

>>> sys.modules
{'os.path': <module 'ntpath' from 'C:\Program Files\Python\Lib\ntpath.pyc'>,...

>>> sys.modules.keys( )
['os.path', 'os', 'exceptions', '__main__', 'ntpath', 'strop', 'nt', 'sys',
'__builtin__', 'site', 'signal', 'UserDict', 'string', 'stat']

>>> sys
<module 'sys' (built-in)>
>>> sys.modules['sys']
<module 'sys' (built-in)>

We might use such a hook to write programs that display or otherwise process all the modules loaded by a program (just iterate over the keys list of sys.modules). sys also exports tools for getting an object's reference count used by Python's garbage collector (getrefcount), checking which modules are built into this Python (builtin_module_names), and more.

				21.1.2.2.4 Exception Details

Some of the sys module's attributes allow us to fetch all the information related to the most recently raised Python exception. This is handy if we want to process exceptions in a more generic fashion. For instance, the sys.exc_info function returns the latest exception's type, value, and traceback object:

>>> try:
...     raise IndexError
... except:
...     print sys.exc_info( )
...
(<class exceptions.IndexError at 7698d0>, <exceptions.IndexError instance at
797140>, <traceback object at 7971a0>)

We might use such information to format our own error message to display in a GUI pop-up window or HTML web page (recall that by default, uncaught exceptions terminate programs with a Python error display). The first two items returned by this call have reasonable string displays when printed directly, and the third is a traceback object that can be processed with the standard traceback module:

>>> import traceback, sys
>>> def grail(x):
...     raise TypeError, 'already got one'
...
>>> try:
...     grail('arthur')
... except:
...     exc_info = sys.exc_info( )
...     print exc_info[0]
...     print exc_info[1]
...     traceback.print_tb(exc_info[2])
...
exceptions.TypeError
already got one
  File "<stdin>", line 2, in ?
  File "<stdin>", line 2, in grail

The traceback module can also format messages as strings and route them to specific file objects; see the Python library manual for more details.


I should make two portability notes. First, the most recent exception type, value, and traceback objects are also available via other names:

>>> try:
...     raise TypeError, "Bad Thing"
... except:
...     print sys.exc_type, sys.exc_value
...
exceptions.TypeError Bad Thing


But these names represent a single, global exception, and they are not specific to a particular thread (threads are covered in Chapter 5). If you mean to raise and catch exceptions in multiple threads, exc_info provides thread-specific exception details. In fact, you are better off using exc_info in all cases today, as the older tools are now documented as deprecated and may be removed in a future version of Python.

It has also been suggested (in the 2.4 library reference manual and the Python 3.0 PEP document) that string-based exceptions may be removed in a future Python release. This seems more radical and less certain. But if you want to avoid potential future work, use class-based exceptions instead. Because they allow you to define categories of exceptions, they are better than strings in terms of code maintenance anyhow; by listing categories, your exception handlers are immune to future changes. Built-in exceptions have been classes since Python 1.5.

				21.1.2.2.5  Other sys Module Exports

The sys module exports additional tools that we will meet in the context of larger topics and examples introduced later in this chapter and book. For instance:

    * Command-line arguments show up as a list of strings called sys.argv.
    * Standard streams are available as sys.stdin, sys.stdout, and sys.stderr.
    * Program exit can be forced with sys.exit calls.

Since all of these lead us to bigger topics, though, we will cover them in sections of their own.

			21.1.2.3 os module
			As mentioned, os is the larger of the two core system modules. It contains all of the usual operating-system calls you may have used in your C programs and shell scripts. Its calls deal with directories, processes, shell variables, and the like. Technically, this module provides POSIX toolsa portable standard for operating-system callsalong with platform-independent directory processing tools as the nested module os.path. Operationally, os serves as a largely portable interface to your computer's system calls: scripts written with os and os.path can usually be run unchanged on any platform.

In fact, if you read the os module's source code, you'll notice that it really just imports whatever platform-specific system module you have on your computer (e.g., nt, mac, posix). See the os.py file in the Python source library directoryit simply runs a from* statement to copy all names out of a platform-specific module. By always importing os rather than platform-specific modules, though, your scripts are mostly immune to platform implementation differences. On some platforms, os includes extra tools available just for that platform (e.g., low-level process calls on Unix); by and large, though, it is as cross-platform as it is technically feasible.

				In fact, if you read the os module's source code, you'll notice that it really just imports whatever platform-specific system module you have on your computer (e.g., nt, mac, posix). See the os.py file in the Python source library directoryit simply runs a from* statement to copy all names out of a platform-specific module. By always importing os rather than platform-specific modules, though, your scripts are mostly immune to platform implementation differences. On some platforms, os includes extra tools available just for that platform (e.g., low-level process calls on Unix); by and large, though, it is as cross-platform as it is technically feasible.

				21.1.2.3.1  The Big os Lists

Let's take a quick look at the basic interfaces in os. As a preview, the following table summarizes some of the most commonly used tools in the os module organized by functional area.

Commonly used os module tools
Tasks	Tools
Shell variables	os.environ
Running programs	os.system, os.popen, os.popen2/3/4, os.startfile
Spawning processes	os.fork, os.pipe, os.exec, os.waitpid, os.kill
Descriptor files, locks	os.open, os.read, os.write
File processing	os.remove, os.rename, os.mkfifo, os.mkdir, os.rmdir
Administrative tools	os.getcwd, os.chdir, os.chmod, os.getpid, os.listdir
Portability tools	os.sep, os.pathsep, os.curdir, os.path.split, os.path.join
Pathname tools	os.path.exists('path'), os.path.isdir('path'), os.path.getsize('path')

If you inspect this module's attributes interactively, you get a huge list of names that will vary per Python release, will likely vary per platform, and isn't incredibly useful until you've learned what each name means (I've removed most of this list to save spacerun the command on your own):

>>> import os 
>>> dir(os) 
['F_OK', 'O_APPEND', 'O_BINARY', 'O_CREAT', 'O_EXCL', 'O_NOINHERIT',
'O_RANDOM', 'O_RDONLY', 'O_RDWR', 'O_SEQUENTIAL', 'O_SHORT_LIVED',
'O_TEMPORARY', 'O_TEXT', 'O_TRUNC', 'O_WRONLY', 'P_DETACH', 'P_NOWAIT',
...
 ...10 lines removed here...
...
'popen4', 'putenv', 'read', 'remove', 'removedirs', 'rename', 'renames', 'rmdir',
'sep', 'spawnl', 'spawnle', 'spawnv', 'spawnve', 'startfile', 'stat',
'stat_float_times', 'stat_result', 'statvfs_result', 'strerror', 'sys', 'system',
'tempnam', 'times', 'tmpfile', 'tmpnam', 'umask', 'unlink', 'unsetenv', 'urandom',
'utime', 'waitpid', 'walk', 'write']


					  


Besides all of these, the nested os.path module exports even more tools, most of which are related to processing file and directory names portably:

>>> dir(os.path)
['__all__', '__builtins__', '__doc__', '__file__', '__name__', 'abspath',
'altsep', 'basename', 'commonprefix', 'curdir', 'defpath', 'devnull', 'dirname',
'exists', 'expanduser', 'expandvars', 'extsep', 'getatime', 'getctime', 'getmtime',
'getsize', 'isabs', 'isdir', 'isfile', 'islink', 'ismount', 'join', 'lexists',
'normcase', 'normpath', 'os', 'pardir', 'pathsep', 'realpath', 'sep', 'split',
'splitdrive', 'splitext', 'splitunc', 'stat', 'supports_unicode_filenames', 'sys',
'walk']


				21.1.2.3.2  Administrative Tools

Just in case those massive listings aren't quite enough to go on, let's experiment interactively with some of the simpler os tools. Like sys, the os module comes with a collection of informational and administrative tools:

>>> os.getpid( )
-510737
>>> os.getcwd( )
'C:\\PP3rdEd\\Examples\\PP3E\\System'

>>> os.chdir(r'c:\temp')
>>> os.getcwd( )
'c:\\temp'


As shown here, the os.getpid function gives the calling process's process ID (a unique system-defined identifier for a running program), and os.getcwd returns the current working directory. The current working directory is where files opened by your script are assumed to live, unless their names include explicit directory paths. That's why earlier I told you to run the following command in the directory where more.py lives:

C:\...\PP3E\System>python more.py more.py


The input filename argument here is given without an explicit directory path (though you could add one to page files in another directory). If you need to run in a different working directory, call the os.chdir function to change to a new directory; your code will run relative to the new directory for the rest of the program (or until the next os.chdir call). This chapter will have more to say about the notion of a current working directory, and its relation to module imports when it explores script execution context.

				21.1.2.3.3  Portability Constants

The os module also exports a set of names designed to make cross-platform programming simpler. The set includes platform-specific settings for path and directory separator characters, parent and current directory indicators, and the characters used to terminate lines on the underlying computer:[*]

    [*] os.linesep comes back as \r\n herethe symbolic escape code equivalent of \015\012, which reflect the carriage-return + line-feed line terminator convention on Windows. In older versions of Python, you may still see these displayed in their octal or hexadecimal escape forms. See the discussion of end-of-line translations in the next chapter.

>>> os.pathsep, os.sep, os.pardir, os.curdir, os.linesep
(';', '\\', '..', '.', '\r\n')


os.sep is whatever character is used to separate directory components on the platform on which Python is running; it is automatically preset to \ on Windows, / for POSIX machines, and : on the Mac. Similarly, os.pathsep provides the character that separates directories on directory lists: for POSIX and ; for DOS and Windows.

By using such attributes when composing and decomposing system-related strings in our scripts, the scripts become fully portable. For instance, a call of the form os.sep.split(dirpath) will correctly split platform-specific directory names into components, even though dirpath may look like dir\dir on Windows, dir/dir on Linux, and dir:dir on Macintosh. As previously mentioned, on Windows you can usually use forward slashes rather than backward slashes when giving filenames to be opened; but these portability constants allow scripts to be platform neutral in directory processing code.

				21.1.2.3.4  Basic os.path Tools

The nested module os.path provides a large set of directory-related tools of its own. For example, it includes portable functions for tasks such as checking a file's type (isdir, isfile, and others), testing file existence (exists), and fetching the size of a file by name (getsize):

- check is file or dir exists

>>> os.path.isdir(r'C:\temp'),        os.path.isfile(r'C:\temp')
(True, False)
>>> os.path.isdir(r'C:\config.sys'),  os.path.isfile(r'C:\config.sys')
(False, Tuue)
>>> os.path.isdir('nonesuch'),        os.path.isfile('nonesuch')
(False, False)

>>> os.path.exists(r'c:\temp\data.txt')
0
>>> os.path.getsize(r'C:\autoexec.bat')
260


The os.path.isdir and os.path.isfile calls tell us whether a filename is a directory or a simple file; both return False if the named file does not exist. We also get calls for splitting and joining directory path strings, which automatically use the directory name conventions on the platform on which Python is running:
Code View: Scroll / Show All

>>> os.path.split(r'C:\temp\data.txt')
('C:\\temp', 'data.txt')
>>> os.path.join(r'C:\temp', 'output.txt')
'C:\\temp\\output.txt'

>>> name = r'C:\temp\data.txt'                            # Windows paths
>>> os.path.basename(name), os.path.dirname(name)
('data.txt', 'C:\\temp')

>>> name = '/home/lutz/temp/data.txt'                     # Unix-style paths
>>> os.path.basename(name), os.path.dirname(name)
('data.txt', '/home/lutz/temp')

>>> os.path.splitext(r'C:\PP3rdEd\Examples\PP3E\PyDemos.pyw')
('C:\\PP3rdEd\\Examples\\PP3E\\PyDemos', '.pyw')


					  


os.path.split separates a filename from its directory path, and os.path.join puts them back togetherall in entirely portable fashion using the path conventions of the machine on which they are called. The basename and dirname calls here return the second and first items returned by a split simply as a convenience, and splitext strips the file extension (after the last .). The normpath call comes in handy if your paths become a jumble of Unix and Windows separators:

>>> mixed
'C:\\temp\\public/files/index.html'
>>> os.path.normpath(mixed)
'C:\\temp\\public\\files\\index.html'
>>> print os.path.normpath(r'C:\temp\\sub\.\file.ext')
C:\temp\sub\file.ext


This module also has an abspath call that portably returns the full directory pathname of a file; it accounts for adding the current directory, .. parents, and more:

>>> os.getcwd( )
'C:\\PP3rdEd\\cdrom\\WindowsExt'
>>> os.path.abspath('temp')                    # expand to full pathname
'C:\\PP3rdEd\\cdrom\\WindowsExt\\temp'
>>> os.path.abspath(r'..\examples')            # relative paths expanded
'C:\\PP3rdEd\\examples'
>>> os.path.abspath(r'C:\PP3rdEd\chapters')    # absolute paths unchanged
'C:\\PP3rdEd\\chapters'
>>> os.path.abspath(r'C:\temp\spam.txt')       # ditto for filenames
'C:\\temp\\spam.txt'
>>> os.path.abspath('')                        # empty string means the cwd
'C:\\PP3rdEd\\cdrom\\WindowsExt'


Because filenames are relative to the current working directory when they aren't fully specified paths, the os.path.abspath function helps if you want to show users what directory is truly being used to store a file. On Windows, for example, when GUI-based programs are launched by clicking on file explorer icons and desktop shortcuts, the execution directory of the program is the clicked file's home directory, but that is not always obvious to the person doing the clicking; printing a file's abspath can help.

				21.1.2.3.5   Running Shell Commands from Scripts

The os module is also the place where we run shell commands from within Python scripts. This concept is intertwined with others we won't cover until later in this chapter, but since this is a key concept employed throughout this part of the book, let's take a quick first look at the basics here. Two os functions allow scripts to run any command line that you can type in a console window:


os.system

    Runs a shell command from a Python script

os.popen

    Runs a shell command and connect to its input or output streams

					21.1.2.3.5.1  What's a shell command?

To understand the scope of these calls, we first need to define a few terms. In this text, the term shell means the system that reads and runs command-line strings on your computer, and shell command means a command-line string that you would normally enter at your computer's shell prompt.

For example, on Windows, you can start an MS-DOS console window and type DOS commands therecommands such as dir to get a directory listing, and type to view a file, names of programs you wish to start, and so on. DOS is the system shell, and commands such as dir and type are shell commands. On Linux, you can start a new shell session by opening an xterm window and typing shell commands there tools to list directories, cat to view files, and so on. A variety of shells are available on Unix (e.g., csh, ksh), but they all read and run command lines. Here are two shell commands typed and run in an MS-DOS console box on Windows:

C:\temp>dir /B                    ...type a shell command line
about-pp.html                     ...its output shows up here
python1.5.tar.gz                  ...DOS is the shell on Windows
about-pp2e.html
about-ppr2e.html
newdir

C:\temp>type helloshell.py
# a Python program
print 'The Meaning of Life'
 
					21.1.2.3.5.2   Running shell commands

None of this is directly related to Python, of course (despite the fact that Python command-line scripts are sometimes confusingly called "shell tools"). But because the os module's system and popen calls let Python scripts run any sort of command that the underlying system shell understands, our scripts can make use of every command-line tool available on the computer, whether it's coded in Python or not. For example, here is some Python code that runs the two DOS shell commands typed at the shell prompt shown previously:

C:\temp>python
>>> import os
>>> os.system('dir /B')
about-pp.html
python1.5.tar.gz
about-pp2e.html
about-ppr2e.html
newdir
0

>>> os.system('type helloshell.py')
# a Python program
print 'The Meaning of Life'
0


The 0s at the end here are just the return values of the system call itself. The system call can be used to run any command line that we could type at the shell's prompt (here, C:\temp>). The command's output normally shows up in the Python session's or program's standard output stream.

					21.1.2.3.5.3   Communicating with shell commands

But what if we want to grab a command's output within a script? The os.system call simply runs a shell command line, but os.popen also connects to the standard input or output streams of the command; we get back a file-like object connected to the command's output by default (if we pass a w mode flag to popen, we connect to the command's input stream instead). By using this object to read the output of a command spawned with popen, we can intercept the text that would normally appear in the console window where a command line is typed:

>>> open('helloshell.py').read( )
"# a Python program\nprint 'The Meaning of Life'\n"

>>> text = os.popen('type helloshell.py').read( )
>>> text
"# a Python program\nprint 'The Meaning of Life'\n"

>>> listing = os.popen('dir /B').readlines( )
>>> listing
['about-pp.html\n', 'python1.5.tar.gz\n', 'helloshell.py\n',
'about-pp2e.html\n', 'about-ppr2e.html\n', 'newdir\n']


Here, we first fetch a file's content the usual way (using Python files), then as the output of a shell type command. Reading the output of a dir command lets us get a listing of files in a directory that we can then process in a loop (we'll learn other ways to obtain such a list in the next chapter[*]). So far, we've run basic DOS commands; because these calls can run any command line that we can type at a shell prompt, they can also be used to launch other Python scripts:

    [*] In the next chapter, after we've learned about file iterators, we'll also learn that the popen objects have an iterator that reads one line at a time, often making the readlines method call superfluous.

>>> os.system('python helloshell.py')       # run a Python program
The Meaning of Life
0
>>> output = os.popen('python helloshell.py').read( )
>>> output
'The Meaning of Life\n'


In all of these examples, the command-line strings sent to system and popen are hardcoded, but there's no reason Python programs could not construct such strings at runtime using normal string operations (+, %, etc.). Given that commands can be dynamically built and run this way, system and popen turn Python scripts into flexible and portable tools for launching and orchestrating other programs. For example, a Python test "driver" script can be used to run programs coded in any language (e.g., C++, Java, Python) and analyze their output. 

					21.1.2.3.5.4  Shell command limitations

You should keep in mind two limitations of system and popen. First, although these two functions themselves are fairly portable, their use is really only as portable as the commands that they run. The preceding examples that run DOS dir and type shell commands, for instance, work only on Windows, and would have to be changed in order to run ls and cat commands on Unix-like platforms.

Second, it is important to remember that running Python files as programs this way is very different and generally much slower than importing program files and calling functions they define. When os.system and os.popen are called, they must start a brand-new, independent program running on your operating system (they generally run the command in a newly forked process). When importing a program file as a module, the Python interpreter simply loads and runs the file's code in the same process in order to generate a module object. No other program is spawned along the way.[dagger]

    [dagger] The Python execfile built-in function also runs a program file's code, but within the same process that called it. It's similar to an import in that regard, but it works more as if the file's text had been pasted into the calling program at the place where the execfile call appears (unless explicit global or local namespace dictionaries are passed). Unlike imports, execfile unconditionally reads and executes a file's code (it may be run more than once per process), no module object is generated by the file's execution, and unless optional namespace dictionaries are passed in, assignments in the file's code may overwrite variables in the scope where the execfile appears; see the Python library manual for more details.

There are good reasons to build systems as separate programs too, and we'll later explore things such as command-line arguments and streams that allow programs to pass information back and forth. But for most purposes, imported modules are a faster and more direct way to compose systems.

If you plan to use these calls in earnest, you should also know that the os.system call normally blocksthat is, pausesits caller until the spawned command line exits. On Linux and Unix-like platforms, the spawned command can generally be made to run independently and in parallel with the caller by adding an & shell background operator at the end of the command line:

os.system("python program.py arg arg &")


On Windows, spawning with a DOS start command will usually launch the command in parallel too:

os.system("start program.py arg arg")


In fact, this is so useful that an os.startfile call was added in recent Python releases. This call opens a file with whatever program is listed in the Windows registry for the file's typeas though its icon has been clicked with the mouse cursor:

os.startfile("webpage.html")    # open file in your web browser
os.startfile("document.doc")    # open file in Microsoft Word
os.startfile("myscript.py")     # run file with Python


The os.popen call does not generally block its caller (by definition, the caller must be able to read or write the file object returned) but callers may still occasionally become blocked under both Windows and Linux if the pipe object is closede.g., when garbage is collectedbefore the spawned program exits or the pipe is read exhaustively (e.g., with its read( ) method). As we will see in the next chapter, the Unix os.fork/exec and Windows os.spawnv calls can also be used to run parallel programs without blocking.

Because the os module's system and popen calls also fall under the category of program launchers, stream redirectors, and cross-process communication devices, they will show up again in later parts of this chapter and in the following chapters, so we'll defer further details for the time being. If you're looking for more details right away, see the stream redirection section in this chapter and the directory listings section in the next. 

				21.1.2.3.6  Other os Module Exports

Since most other os module tools are even more difficult to appreciate outside the context of larger application topics, we'll postpone a deeper look at them until later sections. But to let you sample the flavor of this module, here is a quick preview for reference. Among the os module's other weapons are these:


os.environ Fetches and sets shell environment variables

os.fork Spawns a new child process on Unix

os.pipe Communicates between programs

os.execlp Starts new programs

os.spawnv Starts new programs with lower-level control

os.open Opens a low-level descriptor-based file

os.mkdir Creates a new directory

os.mkfifo Creates a new named pipe

os.stat Fetches low-level file information

os.remove Deletes a file by its pathname

os.path.walk, os.walk Applies a function or loop body to all parts of an entire directory tree

And so on. One caution up front: the os module provides a set of file open, read, and write calls, but all of these deal with low-level file access and are entirely distinct from Python's built-in stdio file objects that we create with the built-in open function. You should normally use the built-in open function (not the os module) for all but very special file-processing needs (e.g., opening with exclusive access file locking).

Throughout this chapter, we will apply sys and os tools such as these to implement common system-level tasks, but this book doesn't have space to provide an exhaustive list of the contents of modules we will meet along the way. If you have not already done so, you should become acquainted with the contents of modules such as os and sys by consulting the Python library manual. For now, let's move on to explore additional system tools in the context of broader system programming concepts.

			21.1.2.4 Script Execution Context
			Python scripts don't run in a vacuum. Depending on platforms and startup procedures, Python programs may have all sorts of enclosing context; information automatically passed in to the program by the operating system when the program starts up. For instance, scripts have access to the following sorts of system-level inputs and interfaces:


Current working directory 
    os.getcwd gives access to the directory from which a script is started, and many file tools use its value implicitly.

Command-line arguments 
    sys.argv gives access to words typed on the command line that are used to start the program and that serve as script inputs.

Shell variables 
    os.environ provides an interface to names assigned in the enclosing shell (or a parent program) and passed in to the script.

Standard streams 
    sys.stdin, stdout, and stderr export the three input/output streams that are at the heart of command-line shell tools.

Such tools can serve as inputs to scripts, configuration parameters, and so on. In the next few sections, we will explore these context toolsboth their Python interfaces and their typical roles.

			21.1.2.5 Current Working Directory

The notion of the current working directory (CWD) turns out to be a key concept in some scripts' execution: it's always the implicit place where files processed by the script are assumed to reside unless their names have absolute directory paths. As we saw earlier, os.getcwd lets a script fetch the CWD name explicitly, and os.chdir allows a script to move to a new CWD.

Keep in mind, though, that filenames without full pathnames map to the CWD and have nothing to do with your PYTHONPATH setting. Technically, a script is always launched from the CWD, not the directory containing the script file. Conversely, imports always first search the directory containing the script, not the CWD (unless the script happens to also be located in the CWD). Since this distinction is subtle and tends to trip up beginners, let's explore it in more detail.

				21.1.2.5.1 CWD, Files, and Import Paths
				When you run a Python script by typing a shell command line such as python dir1\dir2\file.py, the CWD is the directory you were in when you typed this command, not dir1\dir2. On the other hand, Python automatically adds the identity of the script's home directory to the front of the module search path such that file.py can always import other files in dir1\dir2 no matter where it is run from. To illustrate, let's write a simple script to echo both its CWD and its module search path:

C:\PP3rdEd\Examples\PP3E\System>type whereami.py
import os, sys
print 'my os.getcwd =>', os.getcwd( )            # show my cwd execution dir
print 'my sys.path  =>', sys.path[:6]            # show first 6 import paths
raw_input( )                                     # wait for keypress if clicked

Now, running this script in the directory in which it resides sets the CWD as expected and adds an empty string ('') to the front of the module search path in order to designate the CWD (we met the sys.path module search path earlier).
But if we run this script from other places, the CWD moves with us (it's the directory where we type commands), and Python adds a directory to the front of the module search path that allows the script to still see files in its own home directory. For instance, when running from one level up (..), the System name added to the front of sys.path will be the first directory that Python searches for imports within whereami.py; it points imports back to the directory containing the script that was run. Filenames without complete paths, though, will be mapped to the CWD (C:\PP3rdEd\Examples\PP3E), not the System subdirectory nested there.

The net effect is that filenames without directory paths in a script will be mapped to the place where the command was typed (os.getcwd), but imports still have access to the directory of the script being run (via the front of sys.path). Finally, when a file is launched by clicking its icon, the CWD is just the directory that contains the clicked file. The following output, for example, appears in a new DOS console box when whereami.py is double-clicked in Windows Explorer:
Code View: Scroll / Show All

my os.getcwd => C:\PP3rdEd\Examples\PP3E\System
my sys.path  => ['C:\\PP3RDED\\EXAMPLES\\PP3E\\SYSTEM', 'C:\\PP3rdEd\\Examples',
'C:\\Program Files\\Python\\Lib\\plat-win', 'C:\\Program Files\\Python\\Lib',
'C:\\Program Files\\Python\\DLLs']


In this case, both the CWD used for filenames and the first import search directory are the directory containing the script file. This all usually works out just as you expect, but there are two pitfalls to avoid:

    * Filenames might need to include complete directory paths if scripts cannot be sure from where they will be run.
    * Command-line scripts cannot use the CWD to gain import visibility to files that are not in their own directories; instead, use PYTHONPATH settings and package import paths to access modules in other directories.

For example, files in this book, regardless of how they are run (import filehere), can always import other files in their own home directories without package path imports but must go through the PP3E package root to find files anywhere else in the examples tree (from PP3E.dir1.dir2 import filethere), even if they are run from the directory containing the desired external module. As usual for modules, the PP3E\dir1\dir2 directory name could also be added to PYTHONPATH to make files there visible everywhere without package path imports (though adding more directories to PYTHONPATH increases the likelihood of name clashes). In either case, though, imports are always resolved to the script's home directory or other Python search path settings, not to the CWD.


				21.1.2.5.2 CWD and Command Lines

This distinction between the CWD and import search paths explains why many scripts in this book designed to operate in the current working directory (instead of one whose name is passed in) are run with command lines such as this one:

C:\temp>python %X%\PyTools\cleanpyc-py.py                 process cwd


In this example, the Python script file itself lives in the directory C:\PP3rdEd\Examples\PP3E\PyTools, but because it is run from C:\temp, it processes the files located in C:\temp (i.e., in the CWD, not in the script's home directory). To process files elsewhere with such a script, simply cd to the directory to be processed to change the CWD:

C:\temp>cd C:\PP2nEd\examples
C:\PP3rdEd\examples>python %X%\PyTools\cleanpyc-py.py     process cwd


Because the CWD is always implied, a cd command tells the script which directory to process in no less certain terms than passing a directory name to the script explicitly, like this:

C:\...\PP3E\PyTools>python find.py *.py C:\temp          process named dir

In this command line, the CWD is the directory containing the script to be run (notice that the script filename has no directory path prefix); but since this script processes a directory named explicitly on the command line (C:\temp), the CWD is irrelevant. Finally, if we want to run such a script located in some other directory in order to process files located in yet another directory, we can simply give directory paths to both:

C:\temp>python %X%\PyTools\find.py *.cxx C:\PP3rdEd\Examples\PP3E


Here, the script has import visibility to files in its PP3E\PyTools home directory and processes files in the PP3E root, but the CWD is something else entirely (C:\temp). This last form is more to type, of course, but watch for a variety of CWD and explicit script-path command lines like these in this book.

	

Note, Whenever you see a %X% in command lines such as those in the preceding examples, it refers to the value of the shell environment variable named X. It's just shorthand for the full directory pathname of the PP3E book examples package root directory, which I use to point to scripts' files. On my machines, it is preset in my PP3E\Config setup-pp* files like this:

set X=C:\PP3rdEd\Examples\PP3E             --DOS
setenv X /home/mark/PP3rdEd/Examples/PP3E  --Unix/csh


That is, it is assigned and expanded to the directory where PP3E lives on the system. See the next section in this chapter for more on shell variables. You can instead type full example root paths everywhere you see %X% in this book, but both your fingers and your keyboard are probably better off if you don't.

			21.1.2.6 Command-Line Arguments

The sys module is also where Python makes available the words typed on the command that is used to start a Python script. These words are usually referred to as command-line arguments and show up in sys.argv, a built-in list of strings. C programmers may notice its similarity to the C argv array (an array of C strings). It's not much to look at interactively, because no command-line arguments are passed to start up Python in this mode:

>>> sys.argv
['']


To really see what arguments are about, we need to run a script from the shell command line. Example 3-2 shows an unreasonably simple one that just prints the argv list for inspection.
Example 3-2. PP3E\System\testargv.py

import sys
print sys.argv


Running this script prints the command-line arguments list; note that the first item is always the name of the executed Python script file itself, no matter how the script was started (see the sidebar titled "Executable Scripts on Unix," later in this chapter).

C:\...\PP3E\System>python testargv.py
['testargv.py']

C:\...\PP3E\System>python testargv.py spam eggs cheese
['testargv.py', 'spam', 'eggs', 'cheese']

C:\...\PP3E\System>python testargv.py -i data.txt -o results.txt
['testargv.py', '-i', 'data.txt', '-o', 'results.txt']

The last command here illustrates a common convention. Much like function arguments, command-line options are sometimes passed by position and sometimes by name using a "-name value" word pair. For instance, the pair -i data.txt means the -i option's value is data.txt (e.g., an input filename). Any words can be listed, but programs usually impose some sort of structure on them.

Command-line arguments play the same role in programs that function arguments do in functions: they are simply a way to pass information to a program that can vary per program run. Because they don't have to be hardcoded, they allow scripts to be more generally useful. For example, a file-processing script can use a command-line argument as the name of the file it should process; see the more.py script we met in Example 3-1 for a prime example. Other scripts might accept processing mode flags, Internet addresses, and so on.

Once you start using command-line arguments regularly, though, you'll probably find it inconvenient to keep writing code that fishes through the list looking for words. More typically, programs translate the arguments list on startup into structures that are more conveniently processed. Here's one way to do it: the script in Example 3-3 scans the argv list looking for -optionname optionvalue word pairs and stuffs them into a dictionary by option name for easy retrieval.

estargv2.py

# collect command-line options in a dictionary

def getopts(argv):
    opts = {}
    while argv:
        if argv[0][0] == '-':                  # find "-name value" pairs
            opts[argv[0]] = argv[1]            # dict key is "-name" arg
            argv = argv[2:]
        else:
            argv = argv[1:]
    return opts

if __name__ == '__main__':
    from sys import argv                       # example client code
    myargs = getopts(argv)
    if myargs.has_key('-i'):
        print myargs['-i']
    print myargs


You might import and use such a function in all your command-line tools. When run by itself, this file just prints the formatted argument dictionary:
C:\...\PP3E\System>python testargv2.py
{}

C:\...\PP3E\System>python testargv2.py -i data.txt -o results.txt
data.txt
{'-o': 'results.txt', '-i': 'data.txt'}


Naturally, we could get much more sophisticated here in terms of argument patterns, error checking, and the like. We could also use standard and more advanced command-line processing tools in the Python library to parse arguments; see the standard getopt library module and the newer optparse in the library manual for other options. In general, the more configurable your scripts, the more you must invest in command-line processing logic complexity.

Executable Scripts on Unix

Unix and Linux users: you can also make text files of Python source code directly executable by adding a special line at the top with the path to the Python interpreter and giving the file executable permission. For instance, type this code into a text file called myscript:

#!/usr/bin/python
print 'And nice red uniforms'


The first line is normally taken as a comment by Python (it starts with a #); but when this file is run, the operating system sends lines in this file to the interpreter listed after #! on line 1. If this file is made directly executable with a shell command of the form chmod +x myscript, it can be run directly without typing python in the command, as though it were a binary executable program:

% myscript a b c
And nice red uniforms


When run this way, sys.argv will still have the script's name as the first word in the list: ["myscript", "a", "b", "c"], exactly as if the script had been run with the more explicit and portable command form python myscript a b c. Making scripts directly executable is actually a Unix trick, not a Python feature, but it's worth pointing out that it can be made a bit less machine dependent by listing the Unix env command at the top instead of a hardcoded path to the Python executable:

#!/usr/bin/env python
print 'Wait for it...'


When coded this way, the operating system will employ your environment variable settings to locate your Python interpreter (your PATH variable, on most platforms). If you run the same script on many machines, you need only change your environment settings on each machine (you don't need to edit Python script code). Of course, you can always run Python files with a more explicit command line:

% python myscript a b c


This assumes that the python interpreter program is on your system's search path setting (otherwise, you need to type its full path), but it works on any Python platform with a command line. Since this is more portable, I generally use this convention in the book's examples, but consult your Unix manpages for more details on any of the topics mentioned here. Even so, these special #! lines will show up in many examples in this book just in case readers want to run them as executables on Unix or Linux; on other platforms, they are simply ignored as Python comments.

Note that on recent flavors of Windows, you can usually also type a script's filename directly (without the word python) to make it go, and you don't have to add a #! line at the top. Python uses the Windows registry on this platform to declare itself as the program that opens files with Python extensions (.py and others). This is also why you can launch files on Windows by clicking on them.

			21.1.2.7 Shell Environment Variables
Shell variables, sometimes known as environment variables, are made available to Python scripts as os.environ, a Python dictionary-like object with one entry per variable setting in the shell. Shell variables live outside the Python system; they are often set at your system prompt or within startup files and typically serve as system-wide configuration inputs to programs.

In fact, by now you should be familiar with a prime example: the PYTHONPATH module search path setting is a shell variable used by Python to import modules. By setting it once in your system startup files, its value is available every time a Python program is run. Shell variables can also be set by programs to serve as inputs to other programs in an application; because their values are normally inherited by spawned programs, they can be used as a simple form of interprocess communication.

				21.1.2.7.1 Fetching Shell Variables

In Python, the surrounding shell environment becomes a simple preset object, not special syntax. Indexing os.environ by the desired shell variable's name string (e.g., os.environ['USER']) is the moral equivalent of adding a dollar sign before a variable name in most Unix shells (e.g., $USER), using surrounding percent signs on DOS (%USER%), and calling getenv("USER") in a C program. Let's start up an interactive session to experiment:

>>> import os
>>> os.environ.keys( )
['WINBOOTDIR', 'PATH', 'USER', 'PP2HOME', 'CMDLINE', 'PYTHONPATH', 'BL*ER',
'X', 'TEMP', 'COMSPEC', 'PROMPT', 'WINDIR', 'TMP']
>>> os.environ['TEMP']
'C:\\windows\\TEMP'


Here, the keys method returns a list of set variables, and indexing fetches the value of the shell variable TEMP on Windows. This works the same way on Linux, but other variables are generally preset when Python starts up. Since we know about PYTHONPATH, let's peek at its setting within Python to verify its content (as I wrote this, mine was set to the roots of the book examples trees for the third and second editions):

>>> os.environ['PYTHONPATH']
'C:\\Mark\\PP3E-cd\\Examples;C:\\Mark\\PP2E-cd\\Examples'
>>>
>>> for dir in os.environ['PYTHONPATH'].split(os.pathsep):
...     print dir
...
C:\Mark\PP3E-cd\Examples
C:\Mark\PP2E-cd\Examples


PYTHONPATH is a string of directory paths separated by whatever character is used to separate items in such paths on your platform (e.g., ; on DOS/Window, : on Unix and Linux). To split it into its components, we pass to the split string method an os.pathsep delimitera portable setting that gives the proper separator for the underlying machine.

				21.1.2.7.2 Changing Shell Variables

Like normal dictionaries, the os.environ object supports both key indexing and assignment. As usual, assignments change the value of the key:

>>> os.environ['TEMP'] = r'c:\temp'
>>> os.environ['TEMP']
'c:\\temp'


But something extra happens here. In recent Python releases, values assigned to os.environ keys in this fashion are automatically exported to other parts of the application. That is, key assignments change both the os.environ object in the Python program as well as the associated variable in the enclosing shell environment of the running program's process. Its new value becomes visible to the Python program, all linked-in C modules, and any programs spawned by the Python process.

Internally, key assignments to os.environ call os.putenva function that changes the shell variable outside the boundaries of the Python interpreter. To demonstrate how this works, we need a couple of scripts that set and fetch shell variables; the first is shown in Example 3-4.
setenv.py
import os
print 'setenv...',
print os.environ['USER']                 # show current shell variable value

os.environ['USER'] = 'Brian'             # runs os.putenv behind the scenes
os.system('python echoenv.py')

os.environ['USER'] = 'Arthur'            # changes passed to spawned programs
os.system('python echoenv.py')           # and linked-in C library modules

os.environ['USER'] = raw_input('?')
print os.popen('python echoenv.py').read( )

This setenv.py script simply changes a shell variable, USER, and spawns another script that echoes this variable's value.

echoenv.py

import os
print 'echoenv...',
print 'Hello,', os.environ['USER']


No matter how we run echoenv.py, it displays the value of USER in the enclosing shell; when run from the command line, this value comes from whatever we've set the variable to in the shell itself:

C:\...\PP3E\System\Environment>set USER=Bob

C:\...\PP3E\System\Environment>python echoenv.py
echoenv... Hello, Bob


When spawned by another script such as setenv.py, though, echoenv.py gets whatever USER settings its parent program has made:

C:\...\PP3E\System\Environment>python setenv.py
setenv... Bob
echoenv... Hello, Brian
echoenv... Hello, Arthur
?Gumby
echoenv... Hello, Gumby

C:\...\PP3E\System\Environment>echo %USER%
Bob


This works the same way on Linux. In general terms, a spawned program always inherits environment settings from its parents. Spawned programs are programs started with Python tools such as os.spawnv on Windows, the os.fork/exec combination on Unix and Linux, and os.popen and os.system on a variety of platforms. All programs thus launched get the environment variable settings that exist in the parent at launch time.[*]

    [*] This is by default. Some program-launching tools also let scripts pass environment settings that are different from their own to child programs. For instance, the os.spawnve call is like os.spawnv, but it accepts a dictionary argument representing the shell environment to be passed to the started program. Some os.exec* variants (ones with an "e" at the end of their names) similarly accept explicit environments; see the os.exec call formats for more details.

From a larger perspective, setting shell variables like this before starting a new program is one way to pass information into the new program. For instance, a Python configuration script might tailor the PYTHONPATH variable to include custom directories just before launching another Python script; the launched script will have the custom search path because shell variables are passed down to children.

				21.1.2.7.3  Shell Variable Details

Notice the last command in the preceding examplethe USER variable is back to its original value after the top-level Python program exits. Assignments to os.environ keys are passed outside the interpreter and down the spawned programs chain, but never back up to parent program processes (including the system shell). This is also true in C programs that use the putenv library call, and it isn't a Python limitation per se.

It's also likely to be a nonissue if a Python script is at the top of your application. But keep in mind that shell settings made within a program usually endure only for that program's run and for the run of its spawned children. If you need to export a shell variable setting so that it lives on after Python exits, you may be able to find platform-specific extensions that do this; search http://www.python.org or the Web at large.

Another subtlety: currently, changes to os.environ automatically call os.putenv, which runs the putenv call in the C library if the later is available on your platform; this exports the setting outside Python to any linked-in C code too. However, direct calls to os.putenv do not update os.environ to reflect the change, so os.environ changes are preferred.

Also note that environment settings are loaded into os.environ on startup and not on each fetch; hence, changes made by linked-in C code after startup may not be reflected in os.environ. Python does have an os.getenv call today, but it is translated into an os.environ fetch on most platforms, not into a call to getenv in the C library. Most applications won't need to care, especially if they are pure Python code. On platforms without a putenv call, os.environ can be passed as a parameter to program startup tools to set the spawned program's environment. 

			21.1.2.8  Standard Streams
The sys module is also the place where the standard input, output, and error streams  of your Python programs live:

>>> for f in (sys.stdin, sys.stdout, sys.stderr): print f
...
<open file '<stdin>', mode 'r' at 762210>
<open file '<stdout>', mode 'w' at 762270>
<open file '<stderr>', mode 'w' at 7622d0>


The standard streams are simply preopened Python file objects that are automatically connected to your program's standard streams when Python starts up. By default, all of them are tied to the console window where Python (or a Python program) was started. Because the print statement and raw_input functions are really nothing more than user-friendly interfaces to the standard output and input streams, they are similar to using stdout and stdin in sys directly:

>>> print 'hello stdout world'
hello stdout world

>>> sys.stdout.write('hello stdout world' + '\n')
hello stdout world

>>> raw_input('hello stdin world>')
hello stdin world>spam
'spam'

>>> print 'hello stdin world>',; sys.stdin.readline( )[:-1]
hello stdin world>eggs

'eggs'

Note Standard Streams on Windows

Windows users: if you click a .py Python program's filename in a Windows file explorer to start it (or launch it with os.system), a DOS console box automatically pops up to serve as the program's standard stream. If your program makes windows of its own, you can avoid this console pop-up window by naming your program's source-code file with a .pyw extension, not with a .py extension. The .pyw extension simply means a .py source file without a DOS pop up on Windows (it uses Windows registry settings to run a custom version of Python).

One historical note: in the Python 1.5.2 release and earlier, .pyw files could only be run, not importedthe .pyw was not recognized as a module name. If you wanted a program to be run without a DOS console pop up and be importable elsewhere, you needed both .py and .pyw files; the .pyw simply served as top-level script logic that imported and called the core logic in the .py file. This is no longer required today: .pyw files may be imported as usual.

Also note that because printed output goes to this DOS pop up when a program is clicked, scripts that simply print text and exit will generate an odd "flash"the DOS console box pops up, output is printed into it, and the pop up goes away immediately (not the most user-friendly of features!). To keep the DOS pop-up box around so that you can read printed output, simply add a raw_input( ) call at the bottom of your script to pause for an Enter key press before exiting.

				21.1.2.8.1 Redirecting Streams to Files and Programs

Technically, standard output (and print) text appears in the console window where a program was started, standard input (and raw_input) text comes from the keyboard, and standard error text is used to print Python error messages to the console window. At least that's the default. It's also possible to redirect these streams both to files and to other programs at the system shell, as well as to arbitrary objects within a Python script. On most systems, such redirections make it easy to reuse and combine general-purpose command-line utilities. 

					21.1.2.8.1.1  Redirecting streams to files

Redirection is useful for things like canned (precoded) test inputs: we can apply a single test script to any set of inputs by simply redirecting the standard input stream to a different file each time the script is run. Similarly, redirecting the standard output stream lets us save and later analyze a program's output; for example, testing systems might compare the saved standard output of a script with a file of expected output to detect failures.

Although it's a powerful paradigm, redirection turns out to be straightforward to use. For instance, consider the simple read-evaluate-print loop program in Example.
teststreams.py
# read numbers till eof and show squares

def interact( ):
    print 'Hello stream world'                      # print sends to sys.stdout
    while 1:
        try:
            reply  = raw_input('Enter a number>')   # raw_input reads sys.stdin
        except EOFError:
            break                                   # raises an except on eof
        else:                                       # input given as a string
            num = int(reply)
            print "%d squared is %d" % (num, num ** 2)
    print 'Bye'

if __name__ == '__main__':
    interact( )                                      # when run, not imported


As usual, the interact function here is automatically executed when this file is run, not when it is imported. By default, running this file from a system command line makes that standard stream appear where you typed the Python command. The script simply reads numbers until it reaches end-of-file in the standard input stream (on Windows, end-of-file is usually the two-key combination Ctrl-Z; on Unix, type Ctrl-D instead[*]):

    [*] Notice that raw_input raises an exception to signal end-of-file, but file read methods simply return an empty string for this condition. Because raw_input also strips the end-of-line character at the end of lines, an empty string result means an empty line, so an exception is necessary to specify the end-of-file condition. File read methods retain the end-of-line character and denote an empty line as \n instead of "". This is one way in which reading sys.stdin directly differs from raw_input. The latter also accepts a prompt string that is automatically printed before input is accepted.

C:\...\PP3E\System\Streams>python teststreams.py
Hello stream world
Enter a number>12
12 squared is 144
Enter a number>10
10 squared is 100
Enter a number>
Bye


But on both Windows and Unix-like platforms, we can redirect the standard input stream to come from a file with the < filename shell syntax. Here is a command session in a DOS console box on Windows that forces the script to read its input from a text file, input.txt. It's the same on Linux, but replace the DOS type command with a Unix cat command:

C:\...\PP3E\System\Streams>type input.txt
8
6

C:\...\PP3E\System\Streams>python teststreams.py < input.txt
Hello stream world
Enter a number>8 squared is 64
Enter a number>6 squared is 36
Enter a number>Bye


Here, the input.txt file automates the input we would normally type interactivelythe script reads from this file rather than from the keyboard. Standard output can be similarly redirected to go to a file with the > filename shell syntax. In fact, we can combine input and output redirection in a single command:

C:\...\PP3E\System\Streams>python teststreams.py < input.txt > output.txt

C:\...\PP3E\System\Streams>type output.txt
Hello stream world
Enter a number>8 squared is 64
Enter a number>6 squared is 36
Enter a number>Bye


This time, the Python script's input and output are both mapped to text files, not to the interactive console session.

					21.1.2.8.1.2  Chaining programs with pipes

On Windows and Unix-like platforms, it's also possible to send the standard output of one program to the standard input of another using the | shell character between two commands. This is usually called a "pipe" operation because the shell creates a pipeline that connects the output and input of two commands. Let's send the output of the Python script to the standard more command-line program's input to see how this works:

C:\...\PP3E\System\Streams>python teststreams.py < input.txt | more

Hello stream world
Enter a number>8 squared is 64
Enter a number>6 squared is 36
Enter a number>Bye


Here, teststreams's standard input comes from a file again, but its output (written by print statements) is sent to another program, not to a file or window. The receiving program is more, a standard command-line paging program available on Windows and Unix-like platforms. Because Python ties scripts into the standard stream model, though, Python scripts can be used on both ends. One Python script's output can always be piped into another Python script's input:

C:\...\PP3E\System\Streams>type writer.py
print "Help! Help! I'm being repressed!"
print 42

C:\...\PP3E\System\Streams>type reader.py
print 'Got this" "%s"' % raw_input( )
import sys
data = sys.stdin.readline( )[:-1]
print 'The meaning of life is', data, int(data) * 2

C:\...\PP3E\System\Streams>python writer.py | python reader.py
Got this" "Help! Help! I'm being repressed!"
The meaning of life is 42 84

This time, two Python programs are connected. Script reader gets input from script writer; both scripts simply read and write, oblivious to stream mechanics. In practice, such chaining of programs is a simple form of cross-program communications. It makes it easy to reuse utilities written to communicate via stdin and stdout in ways we never anticipated. For instance, a Python program that sorts stdin text could be applied to any data source we like, including the output of other scripts. Consider the Python command-line utility scripts in Examples below that sort and sum lines in the standard input stream.

sorter.py
import sys                                    # or sorted(sys.stdin)
lines = sys.stdin.readlines( )               # sort stdin input lines,
lines.sort( )                                # send result to stdout
for line in lines: print line,               # for further processing

adder.py
import sys
sum = 0
while True:
    try:
        line = raw_input()                 # or call sys.stdin.readlines( )
    except EOFError:                       # or for line in sys.stdin:
        break                              # raw_input strips \n at end
    else:
        sum += int(line)                   # was sting.atoi( ) in 2nd ed
print sum

We can apply such general-purpose tools in a variety of ways at the shell command line to sort and sum arbitrary files and program outputs (Windows note: on my XP machine, I have to type "python file.py" here, not just "file.py"; otherwise, the input redirection fails):
	-> Input file
[yizaq@yytzhak-wxp:Sun Dec 30:/cygdrive/c/work/KB/python/ProgrammingPythonExamples/system:]$ cat data.txt             
123
000
999
042
	-> sort
[yizaq@yytzhak-wxp:Sun Dec 30:/cygdrive/c/work/KB/python/ProgrammingPythonExamples/system:]$ python sorter.py < data.txt 
000
042
123
999
	-> Sum
[yizaq@yytzhak-wxp:Sun Dec 30:/cygdrive/c/work/KB/python/ProgrammingPythonExamples/system:]$ python adder.py < data.txt 
1164
	-> Sum cat output
[yizaq@yytzhak-wxp:Sun Dec 30:/cygdrive/c/work/KB/python/ProgrammingPythonExamples/system:]$ cat data.txt | python adder.py
1164
[yizaq@yytzhak-wxp:Sun Dec 30:/cygdrive/c/work/KB/python/ProgrammingPythonExamples/system:]$ cat writer2.py 
for data in (123, 0, 999, 42):
    print '%03d' % data
	-> sort py output
[yizaq@yytzhak-wxp:Sun Dec 30:/cygdrive/c/work/KB/python/ProgrammingPythonExamples/system:]$ python writer2.py | python sorter.py
000
042
123
999
[yizaq@yytzhak-wxp:Sun Dec 30:/cygdrive/c/work/KB/python/ProgrammingPythonExamples/system:]$ python writer2.py | python sorter.py | python adder.py 
1164

The last command here connects three Python scripts by standard streamsthe output of each prior script is fed to the input of the next via pipeline shell syntax.

					21.1.2.8.1.3   Coding alternatives

A few coding pointers here: if you look closely, you'll notice that sorter reads all of stdin at once with the readlines method, but adder reads one line at a time. If the input source is another program, some platforms run programs connected by pipes in parallel. On such systems, reading line by line works better if the data streams being passed about are large because readers don't have to wait until writers are completely finished to get busy processing data. Because raw_input just reads stdin, the line-by-line scheme used by adder can always be coded with sys.stdin too:

C:\...\PP3E\System\Streams>type adder2.py
import sys
sum = 0
while True:
    line = sys.stdin.readline( )
    if not line: break
    sum += int(line)
print sum


This version utilizes the fact that the int allows the digits to be surrounded by whitespace (readline returns a line including its \n, but we don't have to use [:-1] or rstrip( ) to remove it for int). In fact, we can use Python's more recent file iterators to achieve the same effectthe for loop, for example, automatically grabs one line each time through when we iterate over a file object directly (more on file iterators in the next section):

C:\...\PP3E\System\Streams>type adder3.py
import sys
sum = 0
for line in sys.stdin: sum += int(line)
print sum


Changing sorter to read line by line this way may not be a big performance boost, though, because the list sort method requires that the list already be complete. As we'll see in Chapter 20, manually coded sort algorithms are likely to be much slower than the Python list sorting method.

Interestingly, these two scripts can also be coded in a much more compact fashion in Python 2.4 by using the new sorted function, list comprehensions, and file iterators. The following work the same way as the originals:

C:\...\PP3E\System\Streams>type sorter24.py
import sys
for line in sorted(sys.stdin): print line,

C:\...\PP3E\System\Streams>type adder24.py
import sys
print sum(int(line) for line in sys.stdin)


The latter of these employs a generator expression, which is much like a list comprehension, but results are returned one at a time, not in a physical list. The net effect is space optimization.

					21.1.2.8.1.4 Redirected streams and user interaction

At the start of the last section, we piped teststreams.py output into the standard more command-line program with a command similar to this one:

C:\...\PP3E\System\Streams>python teststreams.py < input.txt | more


But since we already wrote our own "more" paging utility in Python near the start of this chapter, why not set it up to accept input from stdin too? For example, if we change the last three lines of the more.py file listed earlier in this chapter to this...

if __name__ == '__main__':                       # when run, not when imported
    if len(sys.argv) == 1:                         # page stdin if no cmd args
        more(sys.stdin.read( ))
    else:
        more(open(sys.argv[1]).read( ))

...it almost seems as if we should be able to redirect the standard output of teststreams.py into the standard input of more.py: 
[yizaq@yytzhak-wxp:Sun Dec 30:/cygdrive/c/work/KB/python/ProgrammingPythonExamples/system:]$ python teststreams.py < input.txt | python more2.py

					21.1.2.8.1.5  Reading keyboard input

But there's a subtle problem lurking in the preceding more.py command. Really, chaining worked there only by sheer luck: if the first script's output is long enough that more has to ask the user if it should continue, the script will utterly fail. The problem is that the augmented more.py uses stdin for two disjointed purposes. It reads a reply from an interactive user on stdin by calling raw_input, but now it also accepts the main input text on stdin. When the stdin stream is really redirected to an input file or pipe, we can't use it to input a reply from an interactive user; it contains only the text of the input source. Moreover, because stdin is redirected before the program even starts up, there is no way to know what it meant prior to being redirected in the command line.

If we intend to accept input on stdin and use the console for user interaction, we have to do a bit more. The example below shows a modified version of the more script that pages the standard input stream if called with no arguments but also makes use of lower-level and platform-specific tools to converse with a user at a keyboard if needed.
moreplus.py


#############################################################
# split and interactively page a string, file, or stream of
# text to stdout; when run as a script, page stdin or file
# whose name is passed on cmdline; if input is stdin, can't
# use it for user reply--use platform-specific tools or GUI;
#############################################################

import sys

def getreply( ):
    """
    read a reply key from an interactive user
    even if stdin redirected to a file or pipe
    """
    if sys.stdin.isatty( ):                       # if stdin is console
        return raw_input('?')                     # read reply line from stdin
    else:
        if sys.platform[:3] == 'win':            # if stdin was redirected
            import msvcrt                        # can't use to ask a user
            msvcrt.putch('?')
            key = msvcrt.getche( )                # use windows console tools
            msvcrt.putch('\n')                     # getch( ) does not echo key
            return key
	elif (sys.platform[:5] == 'linux' or sys.platform[:6] == 'cygwin'):        # use linux console device
            print '?',                           # strip eoln at line end
            console = open('/dev/tty')
            line = console.readline( )[:-1]
            return line
        else:
            print '[pause]'                      # else just pause--improve me
            import time                          # see also modules curses, tty
            time.sleep(5)                        # or copy to temp file, rerun
            return 'y'                           # or GUI pop up, tk key bind

def more(text, numlines=10):
    """
    split multiline string to stdout
    """
    lines = text.split('\n')
    while lines:
        chunk = lines[:numlines]
        lines = lines[numlines:]
        for line in chunk: print line
        if lines and getreply( ) not in ['y', 'Y']: break

if __name__ == '__main__':                     # when run, not when imported
    if len(sys.argv) == 1:                        # if no command-line arguments
        more(sys.stdin.read( ))                  # page stdin, no raw_inputs
    else:
        more(open(sys.argv[1]).read( ))          # else page filename argument


					  


Most of the new code in this version shows up in its getreply function. The file's isatty method tells us whether stdin is connected to the console; if it is, we simply read replies on stdin as before. Unfortunately, there is no portable way to input a string from a console user independent of stdin, so we must wrap the non-stdin input logic of this script in a sys.platform test:

    * On Windows, the built-in msvcrt module supplies low-level console input and output calls (e.g., msvcrt.getch( ) reads a single key press).
    * On Linux, the system device file named /dev/tty gives access to keyboard input (we can read it as though it were a simple file).
    * On other platforms, we simply run a built-in time.sleep call to pause for five seconds between displays (this is not at all ideal, but it is better than not stopping at all and it serves until a better nonportable solution can be found).

Of course, we have to add such extra logic only to scripts that intend to interact with console users and take input on stdin. In a GUI application, for example, we could instead pop up dialogs, bind keyboard-press events to run callbacks, and so on.

Armed with the reusable getreply function, though, we can safely run our moreplus utility in a variety of ways. As before, we can import and call this module's function directly, passing in whatever string we wish to page:

>>> from moreplus import more
>>> more(open('System.txt').read( ))
This directory contains operating system interface examples.

Many of the examples in this unit appear elsewhere in the examples
distribution tree, because they are actually used to manage other
programs. See the README.txt files in the subdirectories here
for pointers.


Also as before, when run with a command-line argument, this script interactively pages through the named file's text:

C:\...\PP3E\System>python moreplus.py System.txt
This directory contains operating system interface examples.

Many of the examples in this unit appear elsewhere in the examples
distribution tree, because they are actually used to manage other
programs. See the README.txt files in the subdirectories here
for pointers.

C:\...\PP3E\System>python moreplus.py moreplus.py
#############################################################
# split and interactively page a string, file, or stream of
# text to stdout; when run as a script, page stdin or file
# whose name is passed on cmdline; if input is stdin, can't
# use it for user reply--use platform-specific tools or GUI;
#############################################################

import sys, string

def getreply( ):
?n


But now the script also correctly pages text redirected into stdin from either a file or a command pipe, even if that text is too long to fit in a single display chunk. On most shells, we send such input via redirection or pipe operators like these:

C:\...\PP3E\System>python moreplus.py < moreplus.py
#############################################################
# split and interactively page a string, file, or stream of
# text to stdout; when run as a script, page stdin or file
# whose name is passed on cmdline; if input is stdin, can't
# use it for user reply--use platform-specific tools or GUI;
#############################################################

import sys, string

def getreply( ):
?n

C:\...\PP3E\System>type moreplus.py | python moreplus.py
#############################################################
# split and interactively page a string, file, or stream of
# text to stdout; when run as a script, page stdin or file
# whose name is passed on cmdline; if input is stdin, can't
# use it for user reply--use platform-specific tools or GUI;
#############################################################

import sys, string

def getreply( ):
?n


This works the same way on Linux, but, again, use the cat command rather than type. Finally, piping one Python script's output into this script's input now works as expected, without botching user interaction (and not just because we got lucky):
Code View: Scroll / Show All

......\System\Streams>python teststreams.py < input.txt | python ..\moreplus.py
Hello stream world
Enter a number>8 squared is 64
Enter a number>6 squared is 36
Enter a number>Bye


					  


Here, the standard output of one Python script is fed to the standard input of another Python script located in the parent directory: moreplus.py reads the output of teststreams.py.

All of the redirections in such command lines work only because scripts don't care what standard input and output really areinteractive users, files, or pipes between programs. For example, when run as a script, moreplus.py simply reads stream sys.stdin; the command-line shell (e.g., DOS on Windows, csh on Linux) attaches such streams to the source implied by the command line before the script is started. Scripts use the preopened stdin and stdout file objects to access those sources, regardless of their true nature.

And for readers keeping count, we have run this single more pager script in four different ways: by importing and calling its function, by passing a filename command-line argument, by redirecting stdin to a file, and by piping a command's output to stdin. By supporting importable functions, command-line arguments, and standard streams, Python system tools code can be reused in a wide variety of modes.

				21.1.2.8.2  Redirecting Streams to Python Objects

All of the previous standard stream redirections work for programs written in any language that hooks into the standard streams and rely more on the shell's command-line processor than on Python itself. Command-line redirection syntax like < filename and | program is evaluated by the shell, not by Python. A more Pythonesque form of redirection can be done within scripts themselves by resetting sys.stdin and sys.stdout to file-like objects.

The main trick behind this mode is that anything that looks like a file in terms of methods will work as a standard stream in Python. The object's interface (sometimes called its protocol), and not the object's specific datatype, is all that matters. That is:

    * Any object that provides file-like read methods can be assigned to sys.stdin to make input come from that object's read methods.
    * Any object that defines file-like write methods can be assigned to sys.stdout; all standard output will be sent to that object's methods.

Because print and raw_input simply call the write and readline methods of whatever objects sys.stdout and sys.stdin happen to reference, we can use this technique to both provide and intercept standard stream text with objects implemented as classes.

Such plug-and-play compatibility is usually called polymorphismi.e., it doesn't matter what an object is, and it doesn't matter what its interface does, as long as it provides the expected interface. This liberal approach to datatypes accounts for much of the conciseness and flexibility of Python code. Here, it provides a way for scripts to reset their own streams. The example below shows a utility module that demonstrates this concept.

redirect.py
#############################################################################
# file-like objects that save standard output text in a string and provide
# standard input text from a string; redirect runs a passed-in function
# with its output and input streams reset to these file-like class objects;
#############################################################################

import sys                                      # get built-in modules

class Output:                                   # simulated output file
    def __init__(self):
        self.text = ''                          # empty string when created
    def write(self, string):                    # add a string of bytes
        self.text = self.text + string
    def writelines(self, lines):                # add each line in a list
        for line in lines: self.write(line)

class Input:                                    # simulated input file
    def __init__(self, input=''):                # default argument
        self.text = input                       # save string when created
    def read(self, *size):                      # optional argument
        if not size:                            # read N bytes, or all
            res, self.text = self.text, ''
        else:
            res, self.text = self.text[:size[0]], self.text[size[0]:]
        return res
    def readline(self):
        eoln = self.text.find('\n')             # find offset of next eoln
        if eoln == -1:                          # slice off through eoln
            res, self.text = self.text, ''
        else:
            res, self.text = self.text[:eoln+1], self.text[eoln+1:]
        return res

def redirect(function, args, input):            # redirect stdin/out
    savestreams = sys.stdin, sys.stdout         # run a function object
    sys.stdin   = Input(input)                  # return stdout text
    sys.stdout  = Output( )
    try:
        function(*args)                         # was apply(function, args)
    except:
        sys.stderr.write('error in function! ')
        sys.stderr.write("%s, %s\n" % tuple(sys.exc_info( )[:2]))
    result = sys.stdout.text
    sys.stdin, sys.stdout = savestreams
    return result

This module defines two classes that masquerade as real files:


Output

    Provides the write method interface (a.k.a. protocol) expected of output files but saves all output in an in-memory string as it is written.

Input

    Provides the interface expected of input files, but provides input on demand from an in-memory string passed in at object construction time.

The redirect function at the bottom of this file combines these two objects to run a single function with input and output redirected entirely to Python class objects. The passed-in function to run need not know or care that its print statements, raw_input calls and stdin and stdout method calls, are talking to a class rather than to a real file, pipe, or user.

To demonstrate, import and run the interact function at the heart of the teststreams script that we've been running from the shell (to use the redirection utility function, we need to deal in terms of functions, not files). When run directly, the function reads from the keyboard and writes to the screen, just as if it were run as a program without redirection:

C:\...\PP3E\System\Streams>python
>>> from teststreams import interact
>>> interact( )
Hello stream world
Enter a number>2
2 squared is 4
Enter a number>3
3 squared is 9
Enter a number
>>>

Now, let's run this function under the control of the redirection function in redirect.py and pass in some canned input text. In this mode, the interact function takes its input from the string we pass in ('4\n5\n6\n'three lines with explicit end-of-line characters), and the result of running the function is a string containing all the text written to the standard output stream:

>>> from redirect import redirect
>>> output = redirect(interact, ( ), '4\n5\n6\n')
>>> output
'Hello stream world\nEnter a number>4 squared is 16\nEnter a number>
5 squared is 25\nEnter a number>6 squared is 36\nEnter a number>Bye\n'

The result is a single, long string containing the concatenation of all text written to standard output. To make this look better, we can split it up with the string object's split method:

>>> for line in output.split('\n'): print line
...
Hello stream world
Enter a number>4 squared is 16
Enter a number>5 squared is 25
Enter a number>6 squared is 36
Enter a number>Bye


Better still, we can reuse the more.py module we saw earlier in this chapter; it's less to type and remember, and it's already known to work well:

>>> from PP3E.System.more import more
>>> more(output)
Hello stream world
Enter a number>4 squared is 16
Enter a number>5 squared is 25
Enter a number>6 squared is 36
Enter a number>Bye


This is an artificial example, of course, but the techniques illustrated are widely applicable. For example, it's straightforward to add a GUI interface to a program written to interact with a command-line user. Simply intercept standard output with an object such as the Output class shown earlier and throw the text string up in a window. Similarly, standard input can be reset to an object that fetches text from a graphical interface (e.g., a popped-up dialog box). Because classes are plug-and-play compatible with real files, we can use them in any tool that expects a file. Watch for a GUI stream-redirection module named guiStreams.

Notice the function(*args) syntax in the redirect function of Example 3-10. In the prior edition of this book, this was a built-in function call, apply(function, args), but the apply built-in has been marked as deprecated since that edition (in fact, it's not even listed in the functions section of the library manual). It's unclear whether apply will ever be removed completely, but the new call syntax is more general and concise, and it should be preferred today. The following equivalent calls, for instance, are more complex with apply; the new syntax allows normal arguments to be mixed with argument collection objects, but apply must merge manually:

>>> def m(self, a, b, c): print self, a, b, c

>>> m(1, *(2, 3, 4))
1 2 3 4

>>> apply(m, (1,) + (2, 3, 4))
1 2 3 4


This becomes more useful as call signatures grow more complex:

>>> a=1; b=2; c=3; d=4; e=5
>>> def func(a, *ps, **ks): print a, ps, ks

>>> func(a, b, c=1, *(d, e), **{'f':2})
1 (2, 4, 5) {'c': 1, 'f': 2}

>>> kargs = {'f':2}
>>> kargs.update({'c':1})
>>> apply(func, (a, b) + (d, e), kargs)
1 (2, 4, 5) {'c': 1, 'f': 2}



				21.1.2.8.3  The StringIO Module
The prior section's technique of redirecting streams to objects proved so handy that now a standard library automates the task. It provides an object that maps a file object interface to and from in-memory strings. For example:

>>> from StringIO import StringIO
>>> buff = StringIO( )                   # save written text to a string
>>> buff.write('spam\n')
>>> buff.write('eggs\n')
>>> buff.getvalue( )
'spam\neggs\n'

>>> buff = StringIO('ham\nspam\n')      # provide input from a string
>>> buff.readline( )
'ham\n'
>>> buff.readline( )
'spam\n'
>>> buff.readline( )
''

As in the prior section, instances of StringIO objects can be assigned to sys.stdin and sys.stdout to redirect streams for raw_input and print and can be passed to any code that was written to expect a real file object. Again, in Python, the object interface, not the concrete datatype, is the name of the game:

>>> from StringIO import StringIO
>>> import sys
>>> buff = StringIO( )

>>> temp = sys.stdout
>>> sys.stdout = buff
>>> print 42, 'spam', 3.141               # or print >> buff, ...

>>> sys.stdout = temp                     # restore original stream
>>> buff.getvalue( )
'42 spam 3.141\n'


				21.1.2.8.4  Capturing the stderr Stream
We've been focusing on stdin and stdout redirection, but stderr can be similarly reset to files, pipes, and objects. This is straightforward within a Python script. For instance, assigning sys.stderr to another instance of a class such as Output or a StringIO object in the preceding section's example allows your script to intercept text written to standard error too.

Python itself uses standard error for error message text (and the IDLE GUI interface intercepts it and colors it red by default). However, no higher-level tools for standard error do what print and raw_input( ) do for the output and input streams. If you wish to print to the error stream, you'll want to call sys.stderr.write( ) explicitly or read the next section for a print statement trick that makes this a bit simpler.

Redirecting standard errors from a shell command line is a bit more complex and less portable. On most Unix-like systems, we can usually capture stderr output by using shell-redirection syntax of the form command > output 2>&1. This may not work on some flavors of Windows platforms, though, and can even vary per Unix shell; see your shell's manpages for more details.

				21.1.2.8.5   Redirection Syntax in Print Statements

Because resetting the stream attributes to new objects was so popular, as of Python 2.0 the print statement is also extended to include an explicit file to which output is to be sent. A statement of the form:

print >> file, stuff          # file is an object, not a string name


prints stuff to file instead of to stdout. The net effect is similar to simply assigning sys.stdout to an object, but there is no need to save and restore in order to return to the original output stream (as shown in the section on redirecting streams to objects). For example:

import sys
print >> sys.stderr, 'spam' * 2


will send text the standard error stream object rather than sys.stdout for the duration of this single print statement only. The next normal print statement (without >>) prints to standard output as usual.

				21.1.2.8.6  Other Redirection Options

Earlier in this chapter, we studied the built-in os.popen function, which provides a way to redirect another command's streams from within a Python program. As we saw, this function runs a shell command line (e.g., a string we would normally type at a DOS or csh prompt) but returns a Python file-like object connected to the command's input or output stream.

Because of that, the os.popen tool is another way to redirect streams of spawned programs, and it is a cousin to the techniques we just met: its effect is much like the shell | command-line pipe syntax for redirecting streams to programs (in fact, its name means "pipe open"), but it is run within a script and provides a file-like interface to piped streams. It's similar in spirit to the redirect function, but it's based on running programs (not calling functions), and the command's streams are processed in the spawning script as files (not tied to class objects). That is, os.popen redirects the streams of a program that a script starts instead of redirecting the streams of the script itself.

By passing in the desired mode flag, we redirect a spawned program's input or output streams to a file in the calling scripts:

C:\...\PP3E\System\Streams>type hello-out.py
print 'Hello shell world'

C:\...\PP3E\System\Streams>type hello-in.py
input = raw_input( )
open('hello-in.txt', 'w').write('Hello ' + input + '\n')

C:\...\PP3E\System\Streams>python
>>> import os
>>> pipe = os.popen('python hello-out.py')         # 'r' is default--read stdout
>>> pipe.read( )
'Hello shell world\n'

>>> pipe = os.popen('python hello-in.py', 'w')
>>> pipe.write('Gumby\n')                            # 'w'--write to program stdin
>>> pipe.close( )                                   # \n at end is optional
>>> open('hello-in.txt').read( )
'Hello Gumby\n'


					  


The popen call is also smart enough to run the command string as an independent process on platforms that support such a notion. It accepts an optional third argument that can be used to control buffering of written text.

Additional popen-like tools in the Python library allow scripts to connect to more than one of the commands' streams. For instance, the os.open2 call includes functions for hooking into both a command's input and output streams:

childStdIn, childStdout = os.popen2('python hello-in-out.py')
childStdin.write(input)
output = childStdout.read( )


os.popen3 is similar, but it returns a third pipe for connecting to standard error as well. A related call, os.popen4, returns two pipe file objects; it's like os.popen3, but the output and error streams are tied together into a single pipe:

childStdin, childStdout, childStderr = os.popen3('python hello-in-out.py')
childStdin, childStdout_and_err =      os.popen4('python hello-in-out.py')


The os.popen2/3/4 variants work much like os.popen, but they connect additional streams and accept an optional second argument that specifies text or binary-mode data (t or bmore on the distinction in the next chapter).

The os.popen calls are also Python's portable equivalent of Unix-like shell syntax for redirecting the streams of spawned programs. The Python versions also work on Windows, though, and are the most platform-neutral way to launch another program from a Python script. The command-line strings you pass to them may vary per platform (e.g., a directory listing requires an ls on Unix but a dir on Windows), but the call itself works on all major Python platforms.

On Unix-like platforms, the combination of the calls os.fork, os.pipe, os.dup, and some os.exec variants can be used to start a new independent program with streams connected to the parent program's streams. As such, it's another way to redirect streams and a low-level equivalent to tools such as os.popen.

As of this writing, the os.fork call does not work on the standard version of Python for Windows, however, because it is too much at odds with that system's process model. See Chapter 5 for more on all of these calls, especially its section on pipes, as well its sidebar on Cygwin, a third-party package that includes a library for use on Windows that adds Unix calls such as fork and a version of Python that contains such tools.[*]

    [*] More historical anecdotes for users of older releases: as of Python 2.0, the popen2 and popen3 calls are made available in the os module, and this subsumes the older popen2 module. For example, os.popen2 is the same as the older popen2.popen2 except that the order of stdin and stdout in the call's result tuple is swapped.

In the next chapter, we'll continue our survey of Python system interfaces by exploring the tools available for processing files and directories. Although we'll be shifting focus somewhat, we'll find that some of what we've learned here will already begin to come in handy as general system-related tools. Spawning shell commands, for instance, provides ways to inspect directories, and the file interface we will expand on in the next chapter is at the heart of the stream processing techniques we have studied here.

		21.1.3 File and Directory Tools

		21.1.4 

		21.1.5 

		21.1.6 


	21.2

22. Core Python Programming, Second Edition

	22.1 Getting Started
This "quick start" section is intended to "flash" Python to you so that any constructs recognized from previous programming experience can be used for your immediate needs. The details will be spelled out in succeeding chapters, but a high-level tour is one fast and easy way to get you into Python and show you what it has to offer. The best way to follow along is to bring up the Python interpreter in front of you and try some of these examples, and at the same time you can experiment on your own.

We introduced how to start up the Python interpreter in Chapter 1 as well as in the exercises (Problems 1-4). In all interactive examples, you will see the Python primary ( >>> ) and secondary ( ... ) prompts. The primary prompt is a way for the interpreter to let you know that it is expecting the next Python statement, while the secondary prompt indicates that the interpreter is waiting for additional input to complete the current statement.

You will notice two primary ways that Python "does things" for you: statements and expressions (functions, equations, etc.). Most of you already know the difference between the two, but in case you need to review, a statement is a body of control which involves using keywords. It is similar to issuing a command to the interpreter. You ask Python to do something for you, and it will do it. Statements may or may not lead to a result or output. Let us use the print statement for the programmer's perennial first example, Hello World:

   >>> print 'Hello World!'
   Hello World!


Expressions, on the other hand, do not use keywords. They can be simple equations that you use with mathematical operators, or can be functions which are called with parentheses. They may or may not take input, and they may or may not return a (meaningful) value. (Functions that do not explicitly return a value by the programmer automatically return None, Python's equivalent to NULL.) An example of a function that takes input and has a return value is the abs() function, which takes a number and returns its absolute value is:

  >>> abs(4)
  4
  >>> abs(-4)
  4


We will introduce both statements and expressions in this chapter. Let us continue with more about the print statement.
	
		22.1.1  Program Output, the print Statement, and "Hello World!"
Python's print statement (2.x) or function (3.x) is the tool for displaying program output to your users, similar to C's printf() and echo for shell scripts. In fact, it even supports printf()-style string substitution (using the % string format operator):

>>> print "%s is number %d!" % ("Python", 1)
Python is number 1!


Core Note: Dumping variable contents in interactive interpreter

	

Usually when you want to see the contents of a variable, you use the print statement in your code. However, from within the interactive interpreter, you can use the print statement to give you the string representation of a variable, or just dump the variable raw-this is accomplished by simply giving the name of the variable.

In the following example, we assign a string variable, then use print to display its contents. Following that, we issue just the variable name.

>>> myString = 'Hello World!'
>>> print myString
Hello World!
>>> myString
'Hello World!'


Notice how just giving only the name reveals quotation marks around the string. The reason for this is to allow objects other than strings to be displayed in the same manner as this string-being able to display a printable string representation of any object, not just strings. The quotes are there to indicate that the object whose value you just dumped to the display is a string. Once you become more familiar with Python, you will recognize that str() is used for print statements, while repr() is what the interactive interpreter calls to display your objects.

The underscore (_) also has special meaning in the interactive interpreter: the last evaluated expression. So after the code above has executed, _ will contain the string:

 >>> _
 'Hello World!'



%s means to substitute a string while %d indicates an integer should be substituted. See Section 6.4.1 for more information on the string format operator. Also see the str.format() method added in 2.6.

The print statement can also redirect output to a file. Debugging in 2.0, the >> symbols precede a valid file, such as these 2 examples, one to standard error and the other to a log file:

import sys
print >> sys.stderr, 'Fatal error: invalid input!'

logfile = open('/tmp/mylog.txt', 'a')
print >> logfile, 'Fatal error: invalid input!'
logfile.close()


print is changing to a function [print()] in Python 3.0-also see Python Enhancement Proposal (PEP) 3105 (more in PEPs coming up in the Core Note on p. 54). This is a significant change, so starting in Python 2.6, you can start coding against the new function by adding this special import statement:

from __future__ import print_function


The syntax of the new function is:

print(*args, sep=' ', end='\n', file=None)


For example:

print('Fatal error: invalid input!', file=sys.stderr)

		22.1.2  Program Input and the raw_input()Built-in Function
>>> str=""
>>> while str != 'stop':
...     str=raw_input("\nPlz enter stop to stop\n")
... 

Plz enter stop to stop
stop
		22.1.3  Comments
		22.1.4  Operators
		22.1.5  Variables and Assignment
		22.1.6  Numbers
		22.1.7  Strings
		22.1.8  Lists and Tuples
		22.1.9  Dictionaries
		22.1.10 Code Blocks Use Indentation
		22.1.11 if Statement
		22.1.12 while Loop
		22.1.13 for Loop and the range() Built-in Function
		22.1.14 List Comprehensions
		22.1.15 Files and the open() and file() Built-in Functions
		22.1.16 Errors and Exceptions
		22.1.17 Functions
		22.1.18 Classes
		22.1.19 Modules
		22.1.20 Useful Functions
		22.1.21 Exercises


23. Programming Python, 3rd Edition

24. Python Cookbook, 2nd Edition

25. Python in a Nutshell, 2nd Edition

26. Python Phrasebook: Essential Code and Commands

27. Python Pocket Reference, 3rd Edition

28. Python: Essential Reference, Third Edition

29. Rapid GUI Programming with Python and Qt: The Definitive Guide to PyQt Programming

30. learning python, 3rd edition
	
	30.1. Types and Operations
	
		30.1.1. Introducing Python Object Types

This chapter begins our tour of the Python language. In an informal sense, in Python, we do things with stuff. "Things" take the form of operations like addition and concatenation, and "stuff" refers to the objects on which we perform those operations. In this part of the book, our focus is on that stuff, and the things our programs can do with it.

Somewhat more formally, in Python, data takes the form of objects--either built-in objects that Python provides, or objects we create using Python or external language tools such as C extension libraries. Although we'll firm up this definition later, objects are essentially just pieces of memory, with values and sets of associated operations.

Because objects are the most fundamental notion in Python programming, we'll start this chapter with a survey of Python's built-in object types.

By way of introduction, however, let's first establish a clear picture of how this chapter fits into the overall Python picture. From a more concrete perspective, Python programs can be decomposed into modules, statements, expressions, and objects, as follows:

   |1.  Programs are composed of modules.
   |2.  Modules contain statements.
   |3.  Statements contain expressions.
   |4.  Expressions create and process objects.

The discussion of modules in Chapter 3 introduced the highest level of this hierarchy. This part's chapters begin at the bottom, exploring both built-in objects and the expressions you can code to use them.

			30.1.1.1 Why Use Built-in Types?

If you've used lower-level languages such as C or C++, you know that much of your work centers on implementing objects--also known as data structuresto represent the components in your application's domain. You need to lay out memory structures, manage memory allocation, implement search and access routines, and so on. These chores are about as tedious (and error prone) as they sound, and they usually distract from your program's real goals.

In typical Python programs, most of this grunt work goes away. Because Python provides powerful object types as an intrinsic part of the language, there's usually no need to code object implementations before you start solving problems. In fact, unless you have a need for special processing that built-in types don't provide, you're almost always better off using a built-in object instead of implementing your own. Here are some reasons why:

    * Built-in objects make programs easy to write. For simple tasks, built-in types are often all you need to represent the structure of problem domains. Because you get powerful tools such as collections (lists) and search tables (dictionaries) for free, you can use them immediately. You can get a lot of work done with Python's built-in object types alone.
    * Built-in objects are components of extensions. For more complex tasks, you still may need to provide your own objects, using Python classes or C language interfaces. But as you'll see in later parts of this book, objects implemented manually are often built on top of built-in types such as lists and dictionaries. For instance, a stack data structure may be implemented as a class that manages or customizes a built-in list.
    * Built-in objects are often more efficient than custom data structures. Python's built-in types employ already optimized data structure algorithms that are implemented in C for speed. Although you can write similar object types on your own, you'll usually be hard-pressed to get the level of performance built-in object types provide.
    * Built-in objects are a standard part of the language. In some ways, Python borrows both from languages that rely on built-in tools (e.g., LISP) and languages that rely on the programmer to provide tool implementations or frameworks of their own (e.g., C++). Although you can implement unique object types in Python, you don't need to do so just to get started. Moreover, because Python's built-ins are standard, they're always the same; proprietary frameworks, on the other hand, tend to differ from site to site.

In other words, not only do built-in object types make programming easier, but they're also more powerful and efficient than most of what can be created from scratch. Regardless of whether you implement new object types, built-in objects form the core of every Python program.

				30.1.1.1.1 Python's Core Data Types

Table 4-1 previews Python's built-in object types and some of the syntax used to code their literals--that is, the expressions that generate these objects.[11] Some of these types will probably seem familiar if you've used other languages; for instance, numbers and strings represent numeric and textual values, respectively, and files provide an interface for processing files stored on your computer.

    [11] * In this book, the term literal simply means an expression whose syntax generates an objectsometimes also called a constant. Note that the term "constant" does not imply objects or variables that can never be changed (i.e., this term is unrelated to C++'s const or Python's "immutable"a topic explored later in this chapter).

Table 4-1. Built-in objects preview
Object type 	Example literals/creation
Numbers 	1234, 3.1415, 999L, 3+4j, Decimal
Strings 	'spam', "guido's"
Lists 	[1, [2, 'three'], 4]
Dictionaries 	{'food': 'spam', 'taste': 'yum'}
Tuples 	(1,'spam', 4, 'U')
Files 	myfile = open('eggs', 'r')
Other types 	Sets, types, None, Booleans

Table 4-1 isn't really complete, because everything we process in Python programs is a kind of object. For instance, when we perform text pattern matching in Python, we create pattern objects, and when we perform network scripting, we use socket objects. These other kinds of objects are generally created by importing and using modules, and they have behavior all their own.

We call the object types in Table 4-1 core data types because they are effectively built into the Python languagethat is, there is specific syntax for generating most of them. For instance, when you run the following code:


>>> 'spam'


you are, technically speaking, running a literal expression, which generates and returns a new string object. There is specific Python language syntax to make this object. Similarly, an expression wrapped in square brackets makes a list, one in curly braces makes a dictionary, and so on. Even though, as we'll see, there are no type declarations in Python, the syntax of the expressions you run determines the types of objects you create and use. In fact, object-generation expressions like those in Table 4-1 are generally where types originate in the Python language.

Just as importantly, once you create an object, you bind its operation set for all time--you can perform only string operations on a string and list operations on a list. As you'll learn, Python is dynamically typed (it keeps track of types for you automatically instead of requiring declaration code), but it is also strongly typed (you can only perform on an object operations that are valid for its type).

Functionally, the object types in Table 4-1 are more general and powerful than what you may be accustomed to. For instance, you'll find that lists and dictionaries alone are powerful data representation tools that obviate most of the work you do to support collections and searching in lower-level languages. In short, lists provide ordered collections of other objects, while dictionaries store objects by key; both lists and dictionaries may be nested, can grow and shrink on demand, and may contain objects of any type.

We'll study each of the object types in Table 4-1 in detail in upcoming chapters. Before digging into the details, though, let's begin by taking a quick look at Python's core objects in action. The rest of this chapter provides a preview of the operations we'll explore in more depth in the chapters that follow. Don't expect to find the full story herethe goal of this chapter is just to whet your appetite and introduce some key ideas. Still, the best way to get started is to get started, so let's jump right into some real code.

		30.1.2. Numbers

This chapter begins our in-depth tour of the Python language. In Python, data takes the form of objects--either built-in objects that Python provides, or objects we create using Python tools and other languages, such as C. In fact, objects are the basis of every Python program you will ever write. Because they are the most fundamental notion in Python programming, objects are also our first focus in this book.

In the preceding chapter, we took a quick pass over Python's core object types. Although essential terms were introduced in that chapter, we avoided too many specifics in the interest of space. Here, we'll begin a more careful second look at data type concepts to fill in some of the details we glossed over earlier. We'll also explore some types related to numbers, such as sets and Booleans. Let's get started by exploring our first data type category: the Python number.

			30.1.2.1 Python Numeric Types

Python's number types are fairly typical and will probably seem familiar if you've used almost any other programming language in the past. They can be used to keep track of your bank balance, the distance to Mars, the number of visitors to your web site, and just about any other numeric quantity.

In Python, numbers are not really a single object type, but a category of similar types. Python supports the usual numeric types (integers and floating points), as well as literals for creating numbers and expressions for processing them. In addition, Python provides more advanced numeric programming support, including a complex number type, an unlimited-precision integer type, a fixed-precision decimal type, sets and Booleans, and a variety of numeric tool libraries. The next few sections give an overview of the numeric support in Python.

				30.1.2.1.1 Numeric Literals

Among its basic types, Python provides the usual numeric types: it supports integer and floating-point numbers (with a fractional part), and all their associated syntax and operations. Like the C language, Python also allows you to write integers using hexadecimal and octal literals. But, unlike C, Python additionally offers a complex number type, as well as a long integer type with unlimited precision (it can grow to have as many digits as your memory space allows). Table 5-1 shows what Python's numeric types look like when written out in a program (that is, as literals).

|Table 5-1. Numeric literals
|Literal 	Interpretation
|1234, -24, 0 	Normal integers (C longs)
|9999999999999999999L 	Long integers (unlimited size)
|1.23, 3.14e-10, 4E210, 4.0e+210 	Floating-point numbers (C doubles)
|0177, 0x9ff, 0XFF 	Octal and hex literals for integers
|3+4j, 3.0+4.0j, 3J 	Complex number literals

In general, Python's numeric types are straightforward, but a few coding concepts are worth highlighting here:


Integer and floating-point literals

    Integers are written as strings of decimal digits. Floating-point numbers have an embedded decimal point, and/or an optional signed exponent introduced by an e or E. If you write a number with a decimal point or exponent, Python makes it a floating-point object, and uses floating-point (not integer) math when the object is used in an expression. The rules for writing floating-point numbers are the same in Python as in the C language.

Numeric precision and long integers

    Plain Python integers (row 1 of Table 5-1) are implemented as C "longs" internally (i.e., at least 32 bits), and Python floating-point numbers are implemented as C "doubles"; Python numbers therefore get as much precision as the C compiler used to build the Python interpreter gives to longs and doubles.[14]

        [14] * That is, the standard CPython implementation. In the Jython Java-based implementation, Python types are really Java classes.


Long integer literals

    If, however, an integer literal ends with an l or L, it becomes a Python long integer (not to be confused with a C long) and can grow as large as needed. In Python 2.2 and later, because integers are automatically converted to long integers when their values overflow 32 bits, you don't need to type the letter L yourself--Python automatically converts up to long integer when extra precision is needed.

Hexadecimal and octal literals

    The rules for writing hexadecimal (base 16) and octal (base 8) integers are the same in Python as in C. Octal literals start with a leading zero (0) followed by a string of digits 0-7. Hexadecimals start with a leading 0x or 0X, followed by hexadecimal digits 0-9 and A-F. In hexadecimal literals, hex digits may be coded in lower- or uppercase. Both octal and hexadecimal literals produce integer objects; they are just alternative syntaxes for specifying values.

Complex numbers

    Python complex literals are written as realpart+imaginarypart, where the imaginarypart is terminated with a j or J. The realpart is technically optional, so the imaginarypart may appear on its own. Internally, complex numbers are implemented as pairs of floating-point numbers, but all numeric operations perform complex math when applied to complex numbers.

				30.1.2.1.2 Built-in Numeric Tools and Extensions

Besides the built-in number literals shown in Table 5-1, Python provides a set of tools for processing number objects:


Expression operators

    +, *, >>, **, etc.

Built-in mathematical functions

    pow, abs, etc.

Utility modules

    random, math, etc.

We'll meet all of these as we go along.

Finally, if you need to do serious number crunching, an optional extension for Python called NumPy (Numeric Python) provides advanced numeric programming tools, such as a matrix data type, vector processing, and sophisticated computation libraries. Hardcore scientific programming groups at places like Lawrence Livermore and NASA use Python with NumPy to implement the sorts of tasks they previously coded in C++, FORTRAN, or Matlab.

Because it's so advanced, we won't talk further about NumPy in this book. You can find additional support for advanced numeric programming in Python at the Vaults of Parnassus site, or by searching the Web. Also note that NumPy is currently an optional extension; it doesn't come with Python, and must be installed separately.

	

In Python 3.0, the current integer and long integer types will be unified, such that there will be only one integer type, int. It will support arbitrary precision, like the current long integer type. Most programmers will see little or no change as a result of this. See the Python 3.0 release notes for more details.

			30.1.2.2 Python Expression Operators
	
Perhaps the most fundamental tool that processes numbers is the expression: a combination of numbers (or other objects) and operators that computes a value when executed by Python. In Python, expressions are written using the usual mathematical notation and operator symbols. For instance, to add two numbers X and Y, say X + Y, which tells Python to apply the + operator to the values named by X and Y. The result of the expression is the sum of X and Y, another number object.

Table 5-2 lists all the operator expressions available in Python. Many are self-explanatory; for instance, the usual mathematical operators (+, -, *, /, and so on) are supported. A few will be familiar if you've used C in the past: % computes a division remainder, << performs a bitwise left-shift, & computes a bitwise AND result, etc. Others are more Python-specific, and not all are numeric in nature. For example, the is operator tests object identity (i.e., address, a strict form of equality), lambda creates unnamed functions, and so on. More on most of these later.

Table 5-2. Python expression operators and precedence
Operators 	Description
yield x 	Generator function send protocol (new in Release 2.5)
lambda args: expression 	Anonymous function generation
x if y else z 	Ternary selection expression (new in Release 2.5)
x or y 	Logical OR (y is evaluated only if x is false)
x and y 	Logical AND (y is evaluated only if x is true)
not x 	Logical negation
x < y, x <= y, x > y, x >= y, x == y, x <> y, x != y, x is y, x is not y, x in y, x not in y 	Comparison operators, value equality operators,[15] object identity tests, sequence membership
x | y 	Bitwise OR
x ^ y 	Bitwise eXclusive OR
x & y 	Bitwise AND
x << y, x >> y 	Shift x left or right by y bits
-x + y, x - y 	Addition/concatenation, subtraction
x * y, x % y, x / y, x // y 	Multiplication/repetition, remainder/format, division[16]
-x, +x, tildex, x ** y 	Unary negation, identity, bitwise complement, binary power
x[i], x[i:j], x.attr, x(...) 	Indexing, slicing, qualification, function calls
(...), [...], {...}, '...' 	Tuple, list,[17] dictionary, conversion to string[18]

    [15] a In Python 2.5, value inequality can be written as either X != Y or X <> Y. In Python 3.0, the latter of these options will be removed because it is redundant; use X != Y for value inequality tests.

    [16] b Floor division (X // Y), new in Release 2.2, always truncates fractional remainders. This is further described in "Section 5.3.3."

    [17] c Beginning with Python 2.0, the list syntax ([...]) can represent either a list literal or a list comprehension expression. The latter of these performs an implied loop and collects expression results in a new list.

    [18] d Conversion of objects to their print strings can also be accomplished with the more readable str and repr built-in functions, which are described in "Section 5.3.2" later in this chapter. Due to its obscurity, the backticks expression 'X' is scheduled to be removed in Python 3.0; use repr(X) instead.

				30.1.2.2.1 Mixed Operators Follow Operator Precedence

As in most languages, in Python, more complex expressions are coded by stringing together the operator expressions in Table 5-2. For instance, the sum of two multiplications might be written as a mix of variables and operators:


A * B + C * D


So, how does Python know which operation to perform first? The answer to this question lies in operator precedence. When you write an expression with more than one operator, Python groups its parts according to what are called precedence rules, and this grouping determines the order in which the expression's parts are computed. In Table 5-2, operators lower in the table have higher precedence, and so bind more tightly in mixed expressions.

For example, if you write X + Y * Z, Python evaluates the multiplication first (Y * Z), then adds that result to X because * has higher precedence (is lower in the table) than +. Similarly, in this section's original example, both multiplications (A * B and C * D) will happen before their results are added.

				30.1.2.2.2 Parentheses Group Subexpressions

You can forget about precedence completely if you're careful to group parts of expressions with parentheses. When you enclose subexpressions in parentheses, you override Python's precedence rules; Python always evaluates expressions in parentheses first before using their results in the enclosing expressions.

For instance, instead of coding X + Y * Z, you could write one of the following to force Python to evaluate the expression in the desired order:


(X + Y) * Z
X + (Y * Z)


In the first case, + is applied to X and Y first, because this subexpression is wrapped in parentheses. In the second case, the * is performed first (just as if there were no parentheses at all). Generally speaking, adding parentheses in big expressions is a great idea; it not only forces the evaluation order you want, but also aids readability.

				30.1.2.2.3 Mixed Types Are Converted Up

Besides mixing operators in expressions, you can also mix numeric types. For instance, you can add an integer to a floating-point number: 40 + 3.14

But this leads to another question: what type is the result--integer or floating-point? The answer is simple, especially if you've used almost any other language before: in mixed-type expressions, Python first converts operands up to the type of the most complicated operand, and then performs the math on same-type operands. If you've used C, you'll find this behavior similar to type conversions in that language.

Python ranks the complexity of numeric types like so: integers are simpler than long integers, which are simpler than floating-point numbers, which are simpler than complex numbers. So, when an integer is mixed with a floating point, as in the preceding example, the integer is converted up to a floating-point value first, and floating-point math yields the floating-point result. Similarly, any mixed-type expression, where one operand is a complex number, results in the other operand being converted up to a complex number, and the expression yields a complex result. As you'll see later in this section, as of Release 2.2, Python also automatically converts normal integers to long integers whenever their values are too large to fit in a normal integer.

You can force the issue by calling built-in functions to convert types manually:


|>>> int(3.1415)
|3
|>>> float(3)
|3.0
|>>> long(4)
|4L


However, you won't usually need to do this: because Python automatically converts up to the more complex type within an expression, the results are normally what you want.

Also, keep in mind that all these mixed-type conversions only apply when mixing numeric types (e.g., an integer and a floating-point number) around an operator or comparison. In general, Python does not convert across other type boundaries. Adding a string to an integer, for example, results in an error, unless you manually convert one or the other; watch for an example when we meet strings in Chapter 7.

				30.1.2.2.4 Preview: Operator Overloading

Although we're focusing on built-in numbers right now, keep in mind that all Python operators may be overloaded (i.e., implemented) by Python classes and C extension types to work on objects you create. For instance, you'll see later that objects coded with classes may be added with + expressions, indexed with [i] expressions, and so on.

Furthermore, Python itself automatically overloads some operators, such that they perform different actions depending on the type of built-in objects being processed. For example, the + operator performs addition when applied to numbers, but performs concatenation when applied to sequence objects such as strings and lists. In fact, + can mean anything at all when applied to objects you define with classes.

As we saw in the prior chapter, this property is usually called polymorphism--a term indicating that the meaning of an operation depends on the type of objects being operated on. We'll revisit this concept when we explore functions in Chapter 15 because it becomes a much more obvious feature in that context.

			30.1.2.3 Numbers in Action

	Probably the best way to understand numeric objects and expressions is to see them in action. So, let's start up the interactive command line, and try some basic but illustrative operations (see Chapter 3 for pointers if you need help starting an interactive session).

				30.1.2.3.1 Variables and Basic Expressions

First of all, let's exercise some basic math. In the following interaction, we first assign two variables (a and b) to integers so we can use them later in a larger expression. Variables are simply names, created by you or Python, that are used to keep track of information in your program. We'll say more about this in the next chapter, but in Python:

    * Variables are created when they are first assigned values.
    * Variables are replaced with their values when used in expressions.
    * Variables must be assigned before they can be used in expressions.
    * Variables refer to objects and are never declared ahead of time.

In other words, these assignments cause the variables a and b to spring into existence automatically:


% python 
>>> a = 3  # Name created
>>> b = 4 


I've also used a comment here. Recall that in Python code, text after a # mark and continuing to the end of the line is considered to be a comment and is ignored. Comments are a way to write human-readable documentation for your code. Because code you type interactively is temporary, you won't normally write comments in this context, but I've added them to some of this book's examples to help explain the code.[19] In the next part of the book, we'll meet a related featuredocumentation stringsthat attaches the text of your comments to objects.

    [19] * If you're working along, you don't need to type any of the comment text from the # through the end of the line; comments are simply ignored by Python, and not required parts of the statements we're running.

Now, let's use our new integer objects in expressions. At this point, the values of a and b are still 3 and 4, respectively. Variables like these are replaced with their values whenever they're used inside an expression, and the expression results are echoed back immediately when working interactively:


>>> a + 1, a - 1  # Addition (3 + 1), subtraction (3 - 1)
(4, 2)
>>> b * 3, b / 2  # Multiplication (4 * 3), division (4 / 2)
(12, 2)
>>> a % 2, b ** 2  # Modulus (remainder), power
(1, 16)
>>> 2 + 4.0, 2.0 ** b  # Mixed-type conversions
(6.0, 16.0)


Technically, the results being echoed back here are tuples of two values because the lines typed at the prompt contain two expressions separated by commas; that's why the results are displayed in parentheses (more on tuples later). Note that the expressions work because the variables a and b within them have been assigned values. If you use a different variable that has never been assigned, Python reports an error rather than filling in some default value:


>>> c * 2
Traceback (most recent call last):
  File "<stdin>", line 1, in ?
NameError: name 'c' is not defined


You don't need to predeclare variables in Python, but they must have been assigned at least once before you can use them at all. In practice, this means you have to initialize counters to zero before you can add to them, initialize lists to an empty list before you can append to them, and so on.

Here are two slightly larger expressions to illustrate operator grouping and more about conversions:


|>>> b / 2 + a  # Same as ((4 / 2) + 3)
|5
|>>> print b / (2.0 + a)  # Same as (4 / (2.0 + 3))
|0.8


In the first expression, there are no parentheses, so Python automatically groups the components according to its precedence rulesbecause / is lower in Table 5-2 than +, it binds more tightly, and so is evaluated first. The result is as if the expression had parentheses as shown in the comment to the right of the code. Also, notice that all the numbers are integers in the first expression; because of that, Python performs integer division and addition.

In the second expression, parentheses are added around the + part to force Python to evaluate it first (i.e., before the /). We also made one of the operands floating-point by adding a decimal point: 2.0. Because of the mixed types, Python converts the integer referenced by a to a floating-point value (3.0) before performing the +. It also converts b to a floating-point value (4.0), and performs floating-point division; (4.0 / 5.0) yields a floating-point result of 0.8. If all the numbers in this expression were integers, it would instead invoke integer division (4 / 5), and the result would be the truncated integer 0 (in Python 2.5, at leastsee the discussion of true division ahead).

				30.1.2.3.2 Numeric Display Formats

Notice that we used a print statement in the last of the preceding examples. Without the print, you'll see something that may look a bit odd at first glance:


|>>> b / (2.0 + a)  # Auto echo output: more digits
|0.80000000000000004
|
|>>> print b / (2.0 + a)  # Print rounds off digits
|0.8


The full story behind this odd result has to do with the limitations of floating-point hardware, and its inability to exactly represent some values. Because computer architecture is well beyond this book's scope, though, we'll finesse this by saying that all of the digits in the first output are really there, in your computer's floating-point hardwareit's just that you're not normally accustomed to seeing them. I've used this example to demonstrate the difference in output formattingthe interactive prompt's automatic result echo shows more digits than the print statement. If you don't want to see all the digits, include the print.

Note, however, that not all values have so many digits to display:


|>>> 1 / 2.0
|0.5


and that there are more ways to display the bits of a number inside your computer than prints and automatic echoes:


|>>> num = 1 / 3.0 
|>>> num  # Echoes
|0.33333333333333331
|>>> print num  # Print rounds
|0.333333333333
|
|>>> "%e" % num  # String formatting
|'3.333333e-001'
|>>> "%2.2f" % num  # String formatting
|'0.33'


The last two of these employ string formatting, an expression that allows for format flexibility, which we will explore in the upcoming chapter on strings (Chapter 7).

str and repr Display Formats

Technically, the difference between default interactive echoes and prints corresponds to the difference between the built-in repr and str functions:


|>>> repr(num)  # Used by echoes: as-code form
|'0.33333333333333331'
|>>> str(num)  # Used by print: user-friendly form
|'0.333333333333'


Both of these convert arbitrary objects to their string representations: repr (and the default interactive echo) produces results that look as though they were code; str (and the print statement) converts to a typically more user-friendly format. This notion will resurface when we study strings, and you'll find more on these built-ins in general later in the book.

				30.1.2.3.3 Division: Classic, Floor, and True

Now that you've seen how division works, you should know that it is scheduled for a slight change in a future Python release (currently, 3.0, scheduled to appear some time after this edition is released). In Python 2.5, things work as just described, but there are actually two different division operators (one of which will change):


X / Y

    Classic division. In Python 2.5 and earlier, this operator truncates results for integers, and keeps remainders for floating-point numbers, as described here. This operator will be changed to true divisionalways keeping remainders regardless of typesin a future Python release (3.0).

X // Y

    Floor division. Added in Python 2.2, this operator always truncates fractional remainders down to their floor, regardless of types.

Floor division was added to address the fact that the results of the current classic division model are dependent on operand types, and so can be difficult to anticipate in a dynamically typed language like Python.

Due to possible backward-compatibility issues, division in Python is in a state of flux today. To recap, in version 2.5, / division works as described by default, and // floor division can be used to truncate result remainders to their floor regardless of their types:


>>> (5 / 2), (5 / 2.0), (5 / -2.0), (5 / -2)
(2, 2.5, -2.5, -3)

>>> (5 // 2), (5 // 2.0), (5 // -2.0), (5 // -2)
(2, 2.0, -3.0, -3)

>>> (9 / 3), (9.0 / 3), (9 // 3), (9 // 3.0)
(3, 3.0, 3, 3.0)


In a future Python release, / division is slated to be changed to return a true division result that always retains remainders, even for integersfor example, 1 / 2 will be 0.5, not 0, while 1 // 2 will still be 0.

Until this change is incorporated completely, you can see the way that the / operator will likely work in the future by using a special import of the form: from __future__ import division. This turns the / operator into a true division operator (keeping remainders), but leaves // as is. Here's how / will eventually behave:


>>> from __future__ import division

>>> (5 / 2), (5 / 2.0), (5 / -2.0), (5 / -2)
(2.5, 2.5, -2.5, -2.5)

>>> (5 // 2), (5 // 2.0), (5 // -2.0), (5 // -2)
(2, 2.0, -3.0, -3)

>>> (9 / 3), (9.0 / 3), (9 // 3), (9 // 3.0)
(3.0, 3.0, 3, 3.0)


Watch for a simple prime number while loop example in Chapter 13, and a corresponding exercise at the end of Part 4 that illustrates the sort of code that may be impacted by this / change. In general, any code that depends on / truncating an integer result may be affected (use the new // instead). As I write this, this change is scheduled to occur in Python 3.0, but be sure to try these expressions in your version to see which behavior applies. Also stay tuned for more on the special from command used here; it's discussed further in Chapter 21.

				30.1.2.3.4 Bitwise Operations

Besides the normal numeric operations (addition, subtraction, and so on), Python supports most of the numeric expressions available in the C language. For instance, here it's at work performing bitwise shift and Boolean operations:


|>>> x = 1  # 0001
|>>> x << 2  # Shift left 2 bits: 0100
|4
|>>> x | 2  # bitwise OR: 0011
|3
|>>> x & 1  # bitwise AND: 0001
|1


In the first expression, a binary 1 (in base 2, 0001) is shifted left two slots to create a binary 4 (0100). The last two operations perform a binary OR (0001|0010 = 0011), and a binary AND (0001&0001 = 0001). Such bit-masking operations allow us to encode multiple flags and other values within a single integer.

We won't go into much more detail on "bit-twiddling" here. It's supported if you need it, and it comes in handy if your Python code must deal with things like network packets, or packed binary data produced by a C program. Be aware, though, that bitwise operations are often not as important in a high-level language such as Python as they are in a low-level language such as C. As a rule of thumb, if you find yourself wanting to flip bits in Python, you should think about which language you're really coding. In general, there are often better ways to encode information in Python than bit strings.

				30.1.2.3.5 Long Integers

Now for something more exotic: here's a look at long integers in action. When an integer literal ends with a letter L (or lowercase l), Python creates a long integer. In Python, a long integer can be arbitrarily big. That is, it can have as many digits as you have room for in memory:


|>>> 9999999999999999999999999999999999999L + 1
|10000000000000000000000000000000000000L


The L at the end of the digit string tells Python to create a long integer object with unlimited precision. In fact, as of Python 2.2, even the letter L is optionalPython automatically converts a normal integer up to a long integer whenever its value is too large to fit in a normal integer (technically, when it overflows normal integer precision, which is usually 32 bits). That is, you don't need to code the L yourself, as Python will provide the extra precision if your program needs it:


|>>> 9999999999999999999999999999999999999 + 1
|10000000000000000000000000000000000000L


Long integers are a convenient built-in tool. For instance, you can use them to count the national debt in pennies in Python directly (if you are so inclined, and have enough memory on your computer). They are also why we were able to raise 2 to such large powers in the examples in Chapter 3:


|>>> 2L ** 200
|1606938044258990275541962092341162602522202993782792835301376L
|>>>
|>>> 2 ** 200
|1606938044258990275541962092341162602522202993782792835301376L


Because Python must do extra work to support their extended precision, long integer math is usually substantially slower than normal integer math (which usually maps directly to the hardware). However, if you need the precision, the fact that it's built-in for you to use will likely outweigh its performance penalty.

				30.1.2.3.6 Complex Numbers

Complex numbers are a distinct core object type in Python. If you know what they are, you know why they are useful; if not, consider this section optional reading. Complex numbers are represented as two floating-point numbersthe real and imaginary partsand are coded by adding a j or J suffix to the imaginary part. We can also write complex numbers with a nonzero real part by adding the two parts with a +. For example, the complex number with a real part of 2 and an imaginary part of -3 is written 2 + -3j. Here are some examples of complex math at work:


>>> 1j * 1J
(-1+0j)
>>> 2 + 1j * 3
(2+3j)
>>> (2 + 1j) * 3
(6+3j)


Complex numbers also allow us to extract their parts as attributes, support all the usual mathematical expressions, and may be processed with tools in the standard cmath module (the complex version of the standard math module). Complex numbers typically find roles in engineering-oriented programs. Because they are advanced tools, check Python's language reference manual for additional details.

				30.1.2.3.7 Hexadecimal and Octal Notation

As mentioned earlier in this chapter, Python integers can be coded in hexadecimal (base 16) and octal (base 8) notation, in addition to the normal base 10 decimal coding:

    * Octal literals have a leading 0, followed by a string of octal digits 0-7, each of which represents three bits.
    * Hexadecimal literals have a leading 0x or 0X, followed by a string of hex digits 0-9 and upper- or lowercase A-F, each of which stands for four bits.

Keep in mind that this is simply an alternative syntax for specifying the value of an integer object. For example, the following octal and hexadecimal literals produce normal integers with the specified values:


>>> 01, 010, 0100  # Octal literals
(1, 8, 64)
>>> 0x01, 0x10, 0xFF  # Hex literals
(1, 16, 255)


Here, the octal value 0100 is decimal 64, and hex 0xFF is decimal 255. Python prints in decimal (base 10) by default but provides built-in functions that allow you to convert integers to their octal and hexadecimal digit strings:


>>> oct(64), hex(64), hex(255)
('0100', '0x40', '0xff')


The oct function converts decimal to octal, and hex converts to hexadecimal. To go the other way, the built-in int function converts a string of digits to an integer, and an optional second argument lets you specify the numeric base:


>>> int('0100'), int('0100', 8), int('0x40', 16)
(100, 64, 64)


The eval function, which you'll meet later in this book, treats strings as though they were Python code. Therefore, it has a similar effect (but usually runs more slowlyit actually compiles and runs the string as a piece of a program, and it assumes you can trust the source of the string being run; a clever user might be able to submit a string that deletes files on your machine!):


>>> eval('100'), eval('0100'), eval('0x40')
(100, 64, 64)


Finally, you can also convert integers to octal and hexadecimal strings with a string formatting expression:


>>> "%o %x %X" % (64, 64, 255)
'100 40 FF'


Again, string formatting is covered in Chapter 7.

One warning before moving on: be careful not to begin a string of digits with a leading zero in Python, unless you really mean to code an octal value. Python will treat it as base 8, which may not work as you'd expect010 is always decimal 8, not decimal 10 (despite what you may or may not think!).

				30.1.2.3.8 Other Built-in Numeric Tools

In addition to its core object types, Python also provides both built-in functions and built-in modules for numeric processing. The int and round built-in functions, for instance, truncate and round floating-point numbers, respectively. Here are examples of the built-in math module (which contains most of the tools in the C language's math library) and a few built-in functions at work:


|>>> import math 
|>>> math.pi, math.e  # Common constants
|(3.1415926535897931, 2.7182818284590451)
|
|>>> math.sin(2 * math.pi / 180)  # Sine, tangent, cosine
|0.034899496702500969
|
|>>> math.sqrt(144), math.sqrt(2)  # Square root
|(12.0, 1.4142135623730951)
|
|>>> abs(-42), 2**4, pow(2, 4) 
|(42, 16, 16)
|
|>>> int(2.567), round(2.567), round(2.567, 2)  # Truncate, round
|(2, 3.0, 2.5699999999999998)


As described earlier, the last output here will be (2, 3.0, 2.57) if we include print.

Notice that built-in modules such as math must be imported, but built-in functions such as abs are always available without imports. In other words, modules are external components, but built-in functions live in an implied namespace that Python automatically searches to find names used in your program. This namespace corresponds to the module called __builtin__. There is much more about name resolution in Part 4; for now, when you hear "module," think "import."

The standard library random module must be imported as well. This module provides tools for picking a random floating-point number between 0 and 1, selecting a random integer between two numbers, choosing an item at random from a sequence, and more:


|>>> import random
|>>> random.random(  )
|0.49741978338014803
|>>> random.random(  )
|0.49354866439625611
|
|>>> random.randint(1, 10)
|5
|>>> random.randint(1, 10)
|4

>>> random.choice(['Life of Brian', 'Holy Grail', 'Meaning of Life'])
'Life of Brian'
>>> random.choice(['Life of Brian', 'Holy Grail', 'Meaning of Life'])
'Holy Grail'


The random module can be useful for shuffling cards in games, picking images at random in a slideshow GUI, performing statistical simulations, and much more. For more details, see Python's library manual.

			30.1.2.4 Other Numeric Types

	In this chapter, we've been using Python's core numeric typesinteger, long integer, floating point, and complex. These will suffice for most of the number crunching that most programmers will ever need to do. Python comes with a handful of more exotic numeric types, though, that merit a quick look here.

				30.1.2.4.1 Decimal Numbers

Python 2.4 introduced a new core numeric type: the decimal object. Syntactically, decimals are created by calling a function within an imported module, rather than running a literal expression. Functionally, decimals are like floating-point numbers, with a fixed number of decimal points, and hence, a fixed precision. For example, with decimals, we can have a floating-point value that always retains just two decimal digits. Furthermore, we can specify how to round or truncate the extra decimal digits beyond the object's cutoff. Although it incurs a small performance penalty compared to the normal floating-point type, the decimal type is ideal for representing fixed-precision quantities like sums of money, and for achieving better numeric accuracy.

Floating-point object math is less than exact. For example, the following should yield zero, but it does not. The result is close to zero, but there are not enough bits to be precise here:


|>>> 0.1 + 0.1 + 0.1 - 0.3
|5.5511151231257827e-017


Printing the result to produce the user-friendly display format doesn't completely help, either because the hardware related to floating-point math is inherently limited in terms of accuracy:


| >>> print 0.1 + 0.1 + 0.1 - 0.3
| 5.55111512313e-017


However, with decimals, the result can be dead-on:


>>> from decimal import Decimal
>>> Decimal('0.1') + Decimal('0.1') + Decimal('0.1') - Decimal('0.3')
Decimal("0.0")


As shown here, we can make decimal objects by calling the Decimal constructor function in the decimal module, and passing in strings that have the desired number of decimal digits for the resulting object. When decimals of different precision are mixed in expressions, Python converts up to the largest number of decimal digits automatically:


>>> Decimal('0.1') + Decimal('0.10') + Decimal('0.10') - Decimal('0.30')
Decimal("0.00")


Other tools in the decimal module can be used to set the precision of all decimal numbers, and more. For instance, a context object in this module allows for specifying precision (number of decimal digits), and rounding modes (down, ceiling, etc.):


>>> input decimal
>>> decimal.Decimal(1) / decimal.Decimal(7)
Decimal("0.1428571428571428571428571429")

>>> decimal.getcontext(  ).prec = 4
>>> decimal.Decimal(1) / decimal.Decimal(7)
Decimal("0.1429")


Because use of the decimal type is still somewhat rare in practice, I'll defer to Python's standard library manuals and interactive help for more details.

				30.1.2.4.2 Sets

Python 2.4 also introduced a new collection type, the set. Because they are collections of other objects, sets are arguably outside the scope of this chapter. But because they support mathematical set operations, we'll take a look at their basic utility here. To make a set object, pass in a sequence or other iterable object to the built-in set function (similar utility is available in a module prior to Python 2.4, but no import is required as of 2.4):


>>> x = set('abcde')
>>> y = set('bdxyz')


You get back a set object, which contains all the items in the object passed in (notice that sets do not have a positional ordering, and so are not sequences):


>>> x
set(['a', 'c', 'b', 'e', 'd'])


Sets made this way support the common mathematical set operations with expression operators. Note that we can't perform these operations on plain sequenceswe must create sets from them in order to apply these tools:


>>> 'e' in x  # Set membership
True

>>> x - y  # Set difference
set(['a', 'c', 'e'])

>>> x | y  # Set union
set(['a', 'c', 'b', 'e', 'd', 'y', 'x', 'z'])

>>> x & y  # Set intersection
set(['b', 'd'])


Such operations are convenient when dealing with large data sets; the intersection of two sets contains objects in common to both, for example, and the union contains all items in either set. Here's a more realistic example of set operations at work, applied to lists of people in a hypothetical company:


>>> engineers = set(['bob', 'sue', 'ann', 'vic']) 
>>> managers  = set(['tom', 'sue']) 
>>>
>>> engineers & managers  # Who is both engineer and manager?
set(['sue'])
>>>
>>> engineers | managers  # All people in either category
set(['vic', 'sue', 'tom', 'bob', 'ann'])
>>>
>>> engineers - managers  # Engineers who are not managers
set(['vic', 'bob', 'ann'])


In addition, the set object provides method calls that implement more exotic set operations. Although set operations can be coded manually in Python (and often were in the past), Python's built-in sets use efficient algorithms and implementation techniques to provide quick and standard operation.

For more details on sets, see Python's library reference manual.

	

In Python 3.0, the literal notation {1, 2, 3} is proposed to have the same effect as the current call set([1, 2, 3]), and will thus be another way to create a set object. This is a future extension, so see the 3.0 release notes for details.

				30.1.2.4.3 Booleans

Some argue that the Python Boolean type, bool, is numeric in nature because its two values, True and False, are just customized versions of the integers 1 and 0 that print themselves differently. Although that's all most programmers need to know, let's explore this type in a bit more detail.

Python 2.3 introduced a new explicit Boolean data type called bool, with the values True and False available as new preassigned built-in names. Internally, the new names True and False are instances of bool, which is in turn just a subclass (in the object-oriented sense) of the built-in integer type int. True and False behave exactly like the integers 1 and 0, except that they have customized printing logicthey print themselves as the words True and False, instead of the digits 1 and 0 (technically, bool redefines its str and repr string formats).

Because of this customization, as of Python 2.3, the output of Boolean expressions typed at the interactive prompt prints as the words True and False instead of the older 1 and 0. In addition, Booleans make truth values more explicit. For instance, an infinite loop can now be coded as while True: instead of the less intuitive while 1:. Similarly, flags can be initialized more clearly with flag = False.

Again, though, for all other practical purposes, you can treat True and False as though they are predefined variables set to integer 1 and 0. Most programmers were preassigning True and False to 1 and 0 anyway, so the new type simply makes this a standard technique. Its implementation can lead to curious results, thoughbecause True is just the integer 1 with a custom display format, True + 3 yields 4 in Python!

We'll revisit Booleans in Chapter 9 (to define Python's notion of truth) and again in Chapter 12 (to see how Boolean operators like and and or work).

				30.1.2.4.4 Third-Party Extensions

Beyond Python's own numeric types, you'll find various third-party open source add-ons that provide even more exotic numeric tools. Types such as rational numbers, vectors, and matrixes can be obtained on the Web; do a search for more details.

			30.1.2.5 Chapter Summary
This chapter has taken a tour of Python's numeric object types and the operations we can apply to them. Along the way, we met the standard integer and floating-point types, as well as some more exotic and less commonly used types such as complex numbers and the decimal type. We also explored Python's expression syntax, type conversions, bitwise operations, and various literal forms for coding numbers in scripts.

Later in this part of the book, I'll fill in some details about the next object type, the string. In the next chapter, however, we'll take some time to explore the mechanics of variable assignment in more detail than we have here. This turns out to be perhaps the most fundamental idea in Python, so you should read through the next chapter before moving on. First, though, take the usual chapter quiz.

BRAIN BUILDER

Chapter Quiz

Q:	What is the value of the expression 2 * (3 + 4) in Python?
Q:	What is the value of the expression 2 * 3 + 4 in Python?
Q:	What is the value of the expression 2 + 3 * 4 in Python?
Q:	What tools can you use to find a number's square root, as well as its square?
Q:	What is the type of the result of the expression 1 + 2.0 + 3?
Q:	How could you truncate and round a floating-point number?
Q:	How can you convert an integer to a floating-point number?
Q:	How would you display an integer in octal or hexadecimal notation?
Q:	How might you convert an octal or hexadecimal string to a plain integer?


Quiz Answers

A:	The result value will be 14, the result of 2 * 7, because the parentheses force the addition to happen before the multiplication.
A:	Here, the result will be 10, the result of 6 + 4. Python's operator precedence rules are applied in the absence of parentheses, and multiplication has higher precedence than (i.e., happens before) addition, per Table 5-2.
A:	This expression yields 14, the result of 2 + 12, for the same precedence reasons as in the prior question.
A:	Functions for obtaining the square root, as well as pi, tangents, and more, are available in the imported math module. To find a number's square root, import math and call math.sqrt(N). To get a number's square, use either the exponent expression X ** 2, or the built-in function pow(X, 2).
A:	The result will be a floating-point number: the integers are converted up to floating point, the most complex type in the expression, and floating-point math is used to evaluate it.
A:	The int(N) function truncates, and the round(N, digits?) function rounds.
A:	The float(I) function converts an integer to a floating point; mixing an integer with a floating point within an expression will result in a conversion as well.
A:	The oct(I) and hex(I) built-in functions return the octal and hexadecimal string forms for an integer. The % string formatting expression also provides targets for doing this.
A:	The int(S, base?) function can be used to convert from octal and hexadecimal strings to normal integers (pass in 8 or 16 for the base). The eval(S) function can be used for this purpose too, but it's more expensive to run and can have security issues. Note that integers are always stored in binary in computer memory; these are just display string format conversions.

		30.1.3. The Dynamic Typing Interlude
In the prior chapter, we began exploring Python's core object types in depth with a look at Python numbers. We'll resume our object type tour in the next chapter, but before we move on, it's important that you get a handle on what may be the most fundamental idea in Python programming, and is certainly the basis of much of both the conciseness and flexibility of the Python languagedynamic typing, and the polymorphism it yields.

As you'll see here and later in the book, in Python, we do not declare the specific types of the objects our scripts use. In fact, programs should not even care about specific types; in exchange, they are naturally applicable in more contexts than we can sometimes even plan ahead for. Because dynamic typing is the root of this flexibility, let's take a brief look at the model here.

			30.1.3.1 The Case of the Missing Declaration Statements

If you have a background in compiled or statically typed languages like C, C++, or Java, you might find yourself a bit perplexed at this point in the book. So far, we've been using variables without declaring their existence or their types, and it somehow works. When we type a = 3 in an interactive session or program file, for instance, how does Python know that a should stand for an integer? For that matter, how does Python know what a is at all?

Once you start asking such questions, you've crossed over into the domain of Python's dynamic typing model. In Python, types are determined automatically at runtime, not in response to declarations in your code. This means that you never declare variables ahead of time (a concept that is perhaps simpler to grasp if you keep in mind that it all boils down to variables, objects, and the links between them).

				30.1.3.1.1 Variables, Objects, and References

As you've seen in many of the examples used so far in this book, when you run an assignment statement such as a = 3 in Python, it works even if you've never told Python to use the name a as a variable, or that a should stand for an integer-type object. In the Python language, this all pans out in a very natural way, as follows:


Variable creation

    A variable (i.e., name), like a, is created when your code first assigns it a value. Future assignments change the value of the already created name. Technically, Python detects some names before your code runs, but you can think of it as though initial assignments make variables.

Variable types

    A variable never has any type information or constraints associated with it. The notion of type lives with objects, not names. Variables are generic in nature; they always simply refer to a particular object at a particular point in time.

Variable use

    When a variable appears in an expression, it is immediately replaced with the object that it currently refers to, whatever that may be. Further, all variables must be explicitly assigned before they can be used; use of unassigned variables results in errors.

This model is strikingly different from traditional languages. When you are first starting out, dynamic typing is usually easier to understand if you keep clear the distinction between names and objects. For example, when we say this:


>>> a = 3


at least conceptually, Python will perform three distinct steps to carry out the request. These steps reflect the operation of all assignments in the Python language:

   a.  Create an object to represent the value 3.
   b.  Create the variable a, if it does not yet exist.
   c.  Link the variable a to the new object 3.

The net result will be a structure inside Python that resembles Figure 6-1. As sketched, variables and objects are stored in different parts of memory, and are associated by links (the link is shown as a pointer in the figure). Variables always link to objects, and never to other variables, but larger objects may link to other objects (for instance, a list object has links to the objects it contains).

Figure 6-1. Names and objects, after running the assignment a = 3. Variable a becomes a reference to object 3. Internally, the variable is really a pointer to the object's memory space created by running literal expression 3

Names			Referecences		Objects
                             
+----------+                 |                 +-***--+
|          |                 |                 **   **|
|    a     ----------------------------------->*  3   * 
|          |                 |                 **   **|
+----------+                 |                 +-***--+
                                                      |

These links from variables to objects are called references in Pythonthat is, a reference is a kind of association, implemented as a pointer in memory.[20] Whenever the variables are later used (i.e., referenced), Python automatically follows the variable-to-object links. This is all simpler than the terminology may imply. In concrete terms:

    [20] * Readers with a background in C may find Python references similar to C pointers (memory addresses). In fact, references are implemented as pointers, and they often serve the same roles, especially with objects that can be changed in-place (more on this later). However, because references are always automatically dereferenced when used, you can never actually do anything useful with a reference itself; this is a feature that eliminates a vast category of C bugs. You can think of Python references as C "void*" pointers, which are automatically followed whenever used.

    * Variables are entries in a system table, with spaces for links to objects.
    * Objects are pieces of allocated memory, with enough space to represent the values for which they stand.
    * References are automatically followed pointers from variables to objects.

At least conceptually, each time you generate a new value in your script by running an expression, Python creates a new object (i.e., a chunk of memory) to represent that value. Internally, as an optimization, Python caches and reuses certain kinds of unchangeable objects, such as small integers and strings (each 0 is not really a new piece of memorymore on this caching behavior later). But, from a logical perspective, it works as though each expression's result value is a distinct object, and each object is a distinct piece of memory.

Technically speaking, objects have more structure than just enough space to represent their values. Each object also has two standard header fields: a type designator used to mark the type of the object, and a reference counter used to determine when it's OK to reclaim the object. To understand how these two header fields factor into the model, we need to move on.

				30.1.3.1.2 Types Live with Objects, Not Variables

To see how object types come into play, watch what happens if we assign a variable multiple times:


>>> a = 3  # It's an integer
>>> a = 'spam'  # Now it's a string
>>> a = 1.23  # Now it's a floating point


This isn't typical Python code, but it does worka starts out as an integer, then becomes a string, and finally becomes a floating-point number. This example tends to look especially odd to ex-C programmers, as it appears as though the type of a changes from integer to string when we say a = 'spam'.

However, that's not really what's happening. In Python, things work more simply: names have no types; as stated earlier, types live with objects, not names. In the preceding listing, we've simply changed a to reference different objects. Because variables have no type, we haven't actually changed the type of the variable awe've simply made the variable reference a different type of object. In fact, again, all we can ever say about a variable in Python is that it references a particular object at a particular point in time.

Objects, on the other hand, know what type they areeach object contains a header field that tags the object with its type. The integer object 3, for example, will contain the value 3, plus a designator that tells Python that the object is an integer (strictly speaking, a pointer to an object called int, the name of the integer type). The type designator of the 'spam' string object points to the string type (called str) instead. Because objects know their types, variables don't have to.

To recap, types are associated with objects in Python, not with variables. In typical code, a given variable usually will reference just one kind of object. Because this isn't a requirement, though, you'll find that Python code tends to be much more flexible than you may be accustomed toif you use Python well, your code might work on many types automatically.

I mentioned that objects have two header fields, a type designator and a reference counter. To understand the latter of these, we need to move on, and take a brief look at what happens at the end of an object's life.

				30.1.3.1.3 Objects Are Garbage-Collected

In the prior section's listings, we assigned the variable a to different types of objects in each assignment. But when we reassign a variable, what happens to the value it was previously referencing? For example, after the following statements, what happens to the object 3?


>>> a = 3
>>> a = 'spam'


The answer is that in Python, whenever a name is assigned to a new object, the space held by the prior object is reclaimed (if it is not referenced by any other name or object). This automatic reclamation of objects' space is known as garbage collection.

To illustrate, consider the following example, which sets the name x to a different object on each assignment:


>>> x = 42 
>>> x = 'shrubbery'  # Reclaim 42 now (unless referenced elsewhere)
>>> x = 3.1415  # Reclaim 'shrubbery' now
>>> x = [1, 2, 3]  # Reclaim 3.1415 now


First, notice that x is set to a different type of object each time. Again, though this is not really the case, the effect is as though the type of x is changing over time. In Python, types live with objects, not names. Because names are just generic references to objects, this sort of code works naturally.

Second, notice that references to objects are discarded along the way. Each time x is assigned to a new object, Python reclaims the prior object's space. For instance, when it is assigned the string 'shrubbery', the object 42 is immediately reclaimed (assuming it is not referenced anywhere else)the object's space is automatically thrown back into the free space pool, to be reused for a future object.

Internally, Python accomplishes this feat by keeping a counter in every object that keeps track of the number of references currently pointing to that object. As soon as (and exactly when) this counter drops to zero, the object's memory space is automatically reclaimed. In the preceding listing, we're assuming that each time x is assigned to a new object, the prior object's reference counter drops to zero, causing it to be reclaimed.

The most immediately tangible benefit of garbage collection is that it means you can use objects liberally without ever needing to free up space in your script. Python will clean up unused space for you as your program runs. In practice, this eliminates a substantial amount of bookkeeping code compared to lower-level languages such as C and C++.

			30.1.3.2 Shared References
So far, we've seen what happens as a single variable is assigned references to objects. Now, let's introduce another variable into our interaction, and watch what happens to its names and objects:


>>> a = 3
>>> b = a


Typing these two statements generates the scene captured in Figure 6-2. As before, the second line causes Python to create the variable b; the variable a is being used and not assigned here, so it is replaced with the object it references (3), and b is made to reference that object. The net effect is that variables a and b wind up referencing the same object (that is, pointing to the same chunk of memory). This is called a shared reference in Pythonmultiple names referencing the same object.

Figure 6-2. Names and objects, after next running the assignment b = a. Variable a becomes a reference to object 3. Internally, the variable is really a pointer to the object's memory space created by running literal expression 3


Next, suppose we extend the session with one more statement:


>>> a = 3
>>> b = a
>>> a = 'spam'


As for all Python assignments, this statement simply makes a new object to represent the string value 'spam' and sets a to reference this new object. It does not, however, change the value of b; b still references the original object, the integer 3. The resulting reference structure is shown in Figure 6-3.

Figure 6-3. Names and objects, after finally running assignment a = 'spam'. Variable a references the new object (i.e., piece of memory) created by running literal expression 'spam', but variable b still refers to the original object 3. Because this assignment is not an in-place change to object 3, it only changes variable a, not b.


The same sort of thing would happen if we changed b to 'spam' insteadthe assignment would change only b, not a. This behavior also occurs if there are no type differences at all. For example, consider these three statements:


>>> a = 3
>>> b = a
>>> a = a + 2


In this sequence, the same events transpire: Python makes the variable a reference the object 3, and makes b reference the same object as a, as in Figure 6-2; as before, the last assignment then sets a to a completely different object (in this case, the integer 5, which is the result of the + expression). It does not change b as a side effect. In fact, there is no way to ever overwrite the value of the object 3as introduced in Chapter 4, integers are immutable, and thus can never be changed in-place.

One way to think of this is that, unlike in some languages, in Python, variables are always pointers to objects, not labels of changeable memory areas: setting a variable to a new value does not alter the original object, but rather causes the variable to reference an entirely different object. The net effect is that assignment to a variable can impact only the single variable being assigned. When mutable objects and in-place changes enter the equation, though, the picture changes somewhat; to see how, let's move on.

				30.1.3.2.1 Shared References and In-Place Changes

As you'll see later in this part's chapters, there are objects and operations that perform in-place object changes. For instance, an assignment to an offset in a list actually changes the list object itself in-place, rather than generating a brand new list object. For objects that support such in-place changes, you need to be more aware of shared references, since a change from one name may impact others.

To further illustrate, let's take another look at the list objects introduced in Chapter 4. Recall that lists, which do support in-place assignments to positions, are simply collections of other objects, coded in square brackets:


>>> L1 = [2, 3, 4]
>>> L2 = L1


L1 here is a list containing the objects 2, 3, and 4. Items inside a list are accessed by their positions, so L1[0] refers to object 2, the first item in the list L1. Of course, lists are also objects in their own right, just like integers and strings. After running the two prior assignments, L1 and L2 reference the same object, just like a and b in the prior example (see Figure 6-2). Also, as before, if we now extend this interaction to say the following:


>>> L1 = 24


L1 is simply set to a different object; L2 still references the original list. If we change this statement's syntax slightly, however, it has radically different effect:


>>> L1 = [2, 3, 4]  # A mutable object
>>> L2 = L1  # Make a reference to the same object
>>> L1[0] = 24  # An in-place change

>>> L1  # L1 is different
[24, 3, 4]
>>> L2  # But so is L2!
[24, 3, 4]


Here, we haven't changed L1 itself; we've changed a component of the object that L1 references. This sort of change overwrites part of the list object in-place. Because the list object is shared by (referenced from) other variables, though, an in-place change like this doesn't only affect L1that is, you must be aware that when you make such changes, they can impact other parts of your program. In this example, the effect shows up in L2 as well because it references the same object as L1. Again, we haven't actually changed L2, but its value will appear different.

This behavior is usually what you want, but you should be aware of how it works, so that it's expected. It's also just the default: if you don't want such behavior, you can request that Python copy objects, instead of making references. There are a variety of ways to copy a list, including the built-in list function, and the standard library copy module. Perhaps the most common way is to slice from start to finish (see Chapter 4 and Chapter 7 for more on slicing):


>>> L1 = [2, 3, 4] 
>>> L2 = L1[:]  # Make a copy of L1
>>> L1[0] = 24 

>>> L1 
[24, 3, 4]
>>> L2  # L2 is not changed
[2, 3, 4]


Here, the change made through L1 is not reflected in L2 because L2 references a copy of the object L1 references; that is, the two variables point to different pieces of memory.

Note that this slicing technique won't work on the other mutable core type, dictionaries, because they are not sequencesto copy a dictionary, instead use the D.copy( ) method. Also, note that the standard library copy module has a call for copying any object type generically, as well as a call for copying nested object structures (a dictionary with nested lists, for example):


import copy
X = copy.copy(Y)       # Make a top-level "shallow" copy of any object Y
X = copy.deepcopy(Y)   # Make a deep copy of any object Y: copy all nested parts

					  


We'll explore lists and dictionaries in more depth, and revisit the concept of shared references and copies, in Chapter 8 and Chapter 9. For now, keep in mind that objects that can be changed in-place (that is, mutable objects) are always open to these kinds of effects. In Python, this includes lists, dictionaries, and some objects defined with class statements. If this is not the desired behavior, you can simply copy your objects as needed.

				30.1.3.2.2 Shared References and Equality

In the interest of full disclosure, I should point out that the garbage-collection behavior described earlier in this chapter may be more conceptual than literal for certain types. Consider these statements:


>>> x = 42 
>>> x = 'shrubbery'  # Reclaim 42 now?


Because Python caches and reuses small integers and small strings, as mentioned earlier, the object 42 here is probably not literally reclaimed; instead, it will likely remain in a system table to be reused the next time you generate a 42 in your code. Most kinds of objects, though, are reclaimed immediately when no longer referenced; for those that are not, the caching mechanism is irrelevant to your code.

For instance, because of Python's reference model, there are two different ways to check for equality in a Python program. Let's create a shared reference to demonstrate:


>>> L = [1, 2, 3] 
>>> M = L  # M and L reference the same object
>>> L == M  # Same value
True
>>> L is M  # Same object
True


The first technique here, the == operator, tests whether the two referenced objects have the same values; this is the method almost always used for equality checks in Python. The second method, the is operator, instead tests for object identityit returns True only if both names point to the exact same object, and so is a much stronger form of equality testing.

Really, is simply compares the pointers that implement references, and is a way to detect shared references in your code if needed. It returns False if the names point to equivalent but different objects, as is the case when we run two different literal expressions:


>>> L = [1, 2, 3] 
>>> M = [1, 2, 3]  # M and L reference different objects
>>> L == M  # Same values
True
>>> L is M  # Different objects
False


Watch what happens when we perform the same operations on small numbers:


>>> X = 42 
>>> Y = 42  # Should be two different objects
>>> X == Y 
True
>>> X is Y  # Same object anyhow: caching at work!
True


In this interaction, X and Y should be == (same value), but not is (same object) because we ran two different literal expressions. Because small integers and strings are cached and reused, though, is tells us they reference the same single object.

In fact, if you really want a look under the hood, you can always ask Python how many references there are to an object: the getrefcount function in the standard sys module returns the object's reference count. When I ask about the integer object 1 in the IDLE GUI, for instance, it reports 837 reuses of this same object (most of which are in IDLE's system code, not mine):


>>> import sys 
>>> sys.getrefcount(1)  # 837 pointers to this shared piece of memory
837


This object caching and reuse is irrelevant to your code (unless you run the is check!). Because you cannot change numbers or strings in-place, it doesn't matter how many references there are to the same object. Still, this behavior reflects one of many ways Python optimizes its model for execution speed.

			30.1.3.3 Dynamic Typing Is Everywhere
You don't really need to draw name/object diagrams with circles and arrows to use Python. When you're starting out, though, it sometimes helps you understand unusual cases if you can trace their reference structures. If a mutable object changes out from under you when passed around your program, for example, chances are you are witnessing some of this chapter's subject matter firsthand.

Moreover, even if dynamic typing seems a little abstract at this point, you probably will care about it eventually. Because everything seems to work by assignment and references in Python, a basic understanding of this model is useful in many different contexts. As you'll see, it works the same in assignment statements, function arguments, for loop variables, module imports, and more. The good news is that there is just one assignment model in Python; once you get a handle on dynamic typing, you'll find that it works the same everywhere in the language.

At the most practical level, dynamic typing means there is less code for you to write. Just as importantly, though, dynamic typing is also the root of polymorphisma concept we introduced in Chapter 4, and will revisit again later in this bookin Python. Because we do not constrain types in Python code, it is highly flexible. As you'll see, when used right, dynamic typing and the polymorphism it provides produce code that automatically adapts to new requirements as your systems evolve.

			30.1.3.4 Chapter Summary
This chapter took a deeper look at Python's dynamic typing modelthat is, the way that Python keeps track of object types for us automatically, rather than requiring us to code declaration statements in our scripts. Along the way, we learned how variables and objects are associated by references in Python; we also explored the idea of garbage collection, learned how shared references to objects can affect multiple variables, and saw how references impact the notion of equality in Python.

Because there is just one assignment model in Python, and because assignment pops up everywhere in the language, it's important that you have a handle on the model before moving on. The chapter quiz coming up should help you review some of this chapter's ideas. After that, we'll resume our object tour in the next chapter, with strings.

BRAIN BUILDER

Chapter Quiz

Q:	Consider the following three statements. Do they change the value printed for A?


A = "spam"
B = A
B = "shrubbery"


Q:	Consider these three statements. Do they change the value of A?


A = ["spam"]
B = A
B[0] = "shrubbery"


Q:	How about theseis A changed?


A = ["spam"]
B = A[:]
B[0] = "shrubbery"




Quiz Answers

A:	No: A still prints as "spam". When B is assigned to the string "shrubbery", all that happens is that the variable B is reset to point to the new string object. A and B initially share (i.e., reference, or point to) the same single string object "spam", but two names are never linked together in Python. Thus, setting B to a different object has no effect on A. The same would be true if the last statement here was B = B + 'shrubbery', by the waythe concatenation makes a new object for its result, which is then assigned to B only. We can never overwrite a string (or number, or tuple) in-place, because strings are immutable.
A:	Yes: A now prints as ["shrubbery"]. Technically, we haven't really changed either A or B; instead, we've changed part of the object they both reference (point to) by overwriting that object in-place through the variable B. Because A references the same object as B, the update is reflected in A as well.
A:	No: A still prints as ["spam"]. The in-place assignment through B has no effect this time because the slice expression made a copy of the list object before it was assigned to B. After the second assignment statement, there are two different list objects that have the same value (in Python, we say they are ==, but not is). The third statement changes the value of the list object pointed to by B, but not that pointed to by A.

		30.1.4. ofsmallStrings
The next major type on our built-in object tour is the Python stringan ordered collection of characters used to store and represent text-based information. We looked briefly at strings in Chapter 4. Here, we will revisit them in more depth, filling in some of the details we skipped then.

From a functional perspective, strings can be used to represent just about anything that can be encoded as text: symbols and words (e.g., your name), contents of text files loaded into memory, Internet addresses, Python programs, and so on.

You may have used strings in other languages, too. Python's strings serve the same role as character arrays in languages such as C, but they are a somewhat higher-level tool than arrays. Unlike in C, in Python, strings come with a powerful set of processing tools. Also, unlike languages such as C, Python has no special type for single characters (like C's char); instead, you just use one-character strings.

Strictly speaking, Python strings are categorized as immutable sequences, meaning that the characters they contain have a left-to-right positional order, and that they cannot be changed in-place. In fact, strings are the first representative of the larger class of objects called sequences that we will study here. Pay special attention to the sequence operations introduced in this chapter, because they will work the same on other sequence types we'll explore later, such as lists and tuples.

Table 7-1 previews common string literals and operations we will discuss in this chapter. Empty strings are written as a pair of quotation marks (single or double) with nothing in between, and there are a variety of ways to code strings. For processing, strings support expression operations such as concatenation (combining strings), slicing (extracting sections), indexing (fetching by offset), and so on. Besides expressions, Python also provides a set of string methods that implement common string-specific tasks, as well as modules for more advanced text-processing tasks such as pattern matching. We'll explore all of these later in the chapter.

Table 7-1. Common string literals and operations
Operation 	Interpretation
s1 = '' 	Empty string
s2 = "spam's" 	Double quotes
block = """...""" 	Triple-quoted blocks
s3 = r'\temp\spam' 	Raw strings
s4 = u'spam' 	Unicode strings


s1 + s2

s2 * 3 	Concatenate, repeat


s2[i]

s2[i:j]

len(s2) 	Index, slice, length
"a %s parrot" % type 	String formatting


s2.find('pa')

s2.rstrip( )

s2.replace('pa', 'xx')

s1.split(',')

s1.isdigit( )

s1.lower( ) 	String method calls: search, remove whitespace, replacement, split on delimiter, content test, case conversion, etc.


for x in s2

'span' in s2 	Iteration, membership

Beyond the core set of string tools, Python also supports more advanced pattern-based string processing with the standard library's re (regular expression) module, introduced in Chapter 4. This chapter starts with an overview of string literal forms and basic string expressions, then looks at more advanced tools such as string methods and formatting.

			30.1.4.1 String Literals


By and large, strings are fairly easy to use in Python. Perhaps the most complicated thing about them is that there are so many ways to write them in your code:

    * Single quotes: 'spa"m'
    * Double quotes: "spa'm"
    * Triple quotes: '''... spam ...''', ""... spam ..."""
    * Escape sequences: "s\tp\na\0m"
    * Raw strings: r"C:\new\test.spm"
    * Unicode strings: u'eggs\u0020spam'

The single- and double-quoted forms are by far the most common; the others serve specialized roles. Let's take a quick look at each of these options.

				30.1.4.1.1 Single- and Double-Quoted Strings Are the Same

Around Python strings, single and double quote characters are interchangeable. That is, string literals can be written enclosed in either two single or two double quotesthe two forms work the same and return the same type of object. For example, the following two strings are identical, once coded:


>>> 'shrubbery', "shrubbery"
('shrubbery', 'shrubbery')


The reason for including both is that it allows you to embed a quote character of the other variety inside a string without escaping it with a backslash. You may embed a single quote character in a string enclosed in double quote characters, and vice versa:


>>> 'knight"s', "knight's"
('knight"s', "knight's")


Incidentally, Python automatically concatenates adjacent string literals in any expression, although it is almost as simple to add a + operator between them to invoke concatenation explicitly:


>>> title = "Meaning " 'of' " Life"  # Implicit concatenation
>>> title 
'Meaning of Life'


Notice that adding commas between these strings would make a tuple, not a string. Also, notice in all of these outputs that Python prefers to print strings in single quotes, unless they embed one. You can also embed quotes by escaping them with backslashes:


>>> 'knight\'s', "knight\"s"
("knight's", 'knight"s')


To understand why, you need to know how escapes work in general.

				30.1.4.1.2 Escape Sequences Represent Special Bytes

The last example embedded a quote inside a string by preceding it with a backslash. This is representative of a general pattern in strings: backslashes are used to introduce special byte codings known as escape sequences.

Escape sequences let us embed byte codes in strings that cannot easily be typed on a keyboard. The character \, and one or more characters following it in the string literal, are replaced with a single character in the resulting string object, which has the binary value specified by the escape sequence. For example, here is a five-character string that embeds a newline and a tab:


>>> s = 'a\nb\tc'


The two characters \n stand for a single characterthe byte containing the binary value of the newline character in your character set (usually, ASCII code 10). Similarly, the sequence \t is replaced with the tab character. The way this string looks when printed depends on how you print it. The interactive echo shows the special characters as escapes, but print interprets them instead:


>>> s
'a\nb\tc'
>>> print s
a
b       c


To be completely sure how many bytes are in this string, use the built-in len functionit returns the actual number of bytes in a string, regardless of how it is displayed:


>>> len(s)
5


This string is five bytes long: it contains an ASCII a byte, a newline byte, an ASCII b byte, and so on. Note that the original backslash characters are not really stored with the string in memory. For coding such special bytes, Python recognizes a full set of escape code sequences, listed in Table 7-2.

Table 7-2. String backslash characters
Escape 	Meaning
\newline 	Ignored (continuation)
\\ 	Backslash (keeps a \)
\' 	Single quote (keeps ')
\" 	Double quote (keeps ")
\a 	Bell
\b 	Backspace
\f 	Formfeed
\n 	Newline (linefeed)
\r 	Carriage return
\t 	Horizontal tab
\v 	Vertical tab
\N{ id } 	Unicode database ID
\uhhhh 	Unicode 16-bit hex
\Uhhhh... 	Unicode 32-bit hex[21]
\xhh 	Hex digits value
\ooo 	Octal digits value
\0 	Null (doesn't end string)
\ other 	Not an escape (kept)

    [21] a The \Uhhhh... escape sequence takes exactly eight hexadecimal digits (h); both \u and \U can be used only in Unicode string literals.

Some escape sequences allow you to embed absolute binary values into the bytes of a string. For instance, here's a five-character string that embeds two binary zero bytes:


>>> s = 'a\0b\0c'
>>> s
'a\x00b\x00c'
>>> len(s)
5


In Python, the zero (null) byte does not terminate a string the way it typically does in C. Instead, Python keeps both the string's length and text in memory. In fact, no character terminates a string in Python. Here's a string that is all absolute binary escape codesa binary 1 and 2 (coded in octal), followed by a binary 3 (coded in hexadecimal):


>>> s = '\001\002\x03'
>>> s
'\x01\x02\x03'
>>> len(s)
3


This becomes more important to know when you process binary data files in Python. Because their contents are represented as strings in your scripts, it's okay to process binary files that contain any sorts of binary byte values (more on files in Chapter 9).[22]

    [22] * If you're especially interested in binary data files, the chief distinction is that you open them in binary mode (using open mode flags with a b, such as 'rb', 'wb', and so on). See also the standard struct module introduced in Chapter 9, which can parse binary data loaded from a file.

Finally, as the last entry in Table 7-2 implies, if Python does not recognize the character after a \ as being a valid escape code, it simply keeps the backslash in the resulting string:


>>> x = "C:\py\code"  # Keeps \ literally
>>> x 
'C:\\py\\code'
>>> len(x) 
10


Unless you're able to commit all of Table 7-2 to memory, though, you probably shouldn't rely on this behavior.[23] To code literal backslashes explicitly such that they are retained in your strings, double them up (\\ is an escape for \) or use raw strings, described in the next section.

    [23] dagger In classes, I've met people who have indeed committed most or all of this table to memory; I'd normally think that's really sick, but for the fact that I'm a member of the set, too.

				30.1.4.1.3 Raw Strings Suppress Escapes

As we've seen, escape sequences are handy for embedding special byte codes within strings. Sometimes, though, the special treatment of backslashes for introducing escapes can lead to trouble. It's surprisingly common, for instance, to see Python newcomers in classes trying to open a file with a filename argument that looks something like this:


myfile = open('C:\new\text.dat', 'w')


thinking that they will open a file called text.dat in the directory C:\new. The problem here is that \n is taken to stand for a newline character, and \t is replaced with a tab. In effect, the call tries to open a file named C:(newline)ew(tab)ext.dat, with usually less than stellar results.

This is just the sort of thing that raw strings are useful for. If the letter r (uppercase or lowercase) appears just before the opening quote of a string, it turns off the escape mechanism. The result is that Python retains your backslashes literally, exactly as you type them. Therefore, to fix the filename problem, just remember to add the letter r on Windows:


myfile = open(r'C:\new\text.dat', 'w')


Alternatively, because two backslashes are really an escape sequence for one backslash, you can keep your backslashes by simply doubling them up:


myfile = open('C:\\new\\text.dat', 'w')


In fact, Python itself sometimes uses this doubling scheme when it prints strings with embedded backslashes:


>>> path = r'C:\new\text.dat' 
>>> path  # Show as Python code
'C:\\new\\text.dat'
>>> print path  # User-friendly format
C:\new\text.dat
>>> len(path)  # String length
15


As with numeric representation, the default format at the interactive prompt prints results as if they were code, and therefore escapes backslashes in the output. The print statement provides a more user-friendly format that shows that there is actually only one backslash in each spot. To verify this is the case, you can check the result of the built-in len function, which returns the number of bytes in the string, independent of display formats. If you count the characters in the print path output, you'll see that there really is just one character per backslash, for a total of 15.

Besides directory paths on Windows, raw strings are also commonly used for regular expressions (text pattern matching, supported with the re module introduced in Chapter 4). Also note that Python scripts can usually use forward slashes in directory paths on Windows and Unix because Python tries to interpret paths portably. Raw strings are useful if you code paths using native Windows backslashes, though.

				30.1.4.1.4 Triple Quotes Code Multiline Block Strings

So far, you've seen single quotes, double quotes, escapes, and raw strings in action. Python also has a triple-quoted string literal format, sometimes called a block string, that is a syntactic convenience for coding multiline text data. This form begins with three quotes (of either the single or double variety), is followed by any number of lines of text, and is closed with the same triple-quote sequence that opened it. Single and double quotes embedded in the string's text may be, but do not have to be, escapedthe string does not end until Python sees three unescaped quotes of the same kind used to start the literal. For example:


>>> mantra = """Always look
...  on the bright
... side of life."""
>>>
>>> mantra
'Always look\n on the bright\nside of life.'


This string spans three lines (in some interfaces, the interactive prompt changes to ... on continuation lines; IDLE simply drops down one line). Python collects all the triple-quoted text into a single multiline string, with embedded newline characters (\n) at the places where your code has line breaks. Notice that, as in the literal, the second line in the result has a leading space, but the third does notwhat you type is truly what you get.

Triple-quoted strings are useful any time you need multiline text in your program; for example, to embed multiline error messages or HTML or XML code in your source code files. You can embed such blocks directly in your scripts without resorting to external text files, or explicit concatenation and newline characters.

Triple-quoted strings are also commonly used for documentation strings, which are string literals that are taken as comments when they appear at specific points in your file (more on these later in the book). These don't have to be triple-quoted blocks, but they usually are to allow for multiline comments.

Finally, triple-quoted strings are also often used as a horribly hackish way to temporarily disable lines of code during development (OK, it's not really too horrible, and it's actually a fairly common practice). If you wish to turn off a few lines of code and run your script again, simply put three quotes above and below them, like this:


X = 1
"""
import os
print os.getcwd(  )
"""
Y = 2


I said this was hackish because Python really does make a string out of the lines of code disabled this way, but this is probably not significant in terms of performance. For large sections of code, it's also easier than manually adding hash marks before each line and later removing them. This is especially true if you are using a text editor that does not have support for editing Python code specifically. In Python, practicality often beats aesthetics.


				30.1.4.1.5 Unicode Strings Encode Larger Character Sets

The last way to write strings in your scripts is perhaps the most specialized, and it's rarely observed outside of web and XML processing. Unicode strings are sometimes called "wide" character strings. Because each character may be represented with more than one byte in memory, Unicode strings allow programs to encode richer character sets than standard strings.

Unicode strings are typically used to support internationalization of applications (sometimes referred to as "i18n," to compress the 18 characters between the first and last characters of the term). For instance, they allow programmers to directly support European or Asian character sets in Python scripts. Because such character sets have more characters than can be represented by single bytes, Unicode is normally used to process these forms of text.

In Python, you can code Unicode strings in your scripts by adding the letter U (lower- or uppercase) just before the opening quote:


>>> u'spam'
u'spam'


Technically, this syntax generates a Unicode string object, which is a different data type from the normal string type. However, Python allows you to freely mix Unicode and normal strings in expressions, and converts up to Unicode for mixed-type results (more on + concatenation in the next section):


>>> 'ni' + u'spam'  # Mixed string types
u'nispam'


In fact, Unicode strings are defined to support all the usual string-processing operations you'll meet in the next section, so the difference in types is often trivial to your code. Like normal strings, Unicode strings may be concatenated, indexed, sliced, matched with the re module, and so on, and cannot be changed in-place.

If you ever need to convert between the two types explicitly, you can use the built-in str and unicode functions:


>>> str(u'spam')  # Unicode to normal
'spam'
>>> unicode('spam')  # Normal to Unicode
u'spam'


Because Unicode is designed to handle multibyte characters, you can also use the special \u and \U escapes to encode binary character values that are larger than 8 bits:


>>> u'ab\x20cd'  # 8-bit/1-byte characters
u'ab cd'
>>> u'ab\u0020cd'  # 2-byte characters
u'ab cd'
>>> u'ab\U00000020cd'  # 4-byte characters
u'ab cd'


The first of these statements embeds the binary code for a space character; its binary value in hexadecimal notation is x20. The second and third statements do the same, but give the value in 2-byte and 4-byte Unicode escape notation.

Even if you don't think you will need Unicode strings, you might use them without knowing it. Because some programming interfaces (e.g., the COM API on Windows, and some XML parsers) represent text as Unicode, it may find its way into your scripts as API inputs or results, and you may sometimes need to convert back and forth between normal and Unicode types.

Because Python treats the two string types interchangeably in most contexts, though, the presence of Unicode strings is often transparent to your codeyou can largely ignore the fact that text is being passed around as Unicode objects and use normal string operations.

Unicode is a useful addition to Python, and because support is built-in, it's easy to handle such data in your scripts when needed. Of course, there's a lot more to say about the Unicode story. For example:

    * Unicode objects provide an encode method that converts a Unicode string into a normal 8-bit string using a specific encoding.
    * The built-in function unicode and module codecs support registered Unicode "codecs" (for "COders and DECoders").
    * The open function within the codecs module allows for processing Unicode text files, where each character is stored as more than one byte.
    * The unicodedata module provides access to the Unicode character database.
    * The sys module includes calls for fetching and setting the default Unicode encoding scheme (the default is usually ASCII).
    * You may combine the raw and Unicode string formats (e.g., ur'a\b\c').

Because Unicode is a relatively advanced and not so commonly used tool, we won't discuss it further in this introductory text. See the Python standard manual for the rest of the Unicode story.

	

In Python 3.0, the string type will mutate somewhat: the current str string type will always be Unicode in 3.0, and there will be a new "bytes" type that will be a mutable sequence of small integers useful for representing short character strings. Some file read operations may return bytes instead of str (reading binary files, for example). This is still on the drawing board, so consult 3.0 release notes for details.

			30.1.4.2 Strings in Action

Once you've created a string with the literal expressions we just met, you will almost certainly want to do things with it. This section and the next two demonstrate string basics, formatting, and methodsthe first line of text-processing tools in the Python language.

				30.1.4.2.1 Basic Operations

Let's begin by interacting with the Python interpreter to illustrate the basic string operations listed in Table 7-1. Strings can be concatenated using the + operator and repeated using the * operator:


% python 
>>> len('abc')  # Length: number of items
3
>>> 'abc' + 'def'  # Concatenation: a new string
'abcdef'
>>> 'Ni!' * 4  # Repetition: like "Ni!" + "Ni!" + ...
'Ni!Ni!Ni!Ni!'


Formally, adding two string objects creates a new string object, with the contents of its operands joined. Repetition is like adding a string to itself a number of times. In both cases, Python lets you create arbitrarily sized strings; there's no need to predeclare anything in Python, including the sizes of data structures.[24] The len built-in function returns the length of a string (or any other object with a length).

    [24] * Unlike C character arrays, you don't need to allocate or manage storage arrays when using Python strings; simply create string objects as needed, and let Python manage the underlying memory space. As discussed in the prior chapter, Python reclaims unused objects' memory space automatically, using a reference-count garbage-collection strategy. Each object keeps track of the number of names, data structures, etc., that reference it; when the count reaches zero, Python frees the object's space. This scheme means Python doesn't have to stop and scan all the memory to find unused space to free (an additional garbage component also collects cyclic objects).

Repetition may seem a bit obscure at first, but it comes in handy in a surprising number of contexts. For example, to print a line of 80 dashes, you can count up to 80, or let Python count for you:


>>> print '------- ...more... ---'  # 80 dashes, the hard way
>>> print '-'*80  # 80 dashes, the easy way


Notice that operator overloading is at work here already: we're using the same + and * operators that perform addition and multiplication when using numbers. Python does the correct operation because it knows the types of the objects being added and multiplied. But be careful: the rules aren't quite as liberal as you might expect. For instance, Python doesn't allow you to mix numbers and strings in + expressions: 'abc'+9 raises an error instead of automatically converting 9 to a string.

As shown in the last line in Table 7-1, you can also iterate over strings in loops using for statements, and test membership with the in expression operator, which is essentially a search:


>>> myjob = "hacker" 
>>> for c in myjob: print c,  # Step through items
...
h a c k e r
>>> "k" in myjob  # Found
True
>>> "z" in myjob  # Not found
False


The for loop assigns a variable to successive items in a sequence (here, a string), and executes one or more statements for each item. In effect, the variable c becomes a cursor stepping across the string here. We will discuss iteration tools like these in more detail later in this book.


				30.1.4.2.2 Indexing and Slicing

Because strings are defined as ordered collections of characters, we can access their components by position. In Python, characters in a string are fetched by indexingproviding the numeric offset of the desired component in square brackets after the string. You get back the one-character string at the specified position.

As in the C language, Python offsets start at 0, and end at one less than the length of the string. Unlike C, however, Python also lets you fetch items from sequences such as strings using negative offsets. Technically, a negative offset is added to the length of a string to derive a positive offset. You can also think of negative offsets as counting backward from the end. The following interaction demonstrates:


>>> S = 'spam' 
>>> S[0], S[-2]  # Indexing from front or end
('s', 'a')
>>> S[1:3], S[1:], S[:-1]  # Slicing: extract a section
('pa', 'pam', 'spa')


The first line defines a four-character string, and assigns it the name S. The next line indexes it in two ways: S[0] fetches the item at offset 0 from the left (the one-character string 's'), and S[-2] gets the item at offset 2 from the end (or equivalently, at offset (4 + -2) from the front). Offsets and slices map to cells as shown in Figure 7-1.[25]

    [25] * More mathematically minded readers (and students in my classes) sometimes detect a small asymmetry here: the leftmost item is at offset 0, but the rightmost is at offset -1. Alas, there is no such thing as a distinct -0 value in Python.

Figure 7-1. Offsets and slices: positive offsets start from the left end (offset 0 is the first item), and negatives count back from the right end (offset -1 is the last item). Either kind of offset can be used to give positions in indexing and slicing.


The last line in the preceding example demonstrates slicing. Probably the best way to think of slicing is that it is a form of parsing (analyzing structure), especially when applied to stringsit allows us to extract an entire section (substring) in a single step. Slices can be used to extract columns of data, chop off leading and trailing text, and more. We'll look at another slicing-as-parsing example later in this chapter.

Here's how slicing works. When you index a sequence object such as a string on a pair of offsets separated by a colon, Python returns a new object containing the contiguous section identified by the offset pair. The left offset is taken to be the lower bound (inclusive), and the right is the upper bound (noninclusive). Python fetches all items from the lower bound up to but not including the upper bound, and returns a new object containing the fetched items. If omitted, the left and right bounds default to 0, and the length of the object you are slicing, respectively.

For instance, in the example we just looked at, S[1:3] extracts the items at offsets 1 and 2. That is, it grabs the second and third items, and stops before the fourth item at offset 3. Next S[1:] gets all items beyond the firstthe upper bound, which is not specified, defaults to the length of the string. Finally, S[:-1] fetches all but the last itemthe lower bound defaults to 0, and1 refers to the last item, noninclusive.

This may seem confusing at first glance, but indexing and slicing are simple and powerful tools to use, once you get the knack. Remember, if you're unsure about what a slice means, try it out interactively. In the next chapter, you'll see that it's also possible to change an entire section of a certain object in one step by assigning to a slice. Here's a summary of the details for reference:

    * Indexing (S[i]) fetches components at offsets:
          o The first item is at offset 0.
          o Negative indexes mean to count backward from the end or right.
          o S[0] fetches the first item.
          o S[-2] fetches the second item from the end (like S[len(S)-2]).
    * Slicing (S[i:j]) extracts contiguous sections of a sequence:
          o The upper bound is noninclusive.
          o Slice boundaries default to 0 and the sequence length, if omitted.
          o S[1:3] fetches items at offsets 1 up to, but not including, 3.
          o S[1:] fetches items at offset 1 through the end (length).
          o S[:3] fetches items at offset 0 up to, but not including, 3.
          o S[:-1] fetches items at offset 0 up to, but not including, the last item.
          o S[:] fetches items at offsets 0 through the endthis effectively performs a top-level copy of S.

The last item listed here turns out to be a very common trick: it makes a full top-level copy of a sequence objectan object with the same value, but a distinct piece of memory (you'll find more on copies in Chapter 9). This isn't very useful for immutable objects like strings, but it comes in handy for objects that may be changed in-place, such as lists. In the next chapter, you'll also see that the syntax used to index by offset (square brackets) is used to index dictionaries by key as well; the operations look the same, but have different interpretations.


					30.1.4.2.2.1 Extended slicing: the third limit

In Python 2.3, slice expressions grew support for an optional third index, used as a step (sometimes called a stride). The step is added to the index of each item extracted. The full-blown form of a slice is now X[I:J:K], which means "extract all the items in X, from offset I through J-1, by K." The third limit, K, defaults to 1, which is why normally all items in a slice are extracted from left to right. If you specify an explicit value, however, you can use the third limit to skip items or to reverse their order.

For instance, X[1:10:2] will fetch every other item in X from offsets 19; that is, it will collect the items at offsets 1, 3, 5, 7, and 9. As usual, the first and second limits default to 0, and the length of the sequence, respectively, so X[::2] gets every other item from the beginning to the end of the sequence:


>>> S = 'abcdefghijklmnop'
>>> S[1:10:2]
'bdfhj'
>>> S[::2]
'acegikmo'


You can also use a negative stride. For example, the slicing expression "hello"[::-1] returns the new string "olleh"the first two bounds default to 0, and the length of the sequence, as before, and a stride of -1 indicates that the slice should go from right to left instead of the usual left to right. The effect, therefore, is to reverse the sequence:


>>> S = 'hello'
>>> S[::-1]
'olleh'


With a negative stride, the meanings of the first two bounds are essentially reversed. That is, the slice S[5:1:-1] fetches the items from 2 to 5, in reverse order (the result contains items from offsets 5, 4, 3, and 2):


>>> S = 'abcedfg'
>>> S[5:1:-1]
'fdec'


Skipping and reversing like this are the most common use cases for three-limit slices, but see Python's standard library manual for more details, or run a few experiments interactivelythere is more to the story than we will cover here. We'll revisit three-limit slices again later in this book, in conjunction with the for loop statement.

				30.1.4.2.3 String Conversion Tools

One of Python's design mottos is that it refuses the temptation to guess. As a prime example, you cannot add a number and a string together in Python, even if the string looks like a number (i.e., is all digits):


>>> "42" + 1
TypeError: cannot concatenate 'str' and 'int' objects


This is by design: because + can mean both addition and concatenation, the choice of conversion would be ambiguous. So, Python treats this as an error. In Python, magic is generally omitted if it will make your life more complex.

What to do, then, if your script obtains a number as a text string from a file or user interface? The trick is that you need to employ conversion tools before you can treat a string like a number, or vice versa. For instance:


>>> int("42"), str(42)  # Convert from/to string
(42, '42')
>>> repr(42), '42'  # Convert to as-code string
('42', '42')


The int function converts a string to a number, and the str function converts a number to its string representation (essentially, what it looks like when printed). The repr function and its older equivalent, the backquotes expression, also convert an object to its string representation, but these return the object as a string of code that can be rerun to recreate the object (for strings, the result has quotes around it if displayed with the print statement). See the "str and repr Display Formats" sidebar in Chapter 5 on the difference between str and repr for more on this topic. Of these, int and str are the generally prescribed conversion techniques.

Why You Will Care: Slices

Throughout this book, I will include common use case sidebars (such as this one) to give you a peek at how some of the language features being introduced are typically used in real programs. Because you won't be able to make much sense of real use cases until you've seen most of the Python picture, these sidebars necessarily contain many references to topics not introduced yet; at most, you should consider them previews of ways that you may find these abstract language concepts useful for common programming tasks.

For instance, you'll see later that the argument words listed on a system command line used to launch a Python program are made available in the argv attribute of the built-in sys module:


# File echo.py
import sys
print sys.argv

% python echo.py -a -b -c
['echo.py', '-a', '-b', '-c']


Usually, you're only interested in inspecting the arguments that follow the program name. This leads to a very typical application of slices: a single slice expression can be used to return all but the first item of a list. Here, sys.argv[1:] returns the desired list, ['-a', '-b', '-c']. You can then process this list without having to accommodate the program name at the front.

Slices are also often used to clean up lines read from input files. If you know that a line will have an end-of-line character at the end (a \n newline marker), you can get rid of it with a single expression such as line[:-1], which extracts all but the last character in the line (the lower limit defaults to 0). In both cases, slices do the job of logic that must be explicit in a lower-level language.

Note that calling the line.rstrip method is often preferred for stripping newline characters because this call leaves the line intact if it has no newline character at the enda common case for files created with some text-editing tools. Slicing works if you're sure the line is properly terminated.

Although you can't mix strings and number types around operators such as +, you can manually convert operands before that operation if needed:


>>> S = "42" 
>>> I = 1 
>>> S + I 

TypeError: cannot concatenate 'str' and 'int' objects
>>> int(S) + I  # Force addition
43

>>> S + str(I)  # Force concatenation
'421'


Similar built-in functions handle floating-point number conversions to and from strings:


>>> str(3.1415), float("1.5")
('3.1415', 1.5)

|>>> text = "1.234E-10"
|>>> float(text)
|1.2340000000000001e-010


Later, we'll further study the built-in eval function; it runs a string containing Python expression code, and so can convert a string to any kind of object. The functions int and float convert only to numbers, but this restriction means they are usually faster (and more secure, because they do not accept arbitrary expression code). As we saw in Chapter 5, the string formatting expression also provides a way to convert numbers to strings. We'll discuss formatting further later in this chapter.

					30.1.4.2.3.1 Character code conversions

On the subject of conversions, it is also possible to convert a single character to its underlying ASCII integer code by passing it to the built-in ord functionthis returns the actual binary value of the corresponding byte in memory. The chr function performs the inverse operation, taking an ASCII integer code and converting it to the corresponding character:


>>> ord('s')
115
>>> chr(115)
's'


You can use a loop to apply these functions to all characters in a string. These tools can also be used to perform a sort of string-based math. To advance to the next character, for example, convert and do the math in integer:


>>> S = '5'
>>> S = chr(ord(S) + 1)
>>> S
'6'
>>> S = chr(ord(S) + 1)
>>> S
'7'


At least for single-character strings, this provides an alternative to using the built-in int function to convert from string to integer:


>>> int('5')
5
>>> ord('5') - ord('0')
5


Such conversions can be used in conjunction with a looping statement to convert a string of binary digits to their corresponding integer valueseach time through, multiply the current value by 2, and add the next digit's integer value:


>>> B = '1101'
>>> I = 0
>>> while B:
...     I = I * 2 + (ord(B[0]) - ord('0'))
...     B = B[1:]
...
>>> I
13


A left-shift operation (I << 1) would have the same effect as multiplying by 2 here. Because we haven't studied loops in detail yet, though, we'll leave implementing that as a suggested experiment.

				30.1.4.2.4 Changing Strings

Remember the term "immutable sequence"? The immutable part means that you can't change a string in-place (e.g., by assigning to an index):


>>> S = 'spam'
>>> S[0] = "x"
Raises an error!


So how do you modify text information in Python? To change a string, you need to build and assign a new string using tools such as concatenation and slicing, and then, if desired, assign the result back to the string's original name:


>>> S = S + 'SPAM!'  # To change a string, make a new one
>>> S 
'spamSPAM!'
>>> S = S[:4] + 'Burger' + S[-1] 
>>> S 
'spamBurger!'


The first example adds a substring at the end of S, by concatenation; really, it makes a new string and assigns it back to S, but you can think of this as "changing" the original string. The second example replaces four characters with six by slicing, indexing, and concatenating. As you'll see later in this chapter, you can achieve similar effects with string method calls like replace. Here's a sneak peek:


>>> S = 'splot'
>>> S = S.replace('pl', 'pamal')
>>> S
'spamalot'


Like every operation that yields a new string value, string methods generate new string objects. If you want to retain those objects, you can assign it to a variable name. Generating a new string object for each string change is not as inefficient as it may soundremember, as discussed in the preceding chapter, Python automatically garbage collects (reclaims the space of) old unused string objects as you go, so newer objects reuse the space held by prior values. Python is usually more efficient than you might expect.

Finally, it's also possible to build up new text values with string formatting expressions:


>>> 'That is %d %s bird!' % (1, 'dead')  # Like C sprintf
That is 1 dead bird!


This turns out to be a powerful operation. The next section shows how it works.

			30.1.4.3 String Formatting

Python defines the % binary operator to work on strings (you may recall that this is also the remainder of division, or modulus, operator for numbers). When applied to strings, this operator serves the same role as C's sprintf function; the % provides a simple way to format values as strings, according to a format definition string. In short, the % operator provides a compact way to code multiple string substitutions.

To format strings:

   a.  On the left of the % operator, provide a format string containing one or more embedded conversion targets, each of which starts with a % (e.g., %d).
   b.  On the right of the % operator, provide the object (or objects, in parentheses) that you want Python to insert into the format string on the left in place of the conversion target (or targets).

For instance, in the last example, we looked at in the prior section, the integer 1 replaces the %d in the format string on the left, and the string 'dead' replaces the %s. The result is a new string that reflects these two substitutions.

Technically speaking, string formatting expressions are usually optional-you can generally do similar work with multiple concatenations and conversions. However, formatting allows us to combine many steps into a single operation. It's powerful enough to warrant a few more examples:


>>> exclamation = "Ni"
>>> "The knights who say %s!" % exclamation
'The knights who say Ni!'

>>> "%d %s %d you" % (1, 'spam', 4)
'1 spam 4 you'

>>> "%s -- %s -- %s" % (42, 3.14159, [1, 2, 3])
'42 -- 3.14159 -- [1, 2, 3]'


The first example here plugs the string "Ni" into the target on the left, replacing the %s marker. In the second example, three values are inserted into the target string. Note that when you're inserting more than one value, you need to group the values on the right in parentheses (i.e., put them in a tuple).

The third example again inserts three values-an integer, a floating-point object, and a list object-but notice that all of the targets on the left are %s, which stands for conversion to string. As every type of object can be converted to a string (the one used when printing), every object type works with the %s conversion code. Because of this, unless you will be doing some special formatting, %s is often the only code you need to remember for the formatting expression.

Again, keep in mind that formatting always makes a new string, rather than changing the string on the left; because strings are immutable, it must work this way. As before, assign the result to a variable name if you need to retain it.

				30.1.4.3.1 Advanced String Formatting

For more advanced type-specific formatting, you can use any of the conversion codes listed in Table 7-3 in formatting expressions. C programmers will recognize most of these because Python string formatting supports all the usual C printf format codes (but returns the result, instead of displaying it, like printf). Some of the format codes in the table provide alternative ways to format the same type; for instance, %e, %f, and %g provide alternative ways to format floating-point numbers.

Table 7-3. String-formatting codes
Code 	Meaning
%s 	String (or any object)
%r 	s, but uses repr, not str
%c 	Character
%d 	Decimal (integer)
%i 	Integer
%u 	Unsigned (integer)
%o 	Octal integer
%x 	Hex integer
%X 	x, but prints uppercase
%e 	Floating-point exponent
%E 	e, but prints uppercase
%f 	Floating-point decimal
%g 	Floating-point e or f
%G 	Floating-point E or F
%% 	Literal %

In fact, conversion targets in the format string on the expression's left side support a variety of conversion operations with a fairly sophisticated syntax all their own. The general structure of conversion targets looks like this:


%[(name)][flags][width][.precision]code


The character codes in Table 7-3 show up at the end of the target string. Between the % and the character code, you can do any of the following: provide a dictionary key; list flags that specify things like left justification (-), numeric sign (+), and zero fills (0); give a total field width and the number of digits after a decimal point; and more.

Formatting target syntax is documented in full in the Python standard manuals, but to demonstrate common usage, let's look at a few examples. This one formats integers by default, and then in a six-character field with left justification, and zero padding:


>>> x = 1234
>>> res = "integers: ...%d...%-6d...%06d" % (x, x, x)
>>> res
'integers: ...1234...1234  ...001234'


The %e, %f, and %g formats display floating-point numbers in different ways, as the following interaction demonstrates:


|>>> x = 1.23456789
|>>> x
|1.2345678899999999

>>> '%e | %f | %g' % (x, x, x)
'1.234568e+000 | 1.234568 | 1.23457'


For floating-point numbers, you can achieve a variety of additional formatting effects by specifying left justification, zero padding, numeric signs, field width, and digits after the decimal point. For simpler tasks, you might get by with simply converting to strings with a format expression or the str built-in function shown earlier:


>>> '%-6.2f | %05.2f | %+06.1f' % (x, x, x)
'1.23   | 01.23 | +001.2'

>>> "%s" % x, str(x)
('1.23456789', '1.23456789')


				30.1.4.3.2 Dictionary-Based String Formatting

String formatting also allows conversion targets on the left to refer to the keys in a dictionary on the right to fetch the corresponding values. I haven't told you much about dictionaries yet, so here's an example that demonstrates the basics:


>>> "%(n)d %(x)s" % {"n":1, "x":"spam"}
'1 spam'


Here, the (n) and (x) in the format string refer to keys in the dictionary literal on the right, and fetch their associated values. Programs that generate text such as HTML or XML often use this technique-you can build up a dictionary of values, and substitute them all at once with a single formatting expression that uses key-based references:


>>> reply = """
Greetings...
Hello %(name)s!
Your age squared is %(age)s
"""
>>> values = {'name': 'Bob', 'age': 40}
>>> print reply % values

Greetings...
Hello Bob!
Your age squared is 40


This trick is also used in conjunction with the vars built-in function, which returns a dictionary containing all the variables that exist in the place it is called:


>>> food = 'spam' 
>>> age = 40 
>>> vars(  ) 
{'food': 'spam', 'age': 40, ...many more...  }


When used on the right of a format operation, this allows the format string to refer to variables by name (i.e., by dictionary key):


>>> "%(age)d %(food)s" % vars(  )
'40 spam'


We'll study dictionaries in more depth in Chapter 8. See also Chapter 5 for examples that convert to hexadecimal and octal number strings with the %x and %o formatting target codes.


			30.1.4.4 String Methods

In addition to expression operators, strings provide a set of methods that implement more sophisticated text-processing tasks. Methods are simply functions that are associated with particular objects. Technically, they are attributes attached to objects that happen to reference callable functions. In Python, methods are specific to object types-string methods, for example, work only on string objects.

In finer-grained detail, functions are packages of code, and method calls combine two operations at once (an attribute fetch, and a call):


Attribute fetches

    An expression of the form object.attribute means "fetch the value of attribute in object."

Call expressions

    An expression of the form function(arguments) means "invoke the code of function, passing zero or more comma-separated argument objects to it, and return function's result value."

Putting these two together allows us to call a method of an object. The method call expression object.method(arguments) is evaluated from left to right-that is, Python will first fetch the method of the object, and then call it, passing in the arguments. If the method computes a result, it will come back as the result of the entire method-call expression.

As you'll see throughout this part of the book, most objects have callable methods, and all are accessed using this same method-call syntax. To call an object method, you have to go through an existing object. Let's move on to some examples to see how.

				30.1.4.4.1 String Method Examples: Changing Strings

Table 7-4 summarizes the call patterns for built-in string methods (be sure to check Python's standard library manual for the most up-to-date list, or run a help call on any string interactively). String methods in this table implement higher-level operations such as splitting and joining, case conversions, content tests, and substring searches.

Table 7-4. String method calls
S.capitalize( ) 	S.ljust(width)
S.center(width) 	S.lower( )
S.count(sub [, start [, end]]) 	S.lstrip( )
S.encode([encoding [,errors]]) 	S.replace(old, new [, maxsplit])
S.endswith(suffix [, start [, end]]) 	S.rfind(sub [,start [,end]])
S.expandtabs([tabsize]) 	S.rindex(sub [, start [, end]])
S.find(sub [, start [, end]]) 	S.rjust(width)
S.index(sub [, start [, end]]) 	S.rstrip( )
S.isalnum( ) 	S.split([sep [,maxsplit]])
S.isalpha( ) 	S.splitlines([keepends])
S.isdigit( ) 	S.startswith(prefix [, start [, end]])
S.islower( ) 	S.strip( )
S.isspace( ) 	S.swapcase( )
S.istitle( ) 	S.title( )
S.isupper( ) 	S.translate(table [, delchars])
S.join(seq) 	S.upper( )

Now, let's work through some code that demonstrates some of the most commonly used methods in action, and illustrates Python text-processing basics along the way. As we've seen, because strings are immutable, they cannot be changed in-place directly. To make a new text value from an existing string, you construct a new string with operations such as slicing and concatenation. For example, to replace two characters in the middle of a string, you can use code like this:


>>> S = 'spammy'
>>> S = S[:3] + 'xx' + S[5:]
>>> S
'spaxxy'


But, if you're really just out to replace a substring, you can use the string replace method instead:


>>> S = 'spammy'
>>> S = S.replace('mm', 'xx')
>>> S
'spaxxy'


The replace method is more general than this code implies. It takes as arguments the original substring (of any length), and the string (of any length) to replace it with, and performs a global search and replace:


>>> 'aa$bb$cc$dd'.replace('$', 'SPAM')
'aaSPAMbbSPAMccSPAMdd'


In such a role, replace can be used as a tool to implement template replacements (e.g., in form letters). Notice that this time we simply printed the result, instead of assigning it to a name-you need to assign results to names only if you want to retain them for later use.

If you need to replace one fixed-size string that can occur at any offset, you can do a replacement again, or search for the substring with the string find method and then slice:


>>> S = 'xxxxSPAMxxxxSPAMxxxx' 
>>> where = S.find('SPAM')  # Search for position
>>> where  # Occurs at offset 4
4
>>> S = S[:where] + 'EGGS' + S[(where+4):] 
>>> S 
'xxxxEGGSxxxxSPAMxxxx'


The find method returns the offset where the substring appears (by default, searching from the front), or -1 if it is not found. Another option is to use replace with a third argument to limit it to a single substitution:


>>> S = 'xxxxSPAMxxxxSPAMxxxx' 
>>> S.replace('SPAM', 'EGGS')  # Replace all
'xxxxEGGSxxxxEGGSxxxx'

>>> S.replace('SPAM', 'EGGS', 1)  # Replace one
'xxxxEGGSxxxxSPAMxxxx'


Notice that replace returns a new string object each time. Because strings are immutable, methods never really change the subject strings in-place, even if they are called "replace"!

The fact that concatenation operations and the replace method generate new string objects each time they are run is actually a potential downside of using them to change strings. If you have to apply many changes to a very large string, you might be able to improve your script's performance by converting the string to an object that does support in-place changes:


>>> S = 'spammy'
>>> L = list(S)
>>> L
['s', 'p', 'a', 'm', 'm', 'y']


The built-in list function (or an object construction call) builds a new list out of the items in any sequence-in this case, "exploding" the characters of a string into a list. Once the string is in this form, you can make multiple changes to it without generating a new copy for each change:


>>> L[3] = 'x'  # Works for lists, not strings
>>> L[4] = 'x' 
>>> L 
['s', 'p', 'a', 'x', 'x', 'y']


If, after your changes, you need to convert back to a string (e.g., to write to a file), use the string join method to "implode" the list back into a string:


>>> S = ''.join(L)
>>> S
'spaxxy'


The join method may look a bit backward at first sight. Because it is a method of strings (not of lists), it is called through the desired delimiter. join puts the list's strings together, with the delimiter between list items; in this case, it uses an empty string delimiter to convert from a list back to a string. More generally, any string delimiter and list of strings will do:


>>> 'SPAM'.join(['eggs', 'sausage', 'ham', 'toast'])
'eggsSPAMsausageSPAMhamSPAMtoast'


				30.1.4.4.2 String Method Examples: Parsing Text

Another common role for string methods is as a simple form of text parsing-that is, analyzing structure and extracting substrings. To extract substrings at fixed offsets, we can employ slicing techniques:


>>> line = 'aaa bbb ccc'
>>> col1 = line[0:3]
>>> col3 = line[8:]
>>> col1
'aaa'
>>> col3
'ccc'


Here, the columns of data appear at fixed offsets, and so may be sliced out of the original string. This technique passes for parsing, as long as the components of your data have fixed positions. If instead some sort of delimiter separates the data, you can pull out its components by splitting. This will work even if the data may show up at arbitrary positions within the string:


>>> line = 'aaa bbb  ccc'
>>> cols = line.split(  )
>>> cols
['aaa', 'bbb', 'ccc']


The string split method chops up a string into a list of substrings, around a delimiter string. We didn't pass a delimiter in the prior example, so it defaults to whitespace-the string is split at groups of one or more spaces, tabs, and newlines, and we get back a list of the resulting substrings. In other applications, more tangible delimiters may separate the data. This example splits (and hence parses) the string at commas, a separator common in data returned by some database tools:


>>> line = 'bob,hacker,40'
>>> line.split(',')
['bob', 'hacker', '40']


Delimiters can be longer than a single character, too:


>>> line = "i'mSPAMaSPAMlumberjack"
>>> line.split("SPAM")
["i'm", 'a', 'lumberjack']


Although there are limits to the parsing potential of slicing and splitting, both run very fast, and can handle basic text-extraction chores.

				30.1.4.4.3 Other Common String Methods in Action

Other string methods have more focused roles-for example, to strip off whitespace at the end of a line of text, perform case conversions, test content, and test for a substring at the end:


>>> line = "The knights who sy Ni!\n"
>>> line.rstrip(  )
'The knights who sy Ni!'
>>> line.upper(  )
'THE KNIGHTS WHO SY NI!\n'
>>> line.isalpha(  )
False
>>> line.endswith('Ni!\n')
True


Alternative techniques can also sometimes be used to achieve the same results as string methods-the in membership operator can be used to test for the presence of a substring, for instance, and length and slicing operations can be used to mimic endswith:


>>> line 
'The knights who sy Ni!\n'

>>> line.find('Ni') != -1  # Search via method call or expression
True
>>> 'Ni' in line 
True

>>> sub = 'Ni!\n' 
>>> line.endswith(sub)  # End test via method call or slice
True
>>> line[-len(sub):] == sub 
True


Because there are so many methods available for strings, we won't look at every one here. You'll see some additional string examples later in this book, but for more details, you can also turn to the Python library manual and other documentation sources, or simply experiment interactively on your own.

Note that none of the string methods accepts patterns-for pattern-based text processing, you must use the Python re standard library module, an advanced tool that was introduced in Chapter 4, but is mostly outside the scope of this text. Because of this limitation, though, string methods sometimes run more quickly than the re module's tools.

				30.1.4.4.4 The Original string Module

The history of Python's string methods is somewhat convoluted. For roughly the first decade of Python's existence, it provided a standard library module called string that contained functions that largely mirror the current set of string object methods. In response to user requests, in Python 2.0, these functions were made available as methods of string objects. Because so many people had written so much code that relied on the original string module, however, it was retained for backward compatibility.

Today, you should use only string methods, not the original string module. In fact, the original module-call forms of today's string methods are scheduled to be deleted from Python in Release 3.0, due out soon after this edition is published. However, because you may still see the module in use in older Python code, a brief look is in order here.

The upshot of this legacy is that in Python 2.5, there technically are still two ways to invoke advanced string operations: by calling object methods, or by calling string module functions, and passing in the object as an argument. For instance, given a variable X assigned to a string object, calling an object method:


X.method(arguments)


is usually equivalent to calling the same operation through the string module (provided that you have already imported the module):


string.method(X, arguments)


Here's an example of the method scheme in action:


>>> S = 'a+b+c+'
>>> x = S.replace('+', 'spam')
>>> x
'aspambspamcspam'


To access the same operation through the string module, you need to import the module (at least once in your process) and pass in the object:


>>> import string
>>> y = string.replace(S, '+', 'spam')
>>> y
'aspambspamcspam'


Because the module approach was the standard for so long, and because strings are such a central component of most programs, you will probably see both call patterns in Python code you come across.

Again, though, today you should use method calls instead of the older module calls. There are good reasons for this, besides the fact that the module calls are scheduled to go away in Release 3.0. For one thing, the module call scheme requires you to import the string module (methods do not require imports). For another, the module makes calls a few characters longer to type (when you load the module with import, that is, not using from). And, finally, the module runs more slowly than methods (the current module maps most calls back to the methods and so incurs an extra call along the way).

The original string module will probably be retained in Python 3.0 because it contains additional tools, including predefined string constants, and a template object system (an advanced tool omitted here-see the Python library manual for details on template objects). Unless you really want to change your code when 3.0 rolls out, though, you should consider the basic string operation calls in it to be just ghosts from the past.

			30.1.4.5 General Type Categories

Now that we've explored the first of Python's collection objects, the string, let's pause to define a few general type concepts that will apply to most of the types we look at from here on. With regard to built-in types, it turns out that operations work the same for all the types in the same category, so we'll only need to define most of these ideas once. We've only examined numbers and strings so far, but because they are representative of two of the three major type categories in Python, you already know more about other types than you might think.

				30.1.4.5.1 Types Share Operation Sets by Categories

As you've learned, strings are immutable sequences: they cannot be changed in-place (the immutable part), and they are positionally ordered collections that are accessed by offset (the sequence part). Now, it so happens that all the sequences we'll study in this part of the book respond to the same sequence operations shown in this chapter at work on stringsconcatenation, indexing, iteration, and so on. More formally, there are three type (and operation) categories in Python:


Numbers

    Support addition, multiplication, etc.

Sequences

    Support indexing, slicing, concatenation, etc.

Mappings

    Support indexing by key, etc.

We haven't yet explored mappings on our in-depth tour (dictionaries are discussed in the next chapter), but the other types we encounter will mostly be more of the same. For example, for any sequence objects X and Y:

    * X + Y makes a new sequence object with the contents of both operands.
    * X * N makes a new sequence object with N copies of the sequence operand X.

In other words, these operations work the same on any kind of sequence, including strings, lists, tuples, and some user-defined object types. The only difference is that the new result object you get back is the same type as the operands X and Yif you concatenate lists, you get back a new list, not a string. Indexing, slicing, and other sequence operations work the same on all sequences, too; the type of the objects being processed tells Python which task to perform.


				30.1.4.5.2 Mutable Types Can Be Changed In-Place

The immutable classification is an important constraint to be aware of, yet it tends to trip up new users. If an object type is immutable, you cannot change its value in-place; Python raises an error if you try. Instead, you must run code to make a new object containing the new value. Generally, immutable types give some degree of integrity by guaranteeing that an object won't be changed by another part of a program. For a refresher on why this matters, see the discussion of shared object references in Chapter 6.

			30.1.4.6 Chapter Summary

In this chapter, we took an in-depth tour of the string object type. We learned about coding string literals, and explored string operations, including sequence expressions, string formatting, and string method calls. Along the way, we studied a variety of concepts in depth, such as slicing, method calls, and triple-quoted block strings. We also defined some core ideas common to a variety of types: sequences, for example, share an entire set of operations. In the next chapter, we'll continue our types tour with a look at the most general object collections in Pythonthe list and dictionary. As you'll find, much of what you've learned here will apply to those types as well. First, though, here's another chapter quiz to review the material introduced here.

BRAIN BUILDER

Chapter Quiz

Q:	Can the string find method be used to search a list?
Q:	Can a string slice expression be used on a list?
Q:	How would you convert a character to its ASCII integer code? How would you convert the other way, from an integer to a character?
Q:	How might you go about changing a string in Python?
Q:	Given a string S with the value "s,pa,m", name two ways to extract the two characters in the middle.
Q:	How many characters are there in the string "a\nb\x1f\000d"?
Q:	Why might you use the string module instead of string method calls?


Quiz Answers

A:	No, because methods are always type-specific; that is, they only work on a single data type. Expressions are generic, though, and may work on a variety of types. In this case, for instance, the in membership expression has a similar effect, and can be used to search both strings and lists.
A:	Yes. Unlike methods, expressions are generic, and apply to many types. In this case, the slice expression is really a sequence operationit works on any type of sequence object, including strings, lists, and tuples. The only difference is that when you slice a list, you get back a new list.
A:	The built-in ord(S) function converts from a one-character string to an integer character code; chr(I) converts from the integer code back to a string.
A:	Strings cannot be changed; they are immutable. However, you can achieve a similar effect by creating a new stringby concatenating, slicing, running formatting expressions, or using a method call like replaceand then assigning the result back to the original variable name.
A:	You can slice the string using S[2:4], or split on the comma and index the string using S.split(',')[1]. Try these interactively to see for yourself.
A:	Six. The string "a\nb\x1f\000d" contains the bytes a, newline (\n), b, binary 31 (a hex escape \x1f), binary 0 (an octal escape \000), and d. Pass the string to the built-in len function to verify this, and print each of its characters' ord results to see the actual byte values. See Table 7-2 for more details.
A:	You should never use the string module instead of string object method calls todayit's deprecated, and its calls are slated for removal in Python 3.0. The only reason for using the string module at all is for its other tools, such as predefined constants, and advanced template objects.

		30.1.5. Lists and Dictionaries

This chapter presents the list and dictionary object types, both of which are collections of other objects. These two types are the main workhorses in almost all Python scripts. As you'll see, both types are remarkably flexible: they can be changed in-place, can grow and shrink on demand, and may contain and be nested in any other kind of object. By leveraging these types, you can build up and process arbitrarily rich information structures in your scripts.

			30.1.5.1 Lists
			
The next stop on our built-in object tour is the Python list. Lists are Python's most flexible ordered collection object type. Unlike strings, lists can contain any sort of object: numbers, strings, and even other lists. Also, unlike strings, lists may be changed in-place by assignment to offsets and slices, list method calls, deletion statements, and morethey are mutable objects.

Python lists do the work of most of the collection data structures you might have to implement manually in lower-level languages such as C. Here is a quick look at their main properties. Python lists are:


Ordered collections of arbitrary objects

    From a functional view, lists are just places to collect other objects so you can treat them as groups. Lists also maintain a left-to-right positional ordering among the items they contain (i.e., they are sequences).

Accessed by offset

    Just as with strings, you can fetch a component object out of a list by indexing the list on the object's offset. Because items in lists are ordered by their positions, you can also do tasks such as slicing and concatenation.

Variable-length, heterogeneous, and arbitrarily nestable

    Unlike strings, lists can grow and shrink in-place (their lengths can vary), and can contain any sort of object, not just one-character strings (they're heterogeneous). Because lists can contain other complex objects, they also support arbitrary nesting; you can create lists of lists of lists, and so on.

Of the category mutable sequence

    In terms of our type category qualifiers, lists can be changed in-place (they're mutable), and can respond to all the sequence operations used with strings, such as indexing, slicing, and concatenation. In fact, sequence operations work the same on lists as they do on strings; the only difference is that sequence operations such as concatenation and slicing return new lists instead of new strings when applied to lists. Because lists are mutable, however, they also support other operations that strings don't (such as deletion and index assignment operations, which change the lists in-place).

Arrays of object references

    Technically, Python lists contain zero or more references to other objects. Lists might remind you of arrays of pointers (addresses). Fetching an item from a Python list is about as fast as indexing a C array; in fact, lists really are C arrays inside the standard Python interpreter, not linked structures. As we learned in Chapter 6, though, Python always follows a reference to an object whenever the reference is used, so your program deals only with objects. Whenever you assign an object to a data structure component or variable name, Python always stores a reference to that same object, not a copy of it (unless you request a copy explicitly).

Table 8-1 summarizes common and representative list object operations. As usual, for the full story, see the Python standard library manual, or run a help(list) or dir(list) call interactively for a full list of list methodsyou can pass in a real list, or the word list, which is the name of the list data type.

Table 8-1. Common list literals and operations
Operation 	Interpretation
L1 = [] 	An empty list
L2 = [0, 1, 2, 3] 	Four items: indexes 0..3
L3 = ['abc', ['def', 'ghi']] 	Nested sublists


L2[i]

L3[i][j]

L2[i:j]

len(L2) 	Index, index of index, slice, length


L1 + L2

L2 * 3 	Concatenate, repeat


for x in L2

3 in L2 	Iteration, membership


L2.append(4)

L2.extend([5,6,7])

L2.sort( )

L2.index(1)

L2.insert(I, X)

L2.reverse( ) 	Methods: grow, sort, search, insert, reverse, etc.


del L2[k]

del L2[i:j]

L2.pop( )

L2.remove(2)

L2[i:j] = [] 	Shrinking


L2[i] = 1

L2[i:j] = [4,5,6] 	Index assignment, slice assignment


range(4)

xrange(0, 4) 	Make lists/tuples of integers
L4 = [x**2 for x in range(5)] 	List comprehensions (Chapter 13 and Chapter 17)

When written down as a literal expression, a list is coded as a series of objects (really, expressions that return objects) in square brackets, separated by commas. For instance, the second row in Table 8-1 assigns the variable L2 to a four-item list. A nested list is coded as a nested square-bracketed series (row 3), and the empty list is just a square-bracket pair with nothing inside (row 1).[26]

    [26] * In practice, you won't see many lists written out like this in list-processing programs. It's more common to see code that processes lists constructed dynamically (at runtime). In fact, although it's important to master literal syntax, most data structures in Python are built by running program code at runtime.

Many of the operations in Table 8-1 should look familiar, as they are the same sequence operations we put to work on stringsindexing, concatenation, iteration, and so on. Lists also respond to list-specific method calls (which provide utilities such as sorting, reversing, adding items to the end, etc.), as well as in-place change operations (deleting items, assignment to indexes and slices, and so forth). Lists get these tools for change operations because they are a mutable object type.

			30.1.5.2 Lists in Action

Perhaps the best way to understand lists is to see them at work. Let's once again turn to some simple interpreter interactions to illustrate the operations in Table 8-1.

				30.1.5.2.1 Basic List Operations

Lists respond to the + and * operators much like strings; they mean concatenation and repetition here too, except that the result is a new list, not a string. In fact, lists respond to all of the general sequence operations we used on strings in the prior chapter:


% python 
>>> len([1, 2, 3])  # Length
3
>>> [1, 2, 3] + [4, 5, 6]  # Concatenation
[1, 2, 3, 4, 5, 6]
>>> ['Ni!'] * 4  # Repetition
['Ni!', 'Ni!', 'Ni!', 'Ni!']
>>> 3 in [1, 2, 3]  # Membership
True
>>> for x in [1, 2, 3]: print x,  # Iteration
...
1 2 3


We will talk more about for iteration and the range built-ins in Chapter 13 because they are related to statement syntax. In short, for loops step through items in a sequence from left to right, executing one or more statements for each item. The last entry in Table 8-1, which is list comprehensions, are covered in Chapter 13, and expanded on in Chapter 17; as introduced in Chapter 4, they are a way to build a list by applying an expression to each item in a sequence, in a single step.

Although the + operator works the same for lists and strings, it's important to know that it expects the same sort of sequence on both sidesotherwise, you get a type error when the code runs. For instance, you cannot concatenate a list and a string unless you first convert the list to a string (using tools such as backquotes, str, or % formatting), or convert the string to a list (the list built-in function does the trick):


>>> str([1, 2]) + "34"  # Same as "[1, 2]" + "34"
'[1, 2]34'
>>> [1, 2] + list("34")  # Same as [1, 2] + ["3", "4"]
[1, 2, '3', '4']


				30.1.5.2.2 Indexing, Slicing, and Matrixes

Because lists are sequences, indexing and slicing work the same way for lists as they do for strings. However, the result of indexing a list is whatever type of object lives at the offset you specify, while slicing a list always returns a new list:


>>> L = ['spam', 'Spam', 'SPAM!'] 
>>> L[2]  # Offsets start at zero
'SPAM!'
>>> L[-2]  # Negative: count from the right

'Spam'
>>> L[1:]  # Slicing fetches sections
['Spam', 'SPAM!']


One note here: because you can nest lists (and other object types) within lists, you will sometimes need to string together index operations to go deeper into a data structure. For example, one of the simplest ways to represent matrixes (multidimensional arrays) in Python is as lists with nested sublists. Here's a basic 3 x 3 two-dimensional list-based array:


>>> matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]


With one index, you get an entire row (really, a nested sublist), and with two, you get an item within the row:


>>> matrix[1]
[4, 5, 6]
>>> matrix[1][1]
5
>>> matrix[2][0]
7
>>> matrix = [[1, 2, 3],
...           [4, 5, 6],
...           [7, 8, 9]]
>>> matrix[1][1]
5


Notice in the preceding interaction that lists can naturally span multiple lines if you want them to because they are contained by a pair of brackets (more on syntax in the next part of the book). Later in this chapter, you'll also see a dictionary-based matrix representation. For high-powered numeric work, the NumPy extension mentioned in Chapter 5 provides other ways to handle matrixes.


				30.1.5.2.3 Changing Lists In-Place

Because lists are mutable, they support operations that change a list object in-place. That is, the operations in this section all modify the list object directly, without forcing you to make a new copy, as you had to for strings. Because Python deals only in object references, this distinction between changing an object in-place and creating a new object mattersas discussed in Chapter 6, if you change an object in-place, you might impact more than one reference to it at the same time.

					30.1.5.2.3.1 Index and slice assignments

When using a list, you can change its contents by assigning to a particular item (offset), or an entire section (slice):


>>> L = ['spam', 'Spam', 'SPAM!'] 
>>> L[1] = 'eggs'  # Index assignment
>>> L 
['spam', 'eggs', 'SPAM!']
>>> L[0:2] = ['eat', 'more']  # Slice assignment: delete+insert
>>> L  # Replaces items 0,1
['eat', 'more', 'SPAM!']


Both index and slice assignments are in-place changesthey modify the subject list directly, rather than generating a new list object for the result. Index assignment in Python works much as it does in C and most other languages: Python replaces the object reference at the designated offset with a new one.

Slice assignment, the last operation in the preceding example, replaces an entire section of a list in a single step. Because it can be a bit complex, it is perhaps best thought of as a combination of two steps:

   a.  Deletion. The slice you specify to the left of the = is deleted.
   b.  Insertion. The new items contained in the object to the right of the = are inserted into the list on the left, at the place where the old slice was deleted.[27]

          [27] * This description needs elaboration when the value and the slice being assigned overlap: L[2:5]=L[3:6], for instance, works fine because the value to be inserted is fetched before the deletion happens on the left.

This isn't what really happens, but it tends to help clarify why the number of items inserted doesn't have to match the number of items deleted. For instance, given a list L that has the value [1,2,3], the assignment L[1:2]=[4,5] sets L to the list [1,4,5,3]. Python first deletes the 2 (a one-item slice), then inserts the 4 and 5 where the deleted 2 used to be. This also explains why L[1:2]=[] is really a deletion operationPython deletes the slice (the item at offset 1), and then inserts nothing.

In effect, slice assignment replaces an entire section, or "column," all at once. Because the length of the sequence being assigned does not have to match the length of the slice being assigned to, slice assignment can be used to replace (by overwriting), expand (by inserting), or shrink (by deleting) the subject list. It's a powerful operation, but frankly, one that you may not see very often in practice. There are usually more straightforward ways to replace, insert, and delete (concatenation, and the insert, pop, and remove list methods, for example), which Python programmers tend to prefer in practice.

					30.1.5.2.3.2 List method calls

Like strings, Python list objects also support type-specific method calls:


>>> L.append('please')  # Append method call
>>> L 
['eat', 'more', 'SPAM!', 'please']
>>> L.sort(  )  # Sort list items ('S' < 'e')
>>> L 
['SPAM!', 'eat', 'more', 'please']


Methods were introduced in Chapter 7. In brief, they are functions (really, attributes that reference functions) that are associated with particular objects. Methods provide type-specific tools; the list methods presented here, for instance, are available only for lists.

Perhaps the most commonly used list method is append, which simply tacks a single item (object reference) onto the end of the list. Unlike concatenation, append expects you to pass in a single object, not a list. The effect of L.append(X) is similar to L+[X], but while the former changes L in-place, the latter makes a new list.[28] Another commonly seen method, sort, orders a list in-place; by default, it uses Python standard comparison tests (here, string comparisons), and sorts in ascending order. You can also pass in a comparison function of your own to sort.

    [28] * Unlike + concatenation, append doesn't have to generate new objects, so it's usually faster. You can also mimic append with clever slice assignments: L[len(L):]=[X] is like L.append(X), and L[:0]=[X] is like appending at the front of a list. Both delete an empty slice and insert X, changing L in-place quickly, like append.

	

In Python 2.5 and earlier, comparisons of differently typed objects (e.g., a string and a list) workthe language defines a fixed ordering among different types, which is deterministic, if not aesthetically pleasing. That is, this ordering is based on the names of the types involved: all integers are less than all strings, for example, because "int" is less than "str". Comparisons never automatically convert types, except when comparing numeric type objects.

In Python 3.0, this may change: comparison of mixed types is scheduled to raise an exception instead of falling back on the fixed cross-type ordering. Because sorting uses comparisons internally, this means that [1, 2, 'spam'].sort( ) succeeds in Python 2.x, but will raise an exception as of Python 3.0. See the 3.0 release notes for more details.

One warning here: beware that append and sort change the associated list object in-place, but don't return the list as a result (technically, they both return a value called None). If you say something like L=L.append(X), you won't get the modified value of L (in fact, you'll lose the reference to the list altogether); when you use attributes such as append and sort, objects are changed as a side effect, so there's no reason to reassign.

Like strings, lists have other methods that perform other specialized operations. For instance, reverse reverses the list in-place, and the extend and pop methods insert multiple items at the end, and delete an item from the end of the list, respectively:


>>> L = [1, 2] 
>>> L.extend([3,4,5])  # Append method call
>>> L 
[1, 2, 3, 4, 5]
>>> L.pop(  )  # Delete and return last item
5
>>> L 
[1, 2, 3, 4]
>>> L.reverse(  )  # In-place reversal
>>> L 
[4, 3, 2, 1]


In some types of programs, the list pop method used here is often used in conjunction with append to implement a quick last-in-first-out (LIFO) stack structure. The end of the list serves as the top of the stack:


>>> L = [] 
>>> L.append(1)  # Push onto stack
>>> L.append(2) 
>>> L 
[1, 2]
>>> L.pop(  )  # Pop off stack
2
>>> L 
[1]


Although not shown here, the pop method also accepts an optional offset of the item to be deleted and returned (the default is the last item). Other list methods remove an item by value (remove), insert an item at an offset (insert), search for an item's offset (index), and more; see other documentation sources, or experiment with these calls interactively on your own to learn more.

					30.1.5.2.3.3 Other common list operations

Because lists are mutable, you can use the del statement to delete an item or section in-place:


>>> L 
['SPAM!', 'eat', 'more', 'please']
>>> del L[0]  # Delete one item
>>> L 
['eat', 'more', 'please']
>>> del L[1:]  # Delete an entire section
>>> L  # Same as L[1:] = []
['eat']


Because slice assignment is a deletion plus an insertion, you can also delete a section of a list by assigning an empty list to a slice (L[i:j]=[]); Python deletes the slice named on the left, and then inserts nothing. Assigning an empty list to an index, on the other hand, just stores a reference to the empty list in the specified slot, rather than deleting it:


>>> L = ['Already', 'got', 'one']
>>> L[1:] = []
>>> L
['Already']
>>> L[0] = []
>>> L
[[]]


Although all the operations just discussed are typical, there are additional list methods and operations not illustrated here (including methods for inserting and searching). For a comprehensive and up-to-date list of type tools, you should always consult Python's manuals, Python's dir and help functions (which we first met in Chapter 4), or the Python Pocket Reference (O'Reilly), and other reference texts described in the Preface.

I'd also like to remind you one more time that all the in-place change operations discussed here work only for mutable objects: they won't work on strings (or tuples, discussed in the next chapter), no matter how hard you try. Mutability is an inherent property of each object type.

			30.1.5.3 Dictionaries

Apart from lists, dictionaries are perhaps the most flexible built-in data type in Python. If you think of lists as ordered collections of objects, you can think of dictionaries as unordered collections; the chief distinction is that in dictionaries, items are stored and fetched by key, instead of by positional offset.

Being a built-in type, dictionaries can replace many of the searching algorithms and data structures you might have to implement manually in lower-level languagesindexing a dictionary is a very fast search operation. Dictionaries also sometimes do the work of records and symbol tables used in other languages, can represent sparse (mostly empty) data structures, and much more. Here's a rundown of their main properties. Python dictionaries are:


Accessed by key, not offset

    Dictionaries are sometimes called associative arrays or hashes. They associate a set of values with keys, so you can fetch an item out of a dictionary using the key under which you originally stored it. You use the same indexing operation to get components in a dictionary as in a list, but the index takes the form of a key, not a relative offset.

Unordered collections of arbitrary objects

    Unlike in a list, items stored in a dictionary aren't kept in any particular order; in fact, Python randomizes their left-to-right order to provide quick lookup. Keys provide the symbolic (not physical) locations of items in a dictionary.

Variable-length, heterogeneous, and arbitrarily nestable

    Like lists, dictionaries can grow and shrink in-place (without new copies being made), they can contain objects of any type, and they support nesting to any depth (they can contain lists, other dictionaries, and so on).

Of the category mutable mapping

    Dictionaries can be changed in-place by assigning to indexes (they are mutable), but they don't support the sequence operations that work on strings and lists. Because dictionaries are unordered collections, operations that depend on a fixed positional order (e.g., concatenation, slicing) don't make sense. Instead, dictionaries are the only built-in representatives of the mapping type category (objects that map keys to values).

Tables of object references (hash tables)

    If lists are arrays of object references that support access by position, dictionaries are unordered tables of object references that support access by key. Internally, dictionaries are implemented as hash tables (data structures that support very fast retrieval), which start small and grow on demand. Moreover, Python employs optimized hashing algorithms to find keys, so retrieval is quick. Like lists, dictionaries store object references (not copies).

Table 8-2 summarizes some of the most common and representative dictionary operations (again, see the library manual or run a dir(dict) or help(dict) call for a complete listdict is the name of the type). When coded as a literal expression, a dictionary is written as a series of key:value pairs, separated by commas, enclosed in curly braces.[29] An empty dictionary is an empty set of braces, and dictionaries can be nested by writing one as a value inside another dictionary, or within a list or tuple.

    [29] * As with lists, you won't often see dictionaries constructed using literals. Lists and dictionaries are grown in different ways, though. As you'll see in the next section, dictionaries are typically built up by assigning to new keys at runtime; this approach fails for lists (lists are grown with append instead).

Table 8-2. Common dictionary literals and operations
Operation 	Interpretation
D1 = {} 	Empty dictionary
D2 = {'spam': 2, 'eggs': 3} 	Two-item dictionary
D3 = {'food': {'ham': 1, 'egg': 2}} 	Nesting


D2['eggs']

D3['food']['ham'] 	Indexing by key


D2.has_key('eggs')

'eggs' in D2

D2.keys( )

D2.values( )

D2.copy( )

D2.get(key, default)

D2.update(D1)

D2.pop(key) 	Methods: membership test, keys list, values list, copies, defaults, merge, delete, etc.
len(D1) 	Length (number of stored entries)


D2[key] = 42

del D2[key] 	Adding/changing keys, deleting keys


D4 = dict.fromkeys(['a', 'b'])

D5 = dict(zip(keyslist, valslist))

D6 = dict(name='Bob', age=42) 	Alternative construction techniques 


			30.1.5.4 Dictionaries in Action

As Table 8-2 suggests, dictionaries are indexed by key, and nested dictionary entries are referenced by a series of indexes (keys in square brackets). When Python creates a dictionary, it stores its items in any left-to-right order it chooses; to fetch a value back, you supply the key with which it is associated. Let's go back to the interpreter to get a feel for some of the dictionary operations in Table 8-2.

				30.1.5.4.1 Basic Dictionary Operations

In normal operation, you create dictionaries, and store and access items by key:


% python 
>>> d2 = {'spam': 2, 'ham': 1, 'eggs': 3}  # Make a dictionary
>>> d2['spam']  # Fetch a value by key
2
>>> d2  # Order is scrambled
{'eggs': 3, 'ham': 1, 'spam': 2}


Here, the dictionary is assigned to the variable d2; the value of the key 'spam' is the integer 2, and so on. We use the same square bracket syntax to index dictionaries by key as we did to index lists by offset, but here it means access by key, not by position.

Notice the end of this example: the left-to-right order of keys in a dictionary will almost always be different from what you originally typed. This is on purposeto implement fast key lookup (a.k.a. hashing), keys need to be randomized in memory. That's why operations that assume a fixed left-to-right order (e.g., slicing, concatenation) do not apply to dictionaries; you can fetch values only by key, not by position.

The built-in len function works on dictionaries, too; it returns the number of items stored in the dictionary or, equivalently, the length of its keys list. The dictionary has_key method and the in membership operator allow you to test for key existence, and the keys method returns all the keys in the dictionary, collected in a list. The latter of these can be useful for processing dictionaries sequentially, but you shouldn't depend on the order of the keys list. Because the keys result is a normal list, however, it can always be sorted if order matters:


>>> len(d2)  # Number of entries in dictionary
3
>>> d2.has_key('ham')  # Key membership test
True
>>> 'ham' in d2  # Key membership test alternative
True
>>> d2.keys(  )  # Create a new list of my keys
['eggs', 'ham', 'spam']


Notice the third expression in this listing. As mentioned earlier, the in membership test used for strings and lists also works on dictionariesit checks whether a key is stored in the dictionary, like the has_key method call of the prior line. Technically, this works because dictionaries define iterators that step through their keys lists. Other types provide iterators that reflect their common uses; files, for example, have iterators that read line by line. We'll discuss iterators further in Chapter 17 and Chapter 24.

Later in this chapter and book, you'll learn about two alternative ways to build dictionaries, demonstrated at the end of Table 8-2: you can pass zipped lists of key/value tuples or keyword function arguments to the new dict call (really, a type constructor). We'll explore keyword arguments in Chapter 16. We'll also discuss the zip function in Chapter 13; it's a way to construct a dictionary from key and value lists in a single call. If you cannot predict the set of keys and values in your code, for instance, you can always build them up as lists and zip them together dynamically.


				30.1.5.4.2 Changing Dictionaries In-Place

Let's continue with our interactive session. Dictionaries, like lists, are mutable, so you can change, expand, and shrink them in-place without making new dictionaries: simply assign a value to a key to change or create an entry. The del statement works here, too; it deletes the entry associated with the key specified as an index. Notice also the nesting of a list inside a dictionary in this example (the value of the key 'ham'). All collection data types in Python can nest inside each other arbitrarily:


>>> d2['ham'] = ['grill', 'bake', 'fry']  # Change entry
>>> d2 
{'eggs': 3, 'spam': 2, 'ham': ['grill', 'bake', 'fry']}

>>> del d2['eggs']  # Delete entry
>>> d2 
{'spam': 2, 'ham': ['grill', 'bake', 'fry']}

>>> d2['brunch'] = 'Bacon'  # Add new entry
>>> d2 
{'brunch': 'Bacon', 'spam': 2, 'ham': ['grill', 'bake', 'fry']}


As with lists, assigning to an existing index in a dictionary changes its associated value. Unlike with lists, however, whenever you assign a new dictionary key (one that hasn't been assigned before), you create a new entry in the dictionary, as was done in the previous example for the key 'brunch.' This doesn't work for lists because Python considers an offset beyond the end of a list out of bounds and throws an error. To expand a list, you need to use tools such as the append method or slice assignment instead.


				30.1.5.4.3 More Dictionary Methods

Dictionary methods provide a variety of tools. For instance, the dictionary values and items methods return lists of the dictionary's values and (key,value) pair tuples, respectively:


>>> d2 = {'spam': 2, 'ham': 1, 'eggs': 3}
>>> d2.values(  )
[3, 1, 2]
>>> d2.items(  )
 [('eggs', 3), ('ham', 1), ('spam', 2)]


Such lists are useful in loops that need to step through dictionary entries one by one. Fetching a nonexistent key is normally an error, but the get method returns a default value (None, or a passed-in default) if the key doesn't exist. It's an easy way to fill in a default for a key that isn't present and avoid a missing-key error:


>>> d2.get('spam' )                           # A key that is there
2
>>> print d2.get('toast')  # A key that is missing
None
>>> d2.get('toast', 88) 
88


The update method provides something similar to concatenation for dictionaries. It merges the keys and values of one dictionary into another, blindly overwriting values of the same key:


>>> d2
{'eggs': 3, 'ham': 1, 'spam': 2}
>>> d3 = {'toast':4, 'muffin':5}
>>> d2.update(d3)
>>> d2
{'toast': 4, 'muffin': 5, 'eggs': 3, 'ham': 1, 'spam': 2}


Finally, the dictionary pop method deletes a key from a dictionary, and returns the value it had. It's similar to the list pop method, but it takes a key instead of an optional position:


# pop a dictionary by key
>>> d2 
{'toast': 4, 'muffin': 5, 'eggs': 3, 'ham': 1, 'spam': 2}
>>> d2.pop('muffin') 
5
>>> d2.pop('toast')  # Delete and return from a key
4
>>> d2 
{'eggs': 3, 'ham': 1, 'spam': 2}

# pop a list by position

>>> L = ['aa', 'bb', 'cc', 'dd'] 
>>> L.pop(  )  # Delete and return from the end
'dd'
>>> L 
['aa', 'bb', 'cc']
>>> L.pop(1)  # Delete from a specific position
'bb'
>>> L 
['aa', 'cc']


Dictionaries also provide a copy method; we'll discuss this in the next chapter, as it's a way to avoid the potential side effects of shared references to the same dictionary. In fact, dictionaries come with many more methods than those listed in Table 8-2; see the Python library manual, or other documentation sources for a comprehensive list.

				30.1.5.4.4 A Languages Table

Let's look at a more realistic dictionary example. The following example creates a table that maps programming language names (the keys) to their creators (the values). You fetch creator names by indexing on language names:


>>> table = {'Python':  'Guido van Rossum',
...          'Perl':    'Larry Wall',
...          'Tcl':     'John Ousterhout' }
...
>>> language = 'Python'
>>> creator  = table[language]
>>> creator
'Guido van Rossum'

>>> for lang in table.keys(  ):
...     print lang, '\t', table[lang]
...
Tcl     John Ousterhout
Python  Guido van Rossum
Perl    Larry Wall


The last command uses a for loop, which we haven't covered in detail yet. If you aren't familiar with for loops, this command simply iterates through each key in the table, and prints a tab-separated list of keys and their values. We'll learn more about for loops in Chapter 13.

Because dictionaries aren't sequences, you can't iterate over them directly with a for statement in the way you can with strings and lists. But, if you need to step through the items in a dictionary, it's easy: calling the dictionary keys method returns a list of all stored keys, which you can iterate through with a for. If needed, you can index from key to value inside the for loop, as was done in this code.

In fact, Python also lets you step through a dictionary's keys list without actually calling the keys method in most for loops. For any dictionary D, saying for key in D: works the same as saying the complete for key in D.keys( ):. This is really just another instance of the iterators mentioned earlier, which allow the in membership operator to work on dictionaries as well (more on iterators later in this book).

				30.1.5.4.5 Dictionary Usage Notes

Dictionaries are fairly straightforward tools once you get the hang of them, but here are a few additional pointers and reminders you should be aware of when using them:

    * Sequence operations don't work. Dictionaries are mappings, not sequences; because there's no notion of ordering among their items, things like concatenation (an ordered joining), and slicing (extracting a contiguous section) simply don't apply. In fact, Python raises an error when your code runs if you try to do such things.
    * Assigning to new indexes adds entries. Keys can be created when you write a dictionary literal (in which case they are embedded in the literal itself), or when you assign values to new keys of an existing dictionary object. The end result is the same.
    * Keys need not always be strings. Our examples used strings as keys, but any other immutable objects (i.e., not lists) work just as well. For instance, you can use integers as keys, which makes the dictionary look much like a list (when indexing, at least). Tuples are sometimes used as dictionary keys too, allowing for compound key values. Class instance objects (discussed in Part 6) can be used as keys too, as long as they have the proper protocol methods; roughly, they need to tell Python that their values won't change, as otherwise they would be useless as fixed keys.

					30.1.5.4.5.1 Using dictionaries to simulate flexible lists

The last point in the prior list is important enough to demonstrate with a few examples. When you use lists, it is illegal to assign to an offset that is off the end of the list:


>>> L = []
>>> L[99] = 'spam'
Traceback (most recent call last):
  File "<stdin>", line 1, in ?
IndexError: list assignment index out of range


Although you can use repetition to preallocate as big a list as you'll need (e.g., [0]*100), you can also do something that looks similar with dictionaries, which does not require such space allocations. By using integer keys, dictionaries can emulate lists that seem to grow on offset assignment:


>>> D = {}
>>> D[99] = 'spam'
>>> D[99]
'spam'
>>> D
{99: 'spam'}


Here, it looks as if D is a 100-item list, but it's really a dictionary with a single entry; the value of the key 99 is the string 'spam'. You can access this structure with offsets much like a list, but you don't have to allocate space for all the positions you might ever need to assign values to in the future. When used like this, dictionaries are like more flexible equivalents of lists.

					30.1.5.4.5.2 Using dictionaries for sparse data structures

In a similar way, dictionary keys are also commonly leveraged to implement sparse data structuresfor example, multidimensional arrays where only a few positions have values stored in them:


>>> Matrix = {} 
>>> Matrix[(2, 3, 4)] = 88 
>>> Matrix[(7, 8, 9)] = 99 
>>>
>>> X = 2; Y = 3; Z = 4  # ; separates statements
>>> Matrix[(X, Y, Z)] 
88
>>> Matrix 
{(2, 3, 4): 88, (7, 8, 9): 99}


Here, we've used a dictionary to represent a three-dimensional array that is empty except for the two positions (2,3,4) and (7,8,9). The keys are tuples that record the coordinates of nonempty slots. Rather than allocating a large and mostly empty three-dimensional matrix, we can use a simple two-item dictionary. In this scheme, accessing an empty slot triggers a nonexistent key exception, as these slots are not physically stored:


>>> Matrix[(2,3,6)]
Traceback (most recent call last):
  File "<stdin>", line 1, in ?
KeyError: (2, 3, 6)


					30.1.5.4.5.3 Avoiding missing-key errors

Errors for nonexistent key fetches are common in sparse matrixes, but you probably won't want them to shut down your program. There are at least three ways to fill in a default value instead of getting such an error messageyou can test for keys ahead of time in if statements, use a try statement to catch and recover from the exception explicitly, or simply use the dictionary get method shown earlier to provide a default for keys that do not exist:


>>> if Matrix.has_key((2,3,6)):  # Check for key before fetch
...     print Matrix[(2,3,6)] 
... else: 
...     print 0 
...
0
>>> try: 
...     print Matrix[(2,3,6)]  # Try to index
... except KeyError:  # Catch and recover
...     print 0 
...
0
>>> Matrix.get((2,3,4), 0)  # Exists; fetch and return
88
>>> Matrix.get((2,3,6), 0)  # Doesn't exist; use default arg
0


Of these, the get method is the most concise in terms of coding requirements; we'll study the if and try statements in more detail later in this book.

					30.1.5.4.5.4 Using dictionaries as "records"

As you can see, dictionaries can play many roles in Python. In general, they can replace search data structures (because indexing by key is a search operation), and can represent many types of structured information. For example, dictionaries are one of many ways to describe the properties of an item in your program's domain; that is, they can serve the same role as "records" or "structs" in other languages.

This example fills out a dictionary by assigning to new keys over time:


>>> rec = {}
>>> rec['name'] = 'mel'
>>> rec['age']  = 45
>>> rec['job']  = 'trainer/writer'
>>>
>>> print rec['name']
mel


Especially when nested, Python's built-in data types allow us to easily represent structured information. This example again uses a dictionary to capture object properties, but it codes it all at once (rather than assigning to each key separately), and nests a list and a dictionary to represent structured property values:


>>> mel = {'name': 'Mark',
...        'jobs': ['trainer', 'writer'],
...        'web':  'www.rmi.net/tildelutz',
...        'home': {'state': 'CO', 'zip':80513}}
...


To fetch components of nested objects, simply string together indexing operations:


|>>> mel['name']
|'Mark'
|>>> mel['jobs']
|['trainer', 'writer']
|>>> mel['jobs'][1]
|'writer'
|>>> mel['home']['zip']
|80513


					30.1.5.4.5.5 Other ways to make dictionaries

Finally, note that because dictionaries are so useful, more ways to build them have emerged over time. In Python 2.3 and later, for example, the last two calls to the dict constructor in the following have the same effect as the literal and key-assignment forms above them:


{'name': 'mel', 'age': 45}             # Traditional literal expression

D = {}                                 # Assign by keys dynamically
D['name'] = 'mel'
D['age']  = 45

dict(name='mel', age=45)               # Keyword argument form

dict([('name', 'mel'), ('age', 45)])   # Key/value tuples form


All four of these forms create the same two-key dictionary:

    * The first is handy if you can spell out the entire dictionary ahead of time.
    * The second is of use if you need to create the dictionary one field at a time on the fly.
    * The third keyword form is less code to type than literals, but it requires all keys to be strings.
    * The last form here is useful if you need to build up keys and values as sequences at runtime.

Why You Will Care: Dictionary Interfaces

Besides being a convenient way to store information by key in your programs, some Python extensions also present interfaces that look like and work the same as dictionaries. For instance, Python's interface to DBM access-by-key files looks much like a dictionary that must be opened. Strings are stored and fetched using key indexes:


import anydbm
file = anydbm.open("filename") # Link to file
file['key'] = 'data'           # Store data by key
data = file['key']             # Fetch data by key


Later, you'll see that you can store entire Python objects this way, too, if you replace anydbm in the preceding code with shelve (shelves are access-by-key databases of persistent Python objects). For Internet work, Python's CGI script support also presents a dictionary-like interface. A call to cgi.FieldStorage yields a dictionary-like object with one entry per input field on the client's web page:


import cgi
form = cgi.FieldStorage(  )      # Parse form data
if form.has_key('name'):
    showReply('Hello, ' + form['name'].value)


All of these (and dictionaries) are instances of mappings. Once you learn dictionary interfaces, you'll find that they apply to a variety of built-in tools in Python.

As suggested near the end of Table 8-2, the last form is also commonly used in conjunction with the zip function, to combine separate lists of keys and values obtained dynamically at runtime (parsed out of a data file's columns, for instance).

Provided all the key's values are the same, you can also initialize a dictionary with this special formsimply pass in a list of keys and an initial value for all of them (the default is None):


>>> dict.fromkeys(['a', 'b'], 0)
{'a': 0, 'b': 0}


Although you could get by with just literals and key assignments at this point in your Python career, you'll probably find uses for all of these dictionary-creation forms as you start applying them in realistic, flexible, and dynamic Python programs.

			30.1.5.5 Chapter Summary

In this chapter, we explored the list and dictionary typesprobably the two most common, flexible, and powerful collection types you will see and use in Python code. We learned that the list type supports positionally ordered collections of arbitrary objects, and that it may be freely nested, grown and shrunk on demand, and more. The dictionary type is similar, but it stores items by key instead of by position, and does not maintain any reliable left-to-right order among its items. Both lists and dictionaries are mutable, and so support a variety of in-place change operations not available for strings: for example, lists can be grown by append calls, and dictionaries by assignment to new keys.

In the next chapter, we will wrap up our in-depth core object type tour by looking at tuples and files. After that, we'll move on to statements that code the logic that processes our objects, taking us another step toward writing complete programs. Before we tackle those topics, though, here are some chapter quiz questions to review.

BRAIN BUILDER

Chapter Quiz

Q:	Name two ways to build a list containing five integer zeros.
Q:	Name two ways to build a dictionary with two keys 'a' and 'b' each having an associated value of 0.
Q:	Name four operations that change a list object in-place.
Q:	Name four operations that change a dictionary object in-place.


Quiz Answers

A:	A literal expression like [0, 0, 0, 0, 0] and a repetition expression like [0] * 5 will each create a list of five zeros. In practice, you might also build one up with a loop that starts with an empty list and appends 0 to it in each iteration: L.append(0). A list comprehension ([0 for i in range(5)]) could work here too, but this is more work than you need to do.
A:	A literal expression such as {'a': 0, 'b': 0}, or a series of assignments like D = {}, D['a'] = 0, D['b'] = 0 would create the desired dictionary. You can also use the newer and simpler-to-code dict(a=0, b=0) keyword form, or the more flexible dict([('a', 0), ('b', 0)]) key/value sequences form. Or, because all the values are the same, you can use the special form dict.fromkeys(['a', 'b'], 0).
A:	The append and extend methods grow a list in-place, the sort and reverse methods order and reverse lists, the insert method inserts an item at an offset, the remove and pop methods delete from a list by value and by position, the del statement deletes an item or slice, and index and slice assignment statements replace an item or entire section. Pick any four of these for the quiz.
A:	Dictionaries are primarily changed by assignment to a new or existing key, which creates or changes the key's entry in the table. Also, the del statement deletes a key's entry, the dictionary update method merges one dictionary into another in-place, and D.pop(key) removes a key and returns the value it had. Dictionaries also have other, more exotic in-place change methods not listed in this chapter, such as setdefault; see reference sources for more details.


		30.1.6. Tuples, Files, and Everything Else

This chapter rounds out our in-depth look at the core object types in Python by exploring the tuple (a collection of other objects that cannot be changed), and the file (an interface to external files on your computer). As you'll see, the tuple is a relatively simple object that largely performs operations you've already learned about for strings and lists. The file object is a commonly used and full-featured tool for processing files; the basic overview of files here will be supplemented by further file examples that appear in later chapters of this book.

This chapter also concludes this part of the book by looking at properties common to all the core object types we've metthe notions of equality, comparisons, object copies, and so on. We'll also briefly explore other object types in the Python toolbox; as you'll see, although we've covered all the primary built-in types, the object story in Python is broader than I've implied thus far. Finally, we'll close this part of the book by taking a look at a set of common object type pitfalls, and exploring some exercises that will allow you to experiment with the ideas you've learned.


			30.1.6.1  Tuples

The last collection type in our survey is the Python tuple. Tuples construct simple groups of objects. They work exactly like lists, except that tuples can't be changed in-place (they're immutable), and are usually written as a series of items in parentheses, not square brackets. Although they don't support any method calls, tuples share most of their properties with lists. Here's a quick look at their properties. Tuples are:


Ordered collections of arbitrary objects

    Like strings and lists, tuples are positionally ordered collections of objects (i.e., they maintain a left-to-right order among their contents); like lists, they can embed any kind of object.

Accessed by offset

    Like strings and lists, items in a tuple are accessed by offset (not by key); they support all the offset-based access operations, such as indexing and slicing.

Of the category immutable sequence

    Like strings, tuples are immutable; they don't support any of the in-place change operations applied to lists. Like strings and lists, tuples are sequences; they support many of the same operations.

Fixed-length, heterogeneous, and arbitrarily nestable

    Because tuples are immutable, you cannot change their size without making a copy. On the other hand, tuples can hold other compound objects (e.g., lists, dictionaries, other tuples), and so support arbitrary nesting.

Arrays of object references

    Like lists, tuples are best thought of as object reference arrays; tuples store access points to other objects (references), and indexing a tuple is relatively quick.

Table 9-1 highlights common tuple operations. A tuple is written as a series of objects (technically, expressions that generate objects), separated by commas and enclosed in parentheses. An empty tuple is just a parentheses pair with nothing inside.

Table 9-1. Common tuple literals and operations
Operation 	Interpretation
( ) 	An empty tuple
t1 = (0,) 	A one-item tuple (not an expression)
t2 = (0, 'Ni', 1.2, 3) 	A four-item tuple
t2 = 0, 'Ni', 1.2, 3 	Another four-item tuple (same as prior line)
t3 = ('abc', ('def', 'ghi')) 	Nested tuples


t1[i]

t3[i][j]

t1[i:j]

len(t1) 	Index, index of index, slice, length


t1 + t2

t2 * 3 	Concatenate, repeat


for x in t

'spam' in t2 	Iteration, membership

				30.1.6.1.1  Tuples in Action

As usual, let's start an interactive session to explore tuples at work. Notice in Table 9-1 that tuples have no methods (e.g., an append call won't work here). They do, however, support the usual sequence operations that we saw for strings and lists:


>>> (1, 2) + (3, 4)  # Concatenation
(1, 2, 3, 4)

>>> (1, 2) * 4  # Repetition
(1, 2, 1, 2, 1, 2, 1, 2)

>>> T = (1, 2, 3, 4)  # Indexing, slicing
>>> T[0], T[1:3] 
(1, (2, 3))


					30.1.6.1.1.1  Tuple syntax peculiarities: commas and parentheses

The second and fourth entries in Table 9-1 merit a bit more explanation. Because parentheses can also enclose expressions (see Chapter 5), you need to do something special to tell Python when a single object in parentheses is a tuple object and not a simple expression. If you really want a single-item tuple, simply add a trailing comma after the single item, and before the closing parenthesis:


>>> x = (40)  # An integer
>>> x 
40
>>> y = (40,)  # A tuple containing an integer
>>> y 
(40,)


As a special case, Python also allows you to omit the opening and closing parentheses for a tuple in contexts where it isn't syntactically ambiguous to do so. For instance, the fourth line of the table simply lists four items separated by commas. In the context of an assignment statement, Python recognizes this as a tuple, even though it doesn't have parentheses.

Now, some people will tell you to always use parentheses in your tuples, and some will tell you to never use parentheses in tuples (and still others have lives, and won't tell you what to do with your tuples!). The only significant places where the parentheses are required are when a tuple is passed as a literal in a function call (where parentheses matter), and when one is listed in a print statement (where commas are significant).

For beginners, the best advice is that it's probably easier to use the parentheses than it is to figure out when they are optional. Many programmers also find that parentheses tend to aid script readability by making the tuples more explicit, but your mileage may vary.


					30.1.6.1.1.2 Conversions and immutability

Apart from literal syntax differences, tuple operations (the last three rows in Table 9-1) are identical to string and list operations. The only differences worth noting are that the +, *, and slicing operations return new tuples when applied to tuples, and that tuples don't provide the methods you saw for strings, lists, and dictionaries. If you want to sort a tuple, for example, you'll usually have to first convert it to a list to gain access to a sorting method call and make it a mutable object:


>>> T = ('cc', 'aa', 'dd', 'bb') 
>>> tmp = list(T)  # Make a list from a tuple's items
>>> tmp.sort(  )  # Sort the list
>>> tmp 
['aa', 'bb', 'cc', 'dd']
>>> T = tuple(tmp)  # Make a tuple from the list's items
>>> T 
('aa', 'bb', 'cc', 'dd')


Here, the list and tuple built-in functions are used to convert to a list, and then back to a tuple; really, both calls make new objects, but the net effect is like a conversion.

List comprehensions can also be used to convert tuples. The following, for example, makes a list from a tuple, adding 20 to each item along the way:


>>> T = (1, 2, 3, 4, 5)
>>> L = [x + 20 for x in T]
>>> L
[21, 22, 23, 24, 25]


List comprehensions are really sequence operationsthey always build new lists, but they may be used to iterate over any sequence objects, including tuples, strings, and other lists. As we'll see later, they even work on some things that are not physically stored sequencesany iterable objects will do, including files, which are automatically read line by line.

Also, note that the rule about tuple immutability applies only to the top level of the tuple itself, not to its contents. A list inside a tuple, for instance, can be changed as usual:


>>> T = (1, [2, 3], 4) 
>>> T[1] = 'spam'  # This fails: can't change tuple itself
TypeError: object doesn't support item assignment

>>> T[1][0] = 'spam'  # This works: can change mutables inside
>>> T 
(1, ['spam', 3], 4)


For most programs, this one-level-deep immutability is sufficient for common tuple roles. Which, coincidentally, brings us to the next section.

				30.1.6.1.2 Why Lists and Tuples?

This seems to be the first question that always comes up when teaching beginners about tuples: why do we need tuples if we have lists? Some of the reasoning may be historic; Python's creator is a mathematician by training, and has been quoted as seeing a tuple as a simple association of objects, and a list as a data structure that changes over time.

The best answer, however, seems to be that the immutability of tuples provides some integrityyou can be sure a tuple won't be changed through another reference elsewhere in a program, but there's no such guarantee for lists. Tuples, therefore, serve a similar role to "constant" declarations in other languages, though the notion of constant-ness is associated with objects in Python, not variables.

Tuples can also be used in places that lists cannotfor example, as dictionary keys (see the sparse matrix example in Chapter 8). Some built-in operations may also require or imply tuples, not lists. As a rule of thumb, lists are the tool of choice for ordered collections that might need to change; tuples can handle the other cases of fixed associations.

			30.1.6.2  Files

You may already be familiar with the notion of files, which are named storage compartments on your computer that are managed by your operating system. The last major built-in object type that we'll examine on our in-depth tour provides a way to access those files inside Python programs.

In short, the built-in open function creates a Python file object, which serves as a link to a file residing on your machine. After calling open, you can read and write the associated external file by calling the returned file object's methods. The built-in name file is a synonym for open, and files may technically be opened by calling either open or file. open is generally preferred for opening, though, and file is mostly intended for customizations with object-oriented programming (described later in this book).

Compared to the types you've seen so far, file objects are somewhat unusual. They're not numbers, sequences, or mappings; instead, they export only methods for common file-processing tasks. Most file methods are concerned with performing input from and output to the external file associated with a file object, but other file methods allow us to seek to a new position in the file, flush output buffers, and so on.

				30.1.6.2.1  Opening Files

Table 9-2 summarizes common file operations. To open a file, a program calls the built-in open function, with the external name first, followed by a processing mode. The mode is typically the string 'r' to open for input (the default), 'w' to create and open for output, or 'a' to open for appending to the end; there are other options, but we'll omit them here. For advanced roles, adding a b to the end of the mode string allows for binary data (end-of-line translations are turned off), and adding a + means the file is opened for both input and output (i.e., we can both read and write to the same object).

Table 9-2. Common file operations
Operation 	Interpretation
output = open('/tmp/spam', 'w') 	Create output file ('w' means write)
input = open('data', 'r') 	Create input file ('r' means read)
input = open('data') 	Same as prior line ('r' is the default)
aString = input.read( ) 	Read entire file into a single string
aString = input.read(N) 	Read next N bytes (one or more) into a string
aString = input.readline( ) 	Read next line (including end-of-line marker) into a string
aList = input.readlines( ) 	Read entire file into list of line strings
output.write(aString) 	Write a string of bytes into file
output.writelines(aList) 	Write all line strings in a list into file
output.close( ) 	Manual close (done for you when file is collected)
output.flush( ) 	Flush output buffer to disk without closing
anyFile.seek(N) 	Change file position to offset N for next operation

Both arguments to open must be Python strings, and an optional third argument can be used to control output bufferingpassing a zero means that output is unbuffered (it is transferred to the external file immediately on a write method call). The external filename argument may include a platform-specific and absolute or relative directory path prefix; without a directory path, the file is assumed to exist in the current working directory (i.e., where the script runs).

Author Note: Using os and posix it's possible to emulate shell command line sessions. The following is an example the equivalent of running ls , and then cat <file>:

>>> dir (os)
	>>> ['EX_CANTCREAT', 'EX_CONFIG', 'EX_DATAERR', 'EX_IOERR', 'EX_NOHOST', 'EX_NOINPUT', 'EX_NOPERM', 'EX_NOUSER', 'EX_OK', 'EX_OSERR', 'EX_OSFILE', 'EX_PROTOCOL', 'EX_SOFTWARE', 'EX_TEMPFAIL', 'EX_UNAVAILABLE', 'EX_USAGE', 'F_OK', 'NGROUPS_MAX', 'O_APPEND', 'O_BINARY', 'O_CREAT', 'O_DIRECT', 'O_DSYNC', 'O_EXCL', 'O_NDELAY', 'O_NOCTTY', 'O_NOFOLLOW', 'O_NOINHERIT', 'O_NONBLOCK', 'O_RDONLY', 'O_RDWR', 'O_RSYNC', 'O_SYNC', 'O_TEXT', 'O_TRUNC', 'O_WRONLY', 'P_NOWAIT', 'P_NOWAITO', 'P_WAIT', 'R_OK', 'SEEK_CUR', 'SEEK_END', 'SEEK_SET', 'TMP_MAX', 'UserDict', 'WCOREDUMP', 'WEXITSTATUS', 'WIFEXITED', 'WIFSIGNALED', 'WIFSTOPPED', 'WNOHANG', 'WSTOPSIG', 'WTERMSIG', 'WUNTRACED', 'W_OK', 'X_OK', '_Environ', '__all__', '__builtins__', '__doc__', '__file__', '__name__', '_copy_reg', '_execvpe', '_exists', '_exit', '_get_exports_list', '_make_stat_result', '_make_statvfs_result', '_pickle_stat_result', '_pickle_statvfs_result', '_spawnvef', 'abort', 'access', 'altsep', 'chdir', 'chmod', 'chown', 'chroot', 'close', 'ctermid', 'curdir', 'defpath', 'devnull', 'dup', 'dup2', 'environ', 'errno', 'error', 'execl', 'execle', 'execlp', 'execlpe', 'execv', 'execve', 'execvp', 'execvpe', 'extsep', 'fchdir', 'fdatasync', 'fdopen', 'fork', 'forkpty', 'fpathconf', 'fstat', 'fstatvfs', 'fsync', 'ftruncate', 'getcwd', 'getcwdu', 'getegid', 'getenv', 'geteuid', 'getgid', 'getgroups', 'getlogin', 'getpgid', 'getpgrp', 'getpid', 'getppid', 'getsid', 'getuid', 'isatty', 'kill', 'killpg', 'lchown', 'linesep', 'link', 'listdir', 'lseek', 'lstat', 'major', 'makedev', 'makedirs', 'minor', 'mkdir', 'mkfifo', 'mknod', 'name', 'nice', 'open', 'openpty', 'pardir', 'path', 'pathconf', 'pathconf_names', 'pathsep', 'pipe', 'popen', 'popen2', 'popen3', 'popen4', 'putenv', 'read', 'readlink', 'remove', 'removedirs', 'rename', 'renames', 'rmdir', 'sep', 'setegid', 'seteuid', 'setgid', 'setgroups', 'setpgid', 'setpgrp', 'setregid', 'setreuid', 'setsid', 'setuid', 'spawnl', 'spawnle', 'spawnlp', 'spawnlpe', 'spawnv', 'spawnve', 'spawnvp', 'spawnvpe', 'stat', 'stat_float_times', 'stat_result', 'statvfs', 'statvfs_result', 'strerror', 'symlink', 'sys', 'sysconf', 'sysconf_names', 'system', 'tcgetpgrp', 'tcsetpgrp', 'tempnam', 'times', 'tmpfile', 'tmpnam', 'ttyname', 'umask', 'uname', 'unlink', 'unsetenv', 'urandom', 'utime', 'wait', 'wait3', 'wait4', 'waitpid', 'walk', 'write']
>>> dir (os.sys)
>>> ['__displayhook__', '__doc__', '__excepthook__', '__name__', '__stderr__', '__stdin__', '__stdout__', '_current_frames', '_getframe', 'api_version', 'argv', 'builtin_module_names', 'byteorder', 'call_tracing', 'callstats', 'copyright', 'displayhook', 'exc_clear', 'exc_info', 'exc_type', 'excepthook', 'exec_prefix', 'executable', 'exit', 'getcheckinterval', 'getdefaultencoding', 'getdlopenflags', 'getfilesystemencoding', 'getrecursionlimit', 'getrefcount', 'hexversion', 'last_traceback', 'last_type', 'last_value', 'maxint', 'maxunicode', 'meta_path', 'modules', 'path', 'path_hooks', 'path_importer_cache', 'platform', 'prefix', 'ps1', 'ps2', 'setcheckinterval', 'setdlopenflags', 'setprofile', 'setrecursionlimit', 'settrace', 'stderr', 'stdin', 'stdout', 'subversion', 'version', 'version_info', 'warnoptions']
>>> os.curdir  
>>> '.'
>>> import posix
>>> posix.listdir(os.curdir)
								 >>> >>> >>>   >>> >>> >>> ['!', '.aliases', '.aliases.bak', '.aliases~', '.bashrc', '.bashrc_old', '.bash_aliases', '.bash_history', '.borland', '.c++test_geometry', '.c++test_properties', '.c++test_recent_files', '.dbvis', '.idlerc', '.journal', '.klocwork', '.klocwork_old', '.mc', '.nedit', '.profile', '.qqlrzsc-oph', '.recently-used.xbel', '.sn', '.ssh', '.SunDownloadManager', '.vim', '.viminfo', '.viminfo.tmp', '.vimrc', '.vimrc.bak', '.vimrc.swo', '.vimrc~', '.w3m', '.Xauthority', '1', '1~', 'add', 'Application Data', 'browser.vimrc', 'C++TestFiles', 'Cookies', 'cscope.files', 'DESC.SDI', 'Desktop', 'dominionsKB_back', 'Favorites', 'FILE_ID.DIZ', 'freescope.out', 'ftp%3a%2f%2fftp.chg.ru%2fpub%2fsourceware%2fcygwin', 'ftp%3a%2f%2fftp.chg.ru%2fpub%2fsourceware%2fcygwin%2f', 'ftp%3a%2f%2fftp.eq.uc.pt%2fpub%2fsoftware%2fpc%2fprog%2fcygwin', 'ftp%3a%2f%2fftp.fit.vutbr.cz%2fpub%2fsystems%2fcygwin', 'ftp%3a%2f%2fftp.mipt.ru%2fmirror%2fcygwin%2f', 'ftp%3a%2f%2fftp.ntu.edu.tw%2fcygwin%2f', 'ftp%3a%2f%2fftp.sunet.se%2fpub%2flang%2fcygwin', 'ftp%3a%2f%2fftp.sunet.se%2fpub%2flang%2fcygwin%2f', 'ftp%3a%2f%2fmirror.aarnet.edu.au%2fpub%2fsourceware%2fcygwin', 'ftp%3a%2f%2fmirror.switch.ch%2fmirror%2fcygwin', 'ftp%3a%2f%2fmirrors.tuke.sk%2fcygwin', 'http%3a%2f%2fmirror.cs.vt.edu%2fpub%2fcygwin%2fcygwin%2f', 'http%3a%2f%2fsourceware.mirrors.tds.net%2fpub%2fsourceware.org%2fcygwin', 'INSTALL.EXE', 'kbe_session', 'kb_ed', 'kb_ed_back', 'KB_perl~', 'Klocwork', 'Local Settings', 'model', 'My Documents', 'my_startx', 'NetHood', 'ntuser.dat', 'ntuser.dat.LOG', 'ntuser.ini', 'ntuser.pol', 'presets.ini', 'PrintHood', 'public_html.tar', 'README.TXT', 'READ_ME.1ST', 'READ_ME.BAT', 'Recent', 'SametimeTranscripts', 'SendTo', 'stam', 'stam1', 'stam2', 'stam_session', 'Start Menu', 'temp', 'Templates', 'tmp_file', 'UserData', 'VENDINFO.DIZ', 'VENDOR.DOC', 'VENDOR.TXT', 'WINDOWS', 'workspace', 'xterm.exe.stackdump', 'XWin.exe.stackdump', '_startx', '_viminfo', '_vimperatorrc', '~']
>>> inp = open(os.curdir+'/.aliases','r')
>>> inp
>>> <open file './.aliases', mode 'r' at 0x7fee0e30>
>>> print inp
<open file './.aliases', mode 'r' at 0x7fee0e30>
>>> for lint in inp: print lint,
File content


				30.1.6.2.2 Using Files

Once you have a file object, you can call its methods to read from or write to the associated external file. In all cases, file text takes the form of strings in Python programs; reading a file returns its text in strings, and text is passed to the write methods as strings. Reading and writing methods come in multiple flavors; Table 9-2 lists the most common.

Though the reading and writing methods in the table are common, keep in mind that probably the best way to read lines from a text file today is not to read the file at allas we'll see in Chapter 13, files also have an iterator that automatically reads one line at a time in the context of a for loop, list comprehension, or other iteration context.

Notice that data read from a file always comes back to your script as a string, so you'll have to convert it to a different type of Python object if a string is not what you need. Similarly, unlike with the print statement, Python does not convert objects to strings automatically when you write data to a fileyou must send an already formatted string. Therefore, the tools we have already met to convert strings to and from numbers come in handy when dealing with files (int, float, str, and string formatting expressions, for instance). Python also includes advanced standard library tools for handling generic object storage (such as the pickle module) and for dealing with packed binary data in files (such as the struct module). We'll see both of these at work later in this chapter.

Calling the file close method terminates your connection to the external file. As discussed in Chapter 6, in Python, an object's memory space is automatically reclaimed as soon as the object is no longer referenced anywhere in the program. When file objects are reclaimed, Python also automatically closes the files if needed. This means you don't always need to manually close your files, especially in simple scripts that don't run for long. On the other hand, manual close calls can't hurt, and are usually a good idea in larger systems. Also, strictly speaking, this auto-close-on-collection feature of files is not part of the language definition, and it may change over time. Consequently, manually issuing file close method calls is a good habit to form.


				30.1.6.2.3 Files in Action

Let's work through a simple example that demonstrates file-processing basics. It first opens a new file for output, writes a string (terminated with a newline marker, \n), and closes the file. Later, the example opens the same file again in input mode and reads the line back. Notice that the second readline call returns an empty string; this is how Python file methods tell you that you've reached the end of the file (empty lines in the file come back as strings containing just a newline character, not as empty strings). Here's the complete interaction:


>>> myfile = open('myfile', 'w')  # Open for output (creates file)
>>> myfile.write('hello text file\n')  # Write a line of text
>>> myfile.close(  )  # Flush output buffers to disk

>>> myfile = open('myfile')  # Open for input: 'r' is default
>>> myfile.readline(  )  # Read the line back
'hello text file\n'
>>> myfile.readline(  )  # Empty string: end of file
''


This example writes a single line of text as a string, including its end-of-line terminator, \n; write methods don't add the end-of-line character for us, so we must include it to properly terminate our line (otherwise the next write will simply extend the current line in the file).

					30.1.6.2.3.1 Storing and parsing Python objects in files

Now, let's create a bigger file. The next example writes a variety of Python objects into a text file on multiple lines. Notice that it must convert objects to strings using conversion tools. Again, file data is always strings in our scripts, and write methods do not do any automatic to-string formatting for us:


>>> X, Y, Z = 43, 44, 45  # Native Python objects
>>> S = 'Spam'  # Must be strings to store in file
>>> D = {'a': 1, 'b': 2} 
>>> L = [1, 2, 3] 
>>>
>>> F = open('datafile.txt', 'w')  # Create output file
>>> F.write(S + '\n')  # Terminate lines with \n
>>> F.write('%s,%s,%s\n' % (X, Y, Z))  # Convert numbers to strings
>>> F.write(str(L) + '$' + str(D) + '\n')  # Convert and separate with $
>>> F.close(  ) 


Once we have created our file, we can inspect its contents by opening it and reading it into a string (a single operation). Notice that the interactive echo gives the exact byte contents, while the print statement interprets embedded end-of-line characters to render a more user-friendly display:


>>> bytes = open('datafile.txt').read(  )  # Raw bytes display
>>> bytes 
"Spam\n43,44,45\n[1, 2, 3]${'a': 1, 'b': 2}\n"
>>> print bytes  # User-friendly display
Spam
43,44,45
[1, 2, 3]${'a': 1, 'b': 2}


We now have to use other conversion tools to translate from the strings in the text file to real Python objects. As Python never converts strings to numbers, or other types of objects automatically, this is required if we need to gain access to normal object tools like indexing, addition, and so on:


>>> F = open('datafile.txt')  # Open again
>>> line = F.readline(  )  # Read one line
>>> line 
'Spam\n'
>>> line.rstrip(  )  # Remove end-of-line
'Spam'


For this first line, we used the string rstrip method to get rid of the trailing end-of-line character; a line[:-1] slice would work, too, but only if we can be sure all lines have the \n character (the last line in a file sometimes does not). So far, we've read the line containing the string. Now, let's grab the next line, which contains numbers, and parse out (that is, extract) the objects on that line:


>>> line = F.readline(  )  # Next line from file
>>> line  # It's a string here
'43,44,45\n'
>>> parts = line.split(',')  # Split (parse) on commas
>>> parts 
['43', '44', '45\n']


We used the string split method here to chop up the line on its comma delimiters; the result is a list of substrings containing the individual numbers. We still must convert from strings to integers, though, if we wish to perform math on these:


>>> int(parts[1])  # Convert from string to int
44
>>> numbers = [int(P) for P in parts]  # Convert all in list at once
>>> numbers 
[43, 44, 45]


As we have learned, int translates a string of digits into an integer object, and the list comprehension expression introduced in Chapter 4 can apply the call to each item in our list all at once (you'll find more on list comprehensions later in this book). Notice that we didn't have to run rstrip to delete the \n at the end of the last part; int and some other converters quietly ignore whitespace around digits.

Finally, to convert the stored list and dictionary in the third line of the file, we can run them through eval, a built-in function that treats a string as a piece of executable program code (technically, a string containing a Python expression):


>>> line = F.readline(  ) 
>>> line 
"[1, 2, 3]${'a': 1, 'b': 2}\n"
>>> parts = line.split('$')  # Split (parse) on $
>>> parts 
['[1, 2, 3]', "{'a': 1, 'b': 2}\n"]
>>> eval(parts[0])  # Convert to any object type
[1, 2, 3]
>>> objects = [eval(P) for P in parts]  # Do same for all in list
>>> objects 
[[1, 2, 3], {'a': 1, 'b': 2}]


Because the end result of all this parsing and converting is a list of normal Python objects instead of strings, we can now apply list and dictionary operations to them in our script.


					30.1.6.2.3.2 Storing native Python objects with pickle

Using eval to convert from strings to objects, as demonstrated in the preceding code, is a powerful tool. In fact, sometimes it's too powerful. eval will happily run any Python expressioneven one that might delete all the files on your computer, given the necessary permissions! If you really want to store native Python objects, but you can't trust the source of the data in the file, Python's standard library pickle module is ideal.

The pickle module is an advanced tool that allows us to store almost any Python object in a file directly, with no to- or from-string conversion requirement on our part. It's like a super-general data formatting and parsing utility. To store a dictionary in a file, for instance, we pickle it directly:


>>> F = open('datafile.txt', 'w') 
>>> import pickle 
>>> pickle.dump(D, F)  # Pickle any object to file
>>> F.close(  ) 


Then, to get the dictionary back later, we simply use pickle again to recreate it:


>>> F = open('datafile.txt') 
>>> E = pickle.load(F)  # Load any object from file
>>> E 
{'a': 1, 'b': 2}


We get back an equivalent dictionary object, with no manual splitting or converting required. The pickle module performs what is known as object serializationconverting objects to and from strings of bytesbut requires very little work on our part. In fact, pickle internally translates our dictionary to a string form, though it's not much to look at (and it can be even more convoluted if we pickle in other modes):


>>> open('datafile.txt').read(  )
"(dp0\nS'a'\np1\nI1\nsS'b'\np2\nI2\ns."


Because pickle can reconstruct the object from this format, we don't have to deal with that ourselves. For more on the pickle module, see the Python standard library manual, or import pickle, and pass it to help interactively. While you're exploring, also take a look at the shelve module. shelve is a tool that uses pickle to store Python objects in an access-by-key filesystem, which is beyond our scope here.


id=__python_struct__
					30.1.6.2.3.3 Storing and parsing packed binary data in files

One other file-related note before we move on: some advanced applications also need to deal with packed binary data, created perhaps by a C language program. Python's standard library includes a tool to help in this domainthe struct module knows how to both compose and parse packed binary data. In a sense, this is another data-conversion tool that interprets strings in files as binary data.

To create a packed binary data file, for example, open it in 'wb' (write binary) mode, and pass struct a format string and some Python objects. The format string used here means pack as a 4-byte integer, a 4-character string, and a 2-byte integer, all in big-endian form (other format codes handle padding bytes, floating-point numbers, and more):


>>> F = open('data.bin', 'wb')  # Open binary output file
>>> import struct 
>>> bytes = struct.pack('>i4sh', 7, 'spam', 8)  # Make packed binary data
>>> bytes 
'\x00\x00\x00\x07spam\x00\x08'
>>> F.write(bytes)  # Write byte string
>>> F.close(  ) 


Python creates a binary data string, which we write out to the file normally (this one consists mostly of nonprintable characters printed in hexadecimal escapes). To parse the values out to normal Python objects, we simply read the string back, and unpack it using the same format string. Python extracts the values into normal Python objects (integers and a string):


>>> F = open('data.bin', 'rb') 
>>> data = F.read(  )  # Get packed binary data
>>> data 
'\x00\x00\x00\x07spam\x00\x08'
>>> values = struct.unpack('>i4sh', data)  # Convert to Python objects
>>> values 
(7, 'spam', 8)


Binary data files are advanced and somewhat low-level tools that we won't cover in more detail here; for more help, see the Python library manual, or import and pass struct to the help function interactively. Also note that the binary file-processing modes 'wb' and 'rb' can be used to process a simpler binary file such as an image or audio file as a whole without having to unpack its contents.


					30.1.6.2.3.4 Other File Tools

There are additional, more advanced file methods shown in Table 9-2, and even more that are not in the table. For instance, seek resets your current position in a file (the next read or write happens at that position), flush forces buffered output to be written out to disk (by default, files are always buffered), and so on.

The Python standard library manual and the reference books described in the Preface provide complete lists of file methods; for a quick look, run a dir or help call interactively, passing in the word file (the name of the file object type). For more file-processing examples, watch for the sidebar "Why You Will Care: File Scanners" in Chapter 13. It sketches common file-scanning loop code patterns with statements we have not covered enough yet to use here.

Also, note that although the open function and the file objects it returns are your main interface to external files in a Python script, there are additional file-like tools in the Python toolset. Also available are descriptor files (integer file handles that support lower-level tools such as file locking), found in the os module; sockets, pipes, and FIFO files (file-like objects used to synchronize processes or communicate over networks); access-by-key files known as "shelves" (used to store unaltered Python objects directly, by key); and more. See more advanced Python texts for additional information on these file-like tools.

			30.1.6.3  Type Categories Revisited

Now that we've seen all of Python's core built-in types in action, let's wrap up our object types tour by taking a look at some of the properties they share.

Table 9-3 classifies all the types we've seen according to the type categories introduced earlier. Objects share operations according to their category; for instance, strings, lists, and tuples all share sequence operations. Only mutable objects (lists and dictionaries) may be changed in-place; you cannot change numbers, strings, or tuples in-place. Files only export methods, so mutability doesn't really apply to themthey may be changed when written, but this isn't the same as Python type constraints.

Why You Will Care: Operator Overloading

Later, we'll see that objects we implement with classes can pick and choose from these categories arbitrarily. For instance, if you want to provide a new kind of specialized sequence object that is consistent with built-in sequences, code a class that overloads things like indexing and concatenation:


class MySequence:
     def __getitem__(self, index):
         # Called on self[index], others
     def __add__(self, other):
         # Called on self + other


and so on. You can also make the new object mutable or not by selectively implementing methods called for in-place change operations (e.g., __setitem__ is called on self[index]=value assignments). Although it's beyond this book's scope, it's also possible to implement new objects in an external language like C as C extension types. For these, you fill in C function pointer slots to choose between number, sequence, and mapping operation sets.

Table 9-3. Object classifications
Object type 	Category 	Mutable?
Numbers 	Numeric 	No
Strings 	Sequence 	No
Lists 	Sequence 	Yes
Dictionaries 	Mapping 	Yes
Tuples 	Sequence 	No
Files 	Extension 	N/A

	

			30.1.6.4  Object Flexibility

This part of the book introduced a number of compound object types (collections with components). In general:

    * Lists, dictionaries, and tuples can hold any kind of object.
    * Lists, dictionaries, and tuples can be arbitrarily nested.
    * Lists and dictionaries can dynamically grow and shrink.

Because they support arbitrary structures, Python's compound object types are good at representing complex information in programs. For example, values in dictionaries may be lists, which may contain tuples, which may contain dictionaries, and so on. The nesting can be as deep as needed to model the data to be processed.

Let's look at an example of nesting. The following interaction defines a tree of nested compound sequence objects, shown in Figure 9-1. To access its components, you may include as many index operations as required. Python evaluates the indexes from left to right, and fetches a reference to a more deeply nested object at each step. Figure 9-1 may be a pathologically complicated data structure, but it illustrates the syntax used to access nested objects in general:


|>>> L = ['abc', [(1, 2), ([3], 4)], 5]
|>>> L[1]
|[(1, 2), ([3], 4)]
|>>> L[1][1]
|([3], 4)
|>>> L[1][1][0]
|[3]
|>>> L[1][1][0][0]
|3

			30.1.6.5  References Versus Copies

Chapter 6 mentioned that assignments always store references to objects, not copies of those objects. In practice, this is usually what you want. Because assignments can generate multiple references to the same object, though, it's important to be aware that changing a mutable object in-place may affect other references to the same object elsewhere in your program. If you don't want such behavior, you'll need to tell Python to copy the object explicitly.

We studied this phenomenon in Chapter 6, but it can become more acute when larger objects come into play. For instance, the following example creates a list assigned to X, and another list assigned to L that embeds a reference back to list X. It also creates a dictionary D that contains another reference back to list X:


>>> X = [1, 2, 3] 
>>> L = ['a', X, 'b']  # Embed references to X's object
>>> D = {'x':X, 'y':2} 


At this point, there are three references to the first list created: from the name X, from inside the list assigned to L, and from inside the dictionary assigned to D. The situation is illustrated in Figure 9-2.

Figure 9-2. Shared object references: because the list referenced by variable X is also referenced from within the objects referenced by L and D, changing the shared list from X makes it look different from L and D, too.


Because lists are mutable, changing the shared list object from any of the three references also changes what the other two reference:


>>> X[1] = 'surprise'  # Changes all three references!
>>> L 
['a', [1, 'surprise', 3], 'b']
>>> D 
{'x': [1, 'surprise', 3], 'y': 2}


References are a higher-level analog of pointers in other languages. Although you can't grab hold of the reference itself, it's possible to store the same reference in more than one place (variables, lists, and so on). This is a featureyou can pass a large object around a program without generating expensive copies of it along the way. If you really do want copies, however, you can request them:

    * Slice expressions with empty limits (L[:]) copy sequences.
    * The dictionary copy method (D.copy( )) copies a dictionary.
    * Some built-in functions, such as list, make copies (list(L)).
    * The copy standard library module makes full copies.

For example, say you have a list and a dictionary, and you don't want their values to be changed through other variables:


>>> L = [1,2,3]
>>> D = {'a':1, 'b':2}


To prevent this, simply assign copies to the other variables, not references to the same objects:


>>> A = L[:]  # Instead of A = L (or list(L))
>>> B = D.copy(  )  # Instead of B = D


This way, changes made from the other variables will change the copies, not the originals:


>>> A[1] = 'Ni'
>>> B['c'] = 'spam'
>>>
>>> L, D
([1, 2, 3], {'a': 1, 'b': 2})
>>> A, B
([1, 'Ni', 3], {'a': 1, 'c': 'spam', 'b': 2})


In terms of our original example, you can avoid the reference side effects by slicing the original list instead of simply naming it:


>>> X = [1, 2, 3] 
>>> L = ['a', X[:], 'b']  # Embed copies of X's object
>>> D = {'x':X[:], 'y':2} 


This changes the picture in Table 9-2L and D will now point to different lists than X. The net effect is that changes made through X will impact only X, not L and D; similarly, changes to L or D will not impact X.

One note on copies: empty-limit slices and the dictionary copy method still only make top-level copies; that is, they do not copy nested data structures, if any are present. If you need a complete, fully independent copy of a deeply nested data structure, use the standard copy module: include an import copy statement, and say X = copy.deepcopy(Y) to fully copy an arbitrarily nested object Y. This call recursively traverses objects to copy all their parts. This is the much more rare case, though (which is why you have to say more to make it go). References are usually what you will want; when they are not, slices and copy methods are usually as much copying as you'll need to do.


			30.1.6.6  Comparisons, Equality, and Truth

All Python objects also respond to comparisons: tests for equality, relative magnitude, and so on. Python comparisons always inspect all parts of compound objects until a result can be determined. In fact, when nested objects are present, Python automatically traverses data structures to apply comparisons recursively from left to right, and as deeply as needed. The first difference found along the way determines the comparison result.

For instance, a comparison of list objects compares all their components automatically:


>>> L1 = [1, ('a', 3)]  # Same value, unique objects
>>> L2 = [1, ('a', 3)] 
>>> L1 == L2, L1 is L2  # Equivalent? Same object?
(True, False)


Here, L1 and L2 are assigned lists that are equivalent but distinct objects. Because of the nature of Python references (studied in Chapter 6), there are two ways to test for equality:

    * The== operator tests value equivalence. Python performs an equivalence test, comparing all nested objects recursively.
    * The is operator tests object identity. Python tests whether the two are really the same object (i.e., live at the same address in memory).

In the preceding example, L1 and L2 pass the == test (they have equivalent values because all their components are equivalent), but fail the is check (they reference two different objects, and hence two different pieces of memory). Notice what happens for short strings, though:


>>> S1 = 'spam'
>>> S2 = 'spam'
>>> S1 == S2, S1 is S2
(True, True)


Here, we should again have two distinct objects that happen to have the same value: == should be true, and is should be false. But because Python internally caches and reuses short strings as an optimization, there really is just a single string 'spam' in memory, shared by S1 and S2; hence, the is identity test reports a true result. To trigger the normal behavior, we need to use longer strings:


>>> S1 = 'a longer string'
>>> S2 = 'a longer string'
>>> S1 == S2, S1 is S2
(True, False)


Of course, because strings are immutable, the object caching mechanism is irrelevant to your code-string can't be changed in-place, regardless of how many variables refer to them. If identity tests seem confusing, see Chapter 6 for a refresher on object reference concepts.

As a rule of thumb, the == operator is what you will want to use for almost all equality checks; is is reserved for highly specialized roles. We'll see cases where these operators are put to use later in the book.

Relative magnitude comparisons are also applied recursively to nested data structures:


>>> L1 = [1, ('a', 3)] 
>>> L2 = [1, ('a', 2)] 
>>> L1 < L2, L1 == L2, L1 > L2  # Less, equal, greater: tuple of results
(False, False, True)


Here, L1 is greater than L2 because the nested 3 is greater than 2. The result of the last line above is really a tuple of three objects-the results of the three expressions typed (an example of a tuple without its enclosing parentheses).

In general, Python compares types as follows:

    * Numbers are compared by relative magnitude.
    * Strings are compared lexicographically, character by character ("abc" < "ac").
    * Lists and tuples are compared by comparing each component from left to right.
    * Dictionaries are compared as though comparing sorted (key, value) lists.

In general, comparisons of structured objects proceed as though you had written the objects as literals and compared all their parts one at a time from left to right. In later chapters, we'll see other object types that can change the way they get compared.

				30.1.6.6.1  The Meaning of True and False in Python

Notice that the three values in the tuple returned in the last example of the prior section represent true and false values. They print as the words True and False, but now that we're using logical tests like this one in earnest, I should be a bit more formal about what these names really mean.

In Python, as in most programming languages, an integer 0 represents false, and an integer 1 represents true. In addition, though, Python recognizes any empty data structure as false, and any nonempty data structure as true. More generally, the notions of true and false are intrinsic properties of every object in Python-each object is either true or false, as follows:

    * Numbers are true if nonzero.
    * Other objects are true if nonempty.

Table 9-4 gives examples of true and false objects in Python.

| Table 9-4. Example object truth values
| Object 	Value
| "spam" 	True
| "" 	False
| [] 	False
| {} 	False
| 1 	True
| 0.0 	False
| None 	False

Python also provides a special object called None (the last item in Table 9-4), which is always considered to be false. None was introduced in Chapter 4; it is the only value of a special data type in Python, and typically serves as an empty placeholder, much like a NULL pointer in C.

For example, recall that for lists you cannot assign to an offset unless that offset already exists (the list does not magically grow if you make an out-of-bounds assignment). To preallocate a 100-item list such that you can add to any of the 100 offsets, you can fill one with None objects:


>>> L = [None] * 100
>>>
>>> L
[None, None, None, None, None, None, None, ... ]


The Python Boolean type bool (introduced in Chapter 5) simply augments the notion of true and false in Python. You may have noticed that the results of tests in this chapter display as the words True and False. As we learned in Chapter 5, these are just customized versions of the integers 1 and 0. Because of the way this new type is implemented, this is really just a minor extension to the notions of true and false already described, designed to make truth values more explicit. When used explicitly in truth tests, the words True and False evaluate to true and false because they truly are just specialized versions of the integers 1 and 0. Results of tests also print as the words True and False, instead of 1 and 0.

You are not required to use only Boolean types in logical statements such as if; all objects are still inherently true or false, and all the Boolean concepts mentioned in this chapter still work as described if you use other types. We'll explore Booleans further when we study logical statements in Chapter 12.

			30.1.6.7  Python's Type Hierarchies

Figure 9-3 summarizes all the built-in object types available in Python and their relationships. We've looked at the most prominent of these; most of the other kinds of objects in Figure 9-3  correspond to program units (e.g., functions and modules), or exposed interpreter internals (e.g., stack frames and compiled code).

Figure 9-3. Python's major built-in object types, organized by categories. Everything is a type of object in Python, even the type of an object! The type of any object is an object of type "type."


The main point to notice here is that everything is an object type in a Python system, and may be processed by your Python programs. For instance, you can pass a class to a function, assign it to a variable, stuff it in a list or dictionary, and so on.

In fact, even types themselves are an object type in Python: a call to the built-in function type(X) returns the type object of object X. Type objects can be used for manual type comparisons in Python if statements. However, for reasons introduced in Chapter 4, manual type testing is usually not the right thing to do in Python.

One note on type names: as of Python 2.2, each core type has a new built-in name added to support type customization through object-oriented subclassing: dict, list, str, tuple, int, long, float, complex, unicode, type, and file (file is a synonym for open). Calls to these names are really object constructor calls, not simply conversion functions, though you can treat them as simple functions for basic usage.

The types standard library module also provides additional type names (now largely synonyms for the built-in type names), and it is possible to do type tests with the isinstance function. For example, in Python 2.2 and later, all of the following type tests are true:


isinstance([1],list)
type([1])==list
type([1])==type([])
type([1])==types.ListType


Because types can be subclassed in Python today, the isinstance technique is generally recommended. See Chapter 26 for more on subclassing built-in types in Python 2.2 and later.

			30.1.6.8  Other Types in Python

Besides the core objects studied in this part of the book, a typical Python installation has dozens of other object types available as linked-in C extensions or Python classesregular expression objects, DBM files, GUI widgets, network sockets, and so on.

The main difference between these extra tools and the built-in types we've seen so far is that the built-ins provide special language creation syntax for their objects (e.g., 4 for an integer, [1,2] for a list, and the open function for files). Other tools are generally made available in standard library modules that you must first import to use. For instance, to make a regular expression object, you import re and call re.compile( ). See Python's library reference for a comprehensive guide to all the tools available to Python programs.

			30.1.6.9  Built-in Type Gotchas

That's the end of our look at core data types. We'll wrap up this part of the book with a discussion of common problems that seem to bite new users (and the occasional expert) along with their solutions. Some of this is a review of ideas we've already covered, but these issues are important enough to warn about again here.

				30.1.6.9.1  Assignment Creates References, Not Copies

Because this is such a central concept, I'll mention it again: you need to understand what's going on with shared references in your program. For instance, in the following example, the list object assigned to the name L is referenced from L and from inside the list assigned to the name M. Changing L in-place changes what M references, too:


>>> L = [1, 2, 3] 
>>> M = ['X', L, 'Y']  # Embed a reference to L
>>> M 
['X', [1, 2, 3], 'Y']

>>> L[1] = 0  # Changes M too
>>> M 
['X', [1, 0, 3], 'Y']


This effect usually becomes important only in larger programs, and shared references are often exactly what you want. If they're not, you can avoid sharing objects by copying them explicitly. For lists, you can always make a top-level copy by using an empty-limits slice:


>>> L = [1, 2, 3] 
>>> M = ['X', L[:], 'Y']  # Embed a copy of L
>>> L[1] = 0  # Changes only L, not M
>>> L 
[1, 0, 3]
>>> M 
['X', [1, 2, 3], 'Y']


Remember, slice limits default to 0, and the length of the sequence being sliced; if both are omitted, the slice extracts every item in the sequence, and so makes a top-level copy (a new, unshared object).

				30.1.6.9.2 Repetition Adds One Level Deep

Sequence repetition is like adding a sequence to itself a number of times. That's true, but when mutable sequences are nested, the effect might not always be what you expect. For instance, in the following example X is assigned to L repeated four times, whereas Y is assigned to a list containing L repeated four times:


>>> L = [4, 5, 6] 
>>> X = L * 4  # Like [4, 5, 6] + [4, 5, 6] + ...
>>> Y = [L] * 4  # [L] + [L] + ... = [L, L,...]

>>> X 
[4, 5, 6, 4, 5, 6, 4, 5, 6, 4, 5, 6]
>>> Y 
[[4, 5, 6], [4, 5, 6], [4, 5, 6], [4, 5, 6]]


Because L was nested in the second repetition, Y winds up embedding references back to the original list assigned to L, and so is open to the same sorts of side effects noted in the last section:


>>> L[1] = 0  # Impacts Y but not X
>>> X 
[4, 5, 6, 4, 5, 6, 4, 5, 6, 4, 5, 6]
>>> Y 
[[4, 0, 6], [4, 0, 6], [4, 0, 6], [4, 0, 6]]


The same solutions to this problem apply here as in the previous section, as this is really just another way to create the shared mutable object reference case. If you remember that repetition, concatenation, and slicing copy only the top level of their operand objects, these sorts of cases make much more sense.

				30.1.6.9.3 Beware of Cyclic Data Structures

We actually encountered this concept in a prior exercise: if a collection object contains a reference to itself, it's called a cyclic object. Python prints a [...] whenever it detects a cycle in the object, rather than getting stuck in an infinite loop:


>>> L = ['grail']  # Append reference to same object
>>> L.append(L)  # Generates cycle in object: [...]
>>> L 
['grail', [...]]


Besides understanding that the three dots in square brackets represent a cycle in the object, this case is worth knowing about because it can lead to gotchascyclic structures may cause code of your own to fall into unexpected loops if you don't anticipate them. For instance, some programs keep a list or dictionary of items already visited, and check it to determine whether they're in a cycle. See the solutions to "Section 3.14.1" in Chapter 3 for more on this problem, and check out the reloadall.py program at the end of Chapter 21 for a solution.

Don't use a cyclic reference unless you need to. There are good reasons to create cycles, but unless you have code that knows how to handle them, you probably won't want to make your objects reference themselves very often in practice.


				30.1.6.9.4 Immutable Types Can't Be Changed In-Place

Finally, you can't change an immutable object in-place. Instead, construct a new object with slicing, concatenation, and so on, and assign it back to the original reference, if needed:


T = (1, 2, 3)

T[2] = 4             # Error!

T = T[:2] + (4,)     # OK: (1, 2, 4)


That might seem like extra coding work, but the upside is that the previous gotchas can't happen when you're using immutable objects such as tuples and strings; because they can't be changed in-place, they are not open to the sorts of side effects that lists are.

			30.1.6.10 Chapter Summary

This chapter explored the last two major core object typesthe tuple and the file. We learned that tuples support all the usual sequence operations, but they have no methods, and do not allow any in-place changes because they are immutable. We also learned that files are returned by the built-in open function and provide methods for reading and writing data. We explored how to translate Python objects to and from strings for storing in files, and we looked at the pickle and struct modules for advanced roles (object serialization and binary data). Finally, we wrapped up by reviewing some properties common to all object types (e.g., shared references) and went through a list of common mistakes ("gotchas") in the object type domain.

In the next part, we'll shift gears, turning to the topic of statement syntax in Pythonwe'll explore all of Python's basic procedural statements in the chapters that follow. The next chapter kicks off that part of the book with an introduction to Python's general syntax model, which is applicable to all statement types. Before moving on, though, take the chapter quiz, and then work through the end-of-part lab exercises to review type concepts. Statements largely just create and process objects, so make sure you've mastered this domain by working through all the exercises before reading on.


Chapter Quiz

Q:	How can you determine how large a tuple is?
Q:	Write an expression that changes the first item in a tuple. (4, 5, 6) should become (1, 5, 6) in the process.
Q:	What is the default for the processing-mode argument in a file open call?
Q:	What module might you use to store Python objects in a file without converting them to strings yourself?
Q:	How might you go about copying all parts of a nested structure at once?
Q:	When does Python consider an object true?
Q:	What is your quest?


Quiz Answers

A:	The built-in len function returns the length (number of contained items) for any container object in Python, including tuples. It is a built-in function instead of a type method because it applies to many different types of objects.
A:	Because they are immutable, you can't really change tuples in-place, but you can generate a new tuple with the desired value. Given T = (4, 5, 6), you can change the first item by making a new tuple from its parts by slicing and concatenating: T = (1,) + T[1:]. (Recall that single-item tuples require a trailing comma.) You could also convert the tuple to a list, change it in-place, and convert it back to a tuple, but this is more expensive, and is rarely required in practicesimply use a list if you know that the object will require in-place changes.
A:	The default for the processing-mode argument in a file open call is 'r' for reading input. For input text files, simply pass in the external file's name.
A:	The pickle module can be used to store Python objects in a file without explicitly converting them to strings. The struct module is related, but assumes the data is to be in packed binary format in the file.
A:	Import the copy module, and call copy.deepcopy(X) if you need to copy all parts of a nested structure X. This is also rare to see in practice; references are usually the desired behavior, and shallow copies (e.g., aList[:], aDict.copy( )) usually suffice for most copies.
A:	An object is considered true if it is either a nonzero number, or a nonempty collection object. The built-in words True and False are essentially predefined to mean integer 1 and 0, respectively.
A:	Acceptable answers include "To learn Python," "To move on to the next part of the book," or "To seek the Holy Grail."

			30.1.6.11 BRAIN BUILDER


This session asks you to get your feet wet with built-in object fundamentals. As before, a few new ideas may pop up along the way, so be sure to flip to the answers in Appendix B when you're done (and even when you're not). If you have limited time, I suggest starting with exercises 10 and 11 (the most practical of the bunch), and then working from first to last as time allows. This is all fundamental material, though, so try to do as many of these as you can.

|    1.  The basics. Experiment interactively with the common type operations found in the tables in Part 2. To get started, bring up the Python interactive interpreter, type each of the following expressions, and try to explain what's happening in each case:
|       Code View: Scroll / Show All
| 
| 
|           2 ** 16
|           2 / 5, 2 / 5.0
| 
|           "spam" + "eggs"
|           S = "ham"
|           "eggs " + S
|           S * 5
|           S[:0]
|           "green %s and %s" % ("eggs", S)
| 
|           ('x',)[0]
|           ('x', 'y')[1]
| 
|           L = [1,2,3] + [4,5,6]
|           L, L[:], L[:0], L[-2], L[-2:]
|           ([1,2,3] + [4,5,6])[2:4]
|           [L[2], L[3]]
|           L.reverse(  ); L
|           L.sort(  ); L
|           L.index(4)
| 
|           {'a':1, 'b':2}['b']
|           D = {'x':1, 'y':2, 'z':3}
|           D['w'] = 0
|           D['x'] + D['w']
|           D[(1,2,3)] = 4
|           D.keys(  ), D.values(  ), D.has_key((1,2,3))
| 
|           [[]], ["",[],(  ),{},None]
| 
Answer:
| >>> 2/5, 2/5.0
| (0, 0.40000000000000002)
integer is auto floor, conversion to float

| >>> "spam"+"eggs"
| 'spameggs'
| >>> s= "ham"
| >>> "eggs"+s
| 'eggsham'
| >>> s*5
| 'hamhamhamhamham'
Concatenation

| >>> s[:0]
| ''
empty list slice

| >>> s[-1:0]
| ''
| >>> s[-1]
| 'm'
1 from end

| >>> "green %s and %s"%("eggs",s)
| 'green eggs and ham'
| >>> ('x',)[0]
| 'x'
| >>> ('x','y')[1]
| 'y'
| >>> L=[1,2,3]+[4,5,6]
| >>> L
| [1, 2, 3, 4, 5, 6]
| >>> L, L[:], L[:0], L[-2], L[-2:]
| ([1, 2, 3, 4, 5, 6], [1, 2, 3, 4, 5, 6], [], 5, [5, 6])
list, copy list using slice, empty list slice, 2 from end, slice 2 from end till end

| >>> ([1,2,3]+[4,5,6])[2:4]
| [3, 4]
| >>> (L[2],L[3])
| (3, 4)
| >>> L.reverse(); L
| [6, 5, 4, 3, 2, 1]
reverse and print list

| [1, 2, 3, 4, 5, 6]
| >>> L.sort(); L
| [1, 2, 3, 4, 5, 6]
| >>> L.index(4), L[4]
| (3, 5)
index of 4, element at position 4

| >>> {'a':1, 'b':2}['b']
| 2
| >>> D = {'x':1, 'y':2, 'z':3}
| >>> D
| {'y': 2, 'x': 1, 'z': 3}
| >>> D['w']= 0
| >>> D
| {'y': 2, 'x': 1, 'z': 3, 'w': 0}
| >>> D['x']+D['w']
| 1
| >>> D[(1,2,3)] = 4
| >>> D
| {'y': 2, 'x': 1, 'z': 3, 'w': 0, (1, 2, 3): 4}
| >>> D.keys(), D.values(), D.has_key((1,2,3))
| (['y', 'x', 'z', 'w', (1, 2, 3)], [2, 1, 3, 0, 4], True)
| >>> [[]], ["",[], (), {}, None]
| ([[]], ['', [], (), {}, None])
list containing empty list, list containing empty string, list, tupple, dictionary and None

|       					  
| 
| 
|    2.  Indexing and slicing. At the interactive prompt, define a list named L that contains four strings or numbers (e.g., L=[0,1,2,3]). Then, experiment with some boundary cases:
|          1.  What happens when you try to index out of bounds (e.g., L[4])?
|          2.  What about slicing out of bounds (e.g., L[-1000:100])?
|          3.  Finally, how does Python handle it if you try to extract a sequence in reverse, with the lower bound greater than the higher bound (e.g., L[3:1])? Hint: try assigning to this slice (L[3:1]=['?']), and see where the value is put. Do you think this may be the same phenomenon you saw when slicing out of bounds?

| Answer
| 1
| >>> L[6]
| Traceback (most recent call last):
|   File "<stdin>", line 1, in <module>
| IndexError: list index out of range
| 
| 2 
| >>> L[-10:10]
| [1, 2, 3, 4, 5, 6]
| 
| 3
| >>> L[3:1]
| []
| >>> L[3:1] = '?'
| >>> L
| [1, 2, 3, '?', 4, 5, 6]

|    3.  Indexing, slicing, anddel. Define another list L with four items, and assign an empty list to one of its offsets (e.g., L[2]=[]). What happens? Then, assign an empty list to a slice (L[2:3]=[]). What happens now? Recall that slice assignment deletes the slice, and inserts the new value where it used to be.
| 
|       The del statement deletes offsets, keys, attributes, and names. Use it on your list to delete an item (e.g., del L[0]). What happens if you delete an entire slice (del L[1:])? What happens when you assign a nonsequence to a slice (L[1:2]=1)?

Answer:
>>> L 
[1, 2, 3, 4, 5, 6]
>>> L[6]
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
IndexError: list index out of range
>>> L[-10:10]
[1, 2, 3, 4, 5, 6]
Python scales slices according to list size

>>> L[3:1]
[]
Again, the 1 upper limit is scaled back to 3 since python always slices from left to right.
Note using the third argument it's possible to change that - like L[3:1:-1]

>>> L[3:1] = '?'
>>> L
[1, 2, 3, '?', 4, 5, 6]
>>> L[2] = []
>>> L
[1, 2, [], '?', 4, 5, 6]
>>> L[2:4]= []
>>> L
[1, 2, 4, 5, 6]
>>> del L[0]
>>> L
[2, 4, 5, 6]
>>> del L[1:3]
>>> L
[2, 6]
>>> del L[1:] 
>>> L
[2]
>>> del L[0:] 
>>> L
[]
>>> L[:]=1
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: can only assign an iterable

|    4.  Tuple assignment. Type the following lines:
| 
| 
|           >>> X = 'spam'
|           >>> Y = 'eggs'
|           >>> X, Y = Y, X
| 
| 
|       What do you think is happening to X and Y when you type this sequence?

Answer
>>> x = 'spam'
>>> y = 'eggs'
>>> x,y = y, x
>>> x,y
('eggs', 'spam')

x,y are references. the references change after the assignment

|    5.  Dictionary keys. Consider the following code fragments:
| 
| 
|           >>> D = {}
|           >>> D[1] = 'a'
|           >>> D[2] = 'b'
| 
| 
|       You've learned that dictionaries aren't accessed by offsets, so what's going on here? Does the following shed any light on the subject? (Hint: strings, integers, and tuples share which type category?)

Answer
>>> D= {}
>>> D[1]='a'
>>> D[2]='b'
>>> D
{1: 'a', 2: 'b'}

integers are valid dictionary keys since they are immutable (just like strings and tupples). In-fact using them as indexes is a good implementation for sparse data structures

| 
| 
|           >>> D[(1, 2, 3)] = 'c'
|           >>> D
|           {1: 'a', 2: 'b', (1, 2, 3): 'c'}
| 


| 
|    6.  |       Dictionary indexing. Create a dictionary named D with three entries, for keys 'a', 'b', and 'c'. What happens if you try to index a nonexistent key (D['d'])? What does Python do if you try to assign to a nonexistent key 'd' (e.g., D['d']='spam')? How does this compare to out-of-bounds assignments and references for lists? Does this sound like the rule for variable names?

Answer
>>> D[3]
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
KeyError: 3
>>> D[3] = 's'

referencing nonexistent key results in error. Assigning to non-existing key adds it. 
Since dictionaries are mapps and not sequences they have no boundaries to speak of.

|    7.  Generic operations. Run interactive tests to answer the following questions:
|          1.  What happens when you try to use the + operator on different/mixed types (e.g., string + list, list + tuple)?
Answer
>>> 2+'3'
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: unsupported operand type(s) for +: 'int' and 'str'

|          2.  Does + work when one of the operands is a dictionary?
Answer
>>> 2+D  
>>> Traceback (most recent call last):
>>>   File "<stdin>", line 1, in <module>
>>>   TypeError: unsupported operand type(s) for +: 'int' and 'dict'

|          3.  Does the append method work for both lists and strings? How about using the keys method on lists? (Hint: What does append assume about its subject object?)
Answer
>>> str= 'asdsada'
>>> str.append('xxx')
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
AttributeError: 'str' object has no attribute 'append'
>>> L.append(3)
>>> L
[3]

|          4.  Finally, what type of object do you get back when you slice or concatenate two lists or two strings?
Answer
lists and strings accordingly

|    8.  String indexing. Define a string S of four characters: S = "spam". Then type the following expression: S[0][0][0][0][0]. Any clue as to what's happening this time? (Hint: recall that a string is a collection of characters, but Python characters are one-character strings.) Does this indexing expression still work if you apply it to a list such as ['s', 'p', 'a', 'm']? Why?
Answer
>>> s = 'spam'
>>> s[0][0][0][0][0]
's'
>>> list(s)[0][0][0][0]
>>> 's'
>>>

In both cases first item is not a list so any indexing into it is ignored and it is returned.

|    9.  Immutable types. Define a string S of four characters again: S = "spam". Write an assignment that changes the string to "slam", using only slicing and concatenation. Could you perform the same operation using just indexing and concatenation? How about index assignment?

|   10.  Nesting. Write a data structure that represents your personal information: name (first, middle, last), age, job, address, email address, and phone number. You may build the data structure with any combination of built-in object types you like (lists, tuples, dictionaries, strings, numbers). Then, access the individual components of your data structures by indexing. Do some structures make more sense than others for this object?
Answer
>>> Record = {'name':('yosi', 'izaq'), 'age':33, 'job': 'senior software engineer'} 
>>> Record['name']
('yosi', 'izaq')
>>> Record.keys() 
['job', 'age', 'name']

|   11.  Files. Write a script that creates a new output file called myfile.txt, and writes the string "Hello file world!" into it. Then, write another script that opens myfile.txt and reads and prints its contents. Run your two scripts from the system command line. Does the new file show up in the directory where you ran your scripts? What if you add a different directory path to the filename passed to open? Note: file write methods do not add newline characters to your strings; add an explicit \n at the end of the string if you want to fully terminate the line in the file.
Answer
>>> myfile = open('myfile.txt',w)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'w' is not defined
>>> myfile = open('myfile.txt','w')
>>> myfile.write("hello world\n")
>>> myfile
<open file 'myfile.txt', mode 'w' at 0x7fee0260>
>>> myfile.close()
>>> myfile = open('myfile.txt','r')
>>> for l in myfile: print l,
... 
hello world

|   12.  Thedirfunction revisited. Try typing the following expressions at the interactive prompt. Starting with version 1.5, the dir function has been generalized to list all attributes of any Python object you're likely to be interested in. If you're using an earlier version than 1.5, the __methods__ scheme has the same effect. If you're using Python 2.2, dir is probably the only of these that will work.
| 
| 
|           [].__methods__         # 1.4 or 1.5
|           dir([])                    # 1.5 and later
| 
|           {}.__methods__         # Dictionary
|           dir({})
| 
Answer
>>> dir([])
['__add__', '__class__', '__contains__', '__delattr__', '__delitem__', '__delslice__', '__doc__', '__eq__', '__ge__', '__getattribute__', '__getitem__', '__getslice__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__setslice__', '__str__', 'append', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort']
>>> dir({})
['__class__', '__cmp__', '__contains__', '__delattr__', '__delitem__', '__doc__', '__eq__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__iter__', '__le__', '__len__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setitem__', '__str__', 'clear', 'copy', 'fromkeys', 'get', 'has_key', 'items', 'iteritems', 'iterkeys', 'itervalues', 'keys', 'pop', 'popitem', 'setdefault', 'update', 'values']
>>> dir(())
['__add__', '__class__', '__contains__', '__delattr__', '__doc__', '__eq__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__getslice__', '__gt__', '__hash__', '__init__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmul__', '__setattr__', '__str__']
>>> dir('')
['__add__', '__class__', '__contains__', '__delattr__', '__doc__', '__eq__', '__ge__', '__getattribute__', '__getitem__', '__getnewargs__', '__getslice__', '__gt__', '__hash__', '__init__', '__le__', '__len__', '__lt__', '__mod__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__rmod__', '__rmul__', '__setattr__', '__str__', 'capitalize', 'center', 'count', 'decode', 'encode', 'endswith', 'expandtabs', 'find', 'index', 'isalnum', 'isalpha', 'isdigit', 'islower', 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', 'partition', 'replace', 'rfind', 'rindex', 'rjust', 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith', 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']
>>> dir(1)
['__abs__', '__add__', '__and__', '__class__', '__cmp__', '__coerce__', '__delattr__', '__div__', '__divmod__', '__doc__', '__float__', '__floordiv__', '__getattribute__', '__getnewargs__', '__hash__', '__hex__', '__index__', '__init__', '__int__', '__invert__', '__long__', '__lshift__', '__mod__', '__mul__', '__neg__', '__new__', '__nonzero__', '__oct__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdiv__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rlshift__', '__rmod__', '__rmul__', '__ror__', '__rpow__', '__rrshift__', '__rshift__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__str__', '__sub__', '__truediv__', '__xor__']
>>> myfile = open('myfile.txt','r')
>>> dir(myfile)
['__class__', '__delattr__', '__doc__', '__enter__', '__exit__', '__getattribute__', '__hash__', '__init__', '__iter__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__str__', 'close', 'closed', 'encoding', 'fileno', 'flush', 'isatty', 'mode', 'name', 'newlines', 'next', 'read', 'readinto', 'readline', 'readlines', 'seek', 'softspace', 'tell', 'truncate', 'write', 'writelines', 'xreadlines']

Textbook answers:
|      # The basics. Here are the sorts of results you should get, along with a few comments about their meaning. Note that ; is used in a few of these to squeeze more than one statement onto a single line; the ; is a statement separator:
|      Code View: Scroll / Show All
|      
|      
|      # Numbers
|      >>> 2 ** 16  # 2 raised to the power 16
|      65536
|      >>> 2 / 5, 2 / 5.0  # Integer / truncates, float / doesn't
|      (0, 0.40000000000000002)
|      
|      # Strings
|      
|      >>> "spam" + "eggs"  # Concatenation
|      'spameggs'
|      >>> S = "ham" 
|      >>> "eggs " + S 
|      'eggs ham'
|      >>> S * 5  # Repetition
|      'hamhamhamhamham'
|      >>> S[:0]  # An empty slice at the front -- [0:0]
|      ''
|      
|      >>> "green %s and %s" % ("eggs", S)  # Formatting
|      'green eggs and ham'
|      
|      # Tuples
|      
|      >>> ('x',)[0]  # Indexing a single-item tuple
|      'x'
|      >>> ('x', 'y')[1]  # Indexing a 2-item tuple
|      'y'
|      
|      # Lists
|      
|      >>> L = [1,2,3] + [4,5,6]  # List operations
|      >>> L, L[:], L[:0], L[-2], L[-2:] 
|      ([1, 2, 3, 4, 5, 6], [1, 2, 3, 4, 5, 6], [], 5, [5, 6])
|      >>> ([1,2,3]+[4,5,6])[2:4] 
|      [3, 4]
|      >>> [L[2], L[3]]  # Fetch from offsets; store in a list
|      [3, 4]
|      >>> L.reverse(  ); L  # Method: reverse list in-place
|      [6, 5, 4, 3, 2, 1]
|      >>> L.sort(  ); L  # Method: sort list in-place
|      [1, 2, 3, 4, 5, 6]
|      >>> L.index(4)  # Method: offset of first 4 (search)
|      3
|      
|      # Dictionaries
|      
|      >>> {'a':1, 'b':2}['b']  # Index a dictionary by key
|      2
|      >>> D = {'x':1, 'y':2, 'z':3} 
|      >>> D['w'] = 0  # Create a new entry
|      >>> D['x'] + D['w'] 
|      1
|      >>> D[(1,2,3)] = 4  # A tuple used as a key (immutable)
|      >>> D 
|      {'w': 0, 'z': 3, 'y': 2, (1, 2, 3): 4, 'x': 1}
|      >>> D.keys(  ), D.values(  ), D.has_key((1,2,3))     # Methods
|      (['w', 'z', 'y', (1, 2, 3), 'x'], [0, 3, 2, 4, 1], 1)
|      
|      # Empties
|      
|      >>> [[]], ["",[],(  ),{},None]  # Lots of nothings: empty objects
|      ([[]], ['', [], (  ), {}, None])
|      
|      					  
|      
|      
|      #
|      
|      Indexing and slicing. Indexing out of bounds (e.g., L[4]) raises an error; Python always checks to make sure that all offsets are within the bounds of a sequence.
|      
|      On the other hand, slicing out of bounds (e.g., L[-1000:100]) works because Python scales out-of-bounds slices so that they always fit (the limits are set to zero and the sequence length, if required).
|      
|      Extracting a sequence in reversewith the lower bound greater than the higher bound (e.g., L[3:1])doesn't really work. You get back an empty slice ([ ]) because Python scales the slice limits to make sure that the lower bound is always less than or equal to the upper bound (e.g., L[3:1] is scaled to L[3:3], the empty insertion point at offset 3). Python slices are always extracted from left to right, even if you use negative indexes (they are first converted to positive indexes by adding the sequence length). Note that Python 2.3's three-limit slices modify this behavior somewhat: L[3:1:-1] does extract from right to left:
|      
|      
|      >>> L = [1, 2, 3, 4]
|      >>> L[4]
|      Traceback (innermost last):
|        File "<stdin>", line 1, in ?
|      IndexError: list index out of range
|      >>> L[-1000:100]
|      [1, 2, 3, 4]
|      >>> L[3:1]
|      []
|      >>> L
|      [1, 2, 3, 4]
|      >>> L[3:1] = ['?']
|      >>> L
|      [1, 2, 3, '?', 4]
|      
|      
|      #
|      
|      Indexing, slicing, anddel. Your interaction with the interpreter should look something like the following code. Note that assigning an empty list to an offset stores an empty list object there, but assigning an empty list to a slice deletes the slice. Slice assignment expects another sequence, or you'll get a type error; it inserts items inside the sequence assigned, not the sequence itself:
|      
|      
|      >>> L = [1,2,3,4]
|      >>> L[2] = []
|      >>> L
|      [1, 2, [], 4]
|      >>> L[2:3] = []
|      >>> L
|      [1, 2, 4]
|      >>> del L[0]
|      >>> L
|      [2, 4]
|      >>> del L[1:]
|      >>> L
|      [2]
|      >>> L[1:2] = 1
|      Traceback (innermost last):
|        File "<stdin>", line 1, in ?
|      TypeError: illegal argument type for built-in operation
|      
|      
|      #
|      
|      Tuple assignment. The values of X and Y are swapped. When tuples appear on the left and right of an assignment symbol (=), Python assigns objects on the right to targets on the left according to their positions. This is probably easiest to understand by noting that the targets on the left aren't a real tuple, even though they look like one; they are simply a set of independent assignment targets. The items on the right are a tuple, which gets unpacked during the assignment (the tuple provides the temporary assignment needed to achieve the swap effect):
|      
|      
|      >>> X = 'spam'
|      >>> Y = 'eggs'
|      >>> X, Y = Y, X
|      >>> X
|      'eggs'
|      >>> Y
|      'spam'
|      
|      
|      #
|      
|      Dictionary keys. Any immutable object can be used as a dictionary keyincluding integers, tuples, strings, and so on. This really is a dictionary, even though some of its keys look like integer offsets. Mixed-type keys work fine, too:
|      
|      
|      >>> D = {}
|      >>> D[1] = 'a'
|      >>> D[2] = 'b'
|      >>> D[(1, 2, 3)] = 'c'
|      >>> D
|      {1: 'a', 2: 'b', (1, 2, 3): 'c'}
|      
|      
|      #
|      
|      Dictionary indexing. Indexing a nonexistent key (D['d']) raises an error; assigning to a nonexistent key (D['d']='spam') creates a new dictionary entry. On the other hand, out-of-bounds indexing for lists raises an error too, but so do out-of-bounds assignments. Variable names work like dictionary keys; they must have already been assigned when referenced, but are created when first assigned. In fact, variable names can be processed as dictionary keys if you wish (they're made visible in module namespace or stack-frame dictionaries):
|      
|      
|      >>> D = {'a':1, 'b':2, 'c':3}
|      >>> D['a']
|      1
|      >>> D['d']
|      Traceback (innermost last):
|        File "<stdin>", line 1, in ?
|      KeyError: d
|      >>> D['d'] = 4
|      >>> D
|      {'b': 2, 'd': 4, 'a': 1, 'c': 3}
|      >>>
|      >>> L = [0, 1]
|      >>> L[2]
|      Traceback (innermost last):
|        File "<stdin>", line 1, in ?
|      IndexError: list index out of range
|      >>> L[2] = 3
|      Traceback (innermost last):
|        File "<stdin>", line 1, in ?
|      IndexError: list assignment index out of range
|      
|      
|      #
|      
|      Generic operations. Question answers:
|      
|          *
|      
|            The + operator doesn't work on different/mixed types (e.g., string + list, list + tuple).
|          *
|      
|            + doesn't work for dictionaries, as they aren't sequences.
|          *
|      
|            The append method works only for lists, not strings, and keys works only on dictionaries. append assumes its target is mutable, since it's an in-place extension; strings are immutable.
|          *
|      
|            Slicing and concatenation always return a new object of the same type as the objects processed.
|            Code View: Scroll / Show All
|      
|      
|            >>> "x" + 1
|            Traceback (innermost last):
|              File "<stdin>", line 1, in ?
|            TypeError: illegal argument type for built-in operation
|            >>>
|            >>> {} + {}
|            Traceback (innermost last):
|              File "<stdin>", line 1, in ?
|            TypeError: bad operand type(s) for +
|            >>>
|            >>> [].append(9)
|            >>> "".append('s')
|            Traceback (innermost last):
|              File "<stdin>", line 1, in ?
|            AttributeError: attribute-less object
|            >>>
|            >>> {}.keys(  )
|            []
|            >>> [].keys(  )
|            Traceback (innermost last):
|              File "<stdin>", line 1, in ?
|            AttributeError: keys
|            >>>
|            >>> [][:]
|            []
|            >>> ""[:]
|            ''
|      
|            					  
|      
|      
|      #
|      
|      String indexing. Because strings are collections of one-character strings, every time you index a string, you get back a string that can be indexed again. S[0][0][0][0][0] just keeps indexing the first character over and over. This generally doesn't work for lists (lists can hold arbitrary objects) unless the list contains strings:
|      
|      
|      >>> S = "spam"
|      >>> S[0][0][0][0][0]
|      's'
|      >>> L = ['s', 'p']
|      >>> L[0][0][0]
|      's'
|      
|      
|      #
|      
|      Immutable types. Either of the following solutions works. Index assignment doesn't, because strings are immutable:
|      
|      
|      >>> S = "spam"
|      >>> S = S[0] + 'l' + S[2:]
|      >>> S
|      'slam'
|      >>> S = S[0] + 'l' + S[2] + S[3]
|      >>> S
|      'slam'
|      
|      
|      #
|      
|      Nesting. Here is a sample:
|      
|      
|      >>> me = {'name':('mark', 'e', 'lutz'), 'age':'?', 'job':'engineer'}
|      >>> me['job']
|      'engineer'
|      >>> me['name'][2]
|      'lutz'
|      
|      
|      #
|      
|      Files. Here's one way to create and read back a text file in Python (ls is a Unix command; use dir on Windows):
|      
|      
|      # File: maker.py
|      file = open('myfile.txt', 'w')
|      file.write('Hello file world!\n')              # Or: open(  ).write(  )
|      
|      file.close(  )                                 # close not always needed
|      
|      # File: reader.py
|      file = open('myfile.txt')                 # 'r' is default open mode
|      print file.read()                         # Or print open(  ).read(  )
|      
|      % python maker.py 
|      % python reader.py 
|      Hello file world!
|      
|      % ls -l myfile.txt 
|      -rwxrwxrwa   1 0        0             19 Apr 13 16:33 myfile.txt
|      
|      
|      #
|      
|      Thedirfunction revisited. The following is what you get for lists; dictionaries do the same, with different method names. Note that the dir result expanded in Python 2.2you'll see a large set of additional underscore names that implement expression operators and support the subclassing in Part 6. The __methods__ attribute disappeared in 2.2 as well because it wasn't consistently implementeduse dir to fetch attribute lists today instead:
|      
|      
|      >>> [].__methods__
|      ['append', 'count', 'index', 'insert', 'remove', 'reverse', 'sort',...]
|      >>> dir([])
|      ['append', 'count', 'index', 'insert', 'remove', 'reverse', 'sort',...]


	30.2. Statements and Syntax

		30.2.1. Introducing Python Statements
Now that you're familiar with Python's core built-in object types, this chapter begins our exploration of its fundamental statement forms. As in the previous part, we'll begin here with a general introduction to statement syntax, and follow up with more details about specific statements in the next few chapters.

In simple terms, statements are the things you write to tell Python what your programs should do. If programs "do things with stuff," statements are the way you specify what sort of things a program does. Python is a procedural, statement-based language; by combining statements, you specify a procedure that Python performs to satisfy a program's goals.

		30.2.2.  Python Program Structure Revisited

Another way to understand the role of statements is to revisit the concept hierarchy introduced in Chapter 4, which talked about built-in objects and the expressions used to manipulate them. This chapter climbs the hierarchy to the next level:

   a.  Programs are composed of modules.
   b.  Modules contain statements.
   c.  Statements contain expressions.
   d.  Expressions create and process objects.

At its core, Python syntax is composed of statements and expressions. Expressions process objects and are embedded in statements. Statements code the larger logic of a program's operationthey use and direct expressions to process the objects we studied in the preceding chapters. Moreover, statements are where objects spring into existence (e.g., in expressions within assignment statements), and some statements create entirely new kinds of objects (functions, classes, and so on). Statements always exist in modules, which themselves are managed with statements.

		30.2.1.1 Python's Statements

Table 10-1 summarizes Python's statement set.[30] This part of the book deals with entries in the table from the top through break and continue. You've informally been introduced to a few of the statements in Table 10-1 already; this part of the book will fill in details that were skipped earlier, introduce the rest of Python's procedural statement set, and cover the overall syntax model. Statements lower in Table 10-1 that have to do with larger program unitsfunctions, classes, modules, and exceptionslead to larger programming ideas, so they will each have a section of their own. More exotic statements like exec (which compiles and executes code constructed as strings) are covered later in the book, or in Python standard documentation.

    [30] * Technically speaking, in Python 2.5, yield became an expression instead of a statement, and the try/except and try/finally statements were merged (the two were formerly separate statements, but we can now say both except and finally in the same try statement). Also, a new with/as statement is to be added in Python 2.6 to encode context managersroughly speaking, it's an alternative to try/finally exception-related operations (in 2.5, with/as is an optional extension, and is not available unless you explicitly turn it on by running the statement from __future__ import with_statement). See Python manuals for more details. Further in the future, in 3.0, print and exec will become function calls instead of statements, and a new nonlocal statement will have a purpose much like that of today's global.

Table 10-1. Python statements
Statement 	Role 	Example
Assignment 	Creating references 	

a, b, c = 'good', 'bad', 'ugly'


Calls 	Running functions 	

log.write("spam, ham\n")


print 	Printing objects 	

print 'The Killer', joke


if/elif/else 	Selecting actions 	

if "python" in text:
print text


for/else 	Sequence iteration 	

for x in mylist:
    print x


while/else 	General loops 	

while X > Y:
    print 'hello'


pass 	Empty placeholder 	

while True:
    pass


break, continue 	Loop jumps 	

while True:
    if not line: break


try/except/ finally 	Catching exceptions 	

try:
    action(  )
except:
    print 'action error'


raise 	Triggering exceptions 	

raise endSearch, location


import, from 	Module access 	

import sys
from sys import stdin


def, return, yield 	Building functions 	

def f(a, b, c=1, *d):
    return a+b+c+d[0]
def gen(n):
    for i in n, yield i*2


class 	Building objects 	

class subclass(Superclass):
    staticData = []


global 	Namespaces 	

def function(  ):
    global x, y
    x = 'new'


del 	Deleting references 	

del data[k]
del data[i:j]
del obj.attr
del variable


exec 	Running code strings 	

exec "import " + modName
exec code in gdict, ldict


assert 	Debugging checks 	

assert X > Y


with/as 	Context managers (2.6) 	

with open('data') as myfile:
    process(myfile)



		30.2.1.2 A Tale of Two ifs

Before we delve into the details of any of the concrete statements in Table 10-1, I want to begin our look at Python statement syntax by showing you what you are not going to type in Python code so you can compare and contrast it with other syntax models you might have seen in the past.

Consider the following if statement, coded in a C-like language:


if (x > y) {
    x = 1;
    y = 2;
}


This might be a statement in C, C++, Java, JavaScript, or Perl. Now, look at the equivalent statement in the Python language:


if x > y:
    x = 1
    y = 2


The first thing that may pop out at you is that the equivalent Python statement is less, well, clutteredthat is, there are fewer syntactic components. This is by design; as a scripting language, one of Python's goals is to make programmers' lives easier by requiring less typing.

More specifically, when you compare the two syntax models, you'll notice that Python adds one new thing to the mix, and that three items that are present in the C-like language are not present in Python code.

			30.2.1.2.1 What Python Adds

The one new syntax component in Python is the colon character (:). All Python compound statements (i.e., statements that have statements nested inside them) follow the same general pattern of a header line terminated in a colon followed by a nested block of code usually indented underneath the header line, like this:


Header line:
    Nested statement block


The colon is required, and omitting it is probably the most common coding mistake among new Python programmersit's certainly one I've witnessed thousands of times in Python training classes. In fact, if you are new to Python, you'll almost certainly forget the colon character very soon. Most Python-friendly editors make this mistake easy to spot, and including it eventually becomes an unconscious habit (so much so that you may start typing colons into your C++ code, too, generating many entertaining error messages from your C++ compiler!).

			30.2.1.2.2 What Python Removes

Although Python requires the extra colon character, there are three things programmers in C-like languages must include that you don't generally have to in Python.

				30.2.1.2.2.1 Parentheses are optional

The first of these is the set of parentheses around the tests at the top of the statement:


if (x < y)


The parentheses here are required by the syntax of many C-like languages. In Python, they are notwe simply omit the parentheses, and the statement works the same way:


if x < y


Technically speaking, because every expression can be enclosed in parentheses, including them will not hurt in this Python code, and they are not treated as an error if present. But don't do that: you'll be wearing out your keyboard needlessly, and broadcasting to the world that you're an ex-C programmer still learning Python (I was once, too). The Python way is to simply omit the parentheses in these kinds of statements altogether.

				30.2.1.2.2.2 End of line is end of statement

The second and more significant syntax component you won't find in Python code is the semicolon. You don't need to terminate statements with semicolons in Python the way you do in C-like languages:


x = 1;


In Python, the general rule is that the end of a line automatically terminates the statement that appears on that line. In other words, you can leave off the semicolons, and it works the same way:


x = 1


There are some ways to work around this rule, as you'll see in a moment. But, in general, you write one statement per line for the vast majority of Python code, and no semicolon is required.

Here, too, if you are pining for your C programming days (if such a state is possible . . . ), you can continue to use semicolons at the end of each statementthe language lets you get away with them if they are present. But don't do that either (really!); again, doing so tells the world that you're still a C programmer who hasn't quite made the switch to Python coding. The Pythonic style is to leave off the semicolons altogether.

				30.2.1.2.2.3 End of indentation is end of block

The third and final syntax component that Python removes, and perhaps the most unusual one to soon-to-be-ex-C programmers (until they use it for 10 minutes, and realize it's actually a feature), is that you do not type anything explicit in your code to syntactically mark the beginning and end of a nested block of code. You don't need to include begin/end, then/endif, or braces around the nested block, as you do in C-like languages:


if (x > y) {
    x = 1;
    y = 2;
}


Instead, in Python, we consistently indent all the statements in a given single nested block the same distance to the right, and Python uses the statements' physical indentation to determine where the block starts and stops:


if x > y:
    x = 1
    y = 2


By indentation, I mean the blank whitespace all the way to the left of the two nested statements here. Python doesn't care how you indent (you may use either spaces or tabs), or how much you indent (you may use any number of spaces or tabs). In fact, the indentation of one nested block can be totally different from that of another. The syntax rule is only that for a given single nested block, all of its statements must be indented the same distance to the right. If this is not the case, you will get a syntax error, and your code will not run until you repair its indentation to be consistent.

			30.2.1.2.3 Why Indentation Syntax?

The indentation rule may seem unusual at first glance to programmers accustomed to C-like languages, but it is a deliberate feature of Python, and one of the main ways that Python almost forces programmers to produce uniform, regular, and readable code. It essentially means that you must line up your code vertically, in columns, according to its logical structure. The net effect is to make your code more consistent and readable (unlike much of the code written in C-like languages).

To put that more strongly, aligning your code according to its logical structure is a major part of making it readable, and thus reusable and maintainable, by yourself and others. In fact, even if you never use Python after reading this book, you should get into the habit of aligning your code for readability in any block-structured language. Python forces the issue by making this a part of its syntax, but it's an important thing to do in any programming language, and it has a huge impact on the usefulness of your code.

Your mileage may vary, but when I was still doing development on a full-time basis, I was mostly paid to work on large old C++ programs that had been worked on by many programmers over the years. Almost invariably, each programmer had his or her own style for indenting code. For example, I'd often be asked to change a while loop coded in the C++ language that began like this:


while (x > 0) {


Before we even get into indentation, there are three or four ways that programmers can arrange these braces in a C-like language, and organizations often have political debates and write standards manuals to address the options (which seems more than a little off-topic for the problem to be solved by programming). Ignoring that, here's the scenario I often encountered in C++ code. The first person who worked on the code indented the loop four spaces:


while (x > 0) {
    --------;
    --------;


That person eventually moved on to management, only to be replaced by someone who liked to indent further to the right:


while (x > 0) {
    --------;
    --------;
           --------;
           --------;


That person later moved on to other opportunities, and someone else picked up the code who liked to indent less:


while (x > 0) {
    --------;
    --------;
           --------;
           --------;
--------;
--------;
}


And so on. Eventually, the block is terminated by a closing brace (}), which of course makes this block-structured code (he says, sarcastically). In any block-structured language, Python or otherwise, if nested blocks are not indented consistently, they become very difficult for the reader to interpret, change, or reuse. Readability matters, and indentation is a major component of readability.

Here is another example that may have burned you in the past if you've done much programming in a C-like language. Consider the following statement in C:

if (x)
     if (y)
          statement1;
else
     statement2;


Which if does the else here go with? Surprisingly, the else is paired with the nested if statement (if (y)), even though it looks visually as though it is associated with the outer if (x). This is a classic pitfall in the C language, and it can lead to the reader completely misinterpreting the code, and changing it incorrectly in ways that might not be uncovered until the Mars rover crashes into a giant rock!

This cannot happen in Pythonbecause indentation is significant, the way the code looks is the way it will work. Consider an equivalent Python statement:


if x:
     if y:
          statement1
else:
     statement2


In this example, the if that the else lines up with vertically is the one it is associated with logically (the outer if x). In a sense, Python is a WYSIWYG languagewhat you see is what you get because the way code looks is the way it runs, regardless of who coded it.

If this still isn't enough to underscore the benefits of Python's syntax, here's another anecdote. Early in my career, I worked at a successful company that developed systems software in the C language, where consistent indentation is not required. Even so, when we checked our code into source control at the end of the day, this company ran an automated script that analyzed the indentation used in the code. If the script noticed that we'd indented our code inconsistently, we received an automated email about it the next morningand so did our bosses!

My point is that even when a language doesn't require it, good programmers know that consistent use of indentation has a huge impact on code readability and quality. The fact that Python promotes this to the level of syntax is seen by most as a feature of the language.

Finally, keep in mind that nearly every programmer-friendly text editor in use today has built-in support for Python's syntax model. In the IDLE Python GUI, for example, lines of code are automatically indented when you are typing a nested block; pressing the Backspace key backs up one level of indentation, and you can customize how far to the right IDLE indents statements in a nested block.

There is no absolute standard for how to indent: four spaces or one tab per level is common, but it's up to you to decide how and how much you wish to indent. Indent further to the right for further nested blocks, and less to close the prior block. Moreover, generating tabs instead of braces is no more difficult in practice for tools that must output Python code. In general, do what you should be doing in a C-like language, anyhow, but get rid of the braces, and your code will satisfy Python's syntax rules.

			30.2.1.2.4 A Few Special Cases

As mentioned previously, in Python's syntax model:

    * The end of a line terminates the statement on that line (without semicolons).
    * Nested statements are blocked and associated by their physical indentation (without braces).

Those rules cover almost all Python code you'll write or see in practice. However, Python also provides some special-purpose rules that allow customization of both statements and nested statement blocks.

				30.2.1.2.4.1 Statement rule special cases

Although statements normally appear one per line, it is possible to squeeze more than one statement onto a single line in Python by separating them with semicolons:


a = 1; b = 2; print a + b                # Three statements on one line


This is the only place in Python where semicolons are required: as statement separators. This only works, though, if the statements thus combined are not themselves compound statements. In other words, you can chain together only simple statements, like assignments, prints, and function calls. Compound statements must still appear on lines of their own (otherwise, you could squeeze an entire program onto one line, which probably would not make you very popular among your coworkers!).

The other special rule for statements is essentially the inverse: you can make a single statement span across multiple lines. To make this work, you simply have to enclose part of your statement in a bracketed pairparentheses (( )), square brackets ([]), or dictionary braces ({}). Any code enclosed in these constructs can cross multiple lines: your statement doesn't end until Python reaches the line containing the closing part of the pair. For instance, to continue a list literal:


mlist = [111,
         222,
         333]


Because the code is enclosed in a square brackets pair, Python simply drops down to the next line until it encounters the closing bracket. Dictionaries can also span lines this way, and parentheses handle tuples, function calls, and expressions. The indentation of the continuation lines does not matter, though common sense dictates that the lines should be aligned somehow for readability.

Parentheses are the catchall devicebecause any expression can be wrapped up in them, simply inserting a left parenthesis allows you to drop down to the next line and continue your statement:


X = (A + B +
     C + D)


This technique works with compound statements, too, by the way. Anywhere you need to code a large expression, simply wrap it in parentheses to continue it on the next line:


if (A == 1 and
    B == 2 and
    C == 3):
        print 'spam' * 3


An older rule also allows for continuation lines when the prior line ends in a backslash:


X = A + B + \
      C + D


But this alternative technique is dated, and somewhat frowned on today because it's difficult to notice and maintain the backslashes, and it's fairly brittle (there can be no spaces after the backslash). It's also another throwback to the C language, where it is commonly used in "#define" macros; again, when in Pythonland, do as Pythonistas do, not as C programmers do.

				30.2.1.2.4.2 Block rule special case

As mentioned previously, statements in a nested block of code are normally associated by being indented the same amount to the right. As one special case here, the body of a compound statement can instead appear on the same line as the header in Python, after the colon:


if x > y: print x


This allows us to code single-line if statements, single-line loops, and so on. Here again, though, this will work only if the body of the compound statement itself does not contain any compound statements. That is, only simple statementsassignments, prints, function calls, and the likeare allowed after the colon. Larger statements must still appear on lines by themselves. Extra parts of compound statements (such as the else part of an if, which we'll meet later) must also be on separate lines of their own. The body can consist of multiple simple statements separated by semicolons, but this tends to be frowned on.

In general, even though it's not always required, if you keep all your statements on individual lines, and always indent your nested blocks, your code will be easier to read and change in the future. To see a prime and common exception to one of these rules in action, however (the use of a single-line if statement to break out of a loop), let's move on to the next section and write some real code.

		30.2.1.3 A Quick Example: Interactive Loops

We'll see all these syntax rules in action when we tour Python's specific compound statements in the next few chapters, but they work the same everywhere in the Python language. To get started, let's work through a brief, realistic example that demonstrates the way that statement syntax and statement nesting come together in practice, and introduces a few statements along the way.

			30.2.1.3.1 A Simple Interactive Loop

Suppose you're asked to write a Python program that interacts with a user in a console window. Maybe you're accepting inputs to send to a database, or reading numbers to be used in a calculation. Regardless of the purpose, you need to code a loop that reads one or more inputs from a user typing on a keyboard, and prints back a result for each. In other words, you need to write a classic read/evaluate/print loop program.

In Python, typical boilerplate code for such an interactive loop might look like this:


while True:
    reply = raw_input('Enter text:')
    if reply == 'stop': break
    print reply.upper(  )


This code makes use of a few new ideas:

    * The code leverages the Python while loop, Python's most general looping statement. We'll study the while statement in more detail later, but in short, it consists of the word while, followed by an expression that is interpreted as a true or false result, followed by a nested block of code that is repeated while the test at the top is true (the word True here is considered always true).
    * The raw_input built-in function we met earlier in the book is used here for general console inputit prints its optional argument string as a prompt, and returns the user's typed reply as a string.
    * A single-line if statement that makes use of the special rule for nested blocks also appears here: the body of the if appears on the header line after the colon instead of being indented on a new line underneath it. This would work either way, but as it's coded, we've saved an extra line.
    * Finally, the Python break statement is used to exit the loop immediatelyit simply jumps out of the loop statement altogether, and the program continues after the loop. Without this exit statement, the while would loop forever, as its test is always true.

In effect, this combination of statements means essentially "read a line from the user and print it in uppercase until the user enters the word 'stop.'" There are other ways to code such a loop, but the form used here is very common in Python code.

Notice that all three lines nested under the while header line are indented the same amountbecause they line up vertically in a column this way, they are the block of code that is associated with the while test and repeated. Either the end of the source file or a lesser-indented statement will terminate the loop body block.

When run, here is the sort of interaction we get from this code:


|     Enter text:spam
|     SPAM
|     Enter text:42
|     42
|     Enter text:stop
|     
|     >>> while 1:
|     ...     inp=raw_input('$')      
|     ...     if inp=='quit': break
|     ...     print inp.upper()
|     ... 
|     $hi
|     HI
|     $yosi
|     YOSI
|     $hello there
|     HELLO THERE
|     $quit


			30.2.1.3.2 Doing Math on User Inputs

Our script works, but now suppose that instead of converting a text string to uppercase, we want to do some math with numeric inputsquaring it, for example, perhaps in some misguided effort to discourage users who happen to be obsessed with youth. We might try statements like these to achieve the desired effect:


>>> reply = '20' 
>>> reply ** 2 ...error text omitted... 
TypeError: unsupported operand type(s) for ** or pow(  ): 'str' and 'int'


This won't quite work in our script, though, because (as discussed in the last part of the book) Python won't convert object types in expressions unless they are all numeric, and input from a user is always returned to our script as a string. We cannot raise a string of digits to a power unless we convert it manually to an integer:


|    >>> int(reply) ** 2
|    400


Armed with this information, we can now recode our loop to perform the necessary math:


while True:
    reply = raw_input('Enter text:')
    if reply == 'stop': break
    print int(reply) ** 2
print 'Bye'


This script uses a single-line if statement to exit on "stop" as before, but also converts inputs to perform the required math. This version also adds an exit message at the bottom. Because the print statement in the last line is not indented as much as the nested block of code, it is not considered part of the loop body, and will run only once, after the loop is exited:


|    Enter text:2
|    4
|    Enter text:40
|    1600
|    Enter text:stop
|    Bye
|    

			30.2.1.3.3 Handling Errors by Testing Inputs

So far so good, but notice what happens when the input is invalid:


Enter text:xxx ...error text omitted... 
ValueError: invalid literal for int(  ) with base 10: 'xxx'


The built-in int function raises an exception here in the face of a mistake. If we want our script to be robust, we can check the string's content ahead of time with the string object's isdigit method:


>>> S = '123'
>>> T = 'xxx'
>>> S.isdigit(  ), T.isdigit(  )
(True, False)


This also gives us an excuse to further nest the statements in our example. The following new version of our interactive script uses a full-blown if statement to work around the exception on errors:


while True:
    reply = raw_input('Enter text:')
    if reply == 'stop':
        break
    elif not reply.isdigit(  ):
        print 'Bad!' * 8
    else:
        print int(reply) ** 2
print 'Bye'


We'll study the if statement in more detail in Chapter 12, but it's a fairly lightweight tool for coding logic in scripts. In its full form, it consists of the word if followed by a test and an associated block of code, one or more optional elif ("else if") tests and code blocks, and an optional else part, with an associated block of code at the bottom to serve as a default. Python runs the block of code associated with the first test that is true, working from top to bottom, or the else part if all tests are false.

The if, elif, and else parts in the preceding example are associated as part of the same statement because they all line up vertically (i.e., share the same level of indentation). The if statement spans from the word if to the start of the print statement on the last line of the script. In turn, the entire if block is part of the while loop because all of it is indented under the loop's header line. Statement nesting is natural once you get the hang of it.

When we run our new script, its code catches errors before they occur, and prints an (arguably silly) error message to demonstrate:


|    Enter text:5
|    25
|    Enter text:xyz
|    Bad!Bad!Bad!Bad!Bad!Bad!Bad!Bad!
|    Enter text:10
|    100
|    Enter text:stop


			30.2.1.3.4 Handling Errors with try Statements

The preceding solution works, but as you'll see later in the book, the most general way to handle errors in Python is to catch and recover from them completely using the Python try statement. We'll explore this statement in depth in the last part of this book, but, as a preview, using a try here can lead to code that some would claim is simpler than the prior version:


while True:
    reply = raw_input('Enter text:')
    if reply == 'stop': break
    try:
        num = int(reply)
    except:
        print 'Bad!' * 8
    else:
        print int(reply) ** 2
print 'Bye'


This version works exactly like the previous one, but we've replaced the explicit error check with code that assumes the conversion will work, and wraps it up in an exception handler for cases when it doesn't. This try statement is composed of the word try followed by the main block of code (the action we are trying to run), followed by an except part that gives the exception handler code, followed by an else part to be run if no exception is raised in the try part. Python first runs the try part, then runs either the except part (if an exception occurs), or the else part (if no exception occurs).

In terms of statement nesting, because the words try, except, and else are all indented to the same level, they are all considered part of the same single try statement. Notice that the else part is associated with the try here, not the if. As we'll see, else can appear in if statements in Python, but also in try statements and loopsits indentation tells you what statement it is a part of.

Again, we'll come back to the try statement later in the book. For now, be aware that because try can be used to intercept any error, it reduces the amount of error-checking code you have to write, and is a very general approach to dealing with unusual cases.

			30.2.1.3.5 Nesting Code Three Levels Deep

Now, let's look at one last mutation of our script. Nesting can take us even further if we need it towe could, for example, branch to one of a set of alternatives, based on the relative magnitude of a valid input:


while True:
    reply = raw_input('Enter text:')
    if reply == 'stop':
        break
    elif not reply.isdigit(  ):
        print 'Bad!' * 8
    else:
        num = int(reply)
        if num < 20:
            print 'low'
        else:
            print num ** 2
print 'Bye'


This version includes an if statement nested in the else clause of another if statement, which is in turn nested in the while loop. When code is conditional, or repeated like this, we simply indent it further to the right. The net effect is like that of the prior versions, but we'll now print "low" for numbers less than 20:


Enter text:19
low
Enter text:20
400
Enter text:spam
Bad!Bad!Bad!Bad!Bad!Bad!Bad!Bad!
Enter text:stop
Bye
		30.2.1.4 Chapter Summary

That concludes our quick look at Python statement syntax. This chapter introduced the general rules for coding statements and blocks of code. As you've learned, in Python, we normally code one statement per line, and indent all the statements in a nested block the same amount (indentation is part of Python's syntax). However, we also looked at a few exceptions to these rules, including continuation lines and single-line tests and loops. Finally, we put these ideas to work in an interactive script that demonstrated a handful of statements, and showed statement syntax in action.

In the next chapter, we'll start to dig deeper by going over each of Python's basic procedural statements in depth. As you'll see, though, all statements follow the same general rules introduced here.

BRAIN BUILDER

Chapter Quiz

Q:	What three things are required in a C-like language, but omitted in Python?
Q:	How is a statement normally terminated in Python?
Q:	How are the statements in a nested block of code normally associated in Python?
Q:	How can you make a statement span over multiple lines?
Q:	How can you code a compound statement on a single line?
Q:	Is there any valid reason to type a semicolon at the end of a statement in Python?
Q:	What is a try statement for?
Q:	What is the most common coding mistake among Python beginners?


Quiz Answers

A:	C-like languages require parentheses around the tests in some statements, semicolons at the end of each statement, and braces around a nested block of code.
A:	The end of a line terminates the statement that appears on that line. Alternatively, if more than one statement appears on the same line, they can be terminated with semicolons; similarly, if a statement spans many lines, you must terminate it by closing a bracketed syntactic pair.
A:	The statements in a nested block are all indented the same number of tabs or spaces.
A:	A statement can be made to span many lines by enclosing part of it in parentheses, square brackets, or curly braces; the statement ends when Python sees a line that contains the closing part of the pair.
>>> a = [1,
... 2,3
... ]
>>> a
[1, 2, 3]

A:	The body of a compound statement can be moved to the header line after the colon, but only if the body consists of only noncompound statements.
>>> while True: print "h"

A:	Only when you need to squeeze more than one statement onto a single line of code. Even then, this only works if all the statements are noncompound, and it's discouraged because it can lead to code that is difficult to read.
A:	The try statement is used to catch and recover from exceptions (errors) in a Python script. It's usually an alternative to manually checking for errors in your code.
A:	Forgetting to type the colon character at the end of the header line in a compound statement is the most common beginner's mistake.


		30.2.2. rwordsAssignment, Expressions, and print

Now that we've had a quick introduction to Python statement syntax, this chapter begins our in-depth tour of specific Python statements. We'll begin with the basics-assignments, expression statements, and printing. We've already seen all these in action, but here, we'll fill in important details we've skipped so far. Although they're fairly simple, as you'll see, there are optional variations for each of these statement types that will come in handy once you begin writing real Python programs.

	
			30.2.2.1 Assignment Statements
We've been using the Python assignment statement for a while to assign objects to names. In its basic form, you write the target of an assignment on the left of an equals sign, and the object to be assigned on the right. The target on the left may be a name or object component, and the object on the right can be an arbitrary expression that computes an object. For the most part, assignments are straightforward, but here are a few properties to keep in mind:

    * Assignments create object references. As discussed in Chapter 6, Python assignments store references to objects in names or data structure components. They always create references to objects instead of copying the objects. Because of that, Python variables are more like pointers than data storage areas.
    * Names are created when first assigned. Python creates variable names the first time you assign them a value (i.e., an object reference), so there's no need to predeclare names ahead of time. Some (but not all) data structure slots are created when assigned, too (e.g., dictionary entries, some object attributes). Once assigned, a name is replaced with the value it references whenever it appears in an expression.
    * Names must be assigned before being referenced. It's an error to use a name to which you haven't yet assigned a value. Python raises an exception if you try, rather than returning some sort of ambiguous default value; if it returned a default instead, it would be more difficult for you to spot typos in your code.
    * Implicit assignments:import, from, def, class, for, function arguments. In this section, we're concerned with the = statement, but assignment occurs in many contexts in Python. For instance, we'll see later that module imports, function and class definitions, for loop variables, and function arguments are all implicit assignments. Because assignment works the same everywhere it pops up, all these contexts simply bind names to object references at runtime.

				30.2.2.1.1 Assignment Statement Forms

Although assignment is a general and pervasive concept in Python, we are primarily interested in assignment statements in this chapter. Table 11-1 illustrates the different assignment statement forms in Python.

Table 11-1. Assignment statement forms
Operation 	Interpretation
spam = 'Spam' 	Basic form
spam, ham = 'yum', 'YUM' 	Tuple assignment (positional)
[spam, ham] = ['yum', 'YUM'] 	List assignment (positional)
a, b, c, d = 'spam' 	Sequence assignment, generalized
spam = ham = 'lunch' 	Multiple-target assignment
spams += 42 	Augmented assignment (equivalent to spams = spams + 42)

The first form in Table 11-1 is by far the most common: binding a name (or data structure component) to a single object. The other table entries represent special and optional forms that programmers often find convenient:


Tuple- and list-unpacking assignments

    The second and third forms in the table are related. When you code tuples or lists on the left side of the =, Python pairs objects on the right side with targets on the left by position, and assigns them from left to right. For example, in the second line of the table, the name spam is assigned the string 'yum', and the name ham is bound to the string 'YUM'. Internally, Python makes a tuple of the items on the right first, so this is often called a tuple-unpacking assignment.

Sequence assignments

    In recent versions of Python, tuple and list assignments have been generalized into instances of what we now call sequence assignment-any sequence of names can be assigned to any sequence of values, and Python assigns the items one at a time by position. In fact, we can mix and match the types of the sequences involved. The fourth line in Table 11-1, for example, pairs a tuple of names with a string of characters: a is assigned 's', b is assigned 'p', and so on.

Multiple-target assignments

    The fifth line in Table 11-1 shows the multiple-target form of assignment. In this form, Python assigns a reference to the same object (the object farthest to the right) to all the targets on the left. In the table, the names spam and ham are both assigned references to the same string object, 'lunch'. The effect is the same as if we coded ham = 'lunch' followed by spam = ham, as ham evaluates to the original string object (i.e., not a separate copy of that object).

Augmented assignments

    The last line in Table 11-1 is an example of augmented assignment-a shorthand that combines an expression and an assignment in a concise way. Saying spam += 42, for example, has the same effect as spam = spam + 42, but the augmented form requires less typing, and is generally quicker to run. There's one augmented assignment statement for every binary expression operator in Python.

				30.2.2.1.2 Sequence Assignments

We've already used basic assignments in this book. Here are a few simple examples of sequence-unpacking assignments in action:


% python 
>>> nudge = 1 
>>> wink  = 2 
>>> A, B = nudge, wink  # Tuple assignment
>>> A, B  # Like A = nudge; B = wink
(1, 2)
>>> [C, D] = [nudge, wink]  # List assignment
>>> C, D 
(1, 2)


Notice that we really are coding two tuples in the third line in this interaction-we've just omitted their enclosing parentheses. Python pairs the values in the tuple on the right side of the assignment operator with the variables in the tuple on the left side, and assigns the values one at a time.

Tuple assignment leads to a common coding trick in Python that was introduced in a solution to the exercises from Part 2. Because Python creates a temporary tuple that saves the original values of the variables on the right while the statement runs, unpacking assignments are also a way to swap two variables' values without creating a temporary variable of your own-the tuple on the right remembers the prior values of the variables automatically:


>>> nudge = 1 
>>> wink  = 2 
>>> nudge, wink = wink, nudge  # Tuples: swaps values
>>> nudge, wink  # Like T = nudge; nudge = wink; wink = T
(2, 1)


In fact, the original tuple and list assignment forms in Python were eventually generalized to accept any type of sequence on the right as long as it is of the same length. You can assign a tuple of values to a list of variables, a string of characters to a tuple of variables, and so on. In all cases, Python assigns items in the sequence on the right to variables in the sequence on the left by position, from left to right:


>>> [a, b, c] = (1, 2, 3)  # Assign tuple of values to list of names
>>> a, c 
(1, 3)
>>> (a, b, c) = "ABC"  # Assign string of characters to tuple
>>> a, c 
('A', 'C')


Technically speaking, sequence assignment actually supports any iterable object on the right, not just any sequence. This is a more general concept that we will explore in Chapter 13 and Chapter 17.

					30.2.2.1.2.1 Advanced sequence assignment patterns
One note here-although we can mix and match sequence types around the = symbol, we still must have the same number of items on the right as we have variables on the left, or we'll get an error:


>>> string = 'SPAM' 
>>> a, b, c, d = string  # Same number on both sides
>>> a, d 
('S', 'M')

>>> a, b, c = string  # Error if not...error text omitted... 
ValueError: too many values to unpack


To be more general, we need to slice. There are a variety of ways to employ slicing to make this last case work:


>>> a, b, c = string[0], string[1], string[2:]  # Index and slice
>>> a, b, c 
('S', 'P', 'AM')

>>> a, b, c = list(string[:2]) + [string[2:]]  # Slice and concatenate
>>> a, b, c 
('S', 'P', 'AM')

>>> a, b = string[:2]  # Same, but simpler
>>> c = string[2:] 
>>> a, b, c 
('S', 'P', 'AM')

>>> (a, b), c = string[:2], string[2:]  # Nested sequences
>>> a, b, c 
('S', 'P', 'AM')


As the last example in this interaction demonstrates, we can even assign nested sequences, and Python unpacks their parts according to their shape, as expected. In this case, we are assigning a tuple of two items, where the first item is a nested sequence (a string) exactly as though we had coded it this way:


>>> ((a, b), c) = ('SP', 'AM')  # Paired up by shape and position
>>> a, b, c 
('S', 'P', 'AM')


Python pairs the first string on the right ('SP') with the first tuple on the left ((a, b)), and assigns one character at a time, before assigning the entire second string ('AM') to the variable c all at once. In this event, the sequence-nesting shape of the object on the left must match that of the object on the right. Nested sequence assignment like this is somewhat advanced, and rare to see, but it can be convenient for picking out the parts of data structures with known shapes. For example, this technique also works in function argument lists because function arguments are passed by assignment (as we'll see in the next part of the book).

Sequence-unpacking assignments also give rise to another common coding idiom in Python-assigning an integer series to a set of variables:


>>> red, green, blue = range(3)
>>> red, blue
(0, 2)


This initializes the three names to the integer codes 0, 1, and 2, respectively (it's Python's equivalent of the enumerated data types you may have seen in other languages). To make sense of this, you need to know that the range built-in function generates a list of successive integers:


>>> range(3)  # Try list(range(3)) in Python 3.0
[0, 1, 2]


Because range is commonly used in for loops, we'll say more about it in Chapter 13. Another place you may see a tuple assignment at work is for splitting a sequence into its front and the rest in loops like this:


>>> L = [1, 2, 3, 4]
>>> while L:
...     front, L = L[0], L[1:]
...     print front, L
...
1 [2, 3, 4]
2 [3, 4]
3 [4]
4 []


The tuple assignment in the loop here could be coded as the following two lines instead, but it's often more convenient to string them together:


...     front = L[0]
...     L = L[1:]


Notice that this code is using the list as a sort of stack data structure-something we can often also achieve with the append and pop methods of list objects; here, front = L.pop(0) would have much the same effect as the tuple assignment statement, but it would be an in-place change. We'll learn more about while loops, and other (and often better) ways to step through a sequence with for loops, in Chapter 13.

				30.2.2.1.3 Multiple-Target Assignments

A multiple-target assignment simply assigns all the given names to the object all the way to the right. The following, for example, assigns the three variables a, b, and c to the string 'spam':


>>> a = b = c = 'spam'
>>> a, b, c
('spam', 'spam', 'spam')


This form is equivalent to (but easier to code) than these three assignments:


>>> c = 'spam'
>>> b = c
>>> a = b


					30.2.2.1.3.1 Multiple-target assignment and shared references

Keep in mind that there is just one object here, shared by all three variables (they all wind up pointing to the same object in memory). This behavior is fine for immutable types-initializing a set of counters to zero, for example (recall that variables must be assigned before they can be used in Python, so you must initialize counters to zero before you can start adding to them):


>>> a = b = 0
>>> b = b + 1
>>> a, b
(0, 1)


Here, changing b only changes b because numbers do not support in-place changes. As long as the object assigned is immutable, it's irrelevant if more than one name references it.

As usual, though, we have to be more cautious when initializing variables to an empty mutable object such as a list or dictionary:


>>> a = b = []
>>> b.append(42)
>>> a, b
([42], [42])


This time, because a and b reference the same object, appending to it in-place through b will impact what we see through a as well. This is really just another example of the shared reference phenomenon we first met in Chapter 6. To avoid the issue, initialize mutable objects in separate statements instead, so that each creates a distinct empty object by running a distinct literal expression:


>>> a = []
>>> b = []
>>> b.append(42)
>>> a, b
([], [42])

				30.2.2.1.4 Augmented Assignments

Beginning with Python 2.0, the set of additional assignment statement formats listed in Table 11-2 became available. Known as augmented assignments, and borrowed from the C language, these formats are mostly just shorthand. They imply the combination of a binary expression, and an assignment. For instance, the following two formats are now roughly equivalent:


X = X + Y                  # Traditional form
X += Y                     # Newer augmented form


Table 11-2. Augmented assignment statements
X += Y 	X &= Y 	X -= Y 	X |= Y
X *= Y 	X ^= Y 	X /= Y 	X >>= Y
X %= Y 	X <<= Y 	X **= Y 	X //= Y

Augmented assignment works on any type that supports the implied binary expression. For example, here are two ways to add 1 to a name:


>>> x = 1 
>>> x = x + 1  # Traditional
>>> x 
2
>>> x += 1  # Augmented
>>> x 
3


When applied to a string, the augmented form performs concatenation instead. Thus, the second line here is equivalent to typing the longer S = S + "SPAM":


>>> S = "spam" 
>>> S += "SPAM"  # Implied concatenation
>>> S 
'spamSPAM'


As shown in Table 11-2, there are analogous augmented assignment forms for every Python binary expression operator (i.e., each operator with values on the left and right side). For instance, X *= Y multiplies and assigns, X >>= Y shifts right and assigns, and so on. X //= Y (for floor division) was added in version 2.2.

Augmented assignments have three advantages:[31]

    [31] * C/C++ programmers take note: although Python now supports statements like X += Y, it still does not have C's auto-increment/decrement operators (e.g., X++, --X). These don't quite map to the Python object model because Python has no notion of in-place changes to immutable objects like numbers.

    * There's less for you to type. Need I say more?
    * The left side only has to be evaluated once. In X += Y, X may be a complicated object expression. In the augmented form, it need only be evaluated once. However, in the long form, X = X + Y, X appears twice, and must be run twice. Because of this, augmented assignments usually run faster.
    * The optimal technique is automatically chosen. For objects that support in-place changes, the augmented forms automatically perform in-place change operations instead of slower copies.

The last point here requires a bit more explanation. For augmented assignments, in-place operations may be applied for mutable objects as an optimization. Recall that lists can be extended in a variety of ways. To add a single item to the end of a list, we can concatenate or call append:


>>> L = [1, 2] 
>>> L = L + [3]  # Concatenate: slower
>>> L 
[1, 2, 3]
>>> L.append(4)  # Faster, but in-place
>>> L 
[1, 2, 3, 4]


And to add a set of items to the end, we can either concatenate again, or call the list extend method:[32]

    [32] dagger As suggested in Chapter 6, we can also use slice assignment (e.g., L[len(L):] = [11,12,13]), but this works roughly the same as the simpler list extend method.


>>> L = L + [5, 6]  # Concatenate: slower
>>> L 
[1, 2, 3, 4, 5, 6]
>>> L.extend([7, 8])  # Faster, but in-place
>>> L 
[1, 2, 3, 4, 5, 6, 7, 8]


In both cases, concatenation is less prone to the side effects of shared object references, but will generally run slower than the in-place equivalent. Concatenation operations must create a new object, copy in the list on the left, and then copy in the list on the right. By contrast, in-place method calls simply add items at the end of a memory block.

When we use augmented assignment to extend a list, we can forget these details-for example, Python automatically calls the quicker extend method instead of using the slower concatenation operation implied by +:


>>> L += [9, 10]  # Mapped to L.extend([9, 10])
>>> L 
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]


					30.2.2.1.4.1 Augmented assignment and shared references

This behavior is usually what we want, but notice that it implies the += is an in-place change for lists; thus, it is not exactly like + concatenation, which always makes a new object. As for all shared reference cases, the difference might matter if other names reference the object being changed:


>>> L = [1, 2] 
>>> M = L  # L and M reference the same object
>>> L = L + [3, 4]  # Concatenation makes a new object
>>> L, M  # Changes L but not M
([1, 2, 3, 4], [1, 2])

>>> L = [1, 2] 
>>> M = L 
>>> L += [3, 4]  # But += really means extend
>>> L, M  # M sees the in-place change too!
([1, 2, 3, 4], [1, 2, 3, 4])


This only matters for mutables like lists and dictionaries, and it is a fairly obscure case (at least, until it impacts your code!). As always, make copies of your mutable objects if you need to break the shared reference structure.
O
				30.2.2.1.5 Variable Name Rules

Now that we've explored assignment statements, it's time to get more formal about the use of variable names. In Python, names come into existence when you assign values to them, but there are a few rules to follow when picking names for things in your programs:


Syntax: (underscore or letter) + (any number of letters, digits, or underscores)

    Variable names must start with an underscore or letter, which can be followed by any number of letters, digits, or underscores. _spam, spam, and Spam_1 are legal names, but 1_Spam, spam$, and @#! are not.

Case matters: SPAM is not the same as spam

    Python always pays attention to case in programs, both in names you create, and in reserved words. For instance, the names X and x refer to two different variables. For portability, case also matters in the names of imported module files, even on platforms where the filesystems are case insensitive.

Reserved words are off-limits

    Names you define cannot be the same as words that mean special things in the Python language. For instance, if you try to use a variable name like class, Python will raise a syntax error, but klass and Class work fine. Table 11-3 lists the words that are currently reserved (and hence off-limits) in Python.

Table 11-3. Python reserved words
and 	elif 	if 	pass
as (in 2.6 and later) 	else 	import 	print
assert 	except 	in 	raise
break 	exec 	is 	return
class 	finally 	lambda 	try
continue 	for 	nonlocal (in 3.0) 	while
def 	from 	not 	with (in 2.6 and later)
del 	global 	or 	yield

	

yield was an optional extension in Python 2.2, but is a standard keyword as of 2.3. It is used in conjunction with generator functions, a newer feature discussed in Chapter 17. This was one of a small handful of instances where Python broke with backward compatibility. Still, yield was phased in over time-it began generating warnings in 2.2, and was not enabled until 2.3.

Similarly, in Python 2.6, the words with and as are scheduled to become new reserved words for use in context managers (a new form of exception handling). These two words are not reserved in 2.5, unless the context manager feature is turned on manually (see Chapter 27 for details). When used in 2.5, with and as generate warnings about the upcoming change-except in the version of IDLE in Python 2.5, which appears to have enabled this feature for you (that is, using these words as variable names does generate errors in 2.5, but only in its version of the IDLE GUI).

Python's reserved words are always all lowercase, and they are truly reserved; unlike names in the built-in scope that you will meet in the next part of this book, you cannot redefine reserved words by assignment (e.g., and = 1 results in a syntax error).[33]

    [33] * In the Jython Java-based implementation of Python, though, user-defined variable names can sometimes be the same as Python reserved words.

Furthermore, because module names in import statements become variables in your script, this constraint extends to your module filenames-you can code files called and.py and my-code.py, but you cannot import them because their names without the ".py" extension become variables in your code, and so must follow all the variable rules just outlined (reserved words are off-limits, and dashes won't work, though underscores will). We'll revisit this idea in Part 5.

					30.2.2.1.5.1 Naming conventions

Besides these rules, there is also a set of naming conventions-rules that are not required, but are followed in normal practice. For instance, because names with two leading and trailing underscores (e.g., __name__) generally have special meaning to the Python interpreter, you should avoid this pattern for your own names. Here is a list of the conventions Python follows:

    * Names that begin with a single underscore (_X) are not imported by a from module import * statement (described in Chapter 19).
    * Names that have two leading and trailing underscores (__X__) are system-defined names that have special meaning to the interpreter.
    * Names that begin with two underscores, and do not end with two more (__X) are localized ("mangled") to enclosing classes (described in Chapter 26).
    * The name that is just a single underscore (_) retains the result of the last expression when working interactively.

In addition to these Python interpreter conventions, there are various other conventions that Python programmers usually follow as well. For instance, later in the book, we'll see that class names commonly start with an uppercase letter, and module names with a lowercase letter, and that the name self, though not reserved, usually has a special role in classes. In Part 4, we'll also study another, larger category of names known as the built-ins, which are predefined, but not reserved (and so can be reassigned: open = 42 works, though sometimes you might wish it didn't!).

					30.2.2.1.5.2 Names have no type, but objects do

This is mostly review, but remember that it's crucial to keep Python's distinction between names and objects clear. As described in "Chapter 6" in Chapter 6, objects have a type (e.g., integer, list), and may be mutable or not. Names (a.k.a. variables), on the other hand, are always just references to objects; they have no notion of mutability, and have no associated type information, apart from the type of the object they happen to reference at a given point in time.

It's OK to assign the same name to different kinds of objects at different times:


>>> x = 0  # x bound to an integer object
>>> x = "Hello"  # Now it's a string
>>> x = [1, 2, 3]  # And now it's a list


In later examples, you'll see that this generic nature of names can be a decided advantage in Python programming.[34] In Part 4, you'll learn that names also live in something called a scope, which defines where they can be used; the place where you assign a name determines where it is visible.

    [34] * If you've used C++, you may be interested to know that there is no notion of C++'s const declaration in Python; certain objects may be immutable, but names can always be assigned. Python also has ways to hide names in classes and modules, but they're not the same as C++'s declarations.
	
			30.2.2.2 Expression Statements

In Python, you can use an expression as a statement, too (that is, on a line by itself). But because the result of the expression won't be saved, it makes sense to do so only if the expression does something useful as a side effect. Expressions are commonly used as statements in two situations:


For calls to functions and methods

    Some functions and methods do lots of work without returning a value. Such functions are sometimes called procedures in other languages. Because they don't return values that you might be interested in retaining, you can call these functions with expression statements.

For printing values at the interactive prompt

    Python echoes back the results of expressions typed at the interactive command line. Technically, these are expression statements, too; they serve as a shorthand for typing print statements.

Table 11-4 lists some common expression statement forms in Python. Calls to functions and methods are coded with zero or more argument objects (really, expressions that evaluate to objects) in parentheses, after the function/method name.

Table 11-4. Common Python expression statements
Operation 	Interpretation
spam(eggs, ham) 	Function calls
spam.ham(eggs) 	Method calls
spam 	Printing variables in the interactive interpreter
spam < ham and ham != eggs 	Compound expressions
spam < ham < eggs 	Range tests

The last line in the table is a special form: Python lets us string together magnitude comparison tests, to code chained comparisons such as range tests. For instance, the expression (A < B < C) tests whether B is between A and C; it's equivalent to the Boolean test (A < B and B < C), but is easier on the eyes (and the keyboard). Compound expressions aren't normally written as statements, but doing so is syntactically legal, and can even be useful at the interactive prompt if you're not sure of an expression's result.

Beware that although expressions can appear as statements in Python, statements can't be used as expressions. For instance, Python doesn't allow you to embed assignment statements (=) in other expressions. The rationale for this is that it avoids common coding mistakes; you can't accidentally change a variable by typing = when you really mean to use the == equality test. You'll see how to code around this when you meet the Python while loop in Chapter 13.

				30.2.2.2.1 Expression Statements and In-Place Changes

This brings up a mistake that is common in Python work. Expression statements are often used to run list methods that change a list in-place:


>>> L = [1, 2] 
>>> L.append(3)  # Append is an in-place change
>>> L 
[1, 2, 3]


However, it's not unusual for Python newcomers to code such an operation as an assignment statement instead, intending to assign L to the larger list:


>>> L = L.append(4)  # But append returns None, not L
>>> print L  # So we lose our list!
None


This doesn't quite work, though-calling an in-place change operation such as append, sort, or reverse on a list always changes the list in-place, but these methods do not return the list they have changed.

In fact, they return the None object. If you assign such an operation's result back to the variable name, you effectively lose the list (and it is probably garbage collected in the process!).

So, don't do this. We'll revisit this phenomenon in the "Section 14.2" warnings section at the end of this part of the book because it can also appear in the context of some looping statements we'll meet in later chapters.


			30.2.2.3 print Statements
	The print statement prints things-it's simply a programmer-friendly interface to the standard output stream. Technically, it converts an object to its textual representation, and sends this to standard output.

The standard output stream is the same as the C language's stdout; it is usually mapped to the window where you started your Python program (unless redirected to a file or pipe in your system's shell).

In Chapter 9, we looked at some file methods that write text. The print statement is similar, but more focused: print writes objects to the stdout stream (with some default formatting), but file write methods write strings to arbitrary files. Because the standard output stream is available in Python as the stdout object in the built-in sys module (i.e., sys.stdout), it's possible to emulate print with file writes, but print is easier to use.

Table 11-5 lists the print statement's forms.

Table 11-5. print statement forms
Operation 	Interpretation
print spam, ham 	Print objects to sys.stdout; add a space between the items and a linefeed at the end
print spam, ham, 	Same, but don't add linefeed at end of text
print >> myfile, spam, ham 	Send text to myfile.write, not to sys.stdout.write

We've seen the basic print statement in action already. By default, it adds a space between the items separated by commas, and adds a linefeed at the end of the current output line:


>>> x = 'a'
>>> y = 'b'
>>> print x, y
a b


This formatting is just a default; you can choose to use it or not. To suppress the linefeed (so you can add more text to the current line later), end your print statement with a comma, as shown in the second line of Table 11-5. To suppress the space between items, don't print this way-instead, build up an output string yourself using the string concatenation and formatting tools covered in Chapter 7, and print the string all at once:


>>> print x + y
ab
>>> print '%s...%s' % (x, y)
a...b


				30.2.2.3.1 The Python "Hello World" Program

To print a "hello world" message, simply print the string:


>>> print 'hello world'  # Print a string object
hello world


Because expression results are echoed on the interactive command line, you often don't even need to use a print statement there; simply type expressions you'd like to have printed, and their results are echoed back:


>>> 'hello world'  # Interactive echoes
'hello world'


Really, the print statement is just an ergonomic feature of Python-it provides a simple interface to the sys.stdout object, with a bit of default formatting. In fact, if you like to work harder than you must, you can also code print operations this way:


>>> import sys  # Printing the hard way
>>> sys.stdout.write('hello world\n') 
hello world


This code explicitly calls the write method of sys.stdout-an attribute preset when Python starts up to an open file object connected to the output stream. The print statement hides most of those details, providing a simple tool for simple printing tasks.

				30.2.2.3.2 Redirecting the Output Stream

So, why did I just show you the hard way to print? The sys.stdout print equivalent turns out to be the basis of a common technique in Python. In general, print and sys.stdout are related as follows. This statement:


print X


is equivalent to the longer:

import sys
sys.stdout.write(str(X) + '\n')


which manually performs a string conversion with str, adds a newline with +, and calls the output stream's write method. The long form isn't all that useful for printing by itself. However, it is useful to know that this is exactly what print statements do because it is possible to reassign sys.stdout to something different from the standard output stream. In other words, this equivalence provides a way for making your print statements send their text to other places. For example:


import sys
sys.stdout = open('log.txt', 'a')       # Redirects prints to file
...
print x, y, x                           # Shows up in log.txt


Here, we reset sys.stdout to a manually opened output file object opened in append mode. After the reset, every print statement anywhere in the program will write its text to the end of the file log.txt instead of to the original output stream. The print statements are happy to keep calling sys.stdout's write method, no matter what sys.stdout happens to refer to. Because there is just one sys module in your process, assigning sys.stdout this way will redirect every print anywhere in your program.

In fact, as this chapter's upcoming sidebar on print and stdout will explain, you can even reset sys.stdout to a nonfile object, as long as it has the expected protocol (a write method); when that object is a class, printed text can be routed and processed arbitrarily.

This trick of resetting the output stream is primarily useful for programs originally coded with print statements. If you know that output should go to a file to begin with, you can always call file write methods instead. To redirect the output of a print-based program, though, resetting sys.stdout provides a convenient alternative to changing every print statement, or using system-shell-based redirection syntax.
				30.2.2.3.3 The print >> file Extension

This trick of redirecting printed text by assigning sys.stdout is very commonly used in practice. But one potential problem with the last section's code is that there is no direct way to restore the original output stream should you need to switch back after printing to a file. Because sys.stdout is just a normal file object, you can always save it and restore it if needed:[35]

    [35] * You can also use the relatively new __stdout__ attribute in the sys module, which refers to the original value sys.stdout had at program startup time. You still need to restore sys.stdout to sys.__stdout__ to go back to this original stream value, though. See the sys module in the library manual for more details.


>>> import sys 
>>> temp = sys.stdout  # Save for restoring later
>>> sys.stdout = open('log.txt', 'a')  # Redirect prints to a file
>>> print 'spam'  # Prints go to file, not here
>>> print 1, 2, 3 
>>> sys.stdout.close(  )  # Flush output to disk
>>> sys.stdout = temp  # Restore original stream

>>> print 'back here'  # Prints show up here again
back here
>>> print open('log.txt').read(  )  # Result of earlier prints
spam
1 2 3


This need crops up fairly regularly, and this manual saving and restoring of the original output stream is just complicated enough that a print extension was added to make it unnecessary. When a print statement begins with a >> followed by an output file (or other) object, that single print statement sends its text to the object's write method, but it does not reset sys.stdout. Because the redirection is temporary, normal print statements keep printing to the original output stream:


log =  open('log.txt', 'a')
print >> log, x, y, z                   # Print to a file-like object
print a, b, c                           # Print to original stdout


The >> form of print is handy if you need to print to both files and the standard output stream in the same program. If you use this form, however, be sure to give it a file object (or an object that has the same write method as a file object), not a file's name string:


>>> log = open('log.txt', 'w')
>>> print >> log, 1, 2, 3
>>> print >> log, 4, 5, 6
>>> log.close(  )
>>> print 7, 8, 9
7 8 9
>>> print open('log.txt').read(  )
1 2 3
4 5 6


This extended form of print is also commonly used to print error messages to the standard error stream, sys.stderr. You can either use its file write methods and format the output manually, or print with redirection syntax:


>>> import sys
>>> sys.stderr.write(('Bad!' * 8) + '\n')
Bad!Bad!Bad!Bad!Bad!Bad!Bad!Bad!

>>> print >> sys.stderr, 'Bad!' * 8
Bad!Bad!Bad!Bad!Bad!Bad!Bad!Bad!


Why You Will Care: print and stdout

The equivalence between the print statement and writing to sys.stdout is important. It makes it possible to reassign sys.stdout to a user-defined object that provides the same methods as files (e.g., write). Because the print statement just sends text to the sys.stdout.write method, you can capture printed text in your programs by assigning sys.stdout to an object whose write method processes the text in arbitrary ways.

For instance, you can send printed text to a GUI window, or tee it off to multiple destinations by defining an object with a write method that does the required routing. You'll see an example of this trick when we study classes later in the book, but abstractly, it looks like this:


class FileFaker:
    def write(self, string):
        # Do something with the string
import sys
sys.stdout = FileFaker(  )
print someObjects             # Sends to class write method


This works because print is what we will call a polymorphic operation in the next part of this book-it doesn't care what SYS.STDOUT is, only that it has a method (i.e., interface) called write. In recent versions of Python, this redirection to objects is made even simpler with the >> extended form of print because we don't need to reset sys.stdout explicitly:


myobj = FileFaker(  )           # Redirect to an object for one print
print >> myobj, someObjects    # Does not reset sys.stdout


Python's built-in raw_input( ) function reads from the sys.stdin file, so you can intercept read requests in a similar way, using classes that implement file-like read methods. See the raw_input and while loop example in Chapter 10 for more background on this.

Notice that because printed text goes to the stdout stream, it's the way to print HTML in CGI scripts. It also enables you to redirect Python script input and output at the operating system's command line, as usual:


python script.py < inputfile > outputfile
python script.py | filterProgram



	

In Python 3.0, the print statement is scheduled to become a built-in function, with equivalent utility, but slightly different syntax. The target file and end-of-line behavior are to be specified with keyword arguments. For instance, the statement print x, y will become the call print(x, y), and print >> f, x will become print(x, file=f, end=' ').

All of this is still in the future, so consult the 3.0 release notes for more details. Current 3.0 plans also call for including a converter script (tentatively named "2to3") that will, among a handful of other things, automatically convert print statements in your existing code to print calls. It should help, if you find yourself torn between 2.x and 3.x (like this book!).		

'
			30.2.2.4 Chapter Summary
In this chapter, we began our in-depth look at Python statements by exploring assignments, expressions, and prints. Although these are generally simple to use, they have some alternative forms that are optional, but often convenient in practice. Augmented assignment statements and the redirection form of print statements, for example, allow us to avoid some manual coding work. Along the way, we also studied the syntax of variable names, stream redirection techniques, and a variety of common mistakes to avoid, such as assigning the result of an append method call back to a variable.

In the next chapter, we continue our statement tour by filling in details about the if statement, Python's main selection tool; there, we'll also revisit Python's syntax model in more depth, and look at the behavior of Boolean expressions. Before we move on, though, the end-of-chapter quiz will test your knowledge of what you've learned here.

BRAIN BUILDER
Chapter Quiz

Q:	Name three ways that you can assign three variables to the same value.
Q:	Why might you need to care when assigning three variables to a mutable object?
Q:	What's wrong with saying L = L.sort( )?
Q:	How might you use the print statement to send text to an external file?


Quiz Answers

A:	You can use multiple-target assignments (A = B = C = 0), sequence assignment (A, B, C = 0, 0, 0), or multiple assignment statements on separate lines (A = 0, B = 0, C = 0). With the latter technique, as introduced in Chapter 10, you can also string the three separate statements together on the same line by separating them with semicolons (A = 0; B = 0; C = 0).
A:	If you assign them this way:


    A = B = C = []


    all three names reference the same object, so changing it in-place from one (e.g., A.append(99)) will affect the others. This is true only for in-place changes to mutable objects like lists and dictionaries; for immutable objects, such as numbers and strings, this issue is irrelevant.
    A:	The list sort method is like append in that is an in-place change to the subject listÂ¿it returns None, not the list it changes. The assignment back to L sets L to None, not the sorted list. As we'll see later in this part of the book, a newer built-in function, sorted, sorts any sequence, and returns a new list with the sorting result; because it is not an in-place change, its result can be meaningfully assigned to a name.
    A:	You can assign sys.stdout to a manually opened file before the print, or use the extended print >> file statement form to print to a file for a single print statement. You can also redirect all of a program's printed text to a file with special syntax in the system shell, but this is outside Python's scope.

		30.2.3. if Tests
This chapter presents the Python if statement, which is the main statement used for selecting from alternative actions based on test results. Because this is our first in-depth look at compound statementsÂ¿statements that embed other statementsÂ¿we will also explore the general concepts behind the Python statement syntax model here in more detail than we did in the introduction in Chapter 10. And, because the if statement introduces the notion of tests, this chapter will also deal with Boolean expressions, and fill in some details on truth tests in general.

			30.2.3.1  if statements
In simple terms, the Python if statement selects actions to perform. It's the primary selection tool in Python, and represents much of the logic a Python program possesses. It's also our first compound statement. Like all compound Python statements, the if statement may contain other statements, including other ifs. In fact, Python lets you combine statements in a program sequentially (so that they execute one after another), and in an arbitrarily nested fashion (so that they execute only under certain conditions).

				30.2.3.1.1  General Format

The Python if statement is typical of if statements in most procedural languages. It takes the form of an if test, followed by one or more optional elif ("else if") tests, and a final optional else block. The tests and the else part each have an associated block of nested statements, indented under a header line. When the if statement runs, Python executes the block of code associated with the first test that evaluates to true, or the else block if all tests prove false. The general form of an if statement looks like this:


if <test1>:               # if test
    <statements1>         # Associated block
elif <test2>:             # Optional elifs
    <statements2>
else:                     # Optional else
    <statements3>


				30.2.3.1.2 Basic Examples

To demonstrate, let's look at a few simple examples of the if statement at work. All parts are optional, except the initial if test and its associated statements; in the simplest case, the other parts are omitted:


>>> if 1:
...     print 'true'
...
true


Notice how the prompt changes to ... for continuation lines in the basic interface used here (in IDLE, you'll simply drop down to an indented line insteadÂ¿hit Backspace to back up); a blank line terminates and runs the entire statement. Remember that 1 is Boolean true, so this statement's test always succeeds. To handle a false result, code the else:


>>> if not 1:
...     print 'true'
... else:
...     print 'false'
...
false


				30.2.3.1.3 Multiway Branching

Now, here's an example of a more complex if statement, with all its optional parts present:


>>> x = 'killer rabbit'
>>> if x == 'roger':
...     print "how's jessica?"
... elif x == 'bugs':
...     print "what's up doc?"
... else:
...     print 'Run away! Run away!'
...
Run away! Run away!


This multiline statement extends from the if line through the else block. When it's run, Python executes the statements nested under the first test that is true, or the else part if all tests are false (in this example, they are). In practice, both the elif and else parts may be omitted, and there may be more than one statement nested in each section. Note that the words if, elif, and else are associated by the fact that they line up vertically, with the same indentation.

If you've used languages like C or Pascal, you might be interested to know that there is no switch or case statement in Python that selects an action based on a variable's value. Instead, multiway branching is coded either as a series of if/elif tests, as in the prior example, or by indexing dictionaries or searching lists. Because dictionaries and lists can be built at runtime, they're sometimes more flexible than hardcoded if logic:


| >>> choice = 'ham' 
| >>> print {'spam':  1.25,  # A dictionary-based 'switch'
| ...        'ham':   1.99,  # Use has_key or get for default
| ...        'eggs':  0.99, 
| ...        'bacon': 1.10}[choice] 
| 1.99


Although it may take a few moments for this to sink in the first time you see it, this dictionary is a multiway branchÂ¿indexing on the key choice branches to one of a set of values, much like a switch in C. An almost equivalent but more verbose Python if statement might look like this:


| >>> if choice == 'spam':
| ...     print 1.25
| ... elif choice == 'ham':
| ...     print 1.99
| ... elif choice == 'eggs':
| ...     print 0.99
| ... elif choice == 'bacon':
| ...     print 1.10
| ... else:
| ...     print 'Bad choice'
| ...
| 1.99


Notice the else clause on the if here to handle the default case when no key matches. As we saw in Chapter 8, dictionary defaults can be coded with has_key tests, get method calls, or exception catching. All of the same techniques can be used here to code a default action in a dictionary-based multiway branch. Here's the get scheme at work with defaults:


| >>> branch = {'spam': 1.25,
| ...           'ham':  1.99,
| ...           'eggs': 0.99}
| >>> print branch.get('spam', 'Bad choice')
| 1.25
| >>> print branch.get('bacon', 'Bad choice')
| Bad choice


Dictionaries are good for associating values with keys, but what about the more complicated actions you can code in if statements? In Part 4, you'll learn that dictionaries can also contain functions to represent more complex branch actions and implement general jump tables. Such functions appear as dictionary values, are often coded as lambdas, and are called by adding parentheses to trigger their actions; stay tuned for more on this topic.

Although dictionary-based multiway branching is useful in programs that deal with more dynamic data, most programmers will probably find that coding an if statement is the most straightforward way to perform multiway branching. As a rule of thumb in coding, when in doubt, err on the side of simplicity and readability.

			30.2.3.2 Python Syntax Rules

I introduced Python's syntax model in Chapter 10. Now that we're stepping up to larger statements like the if, this section reviews and expands on the syntax ideas introduced earlier. In general, Python has a simple, statement-based syntax. But, there are a few properties you need to know about:

    * Statements execute one after another, until you say otherwise. Python normally runs statements in a file or nested block in order from first to last, but statements like if (and, as you'll see, loops) cause the interpreter to jump around in your code. Because Python's path through a program is called the control flow, statements such as if that affect it are often called control-flow statements.
    * Block and statement boundaries are detected automatically. As we've seen, there are no braces or "begin/end" delimiters around blocks of code in Python; instead, Python uses the indentation of statements under a header to group the statements in a nested block. Similarly, Python statements are not normally terminated with semicolons; rather, the end of a line usually marks the end of the statement coded on that line.
    * Compound statements = header, ":," indented statements. All compound statements in Python follow the same pattern: a header line terminated with a colon, followed by one or more nested statements, usually indented under the header. The indented statements are called a block (or sometimes, a suite). In the if statement, the elif and else clauses are part of the if, but are also header lines with nested blocks of their own.
    * Blank lines, spaces, and comments are usually ignored. Blank lines are ignored in files (but not at the interactive prompt). Spaces inside statements and expressions are almost always ignored (except in string literals, and when used for indentation). Comments are always ignored: they start with a # character (not inside a string literal), and extend to the end of the current line.
    * Docstrings are ignored, but saved and displayed by tools. Python supports an additional comment form called documentation strings (docstrings for short), which, unlike # comments, are retained at runtime for inspection. Docstrings are simply strings that show up at the top of program files and some statements. Python ignores their contents, but they are automatically attached to objects at runtime, and may be displayed with documentation tools. Docstrings are part of Python's larger documentation strategy, and are covered in the last chapter in this part of the book.

As you've seen, there are no variable type declarations in Python; this fact alone makes for a much simpler language syntax than what you may be used to. But, for most new users, the lack of the braces and semicolons used to mark blocks and statements in many other languages seems to be the most novel syntactic feature of Python, so let's explore what this means in more detail.

				30.2.3.2.1 Block Delimiters

Python detects block boundaries automatically, by line indentationÂ¿that is, the empty space to the left of your code. All statements indented the same distance to the right belong to the same block of code. In other words, the statements within a block line up vertically, as in a column. The block ends when the end of the file or a lesser-indented line is encountered, and more deeply nested blocks are simply indented further to the right than the statements in the enclosing block.

For instance, Figure 12-1 demonstrates the block structure of the following code:


x = 1
if x:
    y = 2
    if y:
        print 'block2'
    print 'block1'
print 'block0'


Figure 12-1. Nested blocks of code: a nested block starts with a statement indented further to the right, and ends with either a statement that is indented less, or the end of the file.


This code contains three blocks: the first (the top-level code of the file) is not indented at all, the second (within the outer if statement) is indented four spaces, and the third (the print statement under the nested if) is indented eight spaces.

In general, top-level (unnested) code must start in column 1. Nested blocks can start in any column; indentation may consist of any number of spaces and tabs, as long as it's the same for all the statements in a given single block. That is, Python doesn't care how you indent your code; it only cares that it's done consistently. Technically, tabs count for enough spaces to move the current column number up to a multiple of 8, but it's usually not a good idea to mix tabs and spaces within a blockÂ¿use one or the other.

The only major place in Python where whitespace matters is when it's used to the left of your code, for indentation; in most other contexts, space can be coded or not. However, indentation is really part of Python syntax, not just a stylistic suggestion: all the statements within any given single block must be indented to the same level, or Python reports a syntax error. This is intentionalÂ¿because you don't need to explicitly mark the start and end of a nested block of code, some of the syntactic clutter found in other languages is unnecessary in Python.

Making indentation part of the syntax model also enforces consistency, a crucial component of readability in structured programming languages like Python. Python's syntax is sometimes described as "what you see is what you get"Â¿the indentation of each line of code unambiguously tells readers what it is associated with. This consistent appearance makes Python code easier to maintain and reuse.

Consistently indented code always satisfies Python's rules. Moreover, most text editors (including IDLE) make it easy to follow Python's indentation model by automatically indenting code as you type it.

				30.2.3.2.2 Statement Delimiters

A statement in Python normally ends at the end of the line on which it appears. When a statement is too long to fit on a single line, though, a few special rules may be used to make it span multiple lines:

    * Statements may span multiple lines if you're continuing an open syntactic pair. Python lets you continue typing a statement on the next line if you're coding something enclosed in a ( ), {}, or [] pair. For instance, expressions in parentheses, and dictionary and list literals, can span any number of lines; your statement doesn't end until the Python interpreter reaches the line on which you type the closing part of the pair (a ), }, or ]). Continuation lines can start at any indentation level, and should all be vertically aligned.
    * Statements may span multiple lines if they end in a backslash. This is a somewhat outdated feature, but if a statement needs to span multiple lines, you can also add a backslash (\) at the end of the prior line to indicate you're continuing on the next line. Because you can also continue by adding parentheses around long constructs, backslashes are almost never used.
    * Triple-quoted string literals can span multiple lines. Very long string literals can span lines arbitrarily; in fact, the triple-quoted string blocks we met in Chapter 7 are designed to do so.
    * Other rules. There are a few other points to mention with regard to statement delimiters. Although uncommon, you can terminate statements with a semicolonÂ¿this convention is sometimes used to squeeze more than one simple (noncompound) statement onto a single line. Also, comments and blank lines can appear anywhere in a file; comments (which begin with a # character) terminate at the end of the line on which they appear.

				30.2.3.2.3 A Few Special Cases

Here's what a continuation line looks like using the open pairs rule. You can span delimited constructs across any number of lines:


L = ["Good",
     "Bad",
     "Ugly"]                     # Open pairs may span lines


This works for anything in parentheses, too: expressions, function arguments, function headers (see Chapter 15), and so on. If you like using backslashes to continue lines, you can, but it's not common practice in Python:


if a == b and c == d and   \
   d == e and f == g:
   print 'olde'                  # Backslashes allow continuations...


Because any expression can be enclosed in parentheses, you can usually use this technique instead if you need your code to span multiple lines:


if (a == b and c == d and
    d == e and e == f):
    print 'new'                  # But parentheses usually do too


As another special case, Python allows you to write more than one noncompound statement (i.e., statements without nested statements) on the same line, separated by semicolons. Some coders use this form to save program file real estate, but it usually makes for more readable code if you stick to one statement per line for most of your work:


x = 1; y = 2; print x            # More than one simple statement


And, finally, Python lets you move a compound statement's body up to the header line, provided the body is just a simple (noncompound) statement. You'll most often see this used for simple if statements with a single test and action:


if 1: print 'hello'              # Simple statement on header line


You can combine some of these special cases to write code that is difficult to read, but I don't recommend it; as a rule of thumb, try to keep each statement on a line of its own, and indent all but the simplest of blocks. Six months down the road, you'll be happy you did.

			30.2.3.3 Truth Tests

The notions of comparison, equality, and truth values were introduced in Chapter 9. Because the if statement is the first statement we've looked at that actually uses test results, we'll expand on some of these ideas here. In particular, Python's Boolean operators are a bit different from their counterparts in languages like C. In Python:

    * Any nonzero number or nonempty object is true.
    * Zero numbers, empty objects, and the special object None are considered false.
    * Comparisons and equality tests are applied recursively to data structures.
    * Comparisons and equality tests return True or False (custom versions of 1 and 0).
    * Boolean and and or operators return a true or false operand object.

In short, Boolean operators are used to combine the results of other tests. There are three Boolean expression operators in Python:


X and Y

    Is true if both X and Y are true.

X or Y

    Is true if either X or Y is true.

not X

    Is true if X is false (the expression returns True or False).

Here, X and Y may be any truth value, or an expression that returns a truth value (e.g., an equality test, range comparison, and so on). Boolean operators are typed out as words in Python (instead of C's &&, ||, and !). Also, Boolean and and or operators return a true or false object in Python, not the values True or False. Let's look at a few examples to see how this works:


>>> 2 < 3, 3 < 2  # Less-than: return 1 or 0
(True, False)


Magnitude comparisons such as these return True or False as their truth results, which, as we learned in Chapter 5 and Chapter 9, are really just custom versions of the integers 1 and 0 (they print themselves differently, but are otherwise the same).

On the other hand, the and and or operators always return an object insteadÂ¿either the object on the left side of the operator, or the object on the right. If we test their results in if or other statements, they will be as expected (remember, every object is inherently true or false), but we won't get back a simple True or False.

For or tests, Python evaluates the operand objects from left to right, and returns the first one that is true. Moreover, Python stops at the first true operand it finds. This is usually called short-circuit evaluation, as determining a result short-circuits (terminates) the rest of the expression:


>>> 2 or 3, 3 or 2  # Return left operand if true
(2, 3)                   # Else, return right operand (true or false)
>>> [ ] or 3 
3
>>> [ ] or { } 
{ }


In the first line of the preceding example, both operands (2 and 3) are true (i.e., are nonzero), so Python always stops and returns the one on the left. In the other two tests, the left operand is false (an empty object), so Python simply evaluates and returns the object on the right (which may happen to have either a true or false value when tested).

and operations also stop as soon as the result is known; however, in this case, Python evaluates the operands from left to right, and stops at the first false object:


>>> 2 and 3, 3 and 2  # Return left operand if false
(3, 2)                   # Else, return right operand (true or false)
>>> [ ] and { } 
[ ]
>>> 3 and [ ] 
[ ]


Here, both operands are true in the first line, so Python evaluates both sides, and returns the object on the right. In the second test, the left operand is false ([]), so Python stops and returns it as the test result. In the last test, the left side is true (3), so Python evaluates and returns the object on the right (which happens to be a false []).

The end result of all this is the same as in C and most other languagesÂ¿you get a value that is logically true or false, if tested in an if or while. However, in Python, Booleans return either the left or right object, not a simple integer flag.

This behavior of and and or may seem esoteric at first glance, but see this chapter's sidebar "Why You Will Care: Booleans" for examples of how it is sometimes used to advantage in coding by Python programmers.

The if/else Ternary Expression

One common role for Boolean operators in Python is to code an expression that runs the same as an if statement. Consider the following statement, which sets A to either Y or Z, based on the truth value of X:


if X:
    A = Y
else:
    A = Z


Sometimes, as in this example, the items involved in such a statement are so simple that it seems like overkill to spread them across four lines. At other times, we may want to nest such a construct in a larger statement instead of assigning its result to a variable. For these reasons (and, frankly, because the C language has a similar tool[36]), Python 2.5 introduced a new expression format that allows us to say the same thing in one expression:

    [36] * In fact, Python's X if Y else Z has a slightly different order than C's Y ? X : Z. This was reportedly done in response to analysis of common use patterns in Python code, but also partially to discourage ex-C programmers from overusing it! Remember, simple is better than complex, in Python, and elsewhere.


A = Y if X else Z


This expression has the exact same effect as the preceding four-line if statement, but it's simpler to code. As in the statement equivalent, Python runs expression Y only if X turns out to be true, and runs expression Z only if X turns out to be false. That is, it short-circuits, as Boolean operators in general do. Here are some examples of it in action:


>>> A = 't' if 'spam' else 'f'  # Nonempty is true
>>> A 
't'
>>> A = 't' if '' else 'f' 
>>> A 
'f'


Prior to Python 2.5 (and after 2.5, if you insist), the same effect can be achieved by a careful combination of the and and or operators because they return either the object on the left side or the object on the right:


A = ((X and Y) or Z)


This works, but there is a catchÂ¿you have to be able to assume that Y will be Boolean true. If that is the case, the effect is the same: the and runs first, and returns Y if X is true; if it's not, the or simply returns Z. In other words, we get "if X then Y else Z."

This and/or combination also seems to require a "moment of great clarity" to understand the first time you see it, and it's no longer required as of 2.5Â¿use the more mnemonic Y if X else Z instead if you need this as an expression, or use a full if statement if the parts are nontrivial.

As a side note, using the following expression in Python is similar because the bool function will translate X into the equivalent of integer 1 or 0, which can then be used to pick true and false values from a list:


A = [Z, Y][bool(X)]


For example:


>>> ['f', 't'][bool('')]
'f'
>>> ['f', 't'][bool('spam')]
't'


However, this isn't exactly the same, because Python will not short-circuitÂ¿it will always run both Z and Y, regardless of the value of X. Because of such complexities, you're better off using the simpler and more easily understood if/else expression as of Python 2.5. Again, though, you should use even that sparingly, and only if its parts are all fairly simple; otherwise, you're better off coding the full if statement form to make changes easier in the future. Your coworkers will be happy you did.

Still, you may see the and/or version in code written prior to 2.5 (and in code written by C programmers who haven't quite let go of their coding pasts).

Why You Will Care: Booleans

One common way to use the somewhat unusual behavior of Python Boolean operators is to select from a set of objects with an or. A statement such as this:


X = A or B or C or None


sets X to the first nonempty (that is, true) object among A, B, and C, or to None if all of them are empty. This works because the or operator returns one of its two objects, and it turns out to be a fairly common coding paradigm in Python: to select a nonempty object from among a fixed-size set, simply string them together in an or expression.

It's also important to understand short-circuit evaluation because expressions on the right of a Boolean operator might call functions that perform substantial or important work, or have side effects that won't happen if the short-circuit rule takes effect:


if f1(  ) or f2(  ): ...


Here, if f1 returns a true (or nonempty) value, Python will never run f2. To guarantee that both functions will be run, call them before the or:


tmp1, tmp2 = f1(  ), f2(  )
if tmp1 or tmp2: ...


You've already seen another application of this behavior in this chapter: because of the way Booleans work, the expression ((A and B) or C) can be used to emulate an if/else statementÂ¿almost.

Also, notice that because all objects are inherently true or false, it's common and easier in Python to test an object directly (if X:) rather than comparing it to an empty value (if X != '':). For a string, the two tests are equivalent.

			30.2.3.4 Chapter Summary

In this chapter, we studied the Python if statement. Because this was our first compound and logical statement, we also reviewed Python's general syntax rules, and explored the operation of truth tests in more depth than we were able to previously. Along the way, we also looked at how to code multiway branching in Python, and learned about the if/else expression introduced in Python 2.5.

The next chapter continues our look at procedural statements by expanding on the while and for loops. There, we'll learn about alternative ways to code loops in Python, some of which may be better than others. Before that, though, here is the usual chapter quiz.

BRAIN BUILDER

Chapter Quiz

Q:	How might you code a multiway branch in Python?
Q:	How can you code an if/else statement as an expression in Python?
Q:	How can you make a single statement span many lines?
Q:	What do the words True and False mean?


Quiz Answers

A:	An if statement with multiple elif clauses is often the most straightforward way to code multiway branching, though not necessarily the most concise. Dictionary indexing can often achieve the same result, especially if the dictionary contains callable functions coded with def statements or lambda expressions.
A:	In Python 2.5, the expression form Y if X else Z returns Y if X is true, or Z otherwise; it's the same as a four-line if statement. The and/or combination ((X and Y) or Z) can work the same way, but it's more obscure, and requires that the Y part be true.
A:	Wrap up the statement in an open syntactic pair (( ), [], or {}), and it can span as many lines as you like; the statement ends when Python sees the closing, right half of the pair.
A:	True and False are just custom versions of the integers 1 and 0, respectively. They always stand for Boolean true and false values in Python.


		30.2.4. while and for Loops
In this chapter, we'll meet Python's two main looping constructsÂ¿statements that repeat an action over and over. The first of these, the while statement, provides a way to code general loops; the second, the for  statement, is designed for stepping through the items in a sequence object, and running a block of code for each item.

There are other kinds of looping operations in Python, but the two statements covered here are the primary syntax provided for coding repeated actions. We'll also study a few unusual statements (such as break and continue) here because they are used within loops. Additionally, this chapter will explore the related concept of Python's iteration protocol, and fill in some details on list comprehensions, a close cousin to the for loop.

			30.2.4.1 while Loops

Python's while statement is the most general iteration construct in the language. In simple terms, it repeatedly executes a block of (normally indented) statements as long as a test at the top keeps evaluating to a true value. It is called a "loop" because control keeps looping back to the start of the statement until the test becomes false. When the test becomes false, control passes to the statement that follows the while block. The net effect is that the loop's body is executed repeatedly while the test at the top is true; if the test is false to begin with, the body never runs.

As I've just stated, the while statement is one of two looping statements available in Python, along with the for. Besides these statements, Python also provides a handful of tools that implicitly loop (iterate): the map, reduce, and filter functions; the in membership test; list comprehensions; and more. We'll explore some of these in Chapter 17 because they are related to functions.

				30.2.4.1.1 General Format

In its most complex form, the while statement consists of a header line with a test expression, a body of one or more indented statements, and an optional else part that is executed if control exits the loop without a break statement being encountered. Python keeps evaluating the test at the top, and executing the statements nested in the loop body until the test returns a false value:


while <test>:                   # Loop test
    <statements1>               # Loop body
else:                                  # Optional else
    <statements2>               # Run if didn't exit loop with break


				30.2.4.1.2 Examples

To illustrate, let's look at a few simple while loops in action. The first, which consists of a print statement nested in a while loop, just prints a message forever. Recall that True is just a custom version of the integer 1, and always stands for a Boolean true value; because the test is always true, Python keeps executing the body forever, or until you stop its execution. This sort of behavior is usually called an infinite loop:


>>> while True:
...    print 'Type Ctrl-C to stop me!'


The next example keeps slicing off the first character of a string until the string is empty and hence false. It's typical to test an object directly like this instead of using the more verbose equivalent (while x != '':). Later in this chapter, we'll see other ways to step more directly through the items in a string with a for loop. Notice the trailing comma in the print hereÂ¿as we learned in Chapter 11, this makes all the outputs show up on the same line:


>>> x = 'spam' 
>>> while x:  # While x is not empty
...     print x, 
...     x = x[1:]  # Strip first character off x
...
spam pam am m


The following code counts from the value of a up to, but not including, b. We'll see an easier way to do this with a Python for loop and the built-in range function later:


>>> a=0; b=10 
>>> while a < b:  # One way to code counter loops
...     print a, 
...     a += 1  # Or, a = a + 1
...
0 1 2 3 4 5 6 7 8 9


Finally, notice that Python doesn't have what some languages call a "do until" loop statement. However, we can simulate one with a test and break at the bottom of the loop body:


while True:
    ...loop body...
    if exitTest(  ): break


To fully understand how this structure works, we need to move on to the next section, and learn more about the break statement.

			30.2.4.2 break, continue, pass, and the Loop else

Now that we've seen a few Python loops in action, it's time to take a look at two simple statements that have a purpose only when nested inside loopsâthe break and continue statements. While we're looking at oddballs, we will also study the loop else clause here because it is intertwined with break, and Python's empty placeholder statement, the pass. In Python:


break

    Jumps out of the closest enclosing loop (past the entire loop statement).

continue

    Jumps to the top of the closest enclosing loop (to the loop's header line).

pass

    Does nothing at all: it's an empty statement placeholder.

Loop else block

    Runs if and only if the loop is exited normally (i.e., without hitting a break).

				30.2.4.2.1 General Loop Format

Factoring in break and continue statements, the general format of the while loop looks like this:


while <test1>:
    <statements1>
    if <test2>: break              # Exit loop now, skip else
    if <test3>: continue           # Go to top of loop now, to test1
else:
    <statements2>                  # Run if we didn't hit a 'break'


break and continue statements can appear anywhere inside the while (or for) loop's body, but they are usually coded further nested in an if test to take action in response to some condition.

				30.2.4.2.2 Examples

Let's turn to a few simple examples to see how these statements come together in practice.

					30.2.4.2.2.1 pass

The pass statement is a no-operation placeholder that is used when the syntax requires a statement, but you have nothing useful to say. It is often used to code an empty body for a compound statement. For instance, if you want to code an infinite loop that does nothing each time through, do it with a pass:

while 1: pass# Type Ctrl-C to stop me!


Because the body is just an empty statement, Python gets stuck in this loop. pass is roughly to statements as None is to objectsâan explicit nothing. Notice that here the while loop's body is on the same line as the header, after the colon; as with if statements, this only works if the body isn't a compound statement.

This example does nothing forever. It probably isn't the most useful Python program ever written (unless you want to warm up your laptop computer on a cold winter's day!); frankly, though, I couldn't think of a better pass example at this point in the book. We'll see other places where it makes sense laterâfor instance, to define empty classes that implement objects that behave like structs and records in other languages. A pass is also sometime coded to mean "to be filled in later," and to stub out the bodies of functions temporarily:


def func1(  ):
    pass                           # Add real code here later

def func2(  ):
    pass


					30.2.4.2.2.2 continue

The continue statement causes an immediate jump to the top of a loop. It also sometimes lets you avoid statement nesting. The next example uses continue to skip odd numbers. This code prints all even numbers less than 10, and greater than or equal to 0. Remember, 0 means false, and % is the remainder of division operator, so this loop counts down to 0, skipping numbers that aren't multiples of 2 (it prints 8 6 4 2 0):


x = 10
while x:
    x = xâ1                        # Or, x -= 1
    if x % 2 != 0: continue        # Odd? -- skip print
    print x,


Because continue jumps to the top of the loop, you don't need to nest the print statement inside an if test; the print is only reached if the continue is not run. If this sounds similar to a "goto" in other languages, it should. Python has no goto statement, but because continue lets you jump about in a program, many of the warnings about readability and maintainability you may have heard about goto apply. continue should probably be used sparingly, especially when you're first getting started with Python. For instance, the last example might be clearer if the print were nested under the if:


x = 10
while x:
    x = xâ1
    if x % 2 == 0:                 # Even? -- print
        print x,


					30.2.4.2.2.3 break

The break statement causes an immediate exit from a loop. Because the code that follows it in the loop is not executed if the break is reached, you can also sometimes avoid nesting by including a break. For example, here is a simple interactive loop (a variant of a larger example we studied in Chapter 10) that inputs data with raw_input, and exits when the user enters "stop" for the name request:


>>> while 1:
...     name = raw_input('Enter name:')
...     if name == 'stop': break
...     age  = raw_input('Enter age: ')
...     print 'Hello', name, '=>', int(age) ** 2
...
Enter name:mel
Enter age: 40
Hello mel => 1600
Enter name:bob
Enter age: 30
Hello bob => 900
Enter name:stop


Notice how this code converts the age input to an integer with int before raising it to the second power; as you'll recall, this is necessary because raw_input returns user input as a string. In Chapter 29, you'll see that raw_input also raises an exception at end-of-file (e.g., if the user types Ctrl-Z or Ctrl-D); if this matters, wrap raw_input in try statements.

					30.2.4.2.2.4 else

When combined with the loop else clause, the break statement can often eliminate the need for the search status flags used in other languages. For instance, the following piece of code determines whether a positive integer y is prime by searching for factors greater than 1:


x = y / 2                                 # For some y > 1
while x > 1:
    if y % x == 0:                        # Remainder
        print y, 'has factor', x
        break                             # Skip else
    x = xâ1
else:                                     # Normal exit
    print y, 'is prime'


Rather than setting a flag to be tested when the loop is exited, insert a break where a factor is found. This way, the loop else clause can assume that it will be executed only if no factor was found; if you don't hit the break, the number is prime.[37]

    [37] * More or less. Numbers less than 2 are not considered prime by the strict mathematical definition. To be really picky, this code also fails for negative and floating-point numbers and will be broken by the future / "true division" change described in Chapter 5. If you want to experiment with this code, be sure to see the exercise at the end of Part 4, which wraps it in a function.

The loop else clause is also run if the body of the loop is never executed, as you don't run a break in that event either; in a while loop, this happens if the test in the header is false to begin with. Thus, in the preceding example, you still get the "is prime" message if x is initially less than or equal to 1 (e.g., if y is 2).

					30.2.4.2.2.5 More on the loop else clause

Because the loop else clause is unique to Python, it tends to perplex some newcomers. In general terms, the loop else provides explicit syntax for a common coding scenarioâit is a coding structure that lets you catch the "other" way out of a loop, without setting and checking flags or conditions.

Suppose, for instance, that you are writing a loop to search a list for a value, and you need to know whether the value was found after you exit the loop. You might code such a task this way:


found = False
while x and not found:
    if match(x[0]):                  # Value at front?
        print 'Ni'
        found = True
    else:
        x = x[1:]                    # Slice off front and repeat
if not found:
    print 'not found'


Here, we initialize, set, and later test a flag to determine whether the search succeeded or not. This is valid Python code, and it does work; however, this is exactly the sort of structure that the loop else clause is there to handle. Here's an else equivalent:


while x:                             # Exit when x empty
    if match(x[0]):
        print 'Ni'
        break                        # Exit, go around else
    x = x[1:]
else:
    print 'Not found'                # Only here if exhausted x


This version is more concise. The flag is gone, and we've replaced the if test at the loop end with an else (lined up vertically with the word while). Because the break inside the main part of the while exits the loop and goes around the else, this serves as a more structured way to catch the search-failure case.

Some readers might have noticed that the prior example's else clause could be replaced with a test for an empty x after the loop (e.g., if not x:). Although that's true in this example, the else provides explicit syntax for this coding pattern (it's more obviously a search-failure clause here), and such an explicit empty test may not apply in some cases. The loop else becomes even more useful when used in conjunction with the for loopâthe topic of the next sectionâbecause sequence iteration is not under your control.

Why You Will Care: Emulating C while Loops

The section on expression statements in Chapter 11 stated that Python doesn't allow statements such as assignments to appear in places where it expects an expression. That means this common C language coding pattern won't work in Python:


while ((x = next(  )) != NULL) {...process x...}


C assignments return the value assigned, but Python assignments are just statements, not expressions. This eliminates a notorious class of C errors (you can't accidentally type = in Python when you mean ==). But, if you need similar behavior, there are at least three ways to get the same effect in Python while loops without embedding assignments in loop tests. You can move the assignment into the loop body with a break:


while True:
    x = next(  )
    if not x: break
    ...process x...


or move the assignment into the loop with tests:


x = 1
while x:
    x = next(  )
    if x:
        ...process x...


or move the first assignment outside the loop:


x = next(  )
while x:
    ...process x...
    x = next(  )


Of these three coding patterns, the first may be considered by some to be the least structured, but it also seems to be the simplest, and most commonly used. (A simple Python for loop may replace some C loops as well.)

				30.2.4.2.3 for Loops
The for loop is a generic sequence iterator in Python: it can step through the items in any ordered sequence object. The for  statement works on strings, lists, tuples, other built-in iterables, and new objects that we'll see how to create later with classes.

				30.2.4.2.4 General Format

The Python for loop begins with a header line that specifies an assignment target (or targets), along with the object you want to step through. The header is followed by a block of (normally indented) statements that you want to repeat:


for <target> in <object>:             # Assign object items to target
    <statements>                      # Repeated loop body: use target
else:
    <statements>                      # If we didn't hit a 'break'


When Python runs a for loop, it assigns the items in the sequence object to the target one by one, and executes the loop body for each. The loop body typically uses the assignment target to refer to the current item in the sequence as though it were a cursor stepping through the sequence.

The name used as the assignment target in a for header line is usually a (possibly new) variable in the scope where the for statement is coded. There's not much special about it; it can even be changed inside the loop's body, but it will automatically be set to the next item in the sequence when control returns to the top of the loop again. After the loop, this variable normally still refers to the last item visited, which is the last item in the sequence, unless the loop exits with a break statement.

The for statement also supports an optional else block, which works exactly as it does in a while loopâit's executed if the loop exits without running into a break statement (i.e., if all items in the sequence have been visited). The break and continue statements introduced earlier also work the same in a for loop as they do in a while. The for loop's complete format can be described this way:


for <target> in <object>:             # Assign object items to target
    <statements>
    if <test>: break                  # Exit loop now, skip else
    if <test>: continue               # Go to top of loop now
else:
    <statements>                      # If we didn't hit a 'break'

				30.2.4.2.5 Examples

Let's type a few for loops interactively now, so you can see how they are used in practice.

					30.2.4.2.5.1 Basic usage

As mentioned earlier, a for loop can step across any kind of sequence object. In our first example, for instance, we'll assign the name x to each of the three items in a list in turn, from left to right, and the print statement will be executed for each. Inside the print statement (the loop body), the name x refers to the current item in the list:


>>> for x in ["spam", "eggs", "ham"]:
...     print x,
...
spam eggs ham


As noted in Chapter 11, the trailing comma in the print statement is responsible for making all of these strings show up on the same output line.

The next two examples compute the sum and product of all the items in a list. Later in this chapter and book, we'll meet tools that apply operations such as + and * to items in a list automatically, but it's usually just as easy to use a for:


>>> sum = 0
>>> for x in [1, 2, 3, 4]:
...     sum = sum + x
...
>>> sum
10
>>> prod = 1
>>> for item in [1, 2, 3, 4]: prod *= item
...
>>> prod
24

					30.2.4.2.5.2 Other data types

Any sequence works in a for, as it's a generic tool. For example, for loops work on strings and tuples:


>>> S = "lumberjack" 
>>> T = ("and", "I'm", "okay") 

>>> for x in S: print x,  # Iterate over a string
...
l u m b e r j a c k

>>> for x in T: print x,  # Iterate over a tuple
...
and I'm okay


In fact, as we'll see in a moment, for loops can even work on some objects that are not sequences at all!

					30.2.4.2.5.3 Tuple assignment in for

If you're iterating through a sequence of tuples, the loop target itself can actually be a tuple of targets. This is just another case of the tuple-unpacking assignment at work. Remember, the for loop assigns items in the sequence object to the target, and assignment works the same everywhere:


>>> T = [(1, 2), (3, 4), (5, 6)] 
>>> for (a, b) in T:  # Tuple assignment at work
...     print a, b 
...
1 2
3 4
5 6


Here, the first time through the loop is like writing (a,b) = (1,2), the second time is like writing (a,b) = (3,4), and so on. This isn't a special case; any assignment target works syntactically after the word for.

					30.2.4.2.5.4 Nested for loops

Now, let's look at something a bit more sophisticated. The next example illustrates the loop else clause in a for, and statement nesting. Given a list of objects (items) and a list of keys (tests), this code searches for each key in the objects list, and reports on the search's outcome:


>>> items = ["aaa", 111, (4, 5), 2.01]  # A set of objects
>>> tests = [(4, 5), 3.14]  # Keys to search for
>>>
>>> for key in tests:  # For all keys
...     for item in items:  # For all items
...         if item == key:  # Check for match
...             print key, "was found" 
...             break 
...     else: 
...         print key, "not found!" 
...
(4, 5) was found
|3.14 not found!


Because the nested if runs a break when a match is found, the loop else clause can assume that if it is reached, the search has failed. Notice the nesting here. When this code runs, there are two loops going at the same time: the outer loop scans the keys list, and the inner loop scans the items list for each key. The nesting of the loop else clause is critical; it's indented to the same level as the header line of the inner for loop, so it's associated with the inner loop (not the if or the outer for).

Note that this example is easier to code if we employ the in operator to test membership. Because in implicitly scans a list looking for a match, it replaces the inner loop:


>>> for key in tests:  # For all keys
...     if key in items:  # Let Python
check for a match
...         print key, "was found" 
...     else: 
...         print key, "not found!" 
...
(4, 5) was found
|3.14 not found!


In general, it's a good idea to let Python do as much of the work as possible, as in this solution, for the sake of brevity and performance.

The next example performs a typical data-structure task with a forâcollecting common items in two sequences (strings). It's roughly a simple set intersection routine; after the loop runs, res refers to a list that contains all the items found in seq1 and seq2:


>>> seq1 = "spam" 
>>> seq2 = "scam" 
>>>
>>> res = []  # Start empty
>>> for x in seq1:  # Scan first sequence
...     if x in seq2:  # Common item?
...         res.append(x)  # Add to result end
...
>>> res 
['s', 'a', 'm']


Unfortunately, this code is equipped to work only on two specific variables: seq1 and seq2. It would be nice if this loop could somehow be generalized into a tool you could use more than once. As you'll see, that simple idea leads us to functions, the topic of the next part of the book.

Why You Will Care: File Scanners

In general, loops come in handy anywhere you need to repeat an operation or process something more than once. Because files contain multiple characters and lines, they are one of the more typical use cases for loops. To load a file's contents into a string all at once, you simply call read:


file = open('test.txt', 'r')
print file.read(  )


But to load a file in pieces, it's common to code either a while loop with breaks on end-of-file, or a for loop. To read by characters, either of the following codings will suffice:


file = open('test.txt')
while True:
    char = file.read(1)          # Read by character
    if not char: break
    print char,

for char in open('test.txt').read(  ):
    print char


The for here also processes each character, but it loads the file into memory all at once. To read by lines or blocks with a while loop, use code like this:


file = open('test.txt')
while True:
    line = file.readline(  )       # Read line by line
    if not line: break
    print line,

file = open('test.txt', 'rb')
while True:
    chunk = file.read(10)        # Read byte chunks
    if not chunk: break
    print chunk,


To read text files line by line, though, the for loop tends to be easiest to code and the quickest to run:


for line in open('test.txt').readlines(  ):
    print line

for line in open('test.txt').xreadlines(  ):
    print line

for line in open('test.txt'):
    print line


readlines loads a file all at once into a line-string list, while xreadlines instead loads lines on demand to avoid filling memory for large files; the last example here relies on file iterators to achieve the equivalent of xreadlines (iterators are covered in the next section). The name open in all of the above can also be replaced with file as of Python 2.2. See the library manual for more on the calls used here. As a general rule of thumb, the more data you read on each step, the faster your program will run.

				30.2.4.2.4 Iterators: A First Look
In the prior section, I mentioned that the for loop can work on any sequence type in Python, including lists, tuples, and strings, like this:


>>> for x in [1, 2, 3, 4]: print x ** 2,
...
1 4 9 16

>>> for x in (1, 2, 3, 4): print x ** 3,
...
1 8 27 64

>>> for x in 'spam': print x * 2,
...
ss pp aa mm


Actually, the for loop turns out to be even more generic than thisâit works on any iterable object. In fact, this is true of all iteration tools that scan objects from left to right in Python, including for loops, list comprehensions, in membership tests, and the map built-in function.

The concept of "iterable objects" is relatively new in Python. It's essentially a generalization of the notion of sequencesâan object is considered iterable if it is either a physically stored sequence, or an object that produces one result at a time in the context of an iteration tool like a for loop. In a sense, iterable objects include both physical sequences and virtual sequences computed on demand.

					30.2.4.2.4.1 File Iterators

One of the easiest ways to understand what this means is to look at how it works with a built-in type such as the file. Recall that open file objects have a method called readline, which reads one line of text from a file at a timeâeach time we call the readline method, we advance to the next line. At the end of the file, the empty string is returned, which we can detect to break out of the loop:


>>> f = open('script1.py')
>>> f.readline(  )
'import sys\n'
>>> f.readline(  )
'print sys.path\n'
>>> f.readline(  )
'x = 2\n'
>>> f.readline(  )
'print 2 ** 33\n'
>>> f.readline(  )
''


Today, files also have a method named next that has a nearly identical effectâit returns the next line from a file each time it is called. The only noticeable difference is that next raises a built-in StopIteration exception at end-of-file instead of returning an empty string:


>>> f = open('script1.py')
>>> f.next(  )
'import sys\n'
>>> f.next(  )
'print sys.path\n'
>>> f.next(  )
'x = 2\n'
>>> f.next(  )
'print 2 ** 33\n'
>>> f.next(  )
Traceback (most recent call last):
  File "<pyshell#330>", line 1, in <module>
    f.next(  )
StopIteration


This interface is exactly what we call the iteration protocol in Pythonâan object with a next method to advance to a next result, which raises StopIteration at the end of the series of results. Any such object is considered iterable in Python. Any such object may also be stepped through with a for loop or other iteration tool because all iteration tools work internally by calling next on each iteration and catching the StopIteration exception to determine when to exit.

The net effect of this magic is that, as mentioned in Chapter 9, the best way to read a text file line by line today is not to read it at allâinstead, allow the for loop to automatically call next to advance to the next line on each iteration. The following, for example, reads a file line by line (printing the uppercase version of each line along the way) without ever explicitly reading from the file at all:


>>> for line in open('script1.py'):  # Use file iterators
...     print line.upper(  ), 
...
IMPORT SYS
PRINT SYS.PATH
X = 2
PRINT 2 ** 33


This is considered the best way to read text files by lines today, for three reasons: it's the simplest to code, the quickest to run, and the best in terms of memory usage. The older, original way to achieve the same effect with a for loop is to call the file readlines method to load the file's content into memory as a list of line strings:


>>> for line in open('script1.py').readlines(  ):
...     print line.upper(  ),
...
IMPORT SYS
PRINT SYS.PATH
X = 2
PRINT 2 ** 33


This readlines technique still works, but it is not best practice today, and performs poorly in terms of memory usage. In fact, because this version really does load the entire file into memory all at once, it will not even work for files too big to fit into the memory space available on your computer. On the other hand, because it reads one line at a time, the iterator-based version is immune to such memory-explosion issues. Moreover, the iterator version has been greatly optimized by Python, so it should run faster as well.

As mentioned in the earlier sidebar "Why You Will Care: File Scanners," it's also possible to read a file line by line with a while loop:


>>> f = open('script1.py') 
>>> while True: 
...     line = f.readline(  ) 
...     if not line: break 
...     print line.upper(  ), 
...
...same output ...


However, this will likely run slower than the iterator-based for loop version because iterators run at C language speed inside Python, whereas the while loop version runs Python byte code through the Python virtual machine. Any time we trade Python code for C code, speed tends to increase.

					30.2.4.2.4.2 Other Built-in Type Iterators

Technically, there is one more piece to the iteration protocol. When the for loop begins, it obtains an iterator from the iterable object by passing it to the iter built-in function; the object returned has the required next method. This becomes obvious if we look at how for loops internally process built-in sequence types such as lists:


>>> L = [1, 2, 3] 
>>> I = iter(L)  # Obtain an iterator object
>>> I.next(  )  # Call next to advance to next item
1
>>> I.next(  ) 
2
>>> I.next(  ) 
3
>>> I.next(  ) 
Traceback (most recent call last):
  File "<pyshell#343>", line 1, in <module>
    I.next(  )
StopIteration


Besides files and physical sequences like lists, other types have useful iterators as well. The classic way to step through the keys of a dictionary, for example, is to request its keys list explicitly:


>>> D = {'a':1, 'b':2, 'c':3}
>>> for key in D.keys(  ):
...     print key, D[key]
...
a 1
c 3
b 2


In recent versions of Python, though, we no longer need to call the keys methodâdictionaries have an iterator that automatically returns one key at a time in an iteration context, so they do not require that the keys list be physically created in memory all at once. Again, the effect is to optimize execution speed, memory use, and coding effort:


>>> for key in D:
...     print key, D[key]
...
a 1
c 3
b 2


					30.2.4.2.4.3 Other Iteration Contexts

So far, I've been demonstrating iterators in the context of the for loop statement, which is one of the main subjects of this chapter. Keep in mind, though, that every tool that scans from left to right across objects uses the iteration protocol. This includes the for loops we've seen:


>>> for line in open('script1.py'):  # Use file iterators
...     print line.upper(  ), 
...
IMPORT SYS
PRINT SYS.PATH
X = 2
PRINT 2 ** 33


However, list comprehensions, the in membership test, the map built-in function, and other built-ins, such as the sorted and sum calls, also leverage the iteration protocol:


>>> uppers = [line.upper(  ) for line in open('script1.py')]
>>> uppers
['IMPORT SYS\n', 'PRINT SYS.PATH\n', 'X = 2\n', 'PRINT 2 ** 33\n']

>>> map(str.upper, open('script1.py'))
['IMPORT SYS\n', 'PRINT SYS.PATH\n', 'X = 2\n', 'PRINT 2 ** 33\n']

>>> 'y = 2\n' in open('script1.py')
False
>>> 'x = 2\n' in open('script1.py')
True

>>> sorted(open('script1.py'))
['import sys\n', 'print 2 ** 33\n', 'print sys.path\n', 'x = 2\n']


The map call used here, which we'll meet in the next part of this book, is a tool that applies a function call to each item in an iterable object; it's similar to list comprehensions, but more limited because it requires a function instead of an arbitrary expression. Because list comprehensions are related to for loops, we'll explore these again later in this chapter, as well as in the next part of the book.

We saw the sorted function used here at work in Chapter 4. sorted is a relatively new built-in that employs the iteration protocolâit's like the original list sort method, but it returns the new sorted list as a result, and runs on any iterable object. Other newer built-in functions support the iteration protocol as well. For example, the sum call computes the sum of all the numbers in any iterable, and the any and all built-ins return True if any or all items in an iterable are True, respectively:


>>> sorted([3, 2, 4, 1, 5, 0])  # More iteration contexts
[0, 1, 2, 3, 4, 5]
>>> sum([3, 2, 4, 1, 5, 0]) 
15
>>> any(['spam', '', 'ni']) 
True
>>> all(['spam', '', 'ni']) 
False


Interestingly, the iteration protocol is even more pervasive in Python today than the examples so far have demonstratedâeverything in Python's built-in toolset that scans an object from left to right is defined to use the iteration protocol on the subject object. This even includes more esoteric tools such as the list and tuple built-in functions (which build new objects from iterables), the string join method (which puts a substring between strings contained in an iterable), and even sequence assignments. Because of that, all of these will also work on an open file, and automatically read one line at a time:


>>> list(open('script1.py'))
['import sys\n', 'print sys.path\n', 'x = 2\n', 'print 2 ** 33\n']

>>> tuple(open('script1.py'))
('import sys\n', 'print sys.path\n', 'x = 2\n', 'print 2 ** 33\n')

>>> '&&'.join(open('script1.py'))
'import sys\n&&print sys.path\n&&x = 2\n&&print 2 ** 33\n'

>>> a, b, c, d = open('script1.py')
>>> a, d
('import sys\n', 'print 2 ** 33\n')


					30.2.4.2.4.4 User-Defined Iterators

I'll have more to say about iterators in Chapter 17, in conjunction with functions, and in Chapter 24, when we study classes. As you'll see later, it's possible to turn a user-defined function into an iterable object by using yield statements; list comprehensions can also support the protocol today with generator expressions, and user-defined classes can be made iterable with the __iter__ or __getitem__ operator overloading method. User-defined iterators allow arbitrary objects and operations to be used in any of the iteration contexts we've met here.

				30.2.4.2.5 Loop Coding Techniques
The for loop subsumes most counter-style loops. It's enerally simpler to code and quicker to run than a while, so it's the first tool you should reach for whenever you need to step through a sequence. But there are also situations where you will need to iterate in more specialized ways. For example, what if you need to visit every second or third item in a list, or change the list along the way? How about traversing more than one sequence in parallel, in the same for loop?

You can always code such unique iterations with a while loop and manual indexing, but Python provides two built-ins that allow you to specialize the iteration in a for:

    * The built-in range function returns a list of successively higher integers, which can be used as indexes in a for.[38]

          [38] * Python today also provides a built-in called xrange that generates indexes one at a time instead of storing all of them in a list at once like range does. There's no speed advantage to xrange, but it's useful as a space optimization if you have to generate a huge number of values. At this writing, however, it seems likely that xrange may disappear in Python 3.0 altogether, and that range may become a generator object that supports the iteration protocol to produce one item at a time, instead of all at once in a list; check the 3.0 release notes for future developments on this front.

    * The built-in zip function returns a list of parallel-item tuples, which can be used to traverse multiple sequences in a for.

Because for loops typically run quicker than while-based counter loops, it's to your advantage to use tools that allow you to use for when possible. Let's look at each of these built-ins in turn.

					30.2.4.2.5.1 Counter Loops: while and range

The range function is really a general tool that can be used in a variety of contexts. Although it's used most often to generate indexes in a for, you can use it anywhere you need a list of integers:


>>> range(5), range(2, 5), range(0, 10, 2)
([0, 1, 2, 3, 4], [2, 3, 4], [0, 2, 4, 6, 8])


With one argument, range generates a list of integers from zero up to but not including the argument's value. If you pass in two arguments, the first is taken as the lower bound. An optional third argument can give a step; if used, Python adds the step to each successive integer in the result (steps default to 1). Ranges can also be nonpositive and nonascending, if you want them to be:


>>> range(â5, 5)
[â5, â4, â3, â2, â1, 0, 1, 2, 3, 4]

>>> range(5, â5, â1)
[5, 4, 3, 2, 1, 0, â1, â2, â3, â4]


Although such range results may be useful all by themselves, they tend to come in most handy within for loops. For one thing, they provide a simple way to repeat an action a specific number of times. To print three lines, for example, use a range to generate the appropriate number of integers:


>>> for i in range(3):
...     print i, 'Pythons'
...
0 Pythons
1 Pythons
2 Pythons


range is also commonly used to iterate over a sequence indirectly. The easiest and fastest way to step through a sequence exhaustively is always with a simple for, as Python handles most of the details for you:


>>> X = 'spam' 
>>> for item in X: print item,  # Simple iteration
...
s p a m


Internally, the for loop handles the details of the iteration automatically when used this way. If you really need to take over the indexing logic explicitly, you can do it with a while loop:


>>> i = 0 
>>> while i < len(X):  # while loop iteration
...     print X[i],; i += 1 
...
s p a m


But, you can also do manual indexing with a for, if you use range to generate a list of indexes to iterate through:


>>> X 
'spam'
>>> len(X)  # Length of string
4
>>> range(len(X))  # All legal offsets into X
[0, 1, 2, 3]
>>>
>>> for i in range(len(X)): print X[i],  # Manual for indexing
...
s p a m


The example here is stepping over a list of offsets into X, not the actual items of X; we need to index back into X within the loop to fetch each item.

					30.2.4.2.5.2 Nonexhaustive Traversals: range

The last example in the prior section works, but it probably runs more slowly than it has to. It's also more work than we need to do. Unless you have a special indexing requirement, you're always better off using the simple for loop form in Pythonâuse for instead of while whenever possible, and don't resort to range calls in for loops except as a last resort. This simpler solution is better:


>>> for item in X: print item,  # Simple iteration
...


However, the coding pattern used in the prior example does allow us to do more specialized sorts of traversalsâfor instance, to skip items as we go:


>>> S = 'abcdefghijk'
>>> range(0, len(S), 2)
[0, 2, 4, 6, 8, 10]

>>> for i in range(0, len(S), 2): print S[i],
...
a c e g i k


Here, we visit every second item in the string S by stepping over the generated range list. To visit every third item, change the third range argument to be 3, and so on. In effect, using range this way lets you skip items in loops while still retaining the simplicity of the for.

Still, this is probably not the ideal best-practice technique in Python today. If you really want to skip items in a sequence, the extended three-limit form of the slice expression, presented in Chapter 7, provides a simpler route to the same goal. To visit every second character in S, for example, slice with a stride of 2:


>>> for x in S[::2]: print x
...

>>> for i in  range(10)[::3]: print i
... 
0
3
6
9

					30.2.4.2.5.3 Changing Lists: range

Another common place where you may use the range and for combination is in loops that change a list as it is being traversed. Suppose, for example, that you need to add 1 to every item in a list for some reason. Trying this with a simple for loop does something, but probably not what you want:


>>> L = [1, 2, 3, 4, 5]

>>> for x in L:
...     x += 1
...
>>> L
[1, 2, 3, 4, 5]
>>> x
6


This doesn't quite workâit changes the loop variable x, not the list L. The reason is somewhat subtle. Each time through the loop, x refers to the next integer already pulled out of the list. In the first iteration, for example, x is integer 1. In the next iteration, the loop body sets x to a different object, integer 2, but it does not update the list where 1 originally came from.

To really change the list as we march across it, we need to use indexes so we can assign an updated value to each position as we go. The range/len combination can produce the required indexes for us:


>>> L = [1, 2, 3, 4, 5] 

>>> for i in range(len(L)):  # Add one to each item in L
...     L[i] += 1  # Or L[i] = L[i] + 1
...
>>> L 
[2, 3, 4, 5, 6]


When coded this way, the list is changed as we proceed through the loop. There is no way to do the same with a simple for x in L:-style loop here because such a loop iterates through actual items, not list positions. But what about the equivalent while loop? Such a loop requires a bit more work on our part, and likely runs more slowly:


>>> i = 0
>>> while i < len(L):
...     L[i] += 1
...     i += 1
...
>>> L
[3, 4, 5, 6, 7]


Here again, the range solution may not be ideal. A list comprehension expression of the form [x+1 for x in L] would do similar work, albeit without changing the original list in-place (we could assign the expression's new list object result back to L, but this would not update any other references to the original list). Because this is such a central looping concept, we'll revisit list comprehensions later in this chapter.

					30.2.4.2.5.4 Parallel Traversals: zip and map

As we've seen, the range built-in allows us to traverse sequences with for in a nonexhaustive fashion. In the same spirit, the built-in zip function allows us to use for loops to visit multiple sequences in parallel. In basic operation, zip takes one or more sequences as arguments, and returns a list of tuples that pair up parallel items taken from those sequences. For example, suppose we're working with two lists:


>>> L1 = [1,2,3,4]
>>> L2 = [5,6,7,8]


To combine the items in these lists, we can use zip to create a list of tuple pairs:


>>> zip(L1,L2)
[(1, 5), (2, 6), (3, 7), (4, 8)]


Such a result may be useful in other contexts as well, but when wedded with the for loop, it supports parallel iterations:


>>> for (x, y) in zip(L1, L2):
...     print x, y, '--', x+y
...
1 5 -- 6
2 6 -- 8
3 7 -- 10
4 8 -- 12


Here, we step over the result of the zip callâthat is, the pairs of items pulled from the two lists. Notice that this for loop uses tuple assignment again to unpack each tuple in the zip result. The first time through, it's as though we ran the assignment statement (x, y) = (1, 5).

The net effect is that we scan both L1 and L2 in our loop. We could achieve a similar effect with a while loop that handles indexing manually, but it would require more typing, and would likely be slower than the for/zip approach.

The zip function is more general than this example suggests. For instance, it accepts any type of sequence (really, any iterable object, including files), and more than two arguments:


>>> T1, T2, T3 = (1,2,3), (4,5,6), (7,8,9)
>>> T3
(7, 8, 9)
>>> zip(T1,T2,T3)
[(1, 4, 7), (2, 5, 8), (3, 6, 9)]


zip truncates result tuples at the length of the shortest sequence when the argument lengths differ:


>>> S1 = 'abc'
>>> S2 = 'xyz123'
>>>
>>> zip(S1, S2)
[('a', 'x'), ('b', 'y'), ('c', 'z')]


The related (and older) built-in map function pairs items from sequences in a similar fashion, but it pads shorter sequences with None if the argument lengths differ:


>>> map(None, S1, S2)
[('a', 'x'), ('b', 'y'), ('c', 'z'), (None, '1'), (None, '2'), (None,'3')]


This example is actually using a degenerate form of the map built-in. Normally, map takes a function, and one or more sequence arguments, and collects the results of calling the function with parallel items taken from the sequences.

When the function argument is None (as here), it simply pairs items, like zip. map and similar function-based tools are covered in Chapter 17.

						30.2.4.2.5.4.1 Dictionary construction with zip

In Chapter 8, I suggested that the zip call used here can also be handy for generating dictionaries when the sets of keys and values must be computed at runtime. Now that we're becoming proficient with zip, I'll explain how it relates to dictionary construction. As you've learned, you can always create a dictionary by coding a dictionary literal, or by assigning to keys over time:


>>> D1 = {'spam':1, 'eggs':3, 'toast':5}
>>> D1
{'toast': 5, 'eggs': 3, 'spam': 1}

>>> D1 = {}
>>> D1['spam']  = 1
>>> D1['eggs']  = 3
>>> D1['toast'] = 5


What to do, though, if your program obtains dictionary keys and values in lists at runtime, after you've coded your script? For example, say you had the following keys and values lists:


>>> keys = ['spam', 'eggs', 'toast']
>>> vals = [1, 3, 5]


One solution for turning those lists into a dictionary would be to zip the lists and step through them in parallel with a for loop:


>>> zip(keys, vals)
[('spam', 1), ('eggs', 3), ('toast', 5)]

>>> D2 = {}
>>> for (k, v) in zip(keys, vals): D2[k] = v
...
>>> D2
{'toast': 5, 'eggs': 3, 'spam': 1}


It turns out, though, that in Python 2.2 and later, you can skip the for loop altogether and simply pass the zipped keys/values lists to the built-in dict constructor call:


>>> keys = ['spam', 'eggs', 'toast']
>>> vals = [1, 3, 5]

>>> D3 = dict(zip(keys, vals))
>>> D3
{'toast': 5, 'eggs': 3, 'spam': 1}


The built-in name dict is really a type name in Python (you'll learn more about type names, and subclassing them, in Chapter 26). Calling it achieves something like a list-to-dictionary conversion, but it's really an object construction request. Later in this chapter, we'll explore a related but richer concept, the list comprehension, which builds lists in a single expression.

						30.2.4.2.5.5 Generating Both Offsets and Items: enumerate

Earlier, we discussed using range to generate the offsets of items in a string, rather than the items at those offsets. In some programs, though, we need both: the item to use, plus an offset as we go. Traditionally, this was coded with a simple for loop that also kept a counter of the current offset:


>>> S = 'spam'
>>> offset = 0
>>> for item in S:
...     print item, 'appears at offset', offset
...     offset += 1
...
s appears at offset 0
p appears at offset 1
a appears at offset 2
m appears at offset 3


This works, but in more recent Python releases, a new built-in named enumerate does the job for us:


>>> S = 'spam'
>>> for (offset, item) in enumerate(S):
...     print item, 'appears at offset', offset
...
s appears at offset 0
p appears at offset 1
a appears at offset 2
m appears at offset 3


The enumerate function returns a generator objectâa kind of object that supports the iteration protocol we met earlier in this chapter, and will discuss in more detail in the next part of the book. It has a next method that returns an (index, value) tuple each time through the list, which we can unpack with tuple assignment in the for (much like using zip):


>>> E = enumerate(S)
>>> E.next(  )
(0, 's')
>>> E.next(  )
(1, 'p')


As usual, we don't normally see this machinery because iteration contextsâincluding list comprehensions, the subject of the next sectionârun the iteration protocol automatically:


>>> [c * i for (i, c) in enumerate(S)]
['', 'p', 'aa', 'mmm']

						30.2.4.2.5.6 List Comprehensions: A First Look
In the prior section, we learned how to use range to change a list as we step across it:


>>> L = [1, 2, 3, 4, 5]

>>> for i in range(len(L)):
...     L[i] += 10
...
>>> L
[11, 12, 13, 14, 15]


This works, but as I mentioned, it may not be the optimal "best-practice" approach in Python. Today, the list comprehension expression makes many such prior use cases obsolete. Here, for example, we can replace the loop with a single expression that produces the desired result list:


>>> L = [x + 10 for x in L]
>>> L
[21, 22, 23, 24, 25]


The net result is the same, but it requires less coding on our part, and probably runs substantially faster. The list comprehension isn't exactly the same as the for loop statement version because it makes a new list object (which might matter if there are multiple references to the original list), but it's close enough for most applications, and is a common and convenient enough approach to merit a closer look here.

							30.2.4.2.5.6.1 List Comprehension Basics

We first met the list comprehension in Chapter 4. Syntactically, list comprehensions' syntax is derived from a construct in set theory notation that applies an operation to each item in a set, but you don't have to know set theory to use them. In Python, most people find that a list comprehension simply looks like a backward for loop.

Let's look at the prior section's example in more detail. List comprehensions are written in square brackets because they are ultimately a way to construct a new list. They begin with an arbitrary expression that we make up, which uses a loop variable that we make up (x + 10). That is followed by what you should now recognize as the header of a for loop, which names the loop variable, and an iterable object (for x in L).

To run the expression, Python executes an iteration across L inside the interpreter, assigning x to each item in turn, and collects the results of running items through the expression on the left side. The result list we get back is exactly what the list comprehension saysâa new list containing x + 10, for every x in L.

Technically speaking, list comprehensions are never really required because we can always build up a list of expression results manually with for loops that append results as we go:


>>> res = []
>>> for x in L:
...     res.append(x + 10)
...
>>> res
[21, 22, 23, 24, 25]


In fact, this is exactly what the list comprehension does internally.

However, list comprehensions are more concise to write, and because this code pattern of building up result lists is so common in Python work, they turn out to be very handy in many contexts. Moreover, list comprehensions can run much faster than manual for loop statements (in fact, often roughly twice as fast) because their iterations are performed at C language speed inside the interpreter, rather than with manual Python code; especially for larger data sets, there is a major performance advantage to using them.

							30.2.4.2.5.6.2 Using List Comprehensions on Files

Let's work through another common use case for list comprehensions to explore them in more detail. Recall that the file object has a readlines method that loads the file into a list of line strings all at once:


>>> f = open('script1.py')
>>> lines = f.readlines(  )
>>> lines
['import sys\n', 'print sys.path\n', 'x = 2\n', 'print 2 ** 33\n']


This works, but the lines in the result all include the newline character (\n) at the end. For many programs, the newline character gets in the wayâwe have to be careful to avoid double-spacing when printing, and so on. It would be nice if we could get rid of these newlines all at once, wouldn't it?

Any time we start thinking about performing an operation on each item in a sequence, we're in the realm of list comprehensions. For example, assuming the variable lines is as it was in the prior interaction, the following code does the job by running each line in the list through the string rstrip method to remove whitespace on the right side (a line[:â1] slice would work, too, but only if we can be sure all lines are properly terminated):


>>> lines = [line.rstrip(  ) for line in lines]
>>> lines
['import sys', 'print sys.path', 'x = 2', 'print 2 ** 33']


This works, but because list comprehensions are another iteration context just like simple for loops, we don't even have to open the file ahead of time. If we open it inside the expression, the list comprehension will automatically use the iteration protocol we met earlier in this chapter. That is, it will read one line from the file at a time by calling the file's next method, run the line through the rstrip expression, and add it to the result list. Again, we get what we ask forâthe rstrip result of a line, for every line in the file:


>>> lines = [line.rstrip(  ) for line in open('script1.py')]
>>> lines
['import sys', 'print sys.path', 'x = 2', 'print 2 ** 33']


This expression does a lot implicitly, but we're getting a lot of work for free hereâPython scans the file and builds a list of operation results automatically. It's also an efficient way to code this operation: because most of this work is done inside the Python interpreter, it is likely much faster than an equivalent for statement. Again, especially for large files, the speed advantages of list comprehensions can be significant.

							30.2.4.2.5.6.3 Extended List Comprehension Syntax

In fact, list comprehensions can be even more advanced in practice. As one useful extension, the for loop nested in the expression can have an associated if clause to filter out of the result items for which the test is not true.

For example, suppose we want to repeat the prior example, but we need to collect only lines that begin with the letter p (perhaps the first character on each line is an action code of some sort). Adding an if filter clause to our expression does the trick:


>>> lines = [line.rstrip(  ) for line in open('script1.py') if line[0] == 'p']
>>> lines
['print sys.path', 'print 2 ** 33']

					  


Here, the if clause checks each line read from the file, to see whether its first character is p; if not, the line is omitted from the result list. This is a fairly big expression, but it's easy to understand if we translate it to its simple for loop statement equivalent (in general, we can always translate a list comprehension to a for statement by appending as we go and further indenting each successive part):


>>> res = []
>>> for line in open('script1.py'):
...     if line[0] == 'p':
...         res.append(line.rstrip(  ))
...
>>> res
['print sys.path', 'print 2 ** 33']


This for statement equivalent works, but it takes up four lines instead of one, and probably runs substantially slower.

List comprehensions can become even more complex if we need them toâfor instance, they may also contain nested loops, coded as a series of for clauses. In fact, their full syntax allows for any number of for clauses, each of which can have an optional associated if clause (we'll be more formal about their syntax in Chapter 17).

For example, the following builds a list of the concatenation of x + y for every x in one string, and every y in another. It effectively collects the permutation of the characters in two strings:


>>> [x + y for x in 'abc' for y in 'lmn']
['al', 'am', 'an', 'bl', 'bm', 'bn', 'cl', 'cm', 'cn']


Again, one way to understand this expression is to convert it to statement form by indenting its parts. The following is an equivalent, but likely slower, alternative way to achieve the same effect:


>>> res = []
>>> for x in 'abc':
...     for y in 'lmn':
...         res.append(x + y)
...
>>> res
['al', 'am', 'an', 'bl', 'bm', 'bn', 'cl', 'cm', 'cn']


Beyond this complexity level, though, list comprehension expressions can become too compact for their own good. In general, they are intended for simple types of iterations; for more involved work, a simpler for statement structure will probably be easier to understand and modify in the future. As usual in programming, if something is difficult for you to understand, it's probably not a good idea.

We'll revisit iterators and list comprehensions in Chapter 17, in the context of functional programming tools; as we'll see, they turn out to be just as related to functions as they are to looping statements.

				30.2.4.2.7 Chapter Summary
In this chapter, we explored Python's looping statements as well as some concepts related to looping in Python. We looked at the while and for loop statements in depth, and learned about their associated else clauses. We also studied the break and continue statements, which have meaning only inside loops.

Additionally, we took our first substantial look at the iteration protocol in Pythonâa way for nonsequence objects to take part in iteration loopsâand at list comprehensions. As we saw, list comprehensions, which apply expressions to all the items in any iterable object, are similar to for loops.

This wraps up our tour of specific procedural statements. The next chapter closes out this part of the book by discussing documentation options for Python code. Documentation is also part of the general syntax model, and it's an important component of well-written programs. In the next chapter, we'll also dig into a set of exercises for this part of the book before we turn our attention to larger structures such as functions. As always, though, before moving on, first exercise what you've picked up here with a quiz.

BRAIN BUILDER

Chapter Quiz

Q:	When is a loop's else clause executed?
Q:	How can you code a counter-based loop in Python?
Q:	How are for loops and iterators related?
Q:	How are for loops and list comprehensions related?
Q:	Name four iteration contexts in the Python language.
Q:	What is the best way to read line by line from a text file today?
Q:	What sort of weapons would you expect to see employed by the Spanish Inquisition?


Quiz Answers

A:	The else clause in a while or for loop will be run once as the loop is exiting, if the loop exits normally (without running into a break statement). A break exits the loop immediately, skipping the else part on the way out (if there is one).
A:	Counter loops can be coded with a while statement that keeps track of the index manually, or with a for loop that uses the range built-in function to generate successive integer offsets. Neither is the preferred way to work in Python, if you need to simply step across all the items in a sequenceâuse a simple for loop instead, without range or counters, whenever possible. It will be easier to code, and usually quicker to run.
A:	The for loop uses the iteration protocol to step through items in the object across which it is iterating. It calls the object's next method on each iteration, and catches the StopIteration exception to determine when to stop looping.
A:	Both are iteration tools. List comprehensions are a concise and efficient way to perform a common for loop task: collecting the results of applying an expression to all items in an iterable object. It's always possible to translate a list comprehension to a for loop, and part of the list comprehension expression looks like the header of a for loop syntactically.
A:	Iteration contexts in Python include the for loop; list comprehensions; the map built-in function; the in membership test expression; and the built-in functions sorted, sum, any, and all. This category also includes the list and tuple built-ins, string join methods, and sequence assignments, all of which use the iteration protocol (the next method) to step across iterable objects one item at a time.
A:	The best way to read lines from a text file today is to not read it explicitly at all: instead, open the file within an iteration context such as a for loop or list comprehension, and let the iteration tool automatically scan one line at a time by running the file's next method on each iteration. This approach is generally best in terms of coding simplicity, execution speed, and memory space requirements.
A:	I'll accept any of the following as correct answers: fear, intimidation, nice red uniforms, a comfy couch, and soft pillows.

		30.2.5. The Documentation Interlude
This chapter concludes Part 3  with a look at techniques and tools used for documenting Python code. Although Python code is designed to be readable, a few well-placed human-readable comments can do much to help others understand the workings of your programs. Python includes syntax and tools to make documentation easier.

Although this is something of a tools-related concept, the topic is presented here partly because it involves Python's syntax model, and partly as a resource for readers struggling to understand Python's toolset. For the latter purpose, I'll expand here on documentation pointers first given in Chapter 4. As usual, this chapter ends with some warnings about common pitfalls, a chapter quiz, and a set of exercises for this part of the text.

			30.2.5.1 Python Documentation Sources
By this point in the book, you're probably starting to realize that Python comes with an amazing amount of prebuilt functionalitybuilt-in functions and exceptions, predefined object attributes and methods, standard library modules, and more. Moreover, we've really only scratched the surface of each of these categories.

One of the first questions that bewildered beginners often ask is: how do I find information on all the built-in tools? This section provides hints on the various documentation sources available in Python. It also presents documentation strings (docstrings), and the PyDoc system that makes use of them. These topics are somewhat peripheral to the core language itself, but they become essential knowledge as soon as your code reaches the level of the examples and exercises in this part of the book.

As summarized in Table 14-1, there are a variety of places to look for information on Python with generally increasing verbosity. Because documentation is such a crucial tool in practical programming, we'll explore each of these categories in the sections that follow.

Table 14-1. Python documentation sources
Form 	Role
# comments 	In-file documentation
The dir function 	Lists of attributes available in objects
Docstrings: __doc__ 	In-file documentation attached to objects
PyDoc: The help function 	Interactive help for objects
PyDoc: HTML reports 	Module documentation in a browser
Standard manual set 	Official language and library descriptions
Web resources 	Online tutorials, examples, and so on
Published books 	Commercially available reference texts

				30.2.5.1.1 # Comments

Hash-mark comments are the most basic way to document your code. Python simply ignores all the text following a # (as long as it's not inside a string literal), so you can follow this character with words and descriptions meaningful to programmers. Such comments are accessible only in your source files, though; to code comments that are more widely available, use docstrings.

In fact, current best practice generally dictates that docstings are best for larger functional documentation (e.g., "my file does this"), and # comments are best limited to smaller code documentation (e.g., "this strange expression does this"). More on docstrings in a moment.

				30.2.5.1.2 The dir Function

The built-in dir function is an easy way to grab a list of all the attributes available inside an object (i.e., its methods and simpler data items). It can be called on any object that has attributes. For example, to find out what's available in the standard library's sys module, import it, and pass it to dir:
Code View: Scroll / Show All


>>> import sys 
>>> dir(sys) 
['__displayhook__', '__doc__', '__excepthook__', '__name__',
'__stderr__', '__stdin__', '__stdout__', '_getframe', 'argv',
'builtin_module_names', 'byteorder', 'copyright', 'displayhook',
'dllhandle', 'exc_info', 'exc_type', 'excepthook',
...more names omitted... ]

					    


Only some of the many names are displayed here; run these statements on your machine to see the full list.

To find out what attributes are provided in built-in object types, run dir on a literal of the desired type. For example, to see list and string attributes, you can pass empty objects:


>>> dir([]) 
['__add__', '__class__', ...more... 
'append', 'count', 'extend', 'index', 'insert', 'pop', 'remove',
'reverse', 'sort']

>>> dir('') 
['__add__', '__class__', ...more... 
'capitalize', 'center', 'count', 'decode', 'encode', 'endswith',
'expandtabs', 'find', 'index', 'isalnum', 'isalpha', 'isdigit',
'islower', 'isspace', 'istitle', 'isupper', 'join', 'ljust',
...more names omitted... ]


dir results for any built-in type include a set of attributes that are related to the implementation of that type (technically, operator overloading methods); they all begin and end with double underscores to make them distinct, and you can safely ignore them at this point in the book.

Incidentally, you can achieve the same effect by passing a type name to dir instead of a literal:


>>> dir(str) == dir('')  # Same result as prior example
True
>>> dir(list) == dir([]) 
True


This works because functions like str and list that were once type converters are actually names of types in Python today; calling one of these invokes its constructor to generate an instance of that type. I'll have more to say about constructors and operator overloading methods when we discuss classes in Part 6.

The dir function serves as a sort of memory-joggerit provides a list of attribute names, but it does not tell you anything about what those names mean. For such extra information, we need to move on to the next documentation source.

				30.2.5.1.3 Docstrings: __doc__

Besides # comments, Python supports documentation that is automatically attached to objects, and retained at runtime for inspection. Syntactically, such comments are coded as strings at the tops of module files and function and class statements, before any other executable code (# comments are OK before them). Python automatically stuffs the string, known as a docstring, into the __doc__ attribute of the corresponding object.

					30.2.5.1.3.1 User-defined docstrings

For example, consider the following file, docstrings.py. Its docstrings appear at the beginning of the file and at the start of a function and a class within it. Here, I've used triple-quoted block strings for multiline comments in the file and the function, but any sort of string will work. We haven't studied the def or class statements in detail yet, so ignore everything about them except the strings at their tops:


"""
Module documentation
Words Go Here
"""

spam = 40

def square(x):
    """
    function documentation
    can we have your liver then?
    """
    return x **2

class employee:
    "class documentation"
    pass

print square(4)
print square.__doc__


The whole point of this documentation protocol is that your comments are retained for inspection in __doc__ attributes, after the file is imported. Thus, to display the docstrings associated with the module and its objects, we simply import the file and print their __doc__ attributes, where Python has saved the text:


>>> import docstrings
16

    function documentation
    can we have your liver then?

>>> print docstrings.__doc__

Module documentation
Words Go Here

>>> print docstrings.square.__doc__

    function documentation
    can we have your liver then?

>>> print docstrings.employee.__doc__
    class documentation


Note that you will generally want to explicitly say print to docstrings; otherwise, you'll get a single string with embedded newline characters.

You can also attach docstrings to methods of classes (covered later), but because these are just def statements nested in classes, they're not a special case. To fetch the docstring of a method function inside a class within a module, follow the path, and go through the class: module.class.method.__doc__ (see the example of method docstrings in Chapter 25).

					30.2.5.1.3.2 Docstring standards

There is no broad standard about what should go into the text of a docstring (although some companies have internal standards). There have been various markup language and template proposals (e.g., HTML or XML), but they don't seem to have caught on in the Python world. And, frankly, convincing programmers to document their code using handcoded HTML is probably not going to happen in our lifetimes!

Documentation tends to have a low priority amongst programmers in general. Usually, if you get any comments in a file at all, you count yourself lucky. I strongly encourage you to document your code liberally, thoughit really is an important part of well-written code. The point here is that there is presently no standard on the structure of docstrings; if you want to use them, feel free to do so.
14.1.3.3. Built-in docstrings

As it turns out, built-in modules and objects in Python use similar techniques to attach documentation above and beyond the attribute lists returned by dir. For example, to see an actual human-readable description of a built-in module, import it and print its __doc__ string:


>>> import sys 
>>> print sys.__doc__ 
This module provides access to some objects
used or maintained by the interpreter and to
...more text omitted... 

Dynamic objects:

argv -- command line arguments; argv[0] is the script pathname if known
path -- module search path; path[0] is the script directory, else ''
modules -- dictionary of loaded modules
...more text omitted... 


Functions, classes, and methods within built-in modules have attached descriptions in their __doc__ attributes as well:


>>> print sys.getrefcount.__doc__ 
getrefcount(object) -> integer

Return the current reference count for the object.
...more text omitted... 


You can also read about built-in functions via their docstrings:


>>> print int.__doc__ 
int(x[, base]) -> integer

Convert a string or number to an integer, if possible.
...more text omitted... 

>>> print file.__doc__ 
file(name[, mode[, buffering]]) -> file object

Open a file.  The mode can be 'r', 'w' or 'a' for reading
...more text omitted... 


You can get a wealth of information about built-in tools by inspecting their docstrings this way, but you don't have tothe help function, the topic of the next section, does this automatically for you.

				30.2.5.1.4 PyDoc: The help Function

The docstring technique has proved to be so useful that Python now ships with a tool that makes them even easier to display. The standard PyDoc tool is Python code that knows how to extract docstrings together with automatically extracted structural information, and format them into nicely arranged reports of various types.

There are a variety of ways to launch PyDoc, including command-line script options (see the Python library manual for details). Perhaps the two most prominent PyDoc interfaces are the built-in help function, and the PyDoc GUI/HTML interface. The help function invokes PyDoc to generate a simple textual report (which looks much like a "manpage" on Unix-like systems):


>>> import sys 
>>> help(sys.getrefcount) 
Help on built-in function getrefcount:

getrefcount(...)
    getrefcount(object) -> integer

    Return the current reference count for the object.
    ...more omitted... 


Note that you do not have to import sys in order to call help, but you do have to import sys to get help on sys; it expects an object reference to be passed in. For larger objects such as modules and classes, the help display is broken down into multiple sections, a few of which are shown here. Run this interactively to see the full report:


>>> help(sys) 
Help on built-in module sys:

NAME
    sys

FILE
    (built-in)

DESCRIPTION
    This module provides access to some objects used
    or maintained by the interpreter and to functions
    ...more omitted... 

FUNCTIONS
    __displayhook__ = displayhook(...)
        displayhook(object) -> None

        Print an object to sys.stdout and also save it
    ...more omitted... 
DATA
    __name__ = 'sys'
    __stderr__ = <open file '<stderr>', mode 'w' at 0x0082BEC0>
    ...more omitted... 


Some of the information in this report is docstrings, and some of it (e.g., function call patterns) is structural information that PyDoc gleans automatically by inspecting objects' internals. You can also use help on built-in functions, methods, and types. To get help for a built-in type, use the type name (e.g., dict for dictionary, str for string, list for list). You'll get a large display that describes all the methods available for that type:


>>> help(dict) 
Help on class dict in module __builtin__:

class dict(object)
 |  dict(  ) -> new empty dictionary.
 ...more omitted... 

>>> help(str.replace) 
Help on method_descriptor:

replace(...)
    S.replace (old, new[, maxsplit]) -> string

    Return a copy of string S with all occurrences
    ...more omitted... 

>>> help(ord) 
Help on built-in function ord:

ord(...)
    ord(c) -> integer

    Return the integer ordinal of a one-character string.


Finally, the help function works just as well on your modules as built-ins. Here it is reporting on the docstrings.py file coded earlier; again, some of this is docstrings, and some is information automatically extracted by inspecting objects' structures:
Code View: Scroll / Show All


>>> help(docstrings.square) 
Help on function square in module docstrings:

square(x)
    function documentation
    can we have your liver then?

>>> help(docstrings.employee) ...more omitted... 

>>> help(docstrings) 
Help on module docstrings:

NAME
    docstrings

FILE
    c:\python22\docstrings.py

DESCRIPTION
    Module documentation
    Words Go Here

CLASSES
    employee
    ...more omitted... 
FUNCTIONS
    square(x)
        function documentation
        can we have your liver then?

DATA
    __file__ = 'C:\\PYTHON22\\docstrings.pyc'
    __name__ = 'docstrings'
    spam = 40

					    
				30.2.5.1.5 PyDoc: HTML Reports

The help function is nice for grabbing documentation when working interactively. For a more grandiose display, however, PyDoc also provides a GUI interface (a simple, but portable, Python/Tkinter script), and can render its report in HTML page format, viewable in any web browser. In this mode, PyDoc can run locally or as a remote server in client/server mode; reports contain automatically created hyperlinks that allow you to click your way through the documentation of related components in your application.

To start PyDoc in this mode, you generally first launch the search engine GUI captured in Figure 14-1. You can start this either by selecting the Module Docs item in Python's Start button menu on Windows, or by launching the pydocgui.pyw script in Python's Tools directory (running pydoc.py with a -g command-line argument works, too). Enter the name of a module you're interested in, and press the Enter key; PyDoc will march down your module import search path (sys.path) looking for references to the requested module.

Figure 14-1. The Pydoc top-level search engine GUI: type a name of a module you want documentation for, press Enter, select the module, and then press "go to selected" (or use no module name, and press "open browser" to see all available modules).


Once you've found a promising entry, select it, and click "go to selected." PyDoc will spawn a web browser on your machine to display the report rendered in HTML format. Figure 14-2 shows the information PyDoc displays for the built-in glob module.

Figure 14-2. When you find a module in the Figure 14-1 GUI and press "go to selected," the module's documentation is rendered in HTML and displayed in a web browser window like this onea built-in standard library module.


Notice the hyperlinks in the Modules section of this pageyou can click these to jump to the PyDoc pages for related (imported) modules. For larger pages, PyDoc also generates hyperlinks to sections within the page.

Like the help function interface, the GUI interface works on user-defined modules as well. Figure 14-3 shows the page generated for our docstrings.py module file.

Figure 14-3. PyDoc can serve up documentation pages for both built-in and user-coded modules. Here is the page for a user-defined module, showing all its documentation strings (docstrings) extracted from the source file.


PyDoc can be customized and launched in various ways we won't cover here; see its entry in Python's standard library manual for more details. The main thing to take away from this section is that PyDoc essentially gives you implementation reports "for free"if you are good about using docstrings in your files, PyDoc does all the work of collecting and formatting them for display. PyDoc only helps for objects like functions and modules, but it provides an easy way to access a middle level of documentation for such toolsits reports are more useful than raw attribute lists, and less exhaustive than the standard manuals.

	

Cool PyDoc trick of the day: if you leave the module name empty in the top input field of the window in Figure 14-1 and press the Open Browser button, PyDoc will produce a web page containing a hyperlink to every module you can possibly import on your computer. This includes Python standard library modules, modules of third-party extensions you may have installed, user-defined modules on your import search path, and even statically or dynamically linked-in C-coded modules. Such information is hard to come by otherwise without writing code that inspects a set of module sources.

PyDoc can also be run to save the HTML documentation for a module in a file for later viewing or printing; see its documentation for pointers. Also, note that PyDoc might not work well if run on scripts that read from standard inputPyDoc imports the target module to inspect its contents, and there may be no connection for standard input text when it is run in GUI mode. Modules that can be imported without immediate input requirements will always work under PyDoc, though.

				30.2.5.1.6 Standard Manual Set

For the complete and most up-to-date description of the language and its toolset, Python's standard manuals stand ready to serve. Python's manuals ship in HTML and other formats, and are installed with the Python system on Windowsthey are available in your Start button's menu for Python, and they can also be opened from the Help menu within IDLE. You can also fetch the manual set separately from http://www.python.org in a variety of formats, or read them online at that site (follow the Documentation link). On Windows, the manuals are a compiled help file to support searches, and the online versions at Python.org include a web-based search page.

When opened, the Windows format of the manuals displays a root page like that in Figure 14-4. The two most important entries here are most likely the Library Reference (which documents built-in types, functions, exceptions, and standard library modules), and the Language Reference (which provides a formal description of language-level details). The tutorial listed on this page also provides a brief introduction for newcomers.

Figure 14-4. Python's standard manual set, available online at www.python.org, from IDLE's Help menu, and in the Windows Start button menu. It's a searchable help file on Windows, and there is a search engine for the online version. Of these, the Library manual is the one you'll want to use most of the time.


				30.2.5.1.7 Web Resources

At the official Python programming language web site (http://www.python.org), you'll find links to various Python resources, some of which cover special topics or domains. Click the Documentation link to access an online tutorial and the Beginners Guide to Python. The site also lists non-English Python resources.

You will find numerous Python wikis, blogs, web sites, and a host of other resources on the Web today. To sample the online community, try searching for "Python programming" in Google.
14.1.8. Published Books

As a final resource, you can choose from a large collection of reference books for Python. Bear in mind that books tend to lag behind the cutting edge of Python changes, partly because of the work involved in writing, and partly because of the natural delays built into the publishing cycle. Usually, by the time a book comes out, it's three or more months behind the current Python state. Unlike standard manuals, books are also generally not free.

Still, for many, the convenience and quality of a professionally published text is worth the cost. Moreover, Python changes so slowly that books are usually still relevant years after they are published, especially if their authors post updates on the Web. See the Preface for pointers to other Python books.

			30.2.5.2 Common Coding Gotchas
Before the programming exercises for this part of the book, let's run through some of the most common mistakes beginners make when coding Python statements and programs. Many of these are warnings I've thrown out earlier in this part of the book, collected here for ease of reference. You'll learn to avoid these pitfalls once you've gained a bit of Python coding experience, but a few words now might help you avoid falling into some of these traps initially:

    * Don't forget the colons. Always remember to type a : at the end of compound statement headers (the first line of an if, while, for, etc.). You'll probably forget at first (I did, and so have most of my 3,000 Python students over the years), but you can take some comfort from the fact that it will soon become an unconscious habit.
    * Start in column 1. Be sure to start top-level (unnested) code in column 1. That includes unnested code typed into module files, as well as unnested code typed at the interactive prompt.
    * Blank lines matter at the interactive prompt. Blank lines in compound statements are always ignored in module files, but when you're typing code at the interactive prompt, they end the statement. In other words, blank lines tell the interactive command line that you've finished a compound statement; if you want to continue, don't hit the Enter key at the ... prompt (or in IDLE) until you're really done.
    * Indent consistently. Avoid mixing tabs and spaces in the indentation of a block, unless you know what your text editor does with tabs. Otherwise, what you see in your editor may not be what Python sees when it counts tabs as a number of spaces. This is true in any block-structured language, not just Pythonif the next programmer has her tabs set differently, she will not understand the structure of your code. It's safer to use all tabs or all spaces for each block.
    * Don't code C in Python. A reminder for C/C++ programmers: you don't need to type parentheses around tests in if and while headers (e.g., if (X==1):). You can, if you like (any expression can be enclosed in parentheses), but they are fully superfluous in this context. Also, do not terminate all your statements with semicolons; it's technically legal to do this in Python as well, but it's totally useless unless you're placing more than one statement on a single line (the end of a line normally terminates a statement). And remember, don't embed assignment statements in while loop tests, and don't use {} around blocks (indent your nested code blocks consistently instead).
    * Use simpleforloops instead ofwhileorrange. Another reminder: a simple for loop (e.g., for x in seq:) is almost always simpler to code and quicker to run than a while- or range-based counter loop. Because Python handles indexing internally for a simple for, it can sometimes be twice as fast as the equivalent while. Avoid the temptation to count things in Python!
    * Beware of mutables in assignments. I mentioned this in Chapter 11: you need to be careful about using mutables in a multiple-target assignment (a = b = []), as well as in an augmented assignment (a += [1, 2]). In both cases, in-place changes may impact other variables. See Chapter 11 for details.
    * Don't expect results from functions that change objects in-place. We encountered this one earlier, too: in-place change operations like the list.append and list.sort methods introduced in Chapter 8 do not return values (other than None), so you should call them without assigning the result. It's not uncommon for beginners to say something like mylist = mylist.append(X) to try to get the result of an append, but what this actually does is assign mylist to None, not to the modified list (in fact, you'll lose your reference to the list altogether).

      A more devious example of this pops up when trying to step through dictionary items in a sorted fashion. It's fairly common to see code like for k in D.keys( ).sort( ):. This almost worksthe keys method builds a keys list, and the sort method orders itbut because the sort method returns None, the loop fails because it is ultimately a loop over None (a nonsequence). To code this correctly, either use the newer sorted built-in function, which returns the sorted list, or split the method calls out to statements: Ks = D.keys( ), then Ks.sort( ), and, finally, for k in Ks:. This, by the way, is one case where you'll still want to call the keys method explicitly for looping, instead of relying on the dictionary iteratorsiterators do not sort.
    * Always use parentheses to call a function. You must add parentheses after a function name to call it, whether it takes arguments or not (e.g., use function( ), not function). In Part 4, we'll see that functions are simply objects that have a special operationa call that you trigger with the parentheses.

      In classes, this problem seems to occur most often with files; it's common to see beginners type file.close to close a file, rather than file.close( ). Because it's legal to reference a function without calling it, the first version with no parentheses succeeds silently, but it does not close the file!
    * Don't use extensions or paths in imports and reloads. Omit directory paths and file suffixes in import statements (e.g., say import mod, not import mod.py). (We discussed module basics in Chapter 3, and will continue studying modules in Part 5.) Because modules may have other suffixes besides .py (.pyc, for instance), hardcoding a particular suffix is not only illegal syntax, but doesn't make sense. Any platform-specific directory path syntax comes from module search path settings, not the import statement.

			30.2.5.3 Chapter Summary
This chapter took us on a tour of program documentation conceptsboth documentation we write ourselves for our own programs, and documentation available for built-in tools. We met doctrings, explored the online and manual resources for Python reference, and learned how PyDoc's help function and web page interface provide extra sources of documentation. Because this is the last chapter in this part of the book, we also reviewed common coding mistakes to help you avoid them.

In the next part of this book, we'll start applying what we already know to larger program constructs: functions. Before moving on, however, be sure to work through the set of lab exercises for this part of the book that appear at the end of this chapter. And even before that, let's run through this chapter's quiz.


			30.2.5.4 BRAIN BUILDER

BRAIN BUILDER

Chapter Quiz

Q:	When should you use documentation strings instead of hash-mark comments?
Q:	Name three ways you can view documentation strings.
Q:	How can you obtain a list of the available attributes in an object?
Q:	How can you get a list of all available modules on your computer?
Q:	Which Python book should you purchase after this one?


Quiz Answers

A:	Documentation strings (docstrings) are considered best for larger, functional documentation, describing the use of modules, functions, classes, and methods in your code. Hash-mark comments are today best limited to micro-documentation about arcane expressions or statements. This is partly because docstrings are easier to find in a source file, but also because they can be extracted and displayed by the PyDoc system.
A:	You can see docstrings by printing an object's __doc__ attribute, by passing it to PyDoc's help function, and by selecting modules in PyDoc's GUI search engine in client/server mode. Additionally, PyDoc can be run to save a module's documentation in an HTML file for later viewing or printing.
A:	The built-in dir(X) function returns a list of all the attributes attached to any object.
A:	Run the PyDoc GUI interface, leave the module name blank, and select Open Browser; this opens a web page containing a link to every module available to your programs.
A:	Mine, of course. (Seriously, the Preface lists a few recommended follow-up books, both for reference and for application tuto

Now that you know how to code basic program logic, the following exercises will ask you to implement some simple tasks with statements. Most of the work is in exercise 4, which lets you explore coding alternatives. There are always many ways to arrange statements, and part of learning Python is learning which arrangements work better than others.

See "Section B.3" in Appendix B for the solutions.

1.
      Coding basic loops.
|         1.  Write a for loop that prints the ASCII code of each character in a string named S. Use the built-in function ord(character) to convert each character to an ASCII integer. (Test it interactively to see how it works.)
|         2.  Next, change your loop to compute the sum of the ASCII codes of all characters in a string.
|         3.  Finally, modify your code again to return a new list that contains the ASCII codes of each character in the string. Does the expression map(ord, S) have a similar effect? (Hint: see Part 4.)
|   2.  Backslash characters. What happens on your machine when you type the following code interactively?


          for i in range(50):
              print 'hello %d\n\a' % i


      Beware that if run outside of the IDLE interface, this example may beep at you, so you may not want to run it in a crowded lab. IDLE prints odd characters instead (see the backslash escape characters in Table 7-2).
|   3.

      Sorting dictionaries. In Chapter 8, we saw that dictionaries are unordered collections. Write a for loop that prints a dictionary's items in sorted (ascending) order. Hint: use the dictionary keys and list sort methods, or the newer sorted built-in function.
|   4.

      Program logic alternatives. Consider the following code, which uses a while loop and found flag to search a list of powers of 2 for the value of 2 raised to the 5th power (32). It's stored in a module file called power.py.


          L = [1, 2, 4, 8, 16, 32, 64]
          X = 5

          found = i = 0
          while not found and i < len(L):
              if 2 ** X == L[i]:
                  found = 1
              else:
                  i = i+1

          if found:
              print 'at index', i
          else:
              print X, 'not found'

          C:\book\tests> python power.py
          at index 5


      As is, the example doesn't follow normal Python coding techniques. Follow the steps outlined here to improve it (for all the transformations, you may type your code interactively, or store it in a script file run from the system command lineusing a file makes this exercise much easier):
|         1.  First, rewrite this code with a while loop else clause to eliminate the found flag and final if statement.
|         2.  Next, rewrite the example to use a for loop with an else clause, to eliminate the explicit list-indexing logic. Hint: to get the index of an item, use the list index method (L.index(X) returns the offset of the first X in list L).
|         3.  Next, remove the loop completely by rewriting the example with a simple in operator membership expression. (See Chapter 8 for more details, or type this to test: 2 in [1,2,3].)
|         4.  Finally, use a for loop and the list append method to generate the powers-of-2 list (L) instead of hardcoding a list literal.

      Deeper thoughts:
|         1.  Do you think it would improve performance to move the 2 ** X expression outside the loops? How would you code that?
|         2.  As we saw in exercise 1, Python includes a map(function, list) tool that can generate the powers-of-2 list, too: map(lambda x: 2 ** x, range(7)). Try typing this code interactively; we'll meet lambda more formally in Chapter 17.

	30.3. Functions

 	 	30.3.1. Function Basics
In Part 3, we looked at basic procedural statements in Python. Here, we'll move on to explore a set of additional statements that we can use to create functions of our own.

In simple terms, a function is a device that groups a set of statements so they can be run more than once in a program. Functions also can compute a result value and let us specify parameters that serve as function inputs, which may differ each time the code is run. Coding an operation as a function makes it a generally useful tool, which we can use in a variety of contexts.

More fundamentally, functions are the alternative to programming by cutting and pastingrather than having multiple redundant copies of an operation's code, we can factor it into a single function. In so doing, we reduce our future work radically: if the operation must be changed later, we only have one copy to update, not many.

Functions are the most basic program structure Python provides for maximizing code reuse and minimizing code redundancy. As we'll see, functions are also a design tool that lets us split complex systems into manageable parts. Table 15-1 summarizes the primary function-related tools we'll study in this part of the book.

Table 15-1. Function-related statements and expressions
Statement 	Examples
Calls 	myfunc("spam", "eggs", meat=ham)


def, return, yield 	def adder(a, b=1, *c): return a+b+c[0]


global 	def changer(  ): global x; x = 'new'


lambda 	Funcs = [lambda x: x**2, lambda x: x*3]


			30.3.1.1 Why Use Functions?

Before we get into the details, let's establish a clear picture of what functions are all about. Functions are a nearly universal program-structuring device. You may have come across them before in other languages, where they may have been called subroutines or procedures. As a brief introduction, functions serve two primary development roles:


Maximizing code reuse and minimizing redundancy

    As in most programming languages, Python functions are the simplest way to package logic you may wish to use in more than one place and more than one time. Up until now, all the code we've been writing has run immediately. Functions allow us to group and generalize code to be used arbitrarily many times later. Because they allow us to code an operation in a single place, and use it in many places, Python functions are the most basic factoring tool in the language: they allow us to reduce code redundancy in our programs, and thereby reduce maintenance effort.

Procedural decomposition

    Functions also provide a tool for splitting systems into pieces that have well-defined roles. For instance, to make a pizza from scratch, you would start by mixing the dough, rolling it out, adding toppings, baking it, and so on. If you were programming a pizza-making robot, functions would help you divide the overall "make pizza" task into chunksone function for each subtask in the process. It's easier to implement the smaller tasks in isolation than it is to implement the entire process at once. In general, functions are about procedurehow to do something, rather than what you're doing it to. We'll see why this distinction matters in Part 6.

In this part of the book, we'll explore the tools used to code functions in Python: function basics, scope rules, and argument passing, along with a few related concepts such as generators and functional tools. Because its importance begins to become more apparent at this level of coding, we'll also revisit the notion of polymorphism introduced earlier in the book. As you'll see, functions don't imply much new syntax, but they do lead us to some bigger programming ideas.

				30.3.1.1.1 Coding Functions

Although it wasn't made very formal, we've already used some functions in earlier chapters. For instance, to make a file object, we called the built-in open function; similarly, we used the len built-in function to ask for the number of items in a collection object.

In this chapter, we will explore how to write new functions in Python. Functions we write behave the same way as the built-ins we've already seen: they are called in expressions, are passed values, and return results. But writing new functions requires the application of a few additional ideas that haven't yet been introduced. Moreover, functions behave very differently in Python than they do in compiled languages like C. Here is a brief introduction to the main concepts behind Python functions, all of which we will study in this part of the book:

    * defis executable code. Python functions are written with a new statement, the def. Unlike functions in compiled languages such as C, def is an executable statementyour function does not exist until Python reaches and runs the def. In fact, it's legal (and even occasionally useful) to nest def statements inside if statements, while loops, and even other defs. In typical operation, def statements are coded in module files, and are naturally run to generate functions when a module file is first imported.
    * defcreates an object and assigns it to a name. When Python reaches and runs a def statement, it generates a new function object, and assigns it to the function's name. As with all assignments, the function name becomes a reference to the function object. There's nothing magic about the name of a functionas you'll see, the function object can be assigned to other names, stored in a list, and so on. Functions may also be created with the lambda expression (a more advanced concept deferred until a later chapter).
    * returnsends a result object back to the caller. When a function is called, the caller stops until the function finishes its work, and returns control to the caller. Functions that compute a value send it back to the caller with a return statement; the returned value becomes the result of the function call. Functions known as generators may also use the yield statement to send back a value and suspend their state such that they may be resumed later; this is another advanced topic covered later in this part of the book.
    * Arguments are passed by assignment (object reference). In Python, arguments are passed to functions by assignment (which, as we've learned, means by object reference). As you'll see, Python's model isn't really equivalent to C's passing rules or C++'s reference parametersthe caller and function share objects by references, but there is no name aliasing. Changing an argument name does not also change a name in the caller, but changing passed-in mutable objects can change objects shared by the caller.
    * globaldeclares module-level variables that are to be assigned. By default, all names assigned in a function, are local to that function and exist only while the function runs. To assign a name in the enclosing module, functions need to list it in a global statement. More generally, names are always looked up in scopesplaces where variables are storedand assignments bind names to scopes.
    * Arguments, return values, and variables are not declared. As with everything in Python, there are no type constraints on functions. In fact, nothing about a function needs to be declared ahead of time: you can pass in arguments of any type, return any kind of object, and so on. As one consequence, a single function can often be applied to a variety of object typesany objects that sport a compatible interface (methods and expressions) will do, regardless of their specific type.

If some of the preceding words didn't sink in, don't worrywe'll explore all of these concepts with real code in this part of the book. Let's get started by expanding on some of these ideas and looking at a few examples.

				30.3.1.1.2 def Statements

The def statement creates a function object and assigns it to a name. Its general format is as follows:


def <name>(arg1, arg2,... argN):
    <statements>


As with all compound Python statements, def consists of a header line followed by a block of statements, usually indented (or a simple statement after the colon). The statement block becomes the function's bodythat is, the code Python executes each time the function is called.

The def header line specifies a function name that is assigned the function object, along with a list of zero or more arguments (sometimes called parameters) in parentheses. The argument names in the header are assigned to the objects passed in parentheses at the point of call.

Function bodies often contain a return statement:


def <name>(arg1, arg2,... argN):
    ...
    return <value>


The Python return statement can show up anywhere in a function body; it ends the function call, and sends a result back to the caller. The return statement consists of an object expression that gives the function's result. The return statement is optional; if it's not present, the function exits when the control flow falls off the end of the function body. Technically, a function without a return statement returns the None object automatically, but this return value is usually ignored.

Functions may also contain yield statements, which are designed to produce a series of values over time, but we'll defer discussion of these until we survey advanced function topics in Chapter 17.

				30.3.1.1.3 def Executes at Runtime

The Python def is a true executable statement: when it runs, it creates and assigns a new function object to a name. (Remember, all we have in Python is runtime; there is no such thing as a separate compile time.) Because it's a statement, a def can appear anywhere a statement caneven nested in other statements. For instance, although defs normally are run when the module enclosing them is imported, it's also completely legal to nest a function def inside an if statement to select between alternative definitions:


if test:
    def func(  ):          # Define func this way
        ...
else:
    def func(  ):          # Or else this way
        ...
...
func(  )                   # Call the version selected and built


One way to understand this code is to realize that the def is much like an = statement: it simply assigns a name at runtime. Unlike in compiled languages such as C, Python functions do not need to be fully defined before the program runs. More generally, defs are not evaluated until they are reached and run, and the code inside defs is not evaluated until the functions are later called.

Because function definition happens at runtime, there's nothing special about the function name. What's important is the object to which it refers:


othername = func           # Assign function object
othername(  )              # Call func again


Here, the function was assigned to a different name and called through the new name. Like everything else in Python, functions are just objects; they are recorded explicitly in memory at program execution time.

			30.3.1.2 A First Example: Definitions and Calls

Apart from such runtime concepts (which tend to seem most unique to programmers with backgrounds in traditional compiled languages), Python functions are straightforward to use. Let's code a first real example to demonstrate the basics. As you'll see, there are two sides to the function picture: a definition (the def that creates a function), and a call (an expression that tells Python to run the function's body).

				30.3.1.2.1 Definition

Here's a definition typed interactively that defines a function called times, which returns the product of its two arguments:


>>> def times(x, y):  # Create and assign function
...     return x * y  # Body executed when called
...


When Python reaches and runs this def, it creates a new function object that packages the function's code and assigns the object to the name times. Typically, such a statement is coded in a module file, and runs when the enclosing file is imported; for something this small, though, the interactive prompt suffices.

				30.3.1.2.2 Calls

After the def has run, you can call (run) the function in your program by adding parentheses after the function's name. The parentheses may optionally contain one or more object arguments, to be passed (assigned) to the names in the function's header:


>>> times(2, 4)  # Arguments in parentheses
8


This expression passes two arguments to times. As mentioned previously, arguments are passed by assignment, so, in this case, the name x in the function header is assigned the value 2, y is assigned the value 4, and the function's body is run. For this function, the body is just a return statement that sends back the result as the value of the call expression. The returned object was printed here interactively (as in most languages, 2 * 4 is 8 in Python), but, if we needed to use it later, we could instead assign it to a variable. For example:


| >>> x = times(3.14, 4)  # Save the result object
| >>> x 
| 12.56


Now, watch what happens when the function is called a third time, with very different kinds of objects passed in:


>>> times('Ni', 4)  # Functions are "typeless"
'NiNiNiNi'


This time, our function means something completely different (Monty Python reference again intended). In this third call, a string and an integer are passed to x and y, instead of two numbers. Recall that * works on both numbers and sequences; because we never declare the types of variables, arguments, or return values in Python, we can use times to either multiply numbers or repeat sequences.

In other words, what our times function means and does depends on what we pass into it. This is a core idea in Python (and perhaps the key to using the language well), which we'll explore in the next section.

				30.3.1.2.3 Polymorphism in Python

As we just saw, the very meaning of the expression x * y in our simple times function depends completely upon the kinds of objects that x and y arethus, the same function can perform multiplication in one instance, and repetition in another. Python leaves it up to the objects to do something reasonable for the syntax. Really, * is just a dispatch to the objects being processed.

This sort of type-dependent behavior is known as polymorphism, a term we first met in Chapter 4 that essentially means that the meaning of an operation depends on the objects being operated upon. Because it's a dynamically typed language, polymorphism runs rampant in Python. In fact, every operation is a polymorphic operation in Python: printing, indexing, the * operator, and much more.

This is deliberate, and it accounts for much of the language's conciseness and flexibility. A single function, for instance, can generally be applied to a whole category of object types automatically. As long as those objects support the expected interface (a.k.a. protocol), the function can process them. That is, if the objects passed into a function have the expected methods and expression operators, they are plug-and-play compatible with the function's logic.

Even in our simple times function, this means that any two objects that support a * will work, no matter what they may be, and no matter when they are coded. This function will work on two numbers (performing multiplication), or a string and a number (performing repetition), or any other combination of objects supporting the expected interfaceeven class-based objects we have not even coded yet.

Moreover, if the objects passed in do not support this expected interface, Python will detect the error when the * expression is run, and raise an exception automatically. It's therefore pointless to code error checking ourselves. In fact, doing so would limit our function's utility, as it would be restricted to work only on objects whose types we test for.

This turns out to be a crucial philosophical difference between Python and statically typed languages like C++ and Java: in Python, your code is not supposed to care about specific data types. If it does, it will be limited to working on just the types you anticipated when you wrote it, and it will not support other compatible object types that may be coded in the future. Although it is possible to test for types with tools like the type built-in function, doing so breaks your code's flexibility. By and large, we code to object interfaces in Python, not data types.

Of course, this polymorphic model of programming means we have to test our code to detect errors, rather than providing type declarations a compiler can use to detect some types of errors for us ahead of time. In exchange for an initial bit of testing, though, we radically reduce the amount of code we have to write, and radically increase our code's flexibility. As you'll learn, it's a net win in practice.

			30.3.1.3 A Second Example: Intersecting Sequences

Let's look at a second function example that does something a bit more useful than multiplying arguments and further illustrates function basics.

In Chapter 13, we coded a for loop that collected items held in common in two strings. We noted there that the code wasn't as useful as it could be because it was set up to work only on specific variables and could not be rerun later. Of course, we could copy the code and paste it into each place where it needs to be run, but this solution is neither good nor general¿we'd still have to edit each copy to support different sequence names, and changing the algorithm would then require changing multiple copies.

				30.3.1.3.1 Definition

By now, you can probably guess that the solution to this dilemma is to package the for loop inside a function. Doing so offers a number of advantages:

    * Putting the code in a function makes it a tool that you can run as many times as you like.
    * Because callers can pass in arbitrary arguments, functions are general enough to work on any two sequences (or other iterables) you wish to intersect.
    * When the logic is packaged in a function, you only have to change code in one place if you ever need to change the way the intersection works.
    * Coding the function in a module file means it can be imported and reused by any program run on your machine.

In effect, wrapping the code in a function makes it a general intersection utility:


def intersect(seq1, seq2):
    res = []                     # Start empty
    for x in seq1:               # Scan seq1
        if x in seq2:            # Common item?
            res.append(x)        # Add to end
    return res


The transformation from the simple code of Chapter 13 to this function is straightforward; we've just nested the original logic under a def header, and made the objects on which it operates passed-in parameter names. Because this function computes a result, we've also added a return statement to send a result object back to the caller.

				30.3.1.3.2 Calls

Before you can call a function, you have to make it. To do this, run its def statement, either by typing it interactively, or by coding it in a module file and importing the file. Once you've run the def, you can call the function by passing any two sequence objects in parentheses:


>>> s1 = "SPAM" 
>>> s2 = "SCAM" 
>>> intersect(s1, s2)  # Strings
['S', 'A', 'M']


Here, we've passed in two strings, and we get back a list containing the characters in common. The algorithm the function uses is simple: "for every item in the first argument, if that item is also in the second argument, append the item to the result." It's a little shorter to say that in Python than in English, but it works out the same.

				30.3.1.3.3 Polymorphism Revisited

Like all functions in Python, intersect is polymorphic. That is, it works on arbitrary types, as long as they support the expected object interface:


>>> x = intersect([1, 2, 3], (1, 4))  # Mixed types
>>> x  # Saved result object
[1]


This time, we passed in different types of objects to our function¿a list and a tuple (mixed types)¿and it still picked out the common items. Because you don't have to specify the types of arguments ahead of time, the intersect function happily iterates through any kind of sequence objects you send it, as long as they support the expected interfaces.

For intersect, this means that the first argument has to support the for loop, and the second has to support the in membership test. Any two such objects will work, regardless of their specific types¿that includes physically stored sequences like strings and lists; all the iterable objects we met in Chapter 13, including files and dictionaries; and even any class-based objects we code that apply operator overloading techniques (we'll discuss these later, in Part 6 of this text).[39]

    [39] * Two fine points here. One, technically a file will work as the first argument to the intersect function, but not the second, because the file will have been scanned to end-of-file after the first in membership test has been run. For instance, a call such as intersect(open('data1.txt'), ['line1\n', 'line2\n', 'line3\n']) would work, but a call like intersect(open('data1.txt), open('data2.txt')) would not, unless the first file contained only one line¿for real sequences, the call to iter made by iteration contexts to obtain an iterator always restarts from the beginning, but once a file has been opened and read, its iterator is effectively exhausted. Two, note that for classes, we would probably use the newer __iter__ or older __getitem__ operator overloading methods covered in Chapter 24 to support the expected iteration protocol. If we do this, we can define and control what iteration means for our data.

Here again, if we pass in objects that do not support these interfaces (e.g., numbers), Python will automatically detect the mismatch, and raise an exception for us¿which is exactly what we want, and the best we could do on our own if we coded explicit type tests. By not coding type tests, and allowing Python to detect the mismatches for us, we both reduce the amount of code we need to write, and increase our code's flexibility.

				30.3.1.3.4 Local Variables

The variable res inside intersect is what in Python is called a local variable¿a name that is visible only to code inside the function def, and exists only while the function runs. In fact, because all names assigned in any way inside a function are classified as local variables by default, nearly all the names in intersect are local variables:

    * res is obviously assigned, so it is a local variable.
    * Arguments are passed by assignment, so seq1 and seq2 are, too.
    * The for loop assigns items to a variable, so the name x is also local.

All these local variables appear when the function is called, and disappear when the function exits¿the return statement at the end of intersect sends back the result object, but the name res goes away. To fully explore the notion of locals, though, we need to move on to Chapter 16.

			30.3.1.4 Chapter Summary

This chapter introduced the core ideas behind function definition¿the syntax and operation of the def and return statements, the behavior of function call expressions, and the notion and benefits of polymorphism in Python functions. As we saw, a def statement is executable code that creates a function object at runtime; when the function is later called, objects are passed into it by assignment (recall that assignment means object reference in Python, which, as we learned in Chapter 6, really means pointer internally), and computed values are sent back by return. We also began exploring the concepts of local variables and scopes in this chapter, but we'll save all the details on those topics for Chapter 16. First, though, a quick quiz.

BRAIN BUILDER

Chapter Quiz

Q:	What is the point of coding functions?
Q:	At what time does Python create a function?
Q:	What does a function return if it has no return statement in it?
Q:	When does the code nested inside the function definition statement run?
Q:	What's wrong with checking the types of objects passed into a function?


Quiz Answers

A:	Functions are the most basic way of avoiding code redundancy in Python¿factoring code into functions means that we have only one copy of an operation's code to update in the future. Functions are also the basic unit of code reuse in Python¿wrapping code in functions makes it a reusable tool, callable in a variety of programs. Finally, functions allow us to divide a complex system into manageable parts, each of which may be developed individually.
A:	A function is created when Python reaches and runs the def statement; this statement creates a function object, and assigns it the function's name. This normally happens when the enclosing module file is imported by another module (recall that imports run the code in a file from top to bottom, including any defs), but it can also occur when a def is typed interactively or nested in other statements, such as ifs.
A:	A function returns the None object by default if the control flow falls off the end of the function body without running into a return statement. Such functions are usually called with expression statements, as assigning their None results to variables is generally pointless.
A:	The function body (the code nested inside the function definition statement) is run when the function is later called with a call expression. The body runs anew each time the function is called.
A:	Checking the types of objects passed into a function effectively breaks the function's flexibility, constraining the function to work on specific types only. Without such checks, the function would likely be able to process an entire range of object types¿any objects that support the interface expected by the function will work. (The term interface means the set of methods and expression operators the function runs.)

			30.3.1.5

 	 	30.3.2. Scopes and Arguments

Chapter 15 introduced basic function definitions and calls. As we saw, Python's basic function model is simple to use. This chapter presents the details behind Python's scopesthe places where variables are defined and looked upand behind argument passingthe way that objects are sent to functions as inputs.

			30.3.2.1 Scope Rules

Now that you're ready to start writing your own functions, we need to get more formal about what names mean in Python. When you use a name in a program, Python creates, changes, or looks up the name in what is known as a namespacea place where names live. When we talk about the search for a name's value in relation to code, the term scope refers to a namespace: that is, the location of a name's assignment in your code determines the scope of the name's visibility to your code.

Just about everything related to names, including scope classification, happens at assignment time in Python. As we've seen, names in Python spring into existence when they are first assigned values, and they must be assigned before they are used. Because names are not declared ahead of time, Python uses the location of the assignment of a name to associate it with (i.e., bind it to) a particular namespace. In other words, the place where you assign a name in your source code determines the namespace it will live in, and hence its scope of visibility.

Besides packaging code, functions add an extra namespace layer to your programsby default, all names assigned inside a function are associated with that function's namespace, and no other. This means that:

    * Names defined inside a def can only be seen by the code within that def. You cannot even refer to such names from outside the function.
    * Names defined inside a def do not clash with variables outside the def, even if the same names are used elsewhere. A name X assigned outside a given def (i.e., in a different def or at the top level of a module file) is a completely different variable from a name X assigned inside that def.

In all cases, the scope of a variable (where it can be used) is always determined by where it is assigned in your source code, and has nothing to do with which functions call which. If a variable is assigned inside a def, it is local to that function; if assigned outside a def, it is global to the entire file. We call this lexical scoping because variable scopes are determined entirely by the locations of the variables in the source code of your program files, not by function calls.

For example, in the following module file, the X = 99 assignment creates a global variable named X (visible everywhere in this file), but the X = 88 assignment creates a local variable X (visible only within the def statement):


X = 99

def func(  ):
    X = 88


Even though both variables are named X, their scopes make them different. The net effect is that function scopes help to avoid name clashes in your programs, and help to make functions more self-contained program units.

				30.3.2.1.1 Python Scope Basics

Before we started writing functions, all the code we wrote was at the top level of a module (i.e., not nested in a def), so the names we used either lived in the module itself, or were built-ins predefined by Python (e.g., open).[40] Functions provide nested namespaces (scopes) that localize the names they use, such that names inside a function won't clash with those outside it (in a module or another function). Again, functions define a local scope, and modules define a global scope. The two scopes are related as follows:

    [40] * Code typed at the interactive command prompt is really entered into a built-in module called __main__, so interactively created names live in a module, too, and thus follow the normal scope rules. You'll learn more about modules in Part 5.

    * The enclosing module is a global scope. Each module is a global scopethat is, a namespace in which variables created (assigned) at the top level of the module file live. Global variables become attributes of a module object to the outside world, but can be used as simple variables within a module file.
    * The global scope spans a single file only. Don't be fooled by the word "global" herenames at the top level of a file are only global to code within that single file. There is really no notion of a single, all-encompassing global file-based scope in Python. Instead, names are partitioned into modules, and you must always import a module explicitly if you want to be able to use the names its file defines. When you hear "global" in Python, think "module."
    * Each call to a function creates a new local scope. Every time you call a function, you create a new local scopethat is, a namespace in which the names created inside that function will usually live. You can think of each def statement (and lambda expression) as defining a new local scope, but because Python allows functions to call themselves to loop (an advanced technique known as recursion), the local scope in fact technically corresponds to a function callin other words, each call creates a new local namespace. Recursion is useful when processing structures whose shape can't be predicted ahead of time.
    * Assigned names are local unless declared global. By default, all the names assigned inside a function definition are put in the local scope (the namespace associated with the function call). If you need to assign a name that lives at the top level of the module enclosing the function, you can do so by declaring it in a global statement inside the function.
    * All other names are enclosing locals, globals, or built-ins. Names not assigned a value in the function definition are assumed to be enclosing scope locals (in an enclosing def), globals (in the enclosing module's namespace), or built-ins (in the predefined __builtin__ module Python provides).

Note that any type of assignment within a function classifies a name as local: = statements, imports, defs, argument passing, and so on. Also, notice that in-place changes to objects do not classify names as locals; only actual name assignments do. For instance, if the name L is assigned to a list at the top level of a module, a statement like L.append(X) within a function will not classify L as a local, whereas L = X will. In the former case, L will be found in the global scope as usual, and the statement will change the global list.

				30.3.2.1.2 Name Resolution: The LEGB Rule

If the prior section sounds confusing, it really boils down to three simple rules. With a def statement:

    * Name references search at most four scopes: local, then enclosing functions (if any), then global, then built-in.
    * Name assignments create or change local names by default.
    * Global declarations map assigned names to an enclosing module's scope.

In other words, all names assigned inside a function def statement (or a lambda, an expression we'll meet later) are locals by default; functions can use names in lexically (i.e., physically) enclosing functions and in the global scope, but they must declare globals to change them. Python's name resolution scheme is sometimes called the LEGB rule, after the scope names:

    * When you use an unqualified name inside a function, Python searches up to four scopesthe local (L) scope, then the local scopes of any enclosing (E) defs and lambdas, then the global (G) scope, and then the built-in (B) scopeand stops at the first place the name is found. If the name is not found during this search, Python reports an error. As we learned in Chapter 6, names must be assigned before they can be used.
    * When you assign a name in a function (instead of just referring to it in an expression), Python always creates or changes the name in the local scope, unless it's declared to be global in that function.
    * When you assign a name outside a function (i.e., at the top level of a module file, or at the interactive prompt), the local scope is the same as the global scopethe module's namespace.

Figure 16-1 illustrates Python's four scopes. Note that the second E scope lookup layerthe scopes of enclosing defs or lambdascan technically correspond to more than one lookup layer. It only comes into play when you nest functions within functions.[41]

    [41] * The scope lookup rule was called the "LGB rule" in the first edition of this book. The enclosing def layer was added later in Python to obviate the task of passing in enclosing scope names explicitlya topic usually of marginal interest to Python beginners that we'll defer until later in this chapter.

Figure 16-1. The LEGB scope lookup rule. When a variable is referenced, Python searches for it in this order: in the local scope, in any enclosing functions' local scopes, in the global scope, and, finally, in the built-in scope. The first occurrence wins. The place in your code where a variable is assigned usually determines its scope.


Also, keep in mind that these rules only apply to simple variable names (such as spam). In Parts V and VI, we'll see that qualified attribute names (such as object.spam) live in particular objects and follow a completely different set of lookup rules than the scope ideas covered here. Attribute references (names following periods) search one or more objects, not scopes, and may invoke something called "inheritance" (discussed in Part 6).

				30.3.2.1.3 Scope Example

Let's look at a larger example that demonstrates scope ideas. Suppose we write the following code in a module file:


# Global scope
X = 99                # X and func assigned in module: global

def func(Y):          # Y and Z assigned in function: locals
    # Local scope
    Z = X + Y         # X is a global
    return Z

func(1)               # func in module: result=100


This module and the function it contains use a number of names to do their business. Using Python's scope rules, we can classify the names as follows:


Global names:X, func

    X is global because it's assigned at the top level of the module file; it can be referenced inside the function without being declared global. func is global for the same reason; the def statement assigns a function object to the name func at the top level of the module.

Local names:Y, Z

    Y and Z are local to the function (and exist only while the function runs) because they are both assigned values in the function definition: Z by virtue of the = statement, and Y because arguments are always passed by assignment.

The whole point behind this name segregation scheme is that local variables serve as temporary names that you need only while a function is running. For instance, in the preceding example, the argument Y and the addition result Z exist only inside the function; these names don't interfere with the enclosing module's namespace (or any other function, for that matter).

The local/global distinction also makes functions easier to understand, as most of the names a function uses appear in the function itself, not at some arbitrary place in a module. Also, because you can be sure that local names will not be changed by some remote function in your program, they tend to make programs easier to debug.

				30.3.2.1.4 The Built-in Scope

We've been talking about the built-in scope in the abstract, but it's a bit simpler than you may think. Really, the built-in scope is just a built-in module called __builtin__, but you have to import __builtin__ to use built-in because the name builtin is not itself built-in.

No, I'm serious! The built-in scope is implemented as a standard library module named __builtin__, but that name itself is not placed in the built-in scope, so you have to import it in order to inspect it. Once you do, you can run a dir call to see which names are predefined:


>>> import __builtin__ 
>>> dir(__builtin__) 
['ArithmeticError', 'AssertionError', 'AttributeError',
'DeprecationWarning', 'EOFError', 'Ellipsis',
    ...many more names omitted... 
'str', 'super', 'tuple', 'type', 'unichr', 'unicode',
'vars', 'xrange', 'zip']


The names in this list constitute the built-in scope in Python; roughly the first half are built-in exceptions, and the second half are built-in functions. Because Python automatically searches this module last in its LEGB lookup rule, you get all the names in this list for free; that is, you can use them without importing any modules. Thus, there are two ways to refer to a built-in functionby taking advantage of the LEGB rule, or by manually importing the __builtin__ module:


>>> zip  # The normal way
<built-in function zip>

>>> import __builtin__  # The hard way
>>> __builtin__.zip 
<built-in function zip>


The second of these approaches is sometimes useful in advanced work. The careful reader might also notice that because the LEGB lookup procedure takes the first occurrence of a name that it finds, names in the local scope may override variables of the same name in both the global and built-in scopes, and global names may override built-ins. A function can, for instance, create a local variable called open by assigning to it:


def hider(  ):
    open = 'spam'              # Local variable, hides built-in
    ...
    open('data.txt')           # This won't open a file now in this scope!


However, this will hide the built-in function called open that lives in the built-in (outer) scope. It's also usually a bug, and a nasty one at that, because Python will not issue a warning message about it (there are times in advanced programming where you may really want to replace a built-in name by redefining it in your code).[42]

    [42] * Here's another thing you can do in Python that you probably shouldn'tbecause the names True and False are just variables in the built-in scope, it's possible to reassign them with a statement like True = False. You won't break the logical consistency of the universe in so doing! This statement merely redefines the word True for the single scope in which it appears. For more fun, though, you could say __builtin__.True = False, to reset True to False for the entire Python process! This may be disallowed in the future (and it sends IDLE into a strange panic state that resets the user code process). This technique is, however, useful for tool writers who must change built-ins such as open to customized functions. Also, note that third-party tools such as PyChecker will warn about common programming mistakes, including accidental assignment to built-in names (this is known as "shadowing" a built-in in PyChecker).

Functions can similarly hide global variables of the same name with locals:


X = 88                         # Global X

def func(  ):
    X = 99                     # Local X: hides global

func(  )
print X                        # Prints 88: unchanged


Here, the assignment within the function creates a local X that is a completely different variable from the global X in the module outside the function. Because of this, there is no way to change a name outside a function without adding a global declaration to the def (as described in the next section).

			30.3.2.2 The global Statement

The global statement is the only thing that's remotely like a declaration statement in Python. It's not a type or size declaration, though; it's a namespace declaration. It tells Python that a function plans to change one or more global names¿i.e., names that live in the enclosing module's scope (namespace). We've talked about global in passing already. Here's a summary:

    * Global names are names at the top level of the enclosing module file.
    * Global names must be declared only if they are assigned in a function.
    * Global names may be referenced in a function without being declared.

The global statement consists of the keyword global, followed by one or more names separated by commas. All the listed names will be mapped to the enclosing module's scope when assigned or referenced within the function body. For instance:


X = 88                         # Global X

def func(  ):
    global X
    X = 99                     # Global X: outside def

func(  )
print X                        # Prints 99


We've added a global declaration to the example here, such that the X inside the def now refers to the X outside the def; they are the same variable this time. Here is a slightly more involved example of global at work:


y, z = 1, 2                    # Global variables in module
def all_global(  ):
    global x                   # Declare globals assigned
    x = y + z                  # No need to declare y, z: LEGB rule


Here, x, y, and z are all globals inside the function all_global. y and z are global because they aren't assigned in the function; x is global because it was listed in a global statement to map it to the module's scope explicitly. Without the global here, x would be considered local by virtue of the assignment.

Notice that y and z are not declared global; Python's LEGB lookup rule finds them in the module automatically. Also, notice that x might not exist in the enclosing module before the function runs; if not, the assignment in the function creates x in the module.

				30.3.2.2.1 Minimize Global Variables

By default, names assigned in functions are locals, so if you want to change names outside functions, you have to write extra code (global statements). This is by design¿as is common in Python, you have to say more to do the "wrong" thing. Although there are times when globals are useful, variables assigned in a def are local by default because that is normally the best policy. Changing globals can lead to well-known software engineering problems: because the variables' values are dependent on the order of calls to arbitrarily distant functions, programs can become difficult to debug.

Consider this module file, for example:


X = 99
def func1(  ):
    global X
    X = 88

def func2(  ):
    global X
    X = 77


Now, imagine that it is your job to modify or reuse this module file. What will the value of X be here? Really, that question has no meaning unless qualified with a point of reference in time¿the value of X is timing-dependent, as it depends on which function was called last (something we can't tell from this file alone).

The net effect is that to understand this code, you have to trace the flow of control through the entire program. And, if you need to reuse or modify the code, you have to keep the entire program in your head all at once. In this case, you can't really use one of these functions without bringing along the other. They are dependent (that is, coupled) on the global variable. This is the problem with globals¿they generally make code more difficult to understand and use than code consisting of self-contained functions that rely on locals.

On the other hand, short of using object-oriented programming and classes, global variables are probably the most straightforward way to retain state information (information that a function needs to remember for use the next time it is called) in Python¿local variables disappear when the function returns, but globals do not. Other techniques, such as default mutable arguments and enclosing function scopes, can achieve this, too, but they are more complex than pushing values out to the global scope for retention.

Some programs designate a single module to collect globals; as long as this is expected, it is not as harmful. Also, programs that use multithreading to do parallel processing in Python essentially depend on global variables¿they become shared memory between functions running in parallel threads, and so act as a communication device (threading is beyond this book's scope; see the follow-up texts mentioned in the Preface for more details).

For now, though, especially if you are relatively new to programming, avoid the temptation to use globals whenever you can (try to communicate with passed-in arguments and return values instead). Six months from now, both you and your coworkers will be happy you did.

				30.3.2.2.2 Minimize Cross-File Changes

Here's another scope-related issue: although we can change variables in another file directly, we usually shouldn't. Consider these two module files:


# first.py
X = 99

# second.py
import first
first.X = 88


The first defines a variable X, which the second changes by assignment. Notice that we must import the first module into the second file to get to its variable¿as we've learned, each module is a self-contained namespace (package of variables), and we must import one module to see inside it from another. Really, in terms of this chapter's topic, the global scope of a module file becomes the attribute namespace of the module object once it is imported¿importers automatically have access to all of the file's global variables, so a file's global scope essentially morphs into an object's attribute namespace when it is imported.

After importing the first module, the second module assigns its variable a new value. The problem with the assignment, however, is that it is too implicit: whoever's charged with maintaining or reusing the first module probably has no clue that some arbitrarily far-removed module on the import chain can change X out from under him. In fact, the second module may be in a completely different directory, and so difficult to find. Again, this sets up too strong a coupling between the two files¿because they are both dependent on the value of the variable X, it's difficult to understand or reuse one file without the other.

Here again, the best prescription is generally not to do this¿the best way to communicate across file boundaries is to call functions, passing in arguments, and getting back return values. In this specific case, we would probably be better off coding an accessor function to manage the change:


# first.py
X = 99

def setX(new):
    global X
    X = new

# second.py
import first
first.setX(88)


This requires more code, but it makes a huge difference in terms of readability and maintainability¿when a person reading the first module by itself sees a function, he will know that it is a point of interface, and will expect the change to the variable X. Although we cannot prevent cross-file changes from happening, common sense dictates that they should be minimized unless widely accepted across the program.

				30.3.2.2.3 Other Ways to Access Globals

Interestingly, because global-scope variables morph into the attributes of a loaded module object, we can emulate the global statement by importing the enclosing module and assigning to its attributes, as in the following example module file. Code in this file imports the enclosing module by name, and then by indexing sys.modules, the loaded modules table (more on this table in Chapter 21):
Code View: Scroll / Show All


# thismod.py

var = 99                              # Global variable == module attribute

def local(  ):
    var = 0                           # Change local var

def glob1(  ):
    global var                        # Declare global (normal)
    var += 1                          # Change global var

def glob2(  ):
    var = 0                           # Change local var
    import thismod                    # Import myself
    thismod.var += 1                  # Change global var

def glob3(  ):
    var = 0                           # Change local var
    import sys                        # Import system table
    glob = sys.modules['thismod']     # Get module object (or use __name__)
    glob.var += 1                     # Change global var

def test(  ):
    print var
    local(); glob1(); glob2(  ); glob3(  )
    print var

					    


When run, this adds 3 to the global variable (only the first function does not impact it):


>>> import thismod
>>> thismod.test(  )
99
102
>>> thismod.var
102


This works, and it illustrates the equivalence of globals to module attributes, but it's much more work than using the global statement to make your intentions explicit.

			30.3.2.3 Scopes and Nested Functions

So far, I've omitted one part of Python's scope rules (on purpose, because it's relatively rarely encountered in practice). However, it's time to take a deeper look at the letter E in the LEGB lookup rule. The E layer is fairly new (it was added in Python 2.2); it takes the form of the local scopes of any and all enclosing function defs. Enclosing scopes are sometimes also called statically nested scopes. Really, the nesting is a lexical one¿nested scopes correspond to physically nested code structures in your program's source code.

	

In Python 3.0, a proposed nonlocal statement is planned that will allow write access to variables in enclosing function scopes, much like the global statement does today for variables in the enclosing module scope. This statement will look like the global statement syntactically, but will use the word nonlocal instead. This is still a futurism, so see the 3.0 release notes for details.

				30.3.2.3.1 Nested Scope Details

With the addition of nested function scopes, variable lookup rules become slightly more complex. Within a function:

    * An assignment (X = value) creates or changes the name X in the current local scope, by default. If X is declared global within the function, it creates or changes the name X in the enclosing module's scope instead.
    * A reference (X) looks for the name X first in the current local scope (function); then in the local scopes of any lexically enclosing functions in your source code, from inner to outer; then in the current global scope (the module file); and finally in the built-in scope (the module __builtin__). global declarations make the search begin in the global (module file) scope instead.

Notice that the global declaration still maps variables to the enclosing module. When nested functions are present, variables in enclosing functions may only be referenced, not changed. To clarify these points, let's illustrate with some real code.

				30.3.2.3.2 Nested Scope Examples

Here is an example of a nested scope:


def f1(  ):
    x = 88
    def f2(  ):
        print x
    f2(  )

f1(  )                                     # Prints 88


First off, this is legal Python code: the def is simply an executable statement that can appear anywhere any other statement can¿including nested in another def. Here, the nested def runs while a call to the function f1 is running; it generates a function, and assigns it to the name f2, a local variable within f1's local scope. In a sense, f2 is a temporary function that only lives during the execution of (and is only visible to code in) the enclosing f1.

But, notice what happens inside f2: when it prints the variable x, it refers to the x that lives in the enclosing f1 function's local scope. Because functions can access names in all physically enclosing def statements, the x in f2 is automatically mapped to the x in f1, by the LEGB lookup rule.

This enclosing scope lookup works even if the enclosing function has already returned. For example, the following code defines a function that makes and returns another function:


def f1(  ):
    x = 88
    def f2(  ):
        print x
    return f2

action = f1(  )                            # Make, return function
action(  )                                 # Call it now: prints 88


In this code, the call to action is really running the function we named f2 when f1 ran. f2 remembers the enclosing scope's x in f1, even though f1 is no longer active.

					30.3.2.3.2.1 Factory functions

Depending on whom you ask, this sort of behavior is also sometimes called a closure, or factory function¿a function object that remembers values in enclosing scopes, even though those scopes may not be around any more. Although classes (described in Part 6) are usually best at remembering state because they make it explicit with attribute assignments, such functions provide another alternative.

For instance, factory functions are sometimes used by programs that need to generate event handlers on the fly in response to conditions at runtime (e.g., user inputs that cannot be anticipated). Look at the following function, for example:


>>> def maker(N):
...     def action(X):
...         return X ** N
...     return action
...


This defines an outer function that simply generates and returns a nested function, without calling it. If we call the outer function:


>>> f = maker(2)  # Pass 2 to N
>>> f 
<function action at 0x014720B0>


what we get back is a reference to the generated nested function¿the one created by running the nested def. If we now call what we got back from the outer function:


>>> f(3)  # Pass 3 to X, N remembers 2
9
>>> f(4)  # 4 ** 2
16


it invokes the nested function¿the one called action within maker. The most unusual part of this, though, is that the nested function remembers integer 2, the value of the variable N in maker, even though maker has returned and exited by the time we call action. In effect, N from the enclosing local scope is retained as state information attached to action, and we get back its argument squared.

Now, if we call the outer function again, we get back a new nested function with different state information attached¿we get the argument cubed instead of squared, but the original still squares as before:


>>> g = maker(3) 
>>> g(3)  # 3 ** 3
27
>>> f(3)  # 3 ** 2
9


This is a fairly advanced technique that you're unlikely to see very often in practice, except among programmers with backgrounds in functional programming languages (and sometimes in lambdas, as discussed ahead). In general, classes, which we'll discuss later in the book, are better at "memory" like this because they make the state retention explicit. Short of using classes, though, globals, enclosing scope references like these, and default arguments are the main ways that Python functions can retain state information. Coincidentally, defaults are the topic of the next section.

					30.3.2.3.2.2 Retaining enclosing scopes' state with defaults

In earlier versions of Python, the sort of code in the prior section failed because nested defs did not do anything about scopes¿a reference to a variable within f2 would search only the local (f2), then global (the code outside f1), and then built-in scopes. Because it skipped the scopes of enclosing functions, an error would result. To work around this, programmers typically used default argument values to pass in (remember) the objects in an enclosing scope:


def f1(  ):
    x = 88
    def f2(x=x):
        print x
    f2(  )

f1(  )                                     # Prints 88


This code works in all Python releases, and you'll still see this pattern in some existing Python code. We'll discuss defaults in more detail later in this chapter. In short, the syntax arg = val in a def header means that the argument arg will default to the value val if no real value is passed to arg in a call.

In the modified f2, the x=x means that the argument x will default to the value of x in the enclosing scope¿because the second x is evaluated before Python steps into the nested def, it still refers to the x in f1. In effect, the default remembers what x was in f1 (i.e., the object 88).

All that's fairly complex, and it depends entirely on the timing of default value evaluations. In fact, the nested scope lookup rule was added to Python to make defaults unnecessary for this role¿today, Python automatically remembers any values required in the enclosing scope, for use in nested defs.

Of course, the best prescription is simply to avoid nesting defs within defs, as it will make your programs much simpler. The following is an equivalent of the prior example that banishes the notion of nesting. Notice that it's okay to call a function defined after the one that contains the call, like this, as long as the second def runs before the call of the first function¿code inside a def is never evaluated until the function is actually called:


>>> def f1(  ):
...     x = 88
...     f2(x)
...
>>> def f2(x):
...     print x
...
>>> f1(  )
88


If you avoid nesting this way, you can almost forget about the nested scopes concept in Python, unless you need to code in the factory function style discussed earlier¿at least for def statements. lambdas, which almost naturally appear nested in defs, often rely on nested scopes, as the next section explains.

				30.3.2.3.3 Nested scopes and lambdas

While they're rarely used in practice for defs themselves, you are more likely to care about nested function scopes when you start coding lambda expressions. We won't cover lambda in depth until Chapter 17, but, in short, it's an expression that generates a new function to be called later, much like a def statement. Because it's an expression, though, it can be used in places that def cannot, such as within list and dictionary literals.

Like a def, a lambda expression introduces a new local scope. Thanks to the enclosing scopes lookup layer, lambdas can see all the variables that live in the functions in which they are coded. Thus, the following code works today, but only because the nested scope rules are now applied:


def func(  ):
    x = 4
    action = (lambda n: x ** n)          # x remembered from enclosing def
    return action

x = func(  )
print x(2)                               # Prints 16, 4 ** 2


Prior to the introduction of nested function scopes, programmers used defaults to pass values from an enclosing scope into lambdas, as for defs. For instance, the following works on all Python releases:


def func(  ):
    x = 4
    action = (lambda n, x=x: x ** n)     # Pass x in manually


Because lambdas are expressions, they naturally (and even normally) nest inside enclosing defs. Hence, they are perhaps the biggest beneficiaries of the addition of enclosing function scopes in the lookup rules; in most cases, it is no longer necessary to pass values into lambdas with defaults.

				30.3.2.3.4 Scopes versus defaults with loop variables

There is one notable exception to the rule I just gave: if a lambda or def defined within a function is nested inside a loop, and the nested function references an enclosing scope variable that is changed by the loop, all functions generated within that loop will have the same value¿the value the referenced variable had in the last loop iteration.

For instance, the following attempts to build up a list of functions that each remember the current variable i from the enclosing scope:


>>> def makeActions(  ): 
...     acts = [] 
...     for i in range(5):  # Tries to remember each i
...         acts.append(lambda x: i ** x )     # All remember same last i!
...     return acts 
...
>>> acts = makeActions(  ) 
>>> acts[0] 
<function <lambda> at 0x012B16B0>


This doesn't quite work, though¿because the enclosing variable is looked up when the nested functions are later called, they all effectively remember the same value (the value the loop variable had on the last loop iteration). That is, we get back 4 to the power of 2 for each function in the list because i is the same in all of them:


>>> acts[0](2)  # All are 4 ** 2, value of last i
16
>>> acts[2](2)  # This should be 2 ** 2
16
>>> acts[4](2)  # This should be 4 ** 2
16


This is the one case where we still have to explicitly retain enclosing scope values with default arguments, rather than enclosing scope references. That is, to make this sort of code work, we must pass in the current value of the enclosing scope's variable with a default. Because defaults are evaluated when the nested function is created (not when it's later called), each remembers its own value for i:


>>> def makeActions(  ): 
...     acts = [] 
...     for i in range(5):  # Use defaults instead
...         acts.append(lambda x, i=i: i ** x)  # Remember current i
...     return acts 
...
>>> acts = makeActions(  ) 
>>> acts[0](2)  # 0 ** 2
0
>>> acts[2](2)  # 2 ** 2
4
>>> acts[4](2)  # 4 ** 2
16


This is a fairly obscure case, but it can come up in practice, especially in code that generates callback handler functions for a number of widgets in a GUI (e.g., button press handlers). We'll talk more about both defaults and lambdas in the next chapter, so you may want to return and review this section later.[43]

    [43] * In the "Section 17.9" section for this part at the end of the next chapter, we'll also see that there is an issue with using mutable objects like lists and dictionaries for default arguments (e.g., def f(a=[]))¿because defaults are implemented as single objects, mutable defaults retain state from call to call, rather then being initialized anew on each call. Depending on whom you ask, this is either considered a feature that supports state retention, or a strange wart on the language. More on this in the next chapter.

				30.3.2.3.5 Arbitrary scope nesting

Before ending this discussion, I should note that scopes nest arbitrarily, but only enclosing functions (not classes, described in Part 6) are searched:


>>> def f1(  ): 
...     x = 99 
...     def f2(  ): 
...         def f3(  ): 
...             print x  # Found in f1's local scope!
...         f3(  ) 
...     f2(  ) 
...
>>> f1(  ) 
99


Python will search the local scopes of all enclosing defs, from inner to outer, after the referencing function's local scope, and before the module's global scope. However, this sort of code is unlikely to pop up in practice. In Python, we say flat is better than nested¿your life, and the lives of your coworkers, will generally be better if you minimize nested function definitions.

			30.3.2.4 Passing Arguments

Earlier, I noted that arguments are passed by assignment. This has a few ramifications that aren't always obvious to beginners, which I'll expand on in this section. Here is a rundown of the key points in passing arguments to functions:

    * Arguments are passed by automatically assigning objects to local names. Function arguments¿references to (possibly) shared objects referenced by the caller¿are just another instance of Python assignment at work. Because references are implemented as pointers, all arguments are, in effect, passed by pointer. Objects passed as arguments are never automatically copied.
    * Assigning to argument names inside a function doesn't affect the caller. Argument names in the function header become new, local names when the function runs, in the scope of the function. There is no aliasing between function argument names and names in the caller.
    * Changing a mutable object argument in a function may impact the caller. On the other hand, as arguments are simply assigned to passed-in objects, functions can change passed-in mutable objects, and the results may affect the caller. Mutable arguments can be input and output for functions.

For more details on references, see Chapter 6; everything we learned there also applies to function arguments, though the assignment to argument names is automatic and implicit.

Python's pass-by-assignment scheme isn't quite the same as C++'s reference parameters option, but it turns out to be very similar to the C language's argument-passing model in practice:

    * Immutable arguments are passed "by value." Objects such as integers and strings are passed by object reference instead of by copying, but because you can't change immutable objects in-place anyhow, the effect is much like making a copy.
    * Mutable arguments are passed "by pointer." Objects such as lists and dictionaries are also passed by object reference, which is similar to the way C passes arrays as pointers¿mutable objects can be changed in-place in the function, much like C arrays.

Of course, if you've never used C, Python's argument-passing mode will seem simpler still¿it just involves the assignment of objects to names, and it works the same whether the objects are mutable or not.

				30.3.2.4.1 Arguments and Shared References

Here's an example that illustrates some of these properties at work:


>>> def changer(a, b):  # Function
...    a = 2  # Changes local name's value only
...    b[0] = 'spam'  # Changes shared object in-place
...
>>> X = 1 
>>> L = [1, 2]  # Caller
>>> changer(X, L)  # Pass immutable and mutable objects
>>> X, L  # X is unchanged, L is different
(1, ['spam', 2])


In this code, the changer function assigns values to argument a, and to a component in the object referenced by argument b. The two assignments within the function are only slightly different in syntax, but have radically different results:

    * Because a is a local name in the function's scope, the first assignment has no effect on the caller¿it simply changes the local variable a, and does not change the binding of the name X in the caller.
    * b is a local name, too, but it is passed a mutable object (the list called L in the caller). As the second assignment is an in-place object change, the result of the assignment to b[0] in the function impacts the value of L after the function returns. Really, we aren't changing b, we are changing part of the object that b currently references, and this change impacts the caller.

Figure 16-2 illustrates the name/object bindings that exist immediately after the function has been called, and before its code has run.

Figure 16-2. References: arguments. Because arguments are passed by assignment, argument names may share objects with variables at the call. Hence, in-place changes to mutable arguments in a function can impact the caller. Here, a and b in the function initially reference the objects referenced by variables X and L when the function is first called. Changing the list through variable b makes L appear different after the call returns.


If this example is still confusing, it may help to notice that the effect of the automatic assignments of the passed-in arguments is the same as running a series of simple assignment statements. In terms of the first argument, the assignment has no effect on the caller:


>>> X = 1 
>>> a = X  # They share the same object
>>> a = 2  # Resets 'a' only, 'X' is still 1
>>> print X 
1


But, the assignment through the second argument does affect a variable at the call because it is an in-place object change:


>>> L = [1, 2] 
>>> b = L  # They share the same object
>>> b[0] = 'spam'  # In-place change: 'L' sees the change too
>>> print L 
['spam', 2]


If you recall our discussions about shared mutable objects in Chapter 6 and Chapter 9, you'll recognize the phenomenon at work: changing a mutable object in-place can impact other references to that object. Here, the effect is to make one of the arguments work like an output of the function.

				30.3.2.4.2 Avoiding Mutable Argument Changes

Arguments are passed to functions by reference (a.k.a. pointer) by default in Python because that is what we normally want¿it means we can pass large objects around our programs without making multiple copies along the way, and we can easily update these objects as we go. If we don't want in-place changes within functions to impact objects we pass to them, though, we can simply make explicit copies of mutable objects, as we learned in Chapter 6. For function arguments, we can always copy the list at the point of call:


L = [1, 2]
changer(X, L[:] )            # Pass a copy, so our 'L' does not change


We can also copy within the function itself, if we never want to change passed-in objects, regardless of how the function is called:


def changer(a, b):
   b = b[:]  # Copy input list so we don't impact caller
   a = 2
   b[0] = 'spam'            # Changes our list copy only


Both of these copying schemes don't stop the function from changing the object¿they just prevent those changes from impacting the caller. To really prevent changes, we can always convert to immutable objects to force the issue. Tuples, for example, throw an exception when changes are attempted:


L = [1, 2]
changer(X, tuple(L) )        # Pass a tuple, so changes are errors


This scheme uses the built-in tuple function, which builds a new tuple out of all the items in a sequence (really, any iterable). It's also something of an extreme¿because it forces the function to be written to never change passed-in arguments, this solution might impose more limitations on the function than it should, and so should generally be avoided. You never know when changing arguments might come in handy for other calls in the future. Using this technique will also make the function lose the ability to call any list-specific methods on the argument, including methods that do not change the object in-place.

The main point to remember here is that functions might update mutable objects passed into them (e.g., lists and dictionaries). This isn't necessarily a problem, and it often serves useful purposes. But, you do have to be aware of this property¿if objects change out from under you unexpectedly, check whether a called function might be responsible, and make copies when objects are passed if needed.

				30.3.2.4.3 Simulating Output Parameters

We've already discussed the return statement and used it in a few examples. Here's a neat trick: because return can send back any sort of object, it can return multiple values by packaging them in a tuple or other collection type. In fact, although Python doesn't support what some languages label "call-by-reference" argument passing, we can usually simulate it by returning tuples and assigning the results back to the original argument names in the caller:


>>> def multiple(x, y): 
...     x = 2  # Changes local names only
...     y = [3, 4] 
...     return x, y  # Return new values in a tuple
...
>>> X = 1 
>>> L = [1, 2] 
>>> X, L = multiple(X, L)  # Assign results to caller's names
>>> X, L 
(2, [3, 4])


It looks like the code is returning two values here, but it's really just one¿a two-item tuple with the optional surrounding parentheses omitted. After the call returns, we can use tuple assignment to unpack the parts of the returned tuple. (If you've forgotten why this works, flip back to "Section 4.6" in Chapter 4, and "Section 11.1" in Chapter 11.) The net effect of this coding pattern is to simulate the output parameters of other languages by explicit assignments. X and L change after the call, but only because the code said so.

			30.3.2.5 Special Argument-Matching Modes

As we've just seen, arguments are always passed by assignment in Python; names in the def header are assigned to passed-in objects. On top of this model, though, Python provides additional tools that alter the way the argument objects in a call are matched with argument names in the header prior to assignment. These tools are all optional, but they allow you to write functions that support more flexible calling patterns.

By default, arguments are matched by position, from left to right, and you must pass exactly as many arguments as there are argument names in the function header. You can also specify matching by name, default values, and collectors for extra arguments.

Some of this section gets complicated, and before we go into the syntactic details, I'd like to stress that these special modes are optional, and only have to do with matching objects to names; the underlying passing mechanism after the matching takes place is still assignment. In fact, some of these tools are intended more for people writing libraries than for application developers. But because you may stumble across these modes even if you don't code them yourself, here's a synopsis of the available matching modes:


Positionals: matched from left to right

    The normal case, which we've been using so far, is to match arguments by position.

Keywords: matched by argument name

    Callers can specify which argument in the function is to receive a value by using the argument's name in the call, with the name=value syntax.

Defaults: specify values for arguments that aren't passed

    Functions can specify default values for arguments to receive if the call passes too few values, again using the name=value syntax.

Varargs: collect arbitrarily many positional or keyword arguments

    Functions can use special arguments preceded with * characters to collect an arbitrary number of extra arguments (this feature is often referred to as varargs, after the varargs feature in the C language, which also supports variable-length argument lists).

Varargs: pass arbitrarily many positional or keyword arguments

    Callers can also use the * syntax to unpack argument collections into discrete, separate arguments. This is the inverse of a * in a function header—in the header it means collect arbitrarily many arguments, while in the call it means pass arbitrarily many arguments.

Table 16-1 summarizes the syntax that invokes the special matching modes.

Table 16-1. Function argument-matching forms
Syntax 	Location 	Interpretation
func(value) 	Caller 	Normal argument: matched by position
func(name=value) 	Caller 	Keyword argument: matched by name
func(*name) 	Caller 	Pass all objects in name as individual positional arguments
func(**name) 	Caller 	Pass all key/value pairs in name as individual keyword arguments
def func(name) 	Function 	Normal argument: matches any by position or name
def func(name=value) 	Function 	Default argument value, if not passed in the call
def func(*name) 	Function 	Matches and collects remaining positional arguments (in a tuple)
def func(**name) 	Function 	Matches and collects remaining keyword arguments (in a dictionary)

In a call (the first four rows of the table), simple names are matched by position, but using the name=value form tells Python to match by name instead; these are called keyword arguments. Using a * or ** in a call allows us to package up arbitrarily many positional or keyword objects in sequences and dictionaries, respectively.

In a function header, a simple name is matched by position or name (depending on how the caller passes it), but the name=value form specifies a default value. The *name form collects any extra unmatched positional arguments in a tuple, and the **name form collects extra keyword arguments in a dictionary.

Of these, keyword arguments and defaults are probably the most commonly used in Python code. Keywords allow us to label arguments with their names to make calls more meaningful. We met defaults earlier, as a way to pass in values from the enclosing function's scope, but they actually are more general than that—they allow us to make any argument optional, and provide its default value in a function definition.

Special matching modes let you be fairly liberal about how many arguments must be passed to a function. If a function specifies defaults, they are used if you pass too few arguments. If a function uses the * variable argument list forms, you can pass too many arguments; the * names collect the extra arguments in a data structure.

				30.3.2.5.1 Keyword and Default Examples

This is all simpler in code than the preceding descriptions may imply. Python matches names by position by default like most other languages. For instance, if you define a function that requires three arguments, you must call it with three arguments:


>>> def f(a, b, c): print a, b, c
...


Here, we pass them by position—a is matched to 1, b is matched to 2, and so on:


>>> f(1, 2, 3)
1 2 3


					30.3.2.5.1.1 Keywords

In Python, though, you can be more specific about what goes where when you call a function. Keyword arguments allow us to match by name, instead of by position:


>>> f(c=3, b=2, a=1)
1 2 3


The c=3 in this call, for example, means send 3 to the argument named c. More formally, Python matches the name c in the call to the argument named c in the function definition's header, and then passes the value 3 to that argument. The net effect of this call is the same as that of the prior call, but notice that the left-to-right order of the arguments no longer matters when keywords are used because arguments are matched by name, not by position. It's even possible to combine positional and keyword arguments in a single call. In this case, all positionals are matched first from left to right in the header, before keywords are matched by name:


>>> f(1, c=3, b=2)
1 2 3


When most people see this the first time, they wonder why one would use such a tool. Keywords typically have two roles in Python. First, they make your calls a bit more self-documenting (assuming that you use better argument names than a, b, and c). For example, a call of this form:


func(name='Bob', age=40, job='dev')


is much more meaningful than a call with three naked values separated by commas—the keywords serve as labels for the data in the call. The second major use of keywords occurs in conjunction with defaults, which we'll look at next.

					30.3.2.5.1.2 Defaults

We talked a little about defaults earlier, when discussing nested function scopes. In short, defaults allow us to make selected function arguments optional; if not passed a value, the argument is assigned its default before the function runs. For example, here is a function that requires one argument, and defaults two:


>>> def f(a, b=2, c=3): print a, b, c
...


When we call this function, we must provide a value for a, either by position or by keyword; however, providing values for b and c is optional. If we don't pass values to b and c, they default to 2 and 3, respectively:


>>> f(1)
1 2 3
>>> f(a=1)
1 2 3


If we pass two values, only c gets its default, and with three values, no defaults are used:


>>> f(1, 4)
1 4 3
>>> f(1, 4, 5)
1 4 5


Finally, here is how the keyword and default features interact. Because they subvert the normal left-to-right positional mapping, keywords allow us to essentially skip over arguments with defaults:


>>> f(1, c=6)
1 2 6


Here, a gets 1 by position, c gets 6 by keyword, and b, in between, defaults to 2.

Be careful not to confuse the special name=value syntax in a function header and a function call; in the call, it means a match-by-name keyword argument, and in the header, it specifies a default for an optional argument. In both cases, this is not an assignment statement; it is special syntax for these two contexts, which modifies the default argument-matching mechanics.

				30.3.2.5.2 Arbitrary Arguments Examples

The last two matching extensions, * and **, are designed to support functions that take any number of arguments. Both can appear in either the function definition, or a function call, and they have related purposes in the two locations.

					30.3.2.5.2.1 Collecting arguments

The first use, in the function definition, collects unmatched positional arguments into a tuple:


>>> def f(*args): print args
...


When this function is called, Python collects all the positional arguments into a new tuple, and assigns the variable args to that tuple. Because it is a normal tuple object, it can be indexed, stepped through with a for loop, and so on:


>>> f(  )
(  )
>>> f(1)
(1,)
>>> f(1,2,3,4)
(1, 2, 3, 4)


The ** feature is similar, but it only works for keyword arguments—it collects them into a new dictionary, which can then be processed with normal dictionary tools. In a sense, the ** form allows you to convert from keywords to dictionaries, which you can then step through with keys calls, dictionary iterators, and the like:


>>> def f(**args): print args
...
>>> f(  )
{  }
>>> f(a=1, b=2)
{'a': 1, 'b': 2}


Finally, function headers can combine normal arguments, the *, and the ** to implement wildly flexible call signatures:


>>> def f(a, *pargs, **kargs): print a, pargs, kargs
...
>>> f(1, 2, 3, x=1, y=2)
1 (2, 3) {'y': 2, 'x': 1}


In fact, these features can be combined in even more complex ways that may seem ambiguous at first glance—an idea we will revisit later in this chapter. First, though, let's see what happens when * and ** are coded in function calls instead of definitions.

					30.3.2.5.2.2 Unpacking arguments

In recent Python releases, we can use the * syntax when we call a function, too. In this context, its meaning is the inverse of its meaning in the function definition—it unpacks a collection of arguments, rather than building a collection of arguments. For example, we can pass four arguments to a function in a tuple, and let Python unpack them into individual arguments:


>>> def func(a, b, c, d): print a, b, c, d
...
>>> args = (1, 2)
>>> args += (3, 4)
>>> func(*args)
1 2 3 4


Similarly, the ** syntax in a function call unpacks a dictionary of key/value pairs into separate keyword arguments:


>>> args = {'a': 1, 'b': 2, 'c': 3}
>>> args['d'] = 4
>>> func(**args)
1 2 3 4


Again, we can combine normal, positional, and keyword arguments in the call in very flexible ways:


>>> func(*(1, 2), **{'d': 4, 'c': 4})
1 2 4 4

>>> func(1, *(2, 3), **{'d': 4})
1 2 3 4

>>> func(1, c=3, *(2,), **{'d': 4})
1 2 3 4


This sort of code is convenient when you cannot predict the number of arguments to be passed to a function when you write your script; you can build up a collection of arguments at runtime instead, and call the function generically this way. Again, don't confuse the */** syntax in the function header and the function call—in the header, it collects any number of arguments, and in the call, it unpacks any number of arguments.

We'll revisit this form in the next chapter, when we meet the apply built-in function (a tool that this special call syntax is largely intended to subsume and replace).


																																	     */

				30.3.2.5.3 Combining Keywords and Defaults

Here is a slightly larger example that demonstrates keywords and defaults in action. In the following, the caller must always pass at least two arguments (to match spam and eggs), but the other two are optional. If they are omitted, Python assigns toast and ham to the defaults specified in the header:


def func(spam, eggs, toast=0, ham=0):   # First 2 required
    print (spam, eggs, toast, ham)

func(1, 2)                              # Output: (1, 2, 0, 0)
func(1, ham=1, eggs=0)                  # Output: (1, 0, 0, 1)
func(spam=1, eggs=0)                    # Output: (1, 0, 0, 0)
func(toast=1, eggs=2, spam=3)           # Output: (3, 2, 1, 0)
func(1, 2, 3, 4)                        # Output: (1, 2, 3, 4)


Notice again that when keyword arguments are used in the call, the order in which the arguments are listed doesn't matter; Python matches by name, not by position. The caller must supply values for spam and eggs, but they can be matched by position or by name. Also, notice that the form name=value means different things in the call and the def (a keyword in the call and a default in the header).

				30.3.2.5.4 The min Wakeup Call

To make this more concrete, let's work through an exercise that demonstrates a practical application of argument-matching tools. Suppose you want to code a function that is able to compute the minimum value from an arbitrary set of arguments and an arbitrary set of object data types. That is, the function should accept zero or more arguments—as many as you wish to pass. Moreover, the function should work for all kinds of Python object types: numbers, strings, lists, lists of dictionaries, files, and even None.

The first requirement provides a natural example of how the * feature can be put to good use—we can collect arguments into a tuple, and step over each in turn with a simple for loop. The second part of the problem definition is easy: because every object type supports comparisons, we don't have to specialize the function per type (an application of polymorphism); we can simply compare objects blindly, and let Python perform the correct sort of comparison.

					30.3.2.5.4.1 Full credit

The following file shows three ways to code this operation, at least one of which was suggested by a student at some point along the way:

    * The first function fetches the first argument (args is a tuple), and traverses the rest by slicing off the first (there's no point in comparing an object to itself, especially if it might be a large structure).
    * The second version lets Python pick off the first and rest of the arguments automatically, and so avoids an index and a slice.
    * The third converts from a tuple to a list with the built-in list call, and employs the list sort method.

The sort method is coded in C, so it can be quicker than the others at times, but the linear scans of the first two techniques will make them faster most of the time.[44] The file mins.py contains the code for all three solutions:

    [44] * Actually, this is fairly complicated. The Python sort routine is coded in C, and uses a highly optimized algorithm that attempts to take advantage of partial ordering in the items to be sorted. It's named "timsort" after Tim Peters, its creator, and in its documentation it claims to have "supernatural performance" at times (pretty good, for a sort!). Still, sorting is an inherently exponential operation (it must chop up the sequence, and put it back together many times), and the other versions simply perform one linear, left to right scan. The net effect is that sorting is quicker if the arguments are partially ordered, but likely slower otherwise. Even so, Python performance can change over time, and the fact that sorting is implemented in the C language can help greatly; for an exact analysis, you should time the alternatives with the time or timeit modules we'll meet in the next chapter.


def min1(*args):
    res = args[0]
    for arg in args[1:]:
        if arg < res:
            res = arg
    return res

def min2(first, *rest):
    for arg in rest:
        if arg < first:
            first = arg
    return first

def min3(*args):
    tmp = list(args)            # Or, in Python 2.4+: return sorted(args)[0]
    tmp.sort(  )
    return tmp[0]

print min1(3,4,1,2)
print min2("bb", "aa")
print min3([2,2], [1,1], [3,3])

					    


All three solutions produce the same result when the file is run. Try typing a few calls interactively to experiment with these on your own:


% python mins.py
1
aa
[1, 1]


Notice that none of these three variants tests for the case where no arguments are passed in. They could, but there's no point in doing so here—in all three solutions, Python will automatically raise an exception if no arguments are passed in. The first raises an exception when we try to fetch item 0; the second, when Python detects an argument list mismatch; and the third, when we try to return item 0 at the end.

This is exactly what we want to happen—because these functions support any data type, there is no valid sentinel value that we could pass back to designate an error. There are exceptions to this rule (e.g., if you have to run expensive actions before you reach the error), but, in general, it's better to assume that arguments will work in your functions' code, and let Python raise errors for you when they do not.

					30.3.2.5.4.2 Bonus points

Students and readers can get bonus points here for changing these functions to compute the maximum, rather than minimum, values. This one's easy: the first two versions only require changing < to >, and the third simply requires that we return tmp[−1] instead of tmp[0]. For extra points, be sure to set the function name to "max" as well (though this part is strictly optional).

It's also possible to generalize a single function to compute either a minimum or a maximum value, by evaluating comparison expression strings with a tool like the eval built-in function (see the library manual), or passing in an arbitrary comparison function. The file minmax.py shows how to implement the latter scheme:


def minmax(test, *args):
    res = args[0]
    for arg in args[1:]:
        if test(arg, res):
            res = arg
    return res

def lessthan(x, y): return x < y                 # See also: lambda
def grtrthan(x, y): return x > y

print minmax(lessthan, 4, 2, 1, 5, 6, 3)         # Self-test code
print minmax(grtrthan, 4, 2, 1, 5, 6, 3)

% python minmax.py 
1
6


Functions are another kind of object that can be passed into a function like this one. To make this a max (or other) function, for example, we could simply pass in the right sort of test function. This may seem like extra work, but the main point of generalizing functions this way (instead of cutting and pasting to change just a single character) means we'll only have one version to change in the future, not two.

					30.3.2.5.4.3 The punch line

Of course, all this was just a coding exercise. There's really no reason to code min or max functions because both are built-ins in Python! The built-in versions work almost exactly like ours, but they're coded in C for optimal speed.

				30.3.2.5.5 A More Useful Example: General Set Functions

Now, let's look at a more useful example of special argument-matching modes at work. At the end of the prior chapter, we wrote a function that returned the intersection of two sequences (it picked out items that appeared in both). Here is a version that intersects an arbitrary number of sequences (one or more), by using the varargs matching form *args to collect all the passed-in arguments. Because the arguments come in as a tuple, we can process them in a simple for loop. Just for fun, we'll code a union function that also accepts an arbitrary number of arguments to collect items that appear in any of the operands:


def intersect(*args):
    res = []
    for x in args[0]:                  # Scan first sequence
        for other in args[1:]:         # For all other args
            if x not in other: break   # Item in each one?
        else:                          # No: break out of loop
            res.append(x)              # Yes: add items to end
    return res

def union(*args):
    res = []
    for seq in args:                   # For all args
        for x in seq:                  # For all nodes
            if not x in res:
                res.append(x)          # Add new items to result
    return res


Because these are tools worth reusing (and they're too big to retype interactively), we'll store the functions in a module file called inter2.py (more on modules in Part 5). In both functions, the arguments passed in at the call come in as the args tuple. As in the original intersect, both work on any kind of sequence. Here, they are processing strings, mixed types, and more than two sequences:


% python 
>>> from inter2 import intersect, union 
>>> s1, s2, s3 = "SPAM", "SCAM", "SLAM" 

>>> intersect(s1, s2), union(s1, s2)  # Two operands
(['S', 'A', 'M'], ['S', 'P', 'A', 'M', 'C'])

>>> intersect([1,2,3], (1,4))  # Mixed types
[1]

>>> intersect(s1, s2, s3)  # Three operands
['S', 'A', 'M']

>>> union(s1, s2, s3) 
['S', 'P', 'A', 'M', 'C', 'L']


	

I should note that because Python has a new set object type (described in Chapter 5), none of the set processing examples in this book are strictly required anymore; they are included only as demonstrations of coding functions. (Because it is constantly improving, Python has an uncanny way of conspiring to make my book examples obsolete over time!)

				30.3.2.5.6 Argument Matching: The Gritty Details

If you choose to use and combine the special argument-matching modes, Python will ask you to follow these ordering rules:

    * In a function call, all nonkeyword arguments (name) must appear first, followed by all keyword arguments (name=value), followed by the *name form, and, finally, the **name form, if used.
    * In a function header, arguments must appear in the same order: normal arguments (name), followed by any default arguments (name=value), followed by the *name form if present, followed by **name, if used.

If you mix arguments in any other order, you will get a syntax error because the combinations can be ambiguous. Python internally carries out the following steps to match arguments before assignment:

|   1.  Assign nonkeyword arguments by position.
|   2.  Assign keyword arguments by matching names.
|   3.  Assign extra nonkeyword arguments to *name tuple.
|   4.  Assign extra keyword arguments to **name dictionary.
|   5.  Assign default values to unassigned arguments in header.

After this, Python checks to make sure each argument is passed just one value; if not, an error is raised. This is as complicated as it looks, but tracing Python's matching algorithm will help you to understand some convoluted cases, especially when modes are mixed. We'll postpone looking at additional examples of these special matching modes until the exercises at the end of Part 4.

As you can see, advanced argument-matching modes can be complex. They are also entirely optional; you can get by with just simple positional matching, and it's probably a good idea to do so when you're starting out. However, because some Python tools make use of them, some general knowledge of these modes is important.

Why You Will Care: Keyword Arguments

Keyword arguments play an important role in Tkinter, the de facto standard GUI API for Python. We'll meet Tkinter later in this book, but as a preview, keyword arguments set configuration options when GUI components are built. For instance, a call of the form:


from Tkinter import *
widget = Button(text="Press me", command=someFunction)


creates a new button and specifies its text and callback function, using the text and command keyword arguments. Since the number of configuration options for a widget can be large, keyword arguments let you pick and choose. Without them, you might have to either list all the possible options by position or hope for a judicious positional argument defaults protocol that would handle every possible option arrangement.

			30.3.2.6 Chapter Summary

In this chapter, we studied two key concepts related to functions: scopes (how variables are looked up when used), and arguments (how objects are passed into a function). As we learned, variables are considered local to the function definitions in which they are assigned, unless they are specifically declared global. As we also saw, arguments are passed into a function by assignment, which means by object reference, which really means by pointer.

For both scopes and arguments, we also studied some more advanced extensions—nested function scopes, and default and keyword arguments, for example. Finally, we looked at some general design ideas (avoiding globals and cross-file changes), and saw how mutable arguments can exhibit the same behavior as other shared references to objects—unless the object is explicitly copied when it's sent in, changing a passed-in mutable in a function can impact the caller.

The next chapter concludes our look at functions by exploring some more advanced function-related ideas: lambdas, generators, iterators, functional tools, such as map, and so on. Many of these concepts stem from the fact that functions are normal objects in Python, and so support some advanced and very flexible processing modes. Before diving into those topics, however, take this chapter's quiz to review what we've studied here.

BRAIN BUILDER

Chapter Quiz

Q:	What is the output of the following code, and why?


>>> X = 'Spam'
>>> def func(  ):
...     print X
...
>>> func(  )


Q:	What is the output of this code, and why?


>>> X = 'Spam'
>>> def func(  ):
...     X = 'NI!'
...
>>> func(  )
>>> print X


Q:	What does this code print, and why?


>>> X = 'Spam'
>>> def func(  ):
...     X = 'NI'
...     print X
...
>>> func(  )
>>> print X


Q:	What output does this code produce; again, why?


>>> X = 'Spam'
>>> def func(  ):
...     global X
...     X = 'NI'
...
>>> func(  )
>>> print X


Q:	What about this code—what's the output, and why?


>>> X = 'Spam'
>>> def func(  ):
...     X = 'NI'
...     def nested(  ):
...         print X
...     nested(  )
...
>>> func(  )
>>> X


Q:	One last time: what is the output of this, and why?


>>> def func(a, b, c=3, d=4): print a, b, c, d
...
>>> func(1, *(5,6))


Q:	Name three or four ways to retain state information in a Python function.
Q:	Name three ways that functions can communicate results to a caller.


Quiz Answers

A:	The output here is 'Spam', because the function references a global variable in the enclosing module (because it is not assigned in the function, it is considered global).
A:	The output here is 'Spam' again because assigning the variable inside the function makes it a local and effectively hides the global of the same name. The print statement finds the variable unchanged in the global (module) scope.
A:	It prints 'NI' on one line, and 'Spam' on another because the reference to the variable within the function finds the assigned local, and the reference in the print finds the global.
A:	This time it just prints 'NI' because the global declaration forces the variable assigned inside the function to refer to the variable in the enclosing global scope.
A:	The output in this case is again 'NI' on one line, and 'Spam' on another because the print statement in the nested function finds the name in the enclosing function's local scope, and the print at the end finds the variable in the global scope.
A:	The output here is "1 5 6 4": 1 matches a by position, 5 and 6 match b and c by *name positionals (6 overrides c's default), and d defaults to 4 because it was not passed a value.
A:	Although the values of local variables go away when a function returns, state information can be retained by a Python function by using global variables, enclosing function scope references in nested functions, or default argument values. Another alternative, using OOP with classes, supports state retention better than any of the prior three techniques because it makes it explicit with attribute assignments.
A:	Functions can send back results with return statements, by changing passed-in mutable arguments, and by setting global variables. Globals are generally frowned upon (except for very special cases, like multithreaded programs) because they can make code harder to understand and use. return statements are usually best, but changing mutables is fine, if expected. Functions may also communicate with system devices such as files and sockets, but these are beyond our scope here.

 	 	30.3.3. Advanced Function Topics
This chapter introduces a collection of more advanced function-related topics: the lambda expression, functional programming tools such as map and list comprehensions, generator functions and expressions, and more. Part of the art of using functions lies in the interfaces between them, so we will also explore some general function design principles here. Because this is the last chapter in Part 4, we'll close with the usual sets of gotchas and exercises to help you start coding the ideas you've read about.

			30.3.3.1  Anonymous Functions: lambda

You've seen what it takes to write your own basic functions in Python. The next sections deal with a few more advanced function-related ideas. Most of these are optional features, but they can simplify your coding tasks when used well.

Besides the def statement, Python also provides an expression form that generates function objects. Because of its similarity to a tool in the LISP language, it's called lambda.[45] Like def, this expression creates a function to be called later, but it returns the function instead of assigning it to a name. This is why lambdas are sometimes known as anonymous (i.e., unnamed) functions. In practice, they are often used as a way to inline a function definition, or to defer execution of a piece of code.

    [45] * The name "lambda" seems to scare people more than it should. It comes from LISP, which got it from lambda calculus, which is a form of symbolic logic. In Python, though, it's really just a keyword that introduces the expression syntactically.

				30.3.3.1.1  lambda Expressions

The lambda's general form is the keyword lambda, followed by one or more arguments (exactly like the arguments list you enclose in parentheses in a def header), followed by an expression after a colon:


lambda argument1, argument2,... argumentN : expression using arguments


Function objects returned by running lambda expressions work exactly the same as those created and assigned by def, but there are a few differences that make lambdas useful in specialized roles:

    * lambda is an expression, not a statement. Because of this, a lambda can appear in places a def is not allowed by Python's syntax—inside a list literal, or function call, for example. Also, as an expression, lambda returns a value (a new function) that can optionally be assigned a name; in contrast, the def statement always assigns the new function to the name in the header, instead of returning it as a result.
    * lambda's body is a single expression, not a block of statements. The lambda's body is similar to what you'd put in a def body's return statement; you simply type the result as a naked expression, instead of explicitly returning it. Because it is limited to an expression, a lambda is less general than a def—you can only squeeze so much logic into a lambda body without using statements such as if. This is by design—it limits program nesting: lambda is designed for coding simple functions, and def handles larger tasks.

Apart from those distinctions, defs and lambdas do the same sort of work. For instance, we've seen how to make a function with a def statement:


>>> def func(x, y, z): return x + y + z
...
>>> func(2, 3, 4)
9


But, you can achieve the same effect with a lambda expression by explicitly assigning its result to a name through which you can later call the function:


>>> f = lambda x, y, z: x + y + z
>>> f(2, 3, 4)
9


Here, f is assigned the function object the lambda expression creates; this is how def works, too, but its assignment is automatic.

Defaults work on lambda arguments, just like in a def:


>>> x = (lambda a="fee", b="fie", c="foe": a + b + c)
>>> x("wee")
'weefiefoe'


The code in a lambda body also follows the same scope lookup rules as code inside a def. lambda expressions introduce a local scope much like a nested def, which automatically sees names in enclosing functions, the module, and the built-in scope (via the LEGB rule):


>>> def knights(  ): 
...     title = 'Sir' 
...     action = (lambda x: title + ' ' + x)  # Title in enclosing def
...     return action  # Return a function
...
>>> act = knights(  ) 
>>> act('robin') 
'Sir robin'


In this example, prior to Release 2.2, the value for the name title would typically have been passed in as a default argument value instead; flip back to the scopes coverage in Chapter 16 if you've forgotten why.

				30.3.3.1.2 Why Use lambda?

Generally speaking, lambdas come in handy as a sort of function shorthand that allows you to embed a function's definition within the code that uses it. They are entirely optional (you can always use defs instead), but they tend to be simpler coding constructs in scenarios where you just need to embed small bits of executable code.

For instance, we'll see later that callback handlers are frequently coded as inline lambda expressions embedded directly in a registration call's arguments list, instead of being defined with a def elsewhere in a file, and referenced by name (see the sidebar "Why You Will Care: Callbacks" later in this chapter for an example).

lambdas are also commonly used to code jump tables, which are lists or dictionaries of actions to be performed on demand. For example:


L = [(lambda x: x**2), (lambda x: x**3), (lambda x: x**4)]
for f in L:
    print f(2)                       # Prints 4, 8, 16

print L[0](3)                        # Prints 9


The lambda expression is most useful as a shorthand for def, when you need to stuff small pieces of executable code into places where statements are illegal syntactically. This code snippet, for example, builds up a list of three functions by embedding lambda expressions inside a list literal; a def won't work inside a list literal like this because it is a statement, not an expression.

You can do the same sort of thing with dictionaries and other data structures in Python to build up action tables:


>>> key = 'got'
>>> {'already': (lambda: 2 + 2),
...  'got':     (lambda: 2 * 4),
...  'one':     (lambda: 2 ** 6)
... }[key](  )
8


Here, when Python makes the dictionary, each of the nested lambdas generates and leaves behind a function to be called later; indexing by key fetches one of those functions, and parentheses force the fetched function to be called. When coded this way, a dictionary becomes a more general multiway branching tool than what I could show you in Chapter 12's coverage of if statements.

To make this work without lambda, you'd need to instead code three def statements somewhere else in your file, outside the dictionary in which the functions are to be used:


def f1(  ): return 2 + 2
def f2(  ): return 2 * 4
def f3(  ): return 2 ** 6
...

key = 'one'
{'already': f1, 'got': f2, 'one': f3}[key](  )


This works, too, but your defs may be arbitrarily far away in your file, even if they are just little bits of code. The code proximity that lambdas provide is especially useful for functions that will only be used in a single context—if the three functions here are not useful anywhere else, it makes sense to embed their definitions within the dictionary as lambdas. Moreover, the def form requires you to make up names for these little functions that may clash with other names in this file.

lambdas also come in handy in function argument lists as a way to inline temporary function definitions not used anywhere else in your program; we'll see some examples of such other uses later in this chapter, when we study map.

				30.3.3.1.3 How (Not) to Obfuscate Your Python Code

The fact that the body of a lambda has to be a single expression (not a series of statements) would seem to place severe limits on how much logic you can pack into a lambda. If you know what you're doing, though, you can code most statements in Python as expression-based equivalents.

For example, if you want to print from the body of a lambda function, simply say sys.stdout.write(str(x)+'\n'), instead of print x (recall from Chapter 11 that this is what print really does). Similarly, to nest logic in a lambda, you can use the if/else ternary expression introduced in Chapter 13, or the equivalent but trickier and/or combination also described there. As you learned earlier, the following statement:


if a:
    b
else:
    c


can be emulated by either of these roughly equivalent expressions:


b if a else c
((a and b) or c)


Because expressions like these can be placed inside a lambda, they may be used to implement selection logic within a lambda function:


>>> lower = (lambda x, y: x if x < y else y)
>>> lower('bb', 'aa')
'aa'
>>> lower('aa', 'bb')
'aa'


Furthermore, if you need to perform loops within a lambda, you can also embed things like map calls and list comprehension expressions (tools we met earlier, in Chapter 13, and will revisit later in this chapter):


>>> import sys
>>> showall = (lambda x: map(sys.stdout.write, x))

>>> t = showall(['spam\n', 'toast\n', 'eggs\n'])
spam
toast
eggs

>>> showall = lambda x: [sys.stdout.write(line) for line in x]

>>> t = showall(('bright\n', 'side\n', 'of\n', 'life\n'))
bright
side
of
life


Now that I've shown you these tricks, I am required by law to ask you to please only use them as a last resort. Without due care, they can lead to unreadable (a.k.a. obfuscated) Python code. In general, simple is better than complex, explicit is better than implicit, and full statements are better than arcane expressions. On the other hand, you may find these techniques useful in moderation.

				30.3.3.1.4 Nested lambdas and Scopes

lambdas are the main beneficiaries of nested function scope lookup (the E in the LEGB rule we met in Chapter 16). In the following, for example, the lambda appears inside a def—the typical case—and so can access the value that the name x had in the enclosing function's scope at the time that the enclosing function was called:


>>> def action(x): 
...     return (lambda y: x + y)  # Make and return function, remember x
...
>>> act = action(99) 
>>> act 
<function <lambda> at 0x00A16A88>
>>> act(2) 
101


What wasn't illustrated in the prior chapter's discussion of nested function scopes is that a lambda also has access to the names in any enclosing lambda. This case is somewhat obscure, but imagine if we recoded the prior def with a lambda:


>>> action = (lambda x: (lambda y: x + y))
>>> act = action(99)
>>> act(3)
102
>>> ((lambda x: (lambda y: x + y))(99))(4)
103


Here, the nested lambda structure makes a function that makes a function when called. In both cases, the nested lambda's code has access to the variable x in the enclosing lambda. This works, but it's fairly convoluted code; in the interest of readability, nested lambdas are generally best avoided.

Why You Will Care: Callbacks

Another very common application of lambda is to define inline callback functions for Python's Tkinter GUI API. For example, the following creates a button that prints a message on the console when pressed:


import sys
x = Button(
    text ='Press me',
    command=(lambda:sys.stdout.write('Spam\n')))


Here, the callback handler is registered by passing a function generated with a lambda to the command keyword argument. The advantage of lambda over def here is that the code that handles a button press is right here, embedded in the button creation call.

In effect, the lambda defers execution of the handler until the event occurs: the write call happens on button presses, not when the button is created.

Because the nested function scope rules apply to lambdas as well, they are also easier to use as callback handlers, as of Python 2.2—they automatically see names in the functions in which they are coded, and no longer require passed-in defaults in most cases. This is especially handy for accessing the special self instance argument that is a local variable in enclosing class method functions (more on classes in Part 6):


class MyGui:
    def makewidgets(self):
        Button(command=(lambda: self.display("spam")))
    def display(self, message):
        ...use message...


In prior releases, even self had to be passed in with defaults.

			30.3.3.2  Applying Functions to Arguments
Some programs need to call arbitrary functions in a generic fashion, without knowing their names or arguments ahead of time (we'll see examples of where this can be useful later). Both the apply built-in function, and some special call syntax available in Python, can do the job.

	

At this writing, both apply and the special call syntax described in this section can be used freely in Python 2.5, but it seems likely that apply may go away in Python 3.0. If you wish to future-proof your code, use the equivalent special call syntax, not apply.

				30.3.3.2.1  The apply Built-in

When you need to be dynamic, you can call a generated function by passing it as an argument to apply, along with a tuple of arguments to pass to that function:


>>> def func(x, y, z): return x + y + z
...
>>> apply(func, (2, 3, 4))
9
>>> f = lambda x, y, z: x + y + z
>>> apply(f, (2, 3, 4))
9


The apply function simply calls the passed-in function in the first argument, matching the passed-in arguments tuple to the function's expected arguments. Because the arguments list is passed in as a tuple (i.e., a data structure), a program can build it at runtime.[46]

    [46] * Be careful not to confuse apply with map, the topic of the next section. apply runs a single function call, passing arguments to the function object just once. map calls a function many times instead for each item in a sequence.

The real power of apply is that it doesn't need to know how many arguments a function is being called with. For example, you can use if logic to select from a set of functions and argument lists, and use apply to call any of them:


if <test>:
    action, args = func1, (1,)
else:
    action, args = func2, (1, 2, 3)
...
apply(action, args)


More generally, apply is useful any time you cannot predict the arguments list ahead of time. If your user selects an arbitrary function via a user interface, for instance, you may be unable to hardcode a function call when writing your script. To work around this, simply build up the arguments list with tuple operations, and call the function indirectly through apply:


>>> args = (2,3) + (4,)
>>> args
(2, 3, 4)
>>> apply(func, args)
9


					30.3.3.2.1.1  Passing keyword arguments

The apply call also supports an optional third argument, where you can pass in a dictionary that represents keyword arguments to be passed to the function:


>>> def echo(*args, **kwargs): print args, kwargs
...
>>> echo(1, 2, a=3, b=4)
(1, 2) {'a': 3, 'b': 4}


This allows you to construct both positional and keyword arguments at runtime:


>>> pargs = (1, 2)
>>> kargs = {'a':3, 'b':4}
>>> apply(echo, pargs, kargs)
(1, 2) {'a': 3, 'b': 4}

				30.3.3.2.2 apply-Like Call Syntax

Python also allows you to accomplish the same effect as an apply call with special syntax in the call. This syntax mirrors the arbitrary arguments syntax in def headers that we met in Chapter 16. For example, assuming the names in this example are still as assigned earlier:


>>> apply(func, args)  # Traditional: tuple
9
>>> func(*args)  # New apply-like syntax
9
>>> echo(*pargs, **kargs)  # Keyword dictionaries too
(1, 2) {'a': 3, 'b': 4}


This special call syntax is newer than the apply function, and is generally preferred today. It doesn't have any obvious advantages over an explicit apply call, apart from its symmetry with def headers, and requiring a few less keystrokes. However, the new call syntax alternative also allows us to pass along real additional arguments, and so is more general:


>>> echo(0, *pargs, **kargs)  # Normal, *tuple, **dictionary
(0, 1, 2) {'a': 3, 'b': 4}

			30.3.3.3  Mapping Functions over Sequences: map

One of the more common things programs do with lists and other sequences is apply an operation to each item and collect the results. For instance, updating all the counters in a list can be done easily with a for loop:


>>> counters = [1, 2, 3, 4] 
>>>
>>> updated = [] 
>>> for x in counters: 
...     updated.append(x + 10)  # Add 10 to each item
...
>>> updated 
[11, 12, 13, 14]


But because this is such a common operation, Python actually provides a built-in that does most of the work for you. The map function applies a passed-in function to each item in a sequence object, and returns a list containing all the function call results. For example:


>>> def inc(x): return x + 10  # Function to be run
...
>>> map(inc, counters)  # Collect results
[11, 12, 13, 14]


map was introduced as a parallel loop-traversal tool in Chapter 13. As you may recall, we passed in None for the function argument to pair up items. Here, we make better use of it by passing in a real function to be applied to each item in the list—map calls inc on each list item, and collects all the return values into a list.

Because map expects a function to be passed in, it also happens to be one of the places where lambdas commonly appear:


>>> map((lambda x: x + 3), counters)  # Function expression
[4, 5, 6, 7]


Here, the function adds 3 to each item in the counters list; as this function isn't needed elsewhere, it was written inline as a lambda. Because such uses of map are equivalent to for loops, with a little extra code, you can always code a general mapping utility yourself:


>>> def mymap(func, seq):
...     res = []
...     for x in seq: res.append(func(x))
...     return res
...
>>> map(inc, [1, 2, 3])
[11, 12, 13]
>>> mymap(inc, [1, 2, 3])
[11, 12, 13]


However, as map is a built-in, it's always available, always works the same way, and has some performance benefits (in short, it's faster than a manually coded for loop). Moreover, map can be used in more advanced ways than shown here. For instance, given multiple sequence arguments, it sends items taken from sequences in parallel as distinct arguments to the function:


>>> pow(3, 4) 
81
>>> map(pow, [1, 2, 3], [2, 3, 4])  # 1**2, 2**3, 3**4
[1, 8, 81]


Here, the pow function takes two arguments on each call—one from each sequence passed to map. Although we could simulate this generality, too, there is no obvious point in doing so when a speedy built-in is provided.

	

The map call is similar to the list comprehension expressions we studied in Chapter 13, and will meet again later in this chapter, but map applies a function call to each item instead of an arbitrary expression. Because of this limitation, it is a somewhat less general tool. However, in some cases, map is currently faster to run than a list comprehension (e.g., when mapping a built-in function), and it may require less coding. Because of that, it seems likely that map will still be available in Python 3.0. However, a recent Python 3.0 document did propose removing this call, along with the reduce and filter calls discussed in the next section, from the built-in namespace (they may show up in modules instead).

While map may stick around, it's likely that reduce and filter will be removed in 3.0, partly because they are redundant with list comprehensions (filter is subsumed by list comprehension if clauses), and partly because of their complexity (reduce is one of the most complex tools in the language and is not intuitive). I cannot predict the future on this issue, though, so be sure to watch the Python 3.0 release notes for more on these possible changes. These tools are all still included in this edition of the book because they are part of the current version of Python, and will likely be present in Python code you're likely to see for some time to come.

			30.3.3.4  Functional Programming Tools: filter and reduce
The map function is the simplest representative of a class of Python built-ins used for functional programming—which mostly just means tools that apply functions to sequences. Its relatives filter out items based on a test function (filter), and apply functions to pairs of items and running results (reduce). For example, the following filter call picks out items in a sequence that are greater than zero:


>>> range(−5, 5)
[−5, −4, −3, −2, −1, 0, 1, 2, 3, 4]

>>> filter((lambda x: x > 0), range(−5, 5))
[1, 2, 3, 4]


Items in the sequence for which the function returns true are added to the result list. Like map, this function is roughly equivalent to a for loop, but is built-in, and fast:


>>> res = [  ]
>>> for x in range(−5, 5):
...     if x > 0:
...         res.append(x)
...
>>> res
[1, 2, 3, 4]


reduce is more complex. Here are two reduce calls computing the sum and product of items in a list:
note in python3 reduce is in functools so import functools


>>> reduce((lambda x, y: x + y), [1, 2, 3, 4])
10
>>> reduce((lambda x, y: x * y), [1, 2, 3, 4])
24


At each step, reduce passes the current sum or product, along with the next item from the list, to the passed-in lambda function. By default, the first item in the sequence initializes the starting value. Here's the for loop equivalent to the first of these calls, with the addition hardcoded inside the loop:


>>> L = [1,2,3,4]
>>> res = L[0]
>>> for x in L[1:]:
...     res = res + x
...
>>> res
10


Coding your own version of reduce (for instance, if it is indeed removed in Python 3.0) is actually fairly straightforward:


>>> def myreduce(function, sequence):
...     tally = sequence[0]
...     for next in sequence[1:]:
...         tally = function(tally, next)
...     return tally
...
>>> myreduce((lambda x, y: x + y), [1, 2, 3, 4, 5])
15
>>> myreduce((lambda x, y: x * y), [1, 2, 3, 4, 5])
120


If this has sparked your interest, also see the built-in operator module, which provides functions that correspond to built-in expressions, and so comes in handy for some uses of functional tools:


>>> import operator 
>>> reduce(operator.add, [2, 4, 6])  # Function-based +
12
>>> reduce((lambda x, y: x + y), [2, 4, 6]) 
12


Together with map, filter and reduce support powerful functional programming techniques. Some observers might also extend the functional programming toolset in Python to include lambda and apply, and list comprehensions, which are discussed in the next section.

			30.3.3.5  List Comprehensions Revisited: Mappings

Because mapping operations over sequences and collecting results is such a common task in Python coding, Python 2.0 sprouted a new feature—the list comprehension expression—that makes it even simpler than using the tools we just studied. We met list comprehensions in Chapter 13, but because they're related to functional programming tools like the map and filter calls, we'll resurrect the topic here for one last look. Technically, this feature is not tied to functions—as we'll see, list comprehensions can be a more general tool than map and filter—but it is sometimes best understood by analogy to function-based alternatives.

				30.3.3.5.1  List Comprehension Basics

Let's work through an example that demonstrates the basics. As we saw in Chapter 7, Python's built-in ord function returns the ASCII integer code of a single character (the chr built-in is the converse—it returns the character for an ASCII integer code):


>>> ord('s')
115


Now, suppose we wish to collect the ASCII codes of all characters in an entire string. Perhaps the most straightforward approach is to use a simple for loop, and append the results to a list:


>>> res = []
>>> for x in 'spam':
...     res.append(ord(x))
...
>>> res
[115, 112, 97, 109]


Now that we know about map, though, we can achieve similar results with a single function call without having to manage list construction in the code:


>>> res = map(ord, 'spam')  # Apply function to sequence
>>> res 
[115, 112, 97, 109]


As of Python 2.0, however, we can get the same results from a list comprehension expression:


>>> res = [ord(x) for x in 'spam']  # Apply expression to sequence
>>> res 
[115, 112, 97, 109]


List comprehensions collect the results of applying an arbitrary expression to a sequence of values and return them in a new list. Syntactically, list comprehensions are enclosed in square brackets (to remind you that they construct lists). In their simple form, within the brackets you code an expression that names a variable followed by what looks like a for loop header that names the same variable. Python then collects the expression's results for each iteration of the implied loop.

The effect of the preceding example is similar to that of the manual for loop and the map call. List comprehensions become more convenient, though, when we wish to apply an arbitrary expression to a sequence:


>>> [x ** 2 for x in range(10)]
[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]


Here, we've collected the squares of the numbers 0 through 9 (we're just letting the interactive prompt print the resulting list; assign it to a variable if you need to retain it). To do similar work with a map call, we would probably invent a little function to implement the square operation. Because we won't need this function elsewhere, we'd typically (but not necessarily) code it inline, with a lambda, instead of using a def statement elsewhere:


>>> map((lambda x: x ** 2), range(10))
[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]


This does the same job, and it's only a few keystrokes longer than the equivalent list comprehension. It's also only marginally more complex (at least, once you understand the lambda). For more advanced kinds of expressions, though, list comprehensions will often require considerably less typing. The next section shows why.

				30.3.3.5.2 Adding Tests and Nested Loops, tags: double for nested list comprehension 

List comprehensions are even more general than shown so far. For instance, you can code an if clause after the for, to add selection logic. List comprehensions with if clauses can be thought of as analogous to the filter built-in discussed in the prior section—they skip sequence items for which the if clause is not true. Here are examples of both schemes picking up even numbers from 0 to 4; like the map list comprehension alternative we just looked at, the filter version here invents a little lambda function for the test expression. For comparison, the equivalent for loop is shown here as well:


>>> [x for x in range(5) if x % 2 == 0]
[0, 2, 4]

>>> filter((lambda x: x % 2 == 0), range(5))
[0, 2, 4]

>>> res = [  ]
>>> for x in range(5):
...     if x % 2 == 0:
...         res.append(x)
...
>>> res
[0, 2, 4]


All of these use the modulus (remainder of division) operator, %, to detect even numbers: if there is no remainder after dividing a number by two, it must be even. The filter call is not much longer than the list comprehension here either. However, we can combine an if clause, and an arbitrary expression in our list comprehension, to give it the effect of a filter and a map, in a single expression:


>>> [x ** 2 for x in range(10) if x % 2 == 0]
[0, 4, 16, 36, 64]


This time, we collect the squares of the even numbers from 0 through 9: the for loop skips numbers for which the attached if clause on the right is false, and the expression on the left computes the squares. The equivalent map call would require a lot more work on our part—we would have to combine filter selections with map iteration, making for a noticeably more complex expression:


>>> map((lambda x: x**2), filter((lambda x: x % 2 == 0), range(10)))
[0, 4, 16, 36, 64]


In fact, list comprehensions are more general still. You can code any number of nested for loops in a list comprehension, and each may have an optional associated if test. The general structure of list comprehensions looks like this:


[ expression for target1 in sequence1 [if condition]
             for target2 in sequence2 [if condition] ...
             for targetN in sequenceN [if condition] ]


When for clauses are nested within a list comprehension, they work like equivalent nested for loop statements. For example, the following:


>>> res = [x + y for x in [0, 1, 2] for y in [100, 200, 300]]
>>> res
[100, 200, 300, 101, 201, 301, 102, 202, 302]


has the same effect as this substantially more verbose equivalent:


>>> res = []
>>> for x in [0, 1, 2]:
...     for y in [100, 200, 300]:
...         res.append(x + y)
...
>>> res
[100, 200, 300, 101, 201, 301, 102, 202, 302]


Although list comprehensions construct lists, remember that they can iterate over any sequence or other iterable type. Here's a similar bit of code that traverses strings instead of lists of numbers, and so collects concatenation results:


>>> [x + y for x in 'spam' for y in 'SPAM']
['sS', 'sP', 'sA', 'sM', 'pS', 'pP', 'pA', 'pM',
'aS', 'aP', 'aA', 'aM', 'mS', 'mP', 'mA', 'mM']


Finally, here is a much more complex list comprehension that illustrates the effect of attached if selections on nested for clauses:

>>> [(x, y) for x in range(5) if x % 2 == 0 for y in range(5) if y % 2 == 1]
[(0, 1), (0, 3), (2, 1), (2, 3), (4, 1), (4, 3)]

					  


This expression permutes even numbers from 0 through 4 with odd numbers from 0 through 4. The if clauses filter out items in each sequence iteration. Here's the equivalent statement-based code:


>>> res = []
>>> for x in range(5):
...     if x % 2 == 0:
...         for y in range(5):
...             if y % 2 == 1:
...                 res.append((x, y))
...
>>> res
[(0, 1), (0, 3), (2, 1), (2, 3), (4, 1), (4, 3)]


Recall that if you're confused about what a complex list comprehension does, you can always nest the list comprehension's for and if clauses inside each other (indenting successively further to the right) to derive the equivalent statements. The result is longer, but perhaps clearer.

The map and filter equivalent would be wildly complex and deeply nested, so I won't even try showing it here. I'll leave its coding as an exercise for Zen masters, ex-LISP programmers, and the criminally insane.

				30.3.3.5.3 List Comprehensions and Matrixes

Let's look at one more advanced application of list comprehensions to stretch a few synapses. One basic way to code matrixes (a.k.a. multidimensional arrays) in Python is with nested list structures. The following, for example, defines two 3 × 3 matrixes as lists of nested lists:


>>> M = [[1, 2, 3],
...      [4, 5, 6],
...      [7, 8, 9]]

>>> N = [[2, 2, 2],
...      [3, 3, 3],
...      [4, 4, 4]]


Given this structure, we can always index rows, and columns within rows, using normal index operations:


>>> M[1]
[4, 5, 6]

>>> M[1][2]
6


List comprehensions are powerful tools for processing such structures, though, because they automatically scan rows and columns for us. For instance, although this structure stores the matrix by rows, to collect the second column, we can simply iterate across the rows and pull out the desired column, or iterate through positions in the rows and index as we go:


>>> [row[1] for row in M]
[2, 5, 8]

>>> [M[row][1] for row in (0, 1, 2)]
[2, 5, 8]


Given positions, we can also easily perform tasks such as pulling out a diagonal. The following expression uses range to generate the list of offsets, and then indexes with the row and column the same, picking out M[0][0], then M[1][1], and so on (we assume the matrix has the same number of rows and columns):


>>> [M[i][i] for i in range(len(M))]
[1, 5, 9]


Finally, with a bit of creativity, we can also use list comprehensions to combine multiple matrixes. The following first builds a flat list that contains the result of multiplying the matrixes pairwise, and then builds a nested list structure having the same values by nesting list comprehensions:


>>> [M[row][col] * N[row][col] for row in range(3) for col in range(3)]
[2, 4, 6, 12, 15, 18, 28, 32, 36]

>>> [[M[row][col] * N[row][col] for col in range(3)] for row in range(3)]
[[2, 4, 6], [12, 15, 18], [28, 32, 36]]


This last expression works because the row iteration is an outer loop: for each row, it runs the nested column iteration to build up one row of the result matrix. It's equivalent to this statement-based code:


>>> res = []
>>> for row in range(3):
...     tmp = []
...     for col in range(3):
...         tmp.append(M[row][col] * N[row][col])
...     res.append(tmp)
...
>>> res
[[2, 4, 6], [12, 15, 18], [28, 32, 36]]


Compared to these statements, the list comprehension version requires only one line of code, will probably run substantially faster for large matrixes, and just might make your head explode! Which brings us to the next section.

				30.3.3.5.4 Comprehending List Comprehensions

With such generality, list comprehensions can quickly become, well, incomprehensible, especially when nested. Consequently, my advice is typically to use simple for loops when getting started with Python, and map calls in most other cases (unless they get too complex). The "keep it simple" rule applies here, as always: code conciseness is a much less important goal than code readability.

However, in this case, there is currently a substantial performance advantage to the extra complexity: based on tests run under Python today, map calls are roughly twice as fast as equivalent for loops, and list comprehensions are usually slightly faster than map calls.[47] This speed difference is down to the fact that map and list comprehensions run at C language speed inside the interpreter, which is much faster than stepping through Python for loop code within the PVM.

    [47] * These performance generalizations can depend on call patterns, as well as changes and optimizations in Python itself. Recent Python releases have sped up the simple for loop statement, for example. Usually, though, list comprehensions are still substantially faster than for loops and even faster than map (though map can still win for built-in functions). To time these alternatives yourself, see the standard library's time module's time.clock and time.time calls, the newer timeit module added in Release 2.4, or this chapter's upcoming section "Section 17.7."

Because for loops make logic more explicit, I recommend them in general on the grounds of simplicity. However, map and list comprehensions are worth knowing and using for simpler kinds of iterations, and if your application's speed is an important consideration. In addition, because map and list comprehensions are both expressions, they can show up syntactically in places that for loop statements cannot, such as in the bodies of lambda functions, within list and dictionary literals, and more. Still, you should try to keep your map calls and list comprehensions simple; for more complex tasks, use full statements instead.

Why You Will Care: List Comprehensions and map

Here's a more realistic example of list comprehensions and map in action (we solved this problem with list comprehensions in Chapter 13, but we'll revive it here to add map-based alternatives). Recall that the file readlines method returns lines with \n end-of-line characters at the ends:


>>> open('myfile').readlines(  )
['aaa\n', 'bbb\n', 'ccc\n']


If you don't want the end-of-line characters, you can slice them off all the lines in a single step with a list comprehension or a map call:


>>> [line.rstrip(  ) for line in open('myfile').readlines(  )]
['aaa', 'bbb', 'ccc']

>>> [line.rstrip(  ) for line in open('myfile')]
['aaa', 'bbb', 'ccc']

>>> map((lambda line: line.rstrip(  )), open('myfile'))
['aaa', 'bbb', 'ccc']


The last two of these make use of file iterators (which essentially means that you don't need a method call to grab all the lines in iteration contexts such as these). The map call is just slightly longer than the list comprehension, but neither has to manage result list construction explicitly.

A list comprehension can also be used as a sort of column projection operation. Python's standard SQL database API returns query results as a list of tuples much like the following—the list is the table, tuples are rows, and items in tuples are column values:


listoftuple = [('bob', 35, 'mgr'), ('mel', 40, 'dev')]


A for loop could pick up all the values from a selected column manually, but map and list comprehensions can do it in a single step, and faster:


>>> [age for (name, age, job) in listoftuple]
[35, 40]

>>> map((lambda (name, age, job): age), listoftuple)
[35, 40]


Both of these make use of tuple assignment to unpack row tuples in the list.

See other books and resources for more on Python's database API.

			30.3.3.6  Iterators Revisited: Generators

In this part of the book, we've learned about coding normal functions that receive input parameters and send back a single result immediately. It is also possible, however, to write functions that may send back a value and later be resumed, picking up where they left off. Such functions are known as generators because they generate a sequence of values over time.

Generator functions are like normal functions in most respects, but in Python, they are automatically made to implement the iteration protocol so that they can appear in iteration contexts. We studied iterators in Chapter 13; here, we'll revise them to see how they relate to generators.

Unlike normal functions that return a value and exit, generator functions automatically suspend and resume their execution and state around the point of value generation. Because of that, they are often a useful alternative to both computing an entire series of values up front, and manually saving and restoring state in classes. Generator functions automatically retain their state when they are suspended—because this includes their entire local scope, their local variables keep state information, which is available when the functions are resumed.

The chief code difference between generator and normal functions is that a generator yields a value, rather than returning one—the yield statement suspends the function, and sends a value back to the caller, but retains enough state to enable the function to resume from where it left off. This allows these functions to produce a series of values over time, rather than computing them all at once, and sending them back in something like a list.

Generator functions are bound up with the notion of iterator protocols in Python. In short, functions containing a yield statement are compiled specially as generators; when called, they return a generator object that supports the iterator object interface. Generator functions may also have a return statement, which simply terminates the generation of values.

Iterator objects, in turn, define a next method, which either returns the next item in the iteration, or raises a special exception (StopIteration) to end the iteration. Iterators are fetched with the iter built-in function. Python for loops use this iteration interface protocol to step through a sequence (or sequence generator), if the protocol is supported; if not, for falls back on repeatedly indexing sequences instead.

				30.3.3.6.1  Generator Function Example

Generators and iterators are advanced language features, so please see the Python library manuals for the full story. To illustrate the basics, though, the following code defines a generator function that can be used to generate the squares of a series of numbers over time:[48]

    [48] * Generators are available in Python releases after 2.2; in 2.2, they must be enabled with a special import statement of the form from __future__ import generators (see Chapter 18 for more on this statement form). Iterators were already available in 2.2, largely because the underlying protocol did not require the new, nonbackward-compatible keyword yield.


>>> def gensquares(N): 
...     for i in range(N): 
...         yield i ** 2  # Resume here later
...


This function yields a value, and so returns to its caller, each time through the loop; when it is resumed, its prior state is restored, and control picks up again immediately after the yield statement. For example, when used in the body of a for loop, control returns to the function after its yield statement each time through the loop:


>>> for i in gensquares(5):  # Resume the function
...     print i, ':',  # Print last yielded value
...
0 : 1 : 4 : 9 : 16 :
>>>


To end the generation of values, functions either use a return statement with no value, or simply allow control to fall off the end of the function body.

If you want to see what is going on inside the for, call the generator function directly:


>>> x = gensquares(4)
>>> x
<generator object at 0x0086C378>


You get back a generator object that supports the iterator protocol (i.e., has a next method that starts the function, or resumes it from where it last yielded a value, and raises a StopIteration exception when the end of the series of values is reached):


>>> x.next(  )
0
>>> x.next(  )
1
>>> x.next(  )
4
>>> x.next(  )
9
>>> x.next(  )

Traceback (most recent call last):
  File "<pyshell#453>", line 1, in <module>
    x.next(  )
StopIteration


for loops work with generators in the same way—by calling the next method repeatedly, until an exception is caught. If the object to be iterated over does not support this protocol, for loops instead use the indexing protocol to iterate.

Note that in this example, we could also simply build the list of yielded values all at once:


>>> def buildsquares(n):
...     res = []
...     for i in range(n): res.append(i**2)
...     return res
...
>>> for x in buildsquares(5): print x, ':',
...
0 : 1 : 4 : 9 : 16 :


For that matter, we could use any of the for loop, map, or list comprehension techniques:


>>> for x in [n**2 for n in range(5)]:
...     print x, ':',
...
0 : 1 : 4 : 9 : 16 :

>>> for x in map((lambda x:x**2), range(5)):
...     print x, ':',
...
0 : 1 : 4 : 9 : 16 :


However, generators allow functions to avoid doing all the work up front, which is especially useful when the result lists are large, or when it takes a lot of computation to produce each value. Generators distribute the time required to produce the series of values among loop iterations. Moreover, for more advanced uses, they provide a simpler alternative to manually saving the state between iterations in class objects (more on classes later in Part 6); with generators, function variables are saved and restored automatically.

				30.3.3.6.2 Extended Generator Function Protocol: send Versus next

In Python 2.5, a send method was added to the generator function protocol. The send method advances to the next item in the series of results, just like the next method, but also provides a way for the caller to communicate with the generator, to affect its operation.

Technically, yield is now an expression form that returns the item passed to send, not a statement (though it can be called either way—as yield X, or A = (yield X)). Values are sent into a generator by calling its send(value) method. The generator's code is then resumed, and the yield expression returns the value passed to send. If the regular next( ) method is called, the yield returns None.

The send method can be used, for example, to code a generator that can be terminated by its caller. In addition, generators in 2.5 also support a throw(type) method to raise an exception inside the generator at the latest yield, and a close( ) method that raises a new GeneratorExit exception inside the generator to terminate the iteration. These are advanced features that we won't delve into in more detail here; see Python's standard manuals for more details.

				30.3.3.6.3 Iterators and Built-in Types

As we saw in Chapter 13, built-in data types are designed to produce iterator objects in response to the iter built-in function. Dictionary iterators, for instance, produce key list items on each iteration:


>>> D = {'a':1, 'b':2, 'c':3}
>>> x = iter(D)
>>> x.next(  )
'a'
>>> x.next(  )
'c'


In addition, all iteration contexts (including for loops, map calls, list comprehensions, and the many other contexts we met in Chapter 13) are in turn designed to automatically call the iter function to see whether the protocol is supported. That's why you can loop through a dictionary's keys without calling its keys method, step through lines in a file without calling readlines or xreadlines, and so on:


>>> for key in D:
...     print key, D[key]
...
a 1
c 3
b 2


As we've also seen, for file iterators, Python simply loads lines from the file on demand:


>>> for line in open('temp.txt'):
...     print line,
...
Tis but
a flesh wound.


It is also possible to implement arbitrary generator objects with classes that conform to the iterator protocol, and so may be used in for loops and other iteration contexts. Such classes define a special __iter__ method that returns an iterator object (preferred over the __getitem__ indexing method). However, this is well beyond the scope of this chapter; see Part 6 for more on classes in general, and Chapter 24 for an example of a class that implements the iterator protocol.


				30.3.3.6.4 Generator Expressions: Iterators Meet List Comprehensions

In recent versions of Python, the notions of iterators and list comprehensions are combined in a new feature of the language, generator expressions. Syntactically, generator expressions are just like normal list comprehensions, but they are enclosed in parentheses instead of square brackets:


>>> [x ** 2 for x in range(4)]  # List comprehension: build a list
[0, 1, 4, 9]

>>> (x ** 2 for x in range(4))  # Generator expression: make an iterable
<generator object at 0x011DC648>


Operationally, however, generator expressions are very different—instead of building the result list in memory, they return a generator object, which in turn supports the iteration protocol to yield one piece of the result list at a time in any iteration context:


>>> G = (x ** 2 for x in range(4))
>>> G.next(  )
0
>>> G.next(  )
1
>>> G.next(  )
4
>>> G.next(  )
9
>>> G.next(  )

Traceback (most recent call last):
  File "<pyshell#410>", line 1, in <module>
    G.next(  )
StopIteration


We don't typically see the next iterator machinery under the hood of a generator expression like this because for loops trigger it for us automatically:


>>> for num in (x ** 2 for x in range(4)):
...     print '%s, %s' % (num, num / 2.0)
...
0, 0.0
1, 0.5
4, 2.0
9, 4.5


In fact, every iteration context does this, including the sum, map, and sorted built-in functions, and the other iteration contexts we learned about in Chapter 13, such as the any, all, and list built-in functions.

Notice that the parentheses are not required around a generator expression if they are the sole item enclosed in other parentheses, like those of a function call. Extra parentheses are required, however, in the second call to sorted:


>>> sum(x ** 2 for x in range(4))
14

>>> sorted(x ** 2 for x in range(4))
[0, 1, 4, 9]

>>> sorted((x ** 2 for x in range(4)), reverse=True)
[9, 4, 1, 0]

>>> import math
>>> map(math.sqrt, (x ** 2 for x in range(4)))
[0.0, 1.0, 2.0, 3.0]


Generator expressions are primarily a memory space optimization—they do not require the entire result list to be constructed all at once, as the square-bracketed list comprehension does. They may also run slightly slower in practice, so they are probably best used only for very large result sets—which provides a natural segue to the next section.

			30.3.3.7  Timing Iteration Alternatives
We've met a few iteration alternatives in this book. To summarize, let's take a brief look at a case study that pulls together some of the things we've learned about iteration and functions.

I've mentioned a few times that list comprehensions have a speed performance advantage over for loop statements, and that map performance can be better or worse depending on call patterns. The generator expressions of the prior section tend to be slightly slower than list comprehensions, though they minimize memory requirements.

All that's true today, but relative performance can vary over time (Python is constantly being optimized). If you want to test this for yourself, try running the following script on your own computer, and your version of Python:

# file timerseqs.py
import time, sys
reps = 1000
size = 10000

def tester(func, *args):
    startTime = time.time(  )
    for i in range(reps):
        func(*args)
    elapsed = time.time(  ) - startTime
    return elapsed

def forStatement(  ):
    res = []
    for x in range(size):
        res.append(abs(x))

def listComprehension(  ):
    res = [abs(x) for x in range(size)]

def mapFunction(  ):
    res = map(abs, range(size))

def generatorExpression(  ):
    res = list(abs(x) for x in range(size))

print sys.version
tests = (forStatement, listComprehension, mapFunction, generatorExpression)
for testfunc in tests:
    print testfunc.__name__.ljust(20), '=>', tester(testfunc)

					  

running on my 64bit 4 dual core CPU win7 machine:
:!python /cygdrive/c/work/scripts/python/learn/learning_python/timerseqs.py
2.6.5 (r265:79063, Jun 12 2010, 17:07:01)
[GCC 4.3.4 20090804 (release) 1]
forStatement         => 3.58200001717
listComprehension    => 2.23300004005
mapFunction          => 1.28999996185
generatorExpression  => 2.36800003052   


This script tests all the alternative ways to build lists of results and, as shown, executes on the order of 10 million steps for each—that is, each of the four tests builds a list of 10,000 items 1,000 times.

Notice how we have to run the generator expression though the built-in list call to force it to yield all of its values; if we did not, we would just produce a generator that never does any real work. Also, notice how the code at the bottom steps through a tuple of four function objects, and prints the __name__ of each: this is a built-in attribute that gives a function's name.

When I ran this in IDLE on Windows XP with Python 2.5, here is what I found—list comprehensions were roughly twice as fast as equivalent for loop statements, and map was slightly quicker than list comprehensions when mapping a built-in function such as abs (absolute value):


2.5 (r25:51908, Sep 19 2006, 09:52:17) [MSC v.1310 32 bit (Intel)]
forStatement         => 6.10899996758
listComprehension    => 3.51499986649
mapFunction          => 2.73399996758
generatorExpression  => 4.11600017548


But watch what happens if we change this script to perform a real operation on each iteration, such as addition:


...
...
def forStatement(  ):
    res = []
    for x in range(size):
        res.append(x + 10)

def listComprehension(  ):
    res = [x + 10 for x in range(size)]

def mapFunction(  ):
    res = map((lambda x: x + 10), range(size))

def generatorExpression(  ):
    res = list(x + 10 for x in range(size))
...
...


The function-call requirement of the map call then makes it just as slow as the for loop statements, despite the fact the looping statements version is larger in terms of code:


2.5 (r25:51908, Sep 19 2006, 09:52:17) [MSC v.1310 32 bit (Intel)]
forStatement         => 5.25699996948
listComprehension    => 2.68400001526
mapFunction          => 5.96900010109
generatorExpression  => 3.37400007248


Because the interpreter optimizes so much internally, performance analysis of Python code like this is a very tricky affair. It's virtually impossible to guess which method will perform the best—the best you can do is time your own code, on your computer, with your version of Python. In this case, all we can say for certain is that on this Python, using a user-defined function in map calls can slow it down by at least a factor of 2, and that list comprehensions run quickest for this test.

As I've mentioned before, however, performance should not be your primary concern when writing Python code—write for readability and simplicity first, then optimize later, if and only if needed. It could very well be that any of the four alternatives is quick enough for the data sets the program needs to process; if so, program clarity should be the chief goal.

For more insight, try modifying the repetition counts at the top of this script, or see the newer timeit module, which automates timing of code, and finesses some platform-specific issues (on some platforms, for instance, time.time is preferred over time.clock). Also, see the profile standard library module for a complete source code profiler tool.

			30.3.3.8  Function Design Concepts
When you start using functions, you're faced with choices about how to glue components together—for instance, how to decompose a task into purposeful functions (resulting in cohesion), how your functions should communicate (coupling), and so on. You need to take into account concepts such as cohesion, coupling, and the size of the functions—some of this falls into the category of structured analysis and design. We introduced some ideas related to function and module coupling in the prior chapter, but here is a review of a few general guidelines for Python beginners:

    * Coupling: use arguments for inputs andreturnfor outputs. Generally, you should strive to make a function independent of things outside of it. Arguments and return statements are often the best ways to isolate external dependencies to a small number of well-known places in your code.
    * Coupling: use global variables only when truly necessary. Global variables (i.e., names in the enclosing module) are usually a poor way for functions to communicate. They can create dependencies and timing issues that make programs difficult to debug and change.
    * Coupling: don't change mutable arguments unless the caller expects it. Functions can change parts of passed-in mutable objects, but as with global variables, this implies lots of coupling between the caller and callee, which can make a function too specific and brittle.
    * Cohesion: each function should have a single, unified purpose. When designed well, each of your functions should do one thing—something you can summarize in a simple declarative sentence. If that sentence is very broad (e.g., "this function implements my whole program"), or contains lots of conjunctions (e.g., "this function gives employee raises and submits a pizza order"), you might want to think about splitting it into separate and simpler functions. Otherwise, there is no way to reuse the code behind the steps mixed together in the function.
    * Size: each function should be relatively small. This naturally follows from the preceding goal, but if your functions start spanning multiple pages on your display, it's probably time to split them. Especially given that Python code is so concise to begin with, a long or deeply nested function is often a symptom of design problems. Keep it simple, and keep it short.
    * Coupling: avoid changing variables in another module file directly. We introduced this concept in the prior chapter, and we'll revisit it in the next part of the book when we focus on modules. For reference, though, remember that changing variables across file boundaries sets up a coupling between modules similar to how global variables couple functions—the modules become difficult to understand and reuse. Use accessor functions whenever possible, instead of direct assignment statements.

Figure 17-1 summarizes the ways functions can talk to the outside world; inputs may come from items on the left side, and results may be sent out in any of the forms on the right. Many function designers prefer to use only arguments for inputs, and return statements for outputs.

Figure 17-1. Function execution environment. Functions may obtain input and produce output in a variety of ways, though functions are usually easier to understand and maintain if you use arguments for input, and return statements and anticipated mutable argument changes for output.


Of course, there are plenty of exceptions to the preceding design rules, including some related to Python's OOP support. As you'll see in Part 6, Python classes depend on changing a passed-in mutable object—class functions set attributes of an automatically passed-in argument called self to change per-object state information (e.g., self.name='bob'). Moreover, if classes are not used, global variables are often the best way for functions in modules to retain state between calls. The side effects aren't dangerous if they're expected.

				30.3.3.8.1  Functions Are Objects: Indirect Calls

Because Python functions are objects at runtime, you can write programs that process them generically. Function objects can be assigned, passed to other functions, stored in data structures, and so on, as if they were simple numbers or strings. We've seen some of these uses in earlier examples. Function objects also happen to support a special operation: they can be called by listing arguments in parentheses after a function expression. Still, functions belong to the same general category as other objects.

For instance, there's really nothing special about the name used in a def statement: it's just a variable assigned in the current scope, as if it had appeared on the left of an = sign. After a def runs, the function name is simply a reference to an object, and you can reassign that object to other names, and call it through any reference (not just the original name):


>>> def echo(message):  # echo assigned to a function object
...     print message 
...
>>> x = echo  # Now x references it too
>>> x('Hello world!')  # Call the object by adding (  )
Hello world!


Because arguments are passed by assigning objects, it's just as easy to pass functions to other functions as arguments. The callee may then call the passed-in function just by adding arguments in parentheses:


>>> def indirect(func, arg): 
...     func(arg)  # Call the object by adding (  )
...
>>> indirect(echo, 'Hello jello!')  # Pass the function to a function
Hello jello!


You can even stuff function objects into data structures, as though they were integers or strings. Because Python compound types can contain any sort of object, there's no special case here either:


>>> schedule = [ (echo, 'Spam!'), (echo, 'Ham!') ]
>>> for (func, arg) in schedule:
...     func(arg)
...
Spam!
Ham!


This code simply steps through the schedule list, calling the echo function with one argument each time through (notice the tuple-unpacking assignment in the for loop header, introduced in Chapter 13). Python's lack of type declarations makes for an incredibly flexible programming language.


			30.3.3.9  Function Gotchas
Functions have some jagged edges that you might not expect. They're all obscure, and a few have started to fall away from the language completely in recent releases, but most have been known to trip up new users.

				30.3.3.9.1  Local Names Are Detected Statically

As you know, Python classifies names assigned in a function as locals by default; they live in the function's scope and exist only while the function is running. What I didn't tell you is that Python detects locals statically, when it compiles the def's code, rather than by noticing assignments as they happen at runtime. This leads to one of the most common oddities posted on the Python newsgroup by beginners.

Normally, a name that isn't assigned in a function is looked up in the enclosing module:


>>> X = 99 
>>> def selector(  ):  # X used but not assigned
...     print X  # X found in global scope
...
>>> selector(  ) 
99


Here, the X in the function resolves to the X in the module. But watch what happens if you add an assignment to X after the reference:
>>> def selector(  ): 
...     print X  # Does not yet exist!
...     X = 88  # X classified as a local name (everywhere)
...                           # Can also happen if "import X", "def X" . . .
>>> selector(  ) 
Traceback (most recent call last):
  File "<stdin>", line 1, in ?
  File "<stdin>", line 2, in selector
UnboundLocalError: local variable 'X' referenced before assignment

					  


You get an undefined name error, but the reason is subtle. Python reads and compiles this code when it's typed interactively or imported from a module. While compiling, Python sees the assignment to X, and decides that X will be a local name everywhere in the function. But, when the function is actually run, because the assignment hasn't yet happened when the print executes, Python says you're using an undefined name. According to its name rules, it should say this; the local X is used before being assigned. In fact, any assignment in a function body makes a name local. Imports, =, nested defs, nested classes, and so on, are all susceptible to this behavior.

The problem occurs because assigned names are treated as locals everywhere in a function, not just after the statements where they are assigned. Really, the previous example is ambiguous at best: was the intention to print the global X and then create a local X, or is this a genuine programming error? Because Python treats X as a local everywhere, it is an error; if you really mean to print the global X, you need to declare it in a global statement:


>>> def selector(  ): 
...     global X  # Force X to be global (everywhere)
...     print X 
...     X = 88 
...
>>> selector(  ) 
99


Remember, though, that this means the assignment also changes the global X, not a local X. Within a function, you can't use both local and global versions of the same simple name. If you really meant to print the global, and then set a local of the same name, import the enclosing module, and use module attribute notation to get to the global version:


>>> X = 99 
>>> def selector(  ): 
...     import __main__  # Import enclosing module
...     print __main__.X  # Qualify to get to global version of name
...     X = 88  # Unqualified X classified as local
...     print X  # Prints local version of name
...
>>> selector(  ) 
99
88


Qualification (the .X part) fetches a value from a namespace object. The interactive namespace is a module called __main__, so __main__.X reaches the global version of X. If that isn't clear, check out Part 5.[49]

    [49] * Python has improved on this story somewhat by issuing for this case the more specific "unbound local" error message shown in the example listing (it used to simply raise a generic name error); this gotcha is still present in general, though.

				30.3.3.9.2 Defaults and Mutable Objects

Default argument values are evaluated and saved when a def statement is run, not when the resulting function is called. Internally, Python saves one object per default argument attached to the function itself.

That's usually what you want—because defaults are evaluated at def time, it lets you save values from the enclosing scope, if needed. But because a default retains an object between calls, you have to be careful about changing mutable defaults. For instance, the following function uses an empty list as a default value, and then changes it in-place each time the function is called:


>>> def saver(x=[]):  # Saves away a list object
...     x.append(1)  # Changes same object each time!
...     print x 
...
>>> saver([2])  # Default not used
[2, 1]
>>> saver(  )  # Default used
[1]
>>> saver(  )  # Grows on each call!
[1, 1]
>>> saver(  ) 
[1, 1, 1]


Some see this behavior as a feature—because mutable default arguments retain their state between function calls, they can serve some of the same roles as static local function variables in the C language. In a sense, they work sort of like global variables, but their names are local to the functions, and so will not clash with names elsewhere in a program.

To most observers, though, this seems like a gotcha, especially the first time they run into it. There are better ways to retain state between calls in Python (e.g., using classes, which will be discussed in Part 6).

Moreover, mutable defaults are tricky to remember (and to understand at all). They depend upon the timing of default object construction. In the prior example, there is just one list object for the default value—the one created when the def is executed. You don't get a new list every time the function is called, so the list grows with each new append; it is not reset to empty on each call.

If that's not the behavior you want, simply make a copy of the default at the start of the function body, or move the default value expression into the function body. As long as the value resides in code that's actually executed each time the function runs, you'll get a new object each time through:


>>> def saver(x=None): 
...     if x is None:  # No argument passed?
...         x = []  # Run code to make a new list
...     x.append(1)  # Changes new list object
...     print x 
...
>>> saver([2]) 
[2, 1]
>>> saver(  )  # Doesn't grow here
[1]
>>> saver(  ) 
[1]


By the way, the if statement in this example could almost be replaced by the assignment x = x or [], which takes advantage of the fact that Python's or returns one of its operand objects: if no argument was passed, x would default to None, so the or would return the new empty list on the right.

However, this isn't exactly the same. If an empty list were passed in, the or expression would cause the function to extend and return a newly created list, rather than extending and returning the passed-in list like the if version. (The expression becomes [] or [], which evaluates to the new empty list on the right; see "Section 12.3" in Chapter 12, if you don't recall why). Real program requirements may call for either behavior.

				30.3.3.9.3 Functions Without returns

In Python functions, return (and yield) statements are optional. When a function doesn't return a value explicitly, the function exits when control falls off the end of the function body. Technically, all functions return a value; if you don't provide a return statement, your function returns the None object automatically:


>>> def proc(x): 
...     print x  # No return is a None return
...
>>> x = proc('testing 123...') 
testing 123...
>>> print x 
None


Functions such as this without a return are Python's equivalent of what are called "procedures" in some languages. They're usually invoked as statements, and the None results are ignored, as they do their business without computing a useful result.

This is worth knowing because Python won't tell you if you try to use the result of a function that doesn't return one. For instance, assigning the result of a list append method won't raise an error, but you'll get back None, not the modified list:


>>> list = [1, 2, 3] 
>>> list = list.append(4)  # append is a "procedure"
>>> print list  # append changes list in-place
None


As mentioned in "Section 14.2" in Chapter 14, such functions do their business as a side effect, and are usually designed to be run as statements, not expressions.

				30.3.3.9.4 Enclosing Scope Loop Variables

We described this gotcha in Chapter 16's discussion of enclosing function scopes, but, as a reminder, be careful about relying on enclosing function scope lookup for variables that are changed by enclosing loops—all such references will remember the value of the last loop iteration. Use defaults to save loop variable values instead (see Chapter 16 for more details on this topic).

			30.3.3.10 Chapter Summary
This chapter took us on a tour of advanced function-related concepts—lambda expression functions; generator functions with yield statements; generator expressions; apply-like call syntax; functional tools such as map, filter, and reduce; and general function design ideas. We also revisited iterators and list comprehensions here because they are just as related to functional programming as to looping statements. As a wrap-up for iteration concepts, we also measured the performance of iteration alternatives. Finally, we reviewed common function-related mistakes to help you sidestep potential pitfalls.

This concludes the functions part of this book. In the next part, we will study modules, the topmost organizational structure in Python, and the structure in which our functions always live. After that, we will explore classes, tools that are largely packages of functions with special first arguments. As we'll see, everything we have learned here will apply when functions pop up later in the book.

Before you move on, though, make sure you've mastered function basics by working through this chapter's quiz and the exercises for this part.

BRAIN BUILDER

Chapter Quiz

Q:	What is the difference between enclosing a list comprehension in square brackets and parentheses?
Q:	How are generators and iterators related?
Q:	How can you tell if a function is a generator function?
Q:	What does a yield statement do?
Q:	Given a function object, and a tuple of arguments, how might you call the function?
Q:	How are map calls and list comprehensions related? Compare and contrast the two.
Q:	How are lambda expressions and def statements related? Compare and contrast the two.


Quiz Answers

A:	List comprehensions in square brackets produce the result list all at once in memory. When they are enclosed in parentheses instead, they are actually generator expressions—they have a similar meaning, but do not produce the result list all at once. Instead, generator expressions return a generator object, which yields one item in the result at a time when used in an iteration context.
A:	Generators are objects that support the iteration protocol—they have a next method that repeatedly advances to the next item in a series of results, and raises an exception at the end of the series. In Python, we can code generator functions with def, generator expressions with parenthesized list comprehensions, and generator objects with classes that define a special method named __iter__ (discussed later in the book).
A:	A generator function has a yield statement somewhere in its code. Generator functions are otherwise identical to normal functions.
A:	When present, this statement makes Python compile the function specially as a generator; when called, it returns a generator object that supports the iteration protocol. When the yield statement is run, it sends a result back to the caller, and suspends the function's state; the function can then be resumed in response to a next method call at the caller continuing after the last yield statement. Generator functions may also have a return statement, which terminates the generator.
A:	You can call the function generically with the apply-like call syntax: function(*argstuple). You can also use the built-in function apply(function, args), but this built-in will likely be removed in a future Python release, and is not as general.
A:	The map call is similar to a list comprehension—both build a new list by collecting the results of applying an operation to each item in a sequence or other iterable, one item at a time. The main difference is that map applies a function call to each item, and list comprehensions apply arbitrary expressions. Because of this, list comprehensions are more general; they can apply a function call expression like map, but map requires a function to apply other kinds of expressions. List comprehensions also support extended syntax such as nested for loops and if clauses that subsume the filter built-in.
A:	Both lambda and def create function objects to be called later. Because lambda is an expression, though, it can be used to nest a function definition in places where a def will not work syntactically. Using a lambda is never required—you can always code a def instead, and reference the function by name. lambdas come in handy, though, to embed small pieces of deferred code that are unlikely to be used elsewhere in a program. Syntactically, a lambda only allows for a single return value expression; because it does not support a block of statements, it is not ideal for larger functions.

			30.3.3.11 BRAIN BUILDER
In these exercises, you're going to start coding more sophisticated programs. Be sure to check the solutions in "Section B.4" in Appendix B, and be sure to start writing your code in module files. You won't want to retype these exercises from scratch if you make a mistake.

   |1.  The basics. At the Python interactive prompt, write a function that prints its single argument to the screen and call it interactively, passing a variety of object types: string, integer, list, dictionary. Then, try calling it without passing any argument. What happens? What happens when you pass two arguments?

My solution:
>>> def parg(arg):
...     print arg
... 
>>> parg("a")
a
>>> parg(3)
3
>>> parg({1:5,"a":"b"})
{'a': 'b', 1: 5}
>>> parg()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: parg() takes exactly 1 argument (0 given)
>>> parg(1,2)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: parg() takes exactly 1 argument (2 given)

   |2.  Arguments. Write a function called adder in a Python module file. The function should accept two arguments, and return the sum (or concatenation) of the two. Then, add code at the bottom of the file to call the adder function with a variety of object types (two strings, two lists, two floating points), and run this file as a script from the system command line. Do you have to print the call statement results to see results on your screen?

My solution:
#adder.py

def adder(a,b): return a+b

print adder('a','b')
print adder(range(4),['a','b'])
print adder(5.87,6.04)

:!python /cygdrive/c/work/scripts/python/learn/learning_python/adder.py
ab
[0, 1, 2, 3, 'a', 'b']
11.91
        
   |3.  varargs. Generalize the adder function you wrote in the last exercise to compute the sum of an arbitrary number of arguments, and change the calls to pass more or less than two arguments. What type is the return value sum? (Hints: a slice such as S[:0] returns an empty sequence of the same type as S, and the type built-in function can test types; but see the min examples in Chapter 16 for a simpler approach.) What happens if you pass in arguments of different types? What about passing in dictionaries?

My solution:
#adder.py

def adder(a,b): return a+b

def adder2(*args): 
	sum = args[0]
	for arg in args[1:]:
		sum = sum + arg
	return sum

print adder('a','b')
print adder(range(4),['a','b'])
print adder(5.87,6.04)

print adder2('a','b', 'c')
print adder2(range(4))

:!python /cygdrive/c/work/scripts/python/learn/learning_python/adder.py
ab
[0, 1, 2, 3, 'a', 'b']
11.91
abc
[0, 1, 2, 3]  

Textbook solution:
varargs. Two alternative adder functions are shown in the following file, adders.py. The hard part here is figuring out how to initialize an accumulator to an empty value of whatever type is passed in. The first solution uses manual type testing to look for an integer, and an empty slice of the first argument (assumed to be a sequence) if the argument is determined not to be an integer. The second solution uses the first argument to initialize and scan items 2 and beyond, much like one of the min function variants shown in Chapter 16.

The second solution is better. Both of these assume all arguments are of the same type, and neither works on dictionaries (as we saw in Part 2, + doesn't work on mixed types or dictionaries). You could add a type test and special code to allow dictionaries, too, but that's extra credit:

def adder1(*args):
    print 'adder1',
    if type(args[0]) == type(0):               # Integer?
         sum = 0                               # Init to zero
    else:                                      # else sequence:
         sum = args[0][:0]                     # Use empty slice of arg1
    for arg in args:
        sum = sum + arg
    return sum

def adder2(*args):
    print 'adder2',
    sum = args[0]                              # Init to arg1
    for next in args[1:]:
        sum += next                            # Add items 2..N
    return sum

for func in (adder1, adder2):
    print func(2, 3, 4)
    print func('spam', 'eggs', 'toast')
    print func(['a', 'b'], ['c', 'd'], ['e', 'f'])

% python adders.py 
adder1 9
adder1 spameggstoast
adder1 ['a', 'b', 'c', 'd', 'e', 'f']
adder2 9
adder2 spameggstoast
adder2 ['a', 'b', 'c', 'd', 'e', 'f']

					  
   |4.  Keywords. Change the adder function from exercise 2 to accept and sum/concatenate three arguments: def adder(good, bad, ugly). Now, provide default values for each argument, and experiment with calling the function interactively. Try passing one, two, three, and four arguments. Then, try passing keyword arguments. Does the call adder(ugly=1, good=2) work? Why? Finally, generalize the new adder to accept and sum/concatenate an arbitrary number of keyword arguments. This is similar to what you did in exercise 3, but you'll need to iterate over a dictionary, not a tuple. (Hint: the dict.keys( ) method returns a list you can step through with a for or while.)

#adder.py

def adder(a,b): return a+b

def adder2(*args): 
	print 'adder2'
	sum = args[0]
	for arg in args[1:]:
		sum = sum + arg
	return sum

def adder3(a=1,b=2,c=3): 
	print 'adder3'
	return a+b+c

def adder4(**args): 
	if type(args[args.keys()[0]]) == type(0):               # Integer?
	     sum = 0                               # Init to zero
	elif type(args[args.keys()[0]]) == type('0'):               # String
		sum=''
	else:                                      # else sequence:
	     sum = []                              # Use empty slice of arg1
	print 'adder4'
	for k in args.keys():
		sum = sum + args[k]
	return sum

print adder('a','b')
print adder(range(4),['a','b'])
print adder(5.87,6.04)

print adder2('a','b', 'c')
print adder2(range(4))

print adder3(4,5,6)
#print adder3('a')
#print adder3(range(4))

print adder4(a=1, b=100, c=37651)
print adder4(a='1', b='100', c='37651')
print adder4(a=[1,2,3], b=range(10))

:!python /cygdrive/c/work/scripts/python/learn/learning_python/adder.py
ab
[0, 1, 2, 3, 'a', 'b']
11.91
adder2
abc
adder2
[0, 1, 2, 3]
adder3
15
adder4
37752
adder4
137651100
adder4
[1, 2, 3, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]  


   |5.  Write a function called copyDict(dict) that copies its dictionary argument. It should return a new dictionary containing all the items in its argument. Use the dictionary keys method to iterate (or, in Python 2.2, step over a dictionary's keys without calling keys). Copying sequences is easy (X[:] makes a top-level copy); does this work for dictionaries too?

My solution:
def dict_cp(dict):
	return dict.copy()

Textbook solution:
(and 6.) Here are my solutions to exercises 5 and 6 (file dicts.py). These are just coding exercises, though, because Python 1.5 added the dictionary methods D.copy( ) and D1.update(D2) to handle things like copying and adding (merging) dictionaries. (See Python's library manual or O'Reilly's Python Pocket Reference for more details.) X[:] doesn't work for dictionaries, as they're not sequences (see Chapter 8 for details). Also, remember that if you assign (e = d) rather than copying, you generate a reference to a shared dictionary object; changing d changes e, too:
Code View: Scroll / Show All


def copyDict(old):
    new = {}
    for key in old.keys(  ):
        new[key] = old[key]
    return new

def addDict(d1, d2):
    new = {}
    for key in d1.keys(  ):
        new[key] = d1[key]
    for key in d2.keys(  ):
        new[key] = d2[key]
    return new

% python
>>> from dicts import *
>>> d = {1: 1, 2: 2}
>>> e = copyDict(d)
>>> d[2] = '?'
>>> d
{1: 1, 2: '?'}
>>> e
{1: 1, 2: 2}

>>> x = {1: 1}
>>> y = {2: 2}
>>> z = addDict(x, y)
>>> z
{1: 1, 2: 2}

					  
   |6.  Write a function called addDict(dict1, dict2) that computes the union of two dictionaries. It should return a new dictionary containing all the items in both its arguments (which are assumed to be dictionaries). If the same key appears in both arguments, feel free to pick a value from either. Test your function by writing it in a file and running the file as a script. What happens if you pass lists instead of dictionaries? How could you generalize your function to handle this case, too? (Hint: see the type built-in function used earlier.) Does the order of the arguments passed in matter?

   |7.  More argument-matching examples. First, define the following six functions (either interactively or in a module file that can be imported):


          def f1(a, b): print a, b             # Normal args
          def f2(a, *b): print a, b            # Positional varargs

          def f3(a, **b): print a, b           # Keyword varargs

          def f4(a, *b, **c): print a, b, c    # Mixed modes

          def f5(a, b=2, c=3): print a, b, c   # Defaults

          def f6(a, b=2, *c): print a, b, c    # Defaults and positional varargs


      Now, test the following calls interactively, and try to explain each result; in some cases, you'll probably need to fall back on the matching algorithm shown in Chapter 16. Do you think mixing matching modes is a good idea in general? Can you think of cases where it would be useful?


          >>> f1(1, 2)
	1 2

          >>> f1(b=2, a=1)
	1 2

          >>> f2(1, 2, 3)
	1 (2, 3)

          >>> f3(1, x=2, y=3)
	1 {'y': 3, 'x': 2}

          >>> f4(1, 2, 3, x=2, y=3)
1 (2, 3) {'y': 3, 'x': 2}

          >>> f5(1)
1 2 3

          >>> f5(1, 4)
1 4 3

          >>> f6(1)
1 2 ()

          >>> f6(1, 3, 4)
1 3 (4,)

Textbook solution:
% python 
>>> f1(1, 2)  # Matched by position (order matters)
1 2
>>> f1(b=2, a=1)  # Matched by name (order doesn't matter)
1 2

>>> f2(1, 2, 3)  # Extra positionals collected in a tuple
1 (2, 3)

>>> f3(1, x=2, y=3)  # Extra keywords collected in a dictionary
1 {'x': 2, 'y': 3}

>>> f4(1, 2, 3, x=2, y=3)  # Extra of both kinds
1 (2, 3) {'x': 2, 'y': 3}

>>> f5(1)  # Both defaults kick in
1 2 3
>>> f5(1, 4)  # Only one default used
1 4 3

>>> f6(1)  # One argument: matches "a"
1 2 (  )
>>> f6(1, 3, 4)  # Extra positional collected
1 3 (4,)


   |8.  Primes revisited. Recall the following code snippet from Chapter 13, which simplistically determines whether a positive integer is prime:


          x = y / 2                            # For some y > 1
          while x > 1:
              if y % x == 0:                   # Remainder
                  print y, 'has factor', x
                  break                        # Skip else
              x = x−1
          else:                                # Normal exit
              print y, 'is prime'


      Package this code as a reusable function in a module file, and add some calls to the function at the bottom of your file. While you're at it, replace the first line's / operator with //, so it can handle floating-point numbers, too, and is immune to the true division change planned for the / operator in Python 3.0 (described in Chapter 5). What can you do about negatives, and the values 0 and 1? How about speeding this up? Your outputs should look something like this:


          13 is prime
          13.0 is prime
          15 has factor 5
          15.0 has factor 5.0

Textbook solution:
#from __future__ import division

def prime(y):
    if y <= 1:                                       # For some y > 1
        print y, 'not prime'
    else:
        x = y // 2                                   # Future / fails
        while x > 1:
            if y % x == 0:                           # No remainder?
                print y, 'has factor', x
                break                                # Skip else
            x -= 1
        else:
            print y, 'is prime'

prime(13); prime(13.0)
prime(15); prime(15.0)
prime(3);  prime(2)
prime(1);  prime(-3)


Here is the module in action; the // operator allows it to works for floating-point numbers too, even though it perhaps should not.


% python primes.py
13 is prime
13.0 is prime
15 has factor 5
15.0 has factor 5.0
3 is prime
2 is prime
1 not prime
-3 not prime


This function still isn't very reusable, yet—it could return values, instead of printing—but it's enough to run experiments. It's also not a strict mathematical prime (floating-points work), and is still inefficient. Improvements are left as exercises for more mathematically minded readers. Hint: a for loop over range(y, 1, −1) may be a bit quicker than the while (in fact, it's roughly twice as fast in 2.2), but the algorithm is the real bottleneck here. To time alternatives, use the built-in time module and coding patterns like those used in this general function-call timer (see the library manual for details):


def timer(reps, func, *args):
    import time
    start = time.clock(  )
    for i in xrange(reps):
        apply(func, args)
    return time.clock(  ) - start

   |9.  List comprehensions. Write code to build a new list containing the square roots of all the numbers in this list: [2, 4, 9, 16, 25]. Code this as a for loop first, then as a map call, and finally as a list comprehension. Use the sqrt function in the built-in math module to do the calculation (i.e., import math and say math.sqrt(x)). Of the three, which approach do you like best?

l=[2, 4, 9, 16, 25]
r=[]

-- As for loop
for e in l:
	r.append(e^2)
print r
>>> l=[2, 4, 9, 16, 25]
>>> r=[]
>>> for e in l:
...     r.append(e**2)
... 
>>> print r

-- as map
r = map(lambda x: x**2, l)
>>> r=[]
>>> print r
[]
>>> r = map(lambda x: x**2, l)
>>> print r

-- as list comprehension
r = [x**2 for x in l]
>>> print r
[4, 16, 81, 256, 625]

-- As generator
>>> r = (x**2 for x in l)
>>> print r
<generator object <genexpr> at 0x7ef4452c>
>>> r.next()
4
>>> r.next()
16
>>> r.next()
81
>>> r.next()
256
>>> r.next()
625
>>> r.next()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
StopIteration

	Textbook solution:
>>> values = [2, 4, 9, 16, 25]
>>> import math

>>> res = []
>>> for x in values: res.append(math.sqrt(x))
...
>>> res
[1.4142135623730951, 2.0, 3.0, 4.0, 5.0]

>>> map(math.sqrt, values)
[1.4142135623730951, 2.0, 3.0, 4.0, 5.0]

>>> [math.sqrt(x) for x in values]
[1.4142135623730951, 2.0, 3.0, 4.0, 5.0]


	30.4. Modules


 	 	30.4.1. Modules: The Big Picture
This chapter begins our in-depth look at the Python module, the highest-level program organization unit, which packages program code and data for reuse. In concrete terms, modules usually correspond to Python program files (or extensions coded in external languages such as C, Java, or C#). Each file is a module, and modules import other modules to use the names they define. Modules are processed with two statements, and one important built-in function:


import 
    Lets a client (importer) fetch a module as a whole

from 
    Allows clients to fetch particular names from a module

reload 
    Provides a way to reload a module's code without stopping Python

Chapter 3 introduced module fundamentals, and we've been using them ever since. Part 5 begins by expanding on core module concepts, then moves on to explore more advanced module usage. This first chapter offers a general look at the role of modules in overall program structure. In the next and following chapters, we'll dig into the coding details behind the theory.

Along the way, we'll flesh out module details omitted so far: you'll learn about reloads, the __name__ and __all__ attributes, package imports, and so on. Because modules and classes are really just glorified namespaces, we'll formalize namespace concepts here as well.

			30.4.1.1 Why Use Modules?

In short, modules provide an easy way to organize components into a system by serving as self-contained packages of variables known as namespaces. All the names defined at the top level of a module file become attributes of the imported module object. As we saw in the last part of this book, imports give access to names in a module's global scope. That is, the module file's global scope morphs into the module object's attribute namespace when it is imported. Ultimately, Python's modules allow us to link individual files into a larger program system.

More specifically, from an abstract perspective, modules have at least three roles:


Code reuse

    As discussed in Chapter 3, modules let you save code in files permanently. Unlike code you type at the Python interactive prompt, which goes away when you exit Python, code in module files is persistent—it can be reloaded and rerun as many times as needed. More to the point, modules are a place to define names, known as attributes, that may be referenced by multiple external clients.

System namespace partitioning

    Modules are also the highest-level program organization unit in Python. Fundamentally, they are just packages of names. Modules seal up names into self-contained packages, which helps avoid name clashes—you can never see a name in another file, unless you explicitly import that file. In fact, everything "lives" in a module—code you execute and objects you create—are always implicitly enclosed in modules. Because of that, modules are natural tools for grouping system components.

Implementing shared services or data

    From an operational perspective, modules also come in handy for implementing components that are shared across a system, and hence require only a single copy. For instance, if you need to provide a global object that's used by more than one function or file, you can code it in a module that can then be imported by many clients.

For you to truly understand the role of modules in a Python system, though, we need to digress for a moment, and explore the general structure of a Python program.

			30.4.1.2 Python Program Architecture

So far in this book, I've sugarcoated some of the complexity in my descriptions of Python programs. In practice, programs usually involve more than just one file; for all but the simplest scripts, your programs will take the form of multifile systems. And even if you can get by with coding a single file yourself, you will almost certainly wind up using external files that someone else has already written.

This section introduces the general architecture of Python programs—the way you divide a program into a collection of source files (a.k.a. modules) and link the parts into a whole. Along the way, we'll also explore the central concepts of Python modules, imports, and object attributes.


				30.4.1.2.1 How to Structure a Program

Generally, a Python program consists of multiple text files containing Python statements. The program is structured as one main, top-level file, along with zero or more supplemental files known as modules in Python.

In Python, the top-level file contains the main flow of control of your program—this is the file you run to launch your application. The module files are libraries of tools used to collect components used by the top-level file (and possibly elsewhere). Top-level files use tools defined in module files, and modules use tools defined in other modules.

Module files generally don't do anything when run directly; rather, they define tools intended for use in other files. In Python, a file imports a module to gain access to the tools it defines, which are known as its attributes (i.e., variable names attached to objects such as functions). Ultimately, we import modules and access their attributes to use their tools.

				30.4.1.2.2 Imports and Attributes

Let's make this a bit more concrete. Figure 18-1 sketches the structure of a Python program composed of three files: a.py, b.py, and c.py. The file a.py is chosen to be the top-level file; it will be a simple text file of statements, which is executed from top to bottom when launched. The files b.py and c.py are modules; they are simple text files of statements as well, but they are not usually launched directly. Instead, as explained previously, modules are normally imported by other files that wish to use the tools they define.

Figure 18-1. Program architecture in Python. A program is a system of modules. It has one top-level script file (launched to run the program), and multiple module files (imported libraries of tools). Scripts and modules are both text files containing Python statements, though the statements in modules usually just create objects to be used later. Python's standard library provides a collection of precoded modules.


For instance, suppose the file b.py in Figure 18-1 defines a function called spam, for external use. As we learned in Part 4, b.py will contain a Python def statement to generate the function, which can later be run by passing zero or more values in parentheses after the function's name:


def spam(text):
    print text, 'spam'


Now, suppose a.py wants to use spam. To this end, it might contain Python statements such as the following:


import b
b.spam('gumby')


The first of these, a Python import statement, gives the file a.py access to everything defined by top-level code in the file b.py. It roughly means "load the file b.py (unless it's already loaded), and give me access to all its attributes through the name b." import (and, as you'll see later, from) statements execute and load other files at runtime.

In Python, cross-file module linking is not resolved until such import statements are executed at runtime; their net effect is to assign module names—simple variables—to loaded module objects. In fact, the module name used in an import statement serves two purposes: it identifies the external file to be loaded, but it also becomes a variable assigned to the loaded module. Objects defined by a module are also created at runtime, as the import is executing: import literally runs statements in the target file one at a time to create its contents.

The second of the statements in a.py calls the function spam defined in the module b, using object attribute notation. The code b.spam means "fetch the value of the name spam that lives within the object b." This happens to be a callable function in our example, so we pass a string in parentheses ('gumby'). If you actually type these files, save them, and run a.py, the words "gumby spam" will be printed.

You'll see the object.attribute notation used throughout Python scripts—most objects have useful attributes that are fetched with the "." operator. Some are callable things like functions, and others are simple data values that give object properties (e.g., a person's name).

The notion of importing is also completely general throughout Python. Any file can import tools from any other file. For instance, the file a.py may import b.py to call its function, but b.py might also import c.py to leverage different tools defined there. Import chains can go as deep as you like: in this example, the module a can import b, which can import c, which can import b again, and so on.

Besides serving as the highest organizational structure, modules (and module packages, described in Chapter 20) are also the highest level of code reuse in Python. Coding components in module files makes them useful in your original program, and in any other programs you may write. For instance, if after coding the program in Figure 18-1 we discover that the function b.spam is a general-purpose tool, we can reuse it in a completely different program; all we have to do is import the file b.py again from the other program's files.


				30.4.1.2.3 Standard Library Modules

Notice the rightmost portion of Figure 18-1. Some of the modules that your programs will import are provided by Python itself, and are not files you will code.

Python automatically comes with a large collection of utility modules known as the standard library. This collection, roughly 200 modules large at last count, contains platform-independent support for common programming tasks: operating system interfaces, object persistence, text pattern matching, network and Internet scripting, GUI construction, and much more. None of these tools are part of the Python language itself, but you can use them by importing the appropriate modules on any standard Python installation. Because they are standard library modules, you can also be reasonably sure that they will be available, and will work portably on most platforms on which you will run Python.

You will see a few of the standard library modules in action in this book's examples, but for a complete look, you should browse the standard Python library reference manual, available either with your Python installation (via IDLE or the Python Start button menu on Windows), or online at http://www.python.org.

Because there are so many modules, this is really the only way to get a feel for what tools are available. You can also find tutorials on Python library tools in commercial books that cover application-level programming, such as Programming Python, but the manuals are free, viewable in any web browser (they ship in HTML format), and updated each time Python is rereleased.

			30.4.1.3 How Imports Work

The prior section talked about importing modules without really explaining what happens when you do so. Because imports are at the heart of program structure in Python, this section goes into more detail on the import operation to make this process less abstract.

Some C programmers like to compare the Python module import operation to a C #include, but they really shouldn't—in Python, imports are not just textual insertions of one file into another. They are really runtime operations that perform three distinct steps the first time a program imports a given file:

   |1.  Find the module's file.
   |2.  Compile it to byte code (if needed).
   |3.  Run the module's code to build the objects it defines.


To better understand module imports, we'll explore these steps in turn. Bear in mind that all three of these steps are carried out only the first time a module is imported during a program's execution; later imports of the same module bypass all of these steps, and simply fetch the already loaded module object in memory.

				30.4.1.3.1 1. Find It

First off, Python must locate the module file referenced by an import statement. Notice that the import statement in the prior section's example names the file without a .py suffix and without its directory path: it just says import b, instead of something like import c:\dir1\b.py. In fact, you can only list a simple name; path and suffix details are omitted on purpose, as Python uses a standard module search path to locate the module file corresponding to an import statement.[50] Because this is the main part of the import operation that programmers must know about, let's study this step in more detail.

    [50] * It's actually syntactically illegal to include path and suffix details in a standard import. Package imports, which we'll discuss in Chapter 20, allow import statements to include part of the directory path leading to a file as a set of period-separated names; however, package imports still rely on the normal module search path to locate the leftmost directory in a package path (i.e., they are relative to a directory in the search path). They also cannot make use of any platform-specific directory syntax in the import statements; such syntax only works on the search path. Also, note that module file search path issues are not as relevant when you run frozen executables (discussed in Chapter 2); they typically embed byte code in the binary image.

					30.4.1.3.1.1 The module search path

In many cases, you can rely on the automatic nature of the module import search path, and need not configure this path at all. If you want to be able to import files across user-defined directory boundaries, though, you will need to know how the search path works in order to customize it. Roughly, Python's module search path is composed of the concatenation of these major components, some of which are preset for you, and some of which you can tailor to tell Python where to look:

   |1.  The home directory of the program.
   |2.  PYTHONPATH directories (if set).
   |3.  Standard library directories.
   |4.  The contents of any .pth files (if present).


Ultimately, the concatenation of these four components becomes sys.path, a list of directory name strings that I'll expand upon in the next section. The first and third elements of the search path are defined automatically, but because Python searches the concatenation of these components from first to last, the second and fourth elements can be used to extend the path to include your own source code directories. Here is how Python uses each of these path components:


Home directory

    Python first looks for the imported file in the home directory. Depending on how you are launching code, this is either the directory containing your program's top-level file, or the directory in which you are working interactively. Because this directory is always searched first, if a program is located entirely in a single directory, all of its imports will work automatically with no path configuration required.

PYTHONPATH directories

    Next, Python searches all directories listed in your PYTHONPATH environment variable setting, from left to right (assuming you have set this at all). In brief, PYTHONPATH is simply set to a list of user-defined and platform-specific names of directories that contain Python code files. You can add all the directories from which you wish to be able to import, and Python will use your setting to extend the module search path.

    Because Python searches the home directory first, this setting is only important when importing files across directory boundaries—that is, if you need to import a file that is stored in a different directory from the file that imports it. You'll probably want to set your PYTHONPATH variable once you start writing substantial programs, but when you're first starting out, as long as you save all your module files in the directory in which you're working (i.e., the home directory), your imports will work without you needing to worry about this setting at all.

Standard library directories

    Next, Python automatically searches the directories where the standard library modules are installed on your machine. Because these are always searched, they normally do not need to be added to your PYTHONPATH.

.pth file directories

    Finally, a relatively new feature of Python allows users to add valid directories to the module search path by simply listing them, one per line, in a text file whose name ends with a .pth suffix (for "path"). These path configuration files are a somewhat advanced installation-related feature, and we will not discuss them fully here.

    In short, a text file of directory names dropped in an appropriate directory can serve roughly the same role as the PYTHONPATH environment variable setting. For instance, a file named myconfig.pth may be placed at the top level of the Python install directory on Windows (e.g., in C:\Python25 or C:\Python25\Lib\site-packages) to extend the module search path. Python will add the directories listed on each line of the file, from first to last, near the end of the module search path list. Because they are files rather than shell settings, path files can apply to all users of an installation, instead of just one user or shell.

    This feature is more sophisticated than I've described here. For more details, see the Python library manual (especially its documentation for the standard library module site). I recommend that beginners use PYTHONPATH or a single .pth file, and then only if you must import across directories. See also Appendix A for examples of common ways to extend your module search path with PYTHONPATH or .pth files on various platforms.

This description of the module search path is accurate, but generic; the exact configuration of the search path is prone to changing across platforms and Python releases. Depending on your platform, additional directories may automatically be added to the module search path as well.

For instance, Python may add an entry for the current working directory—the directory from which you launched your program—in the search path after the PYTHONPATH directories, and before the standard library entries. When launching from a command line, the current working directory may not be the same as the home directory of your top-level file (i.e., the directory where your program file resides).[51] Because the current working directory can vary each time your program runs, you normally shouldn't depend on its value for import purposes.[52]

    [51] * See Chapter 3 for more on launching programs from command lines.

    [52] † See also Chapter 21's discussion of the new relative import syntax in Python 2.5; this modifies the search path for from statements when "." characters are used (e.g., from . import string).

					30.4.1.3.1.2 The sys.path list

If you want to see how the module search path is truly configured on your machine, you can always inspect the path as Python knows it by printing the built-in sys.path list (that is, the path attribute of the standard library module sys). This list of directory name strings is the actual search path within Python; on imports, Python searches each directory in this list from left to right.

Really, sys.path is the module search path. Python configures it at program startup, automatically merging any PYTHONPATH and .pth file path settings you've made into the list, and setting the first entry to identify the home directory of the top-level file (possibly as an empty string).

Python exposes this list for two good reasons. First, it provides a way to verify the search path settings you've made—if you don't see your settings somewhere in this list, you need to recheck your work. Second, if you know what you're doing, this list also provides a way for scripts to tailor their search paths manually. As you'll see later in this part of the book, by modifying the sys.path list, you can modify the search path for all future imports. Such changes only last for the duration of the script, however; PYTHONPATH and .pth files offer more permanent ways to modify the path.[53]

    [53] ‡ Some programs really need to change sys.path, though. Scripts that run on web servers, for example, usually run as the user "nobody" to limit machine access. Because such scripts cannot usually depend on "nobody" to have set PYTHONPATH in any particular way, they often set sys.path manually to include required source directories, prior to running any import statements. A sys.path.append(dirname) will often suffice.

>>> import sys
>>> print sys.path
['', '/usr/lib/python26.zip', '/usr/lib/python2.6', '/usr/lib/python2.6/plat-cygwin', '/usr/lib/python2.6/lib-tk', '/usr/lib/python2.6/lib-old', '/usr/lib/python2.6/lib-dynload', '/usr/lib/python2.6/site-packages', '/usr/lib/python2.6/site-packages/gtk-2.0']

					30.4.1.3.1.3 Module file selection

Keep in mind that filename suffixes (e.g., .py) are intentionally omitted from import statements. Python chooses the first file it can find on the search path that matches the imported name. For example, an import statement of the form import b might load:

    * A source code file named b.py.
    * A byte code file named b.pyc.
    * A directory named b, for package imports (described in Chapter 20).
    * A compiled extension module, usually coded in C or C++, and dynamically linked when imported (e.g., b.so on Linux, or b.dll or b.pyd on Cygwin and Windows).
    * A compiled built-in module coded in C and statically linked into Python.
    * A ZIP file component that is automatically extracted when imported.
    * An in-memory image, for frozen executables.
    * A Java class, in the Jython version of Python.
    * A .NET component, in the IronPython version of Python.

C extensions, Jython, and package imports all extend imports beyond simple files. To importers, though, differences in the loaded file type are completely transparent, both when importing and when fetching module attributes. Saying import b gets whatever module b is, according to your module search path, and b.attr fetches an item in the module, be it a Python variable or a linked-in C function. Some standard modules we will use in this book are actually coded in C, not Python; because of this transparency, their clients don't have to care.

If you have both a b.py and a b.so in different directories, Python will always load the one found in the first (leftmost) directory of your module search path during the left-to-right search of sys.path. But what happens if it finds both a b.py and a b.so in the same directory? In this case, Python follows a standard picking order, though this order is not guaranteed to stay the same over time. In general, you should not depend on which type of file Python will choose within a given directory—make your module names distinct, or configure your module search path to make your module selection preferences more obvious.

					30.4.1.3.1.4 Advanced module selection concepts

Normally, imports work as described in this section—they find and load files on your machine. However, it is possible to redefine much of what an import operation does in Python, using what are known as import hooks. These hooks can be used to make imports do various useful things, such as loading files from archives, performing decryption, and so on. In fact, Python itself uses these hooks to enable files to be directly imported from ZIP archives—the archived files are automatically extracted at import time when a .zip file is selected in the import search path. For more details, see the Python standard library manual's description of the built-in __import__ function, the customizable tool that import statements actually run.

Python also supports the notion of .pyo optimized byte code files, created and run with the -O Python command-line flag; because these run only slightly faster than normal .pyc files (typically 5 percent faster), however, they are infrequently used. The Psyco system (see Chapter 2) provides more substantial speedups.

				30.4.1.3.2 2. Compile It (Maybe)

After finding a source code file that matches an import statement by traversing the module search path, Python next compiles it to byte code, if necessary. (We discussed byte code in Chapter 2.)

Python checks the file timestamps and skips the source-to-byte-code compile step if it finds a .pyc byte code file that is not older than the corresponding .py source file. In addition, if Python finds only a byte code file on the search path and no source, it simply loads the byte code directly. In other words, the compile step is bypassed if possible to speed program startup. If you change the source code, Python will automatically regenerate the byte code the next time your program is run. Moreover, you can ship a program as just byte code files, and avoid sending source.

Notice that compilation happens when a file is being imported. Because of this, you will not usually see a .pyc byte code file for the top-level file of your program, unless it is also imported elsewhere—only imported files leave behind a .pyc on your machine. The byte code of top-level files is used internally and discarded; byte code of imported files is saved in files to speed future imports.

Top-level files are often designed to be executed directly and not imported at all. Later, we'll see that it is possible to design a file that serves both as the top-level code of a program, and as a module of tools to be imported. Such a file may be both executed and imported, and thus does generate a .pyc. To learn how this works, watch for the discussion of the special __name__ attribute and __main__ in Chapter 21.

				30.4.1.3.3 3. Run It

The final step of an import operation executes the byte code of the module. All statements in the file are executed in turn, from top to bottom, and any assignments made to names during this step generate attributes of the resulting module object. This execution step therefore generates all the tools that the module's code defines. For instance, def statements in a file are run at import time to create functions and assign attributes within the module to those functions. The functions can then be called later in the program by the file's importers.

Because this last import step actually runs the file's code, if any top-level code in a module file does real work, you'll see its results at import time. For example, top-level print statements in a module show output when the file is imported. Function def statements simply define objects for later use.

As you can see, import operations involve quite a bit of work—they search for files, possibly run a compiler, and run Python code. Because of this, any given module is imported only once per process by default. Future imports skip all three import steps and reuse the already loaded module in memory.[54] If you need to import a file again after it has already been loaded (for example, to support end-user customization), you have to force the issue with a reload call—a tool we'll meet in the next chapter.

    [54] * Technically, Python keeps already loaded modules in the built-in sys.modules dictionary, which it checks at the start of an import operation to determine whether the referenced module is already loaded. If you want to see which modules are loaded, import sys and print sys.modules.keys( ). More on this internal table in Chapter 21.

Third-Party Software: distutils

This chapter's description of module search path settings is targeted mainly at user-defined source code that you write on your own. Third-party extensions for Python typically use the distutils tools in the standard library to automatically install themselves, so no path configuration is required to use their code.

Systems that use distutils generally come with a setup.py script, which is run to install them; this script imports and uses distutils modules to place such systems in a directory that is automatically part of the module search path (usually in the Lib\site-packages subdirectory of the Python install tree, wherever that resides on the target machine).

For more details on distributing and installing with distutils, see the Python standard manual set; its use is beyond the scope of this book (for instance, it also provides ways to automatically compile C-coded extensions on the target machine). Also, see the emerging third-party open source eggs system, which adds dependency checking for installed Python software.

			30.4.1.4 Summary 
			In this chapter, we covered the basics of modules, attributes, and imports, and explored the operation of import statements. We learned that imports find the designated file on the module search path, compile it to byte code, and execute all of its statements to generate its contents. We also learned how to configure the search path to be able to import from other directories than the home directory and the standard library directories, primarily with PYTHONPATH settings.

As this chapter demonstrated, the import operation and modules are at the heart of program architecture in Python. Larger programs are divided into multiple files, which are linked together at runtime by imports. Imports in turn use the module search path to locate files, and modules define attributes for external use.

Of course, the whole point of imports and modules is to provide a structure to your program, which divides its logic into self-contained software components. Code in one module is isolated from code in another; in fact, no file can ever see the names defined in another, unless explicit import statements are run. Because of this, modules minimize name collisions between different parts of your program.

You'll see what this all means in terms of actual code in the next chapter. Before we move on, though, let's run through the chapter quiz.

BRAIN BUILDER

Chapter Quiz

Q:	How does a module source code file become a module object?
Q:	Why might you have to set your PYTHONPATH environment variable?
Q:	Name the four major components of the module import search path.
Q:	Name four file types that Python might load in response to an import operation.
Q:	What is a namespace, and what does a module's namespace contain?


Quiz Answers

A:	A module's source code file automatically becomes a module object when that module is imported. Technically, the module's source code is run during the import, one statement at a time, and all the names assigned in the process become attributes of the module object.
A:	You only need to set PYTHONPATH to import from directories other than the one in which you are working (i.e., the current directory when working interactively, or the directory containing your top-level file).
A:	The four major components of the module import search path are the top-level script's home directory (the directory containing it), all directories listed in the PYTHONPATH environment variable, standard library directories, and all directories in .pth path files located in standard places. Of these, programmers can customize PYTHONPATH and .pth files.
A:	Python might load a source code (.py) file, a byte code (.pyc) file, a C extension module (e.g., a .so file on Linux or a .dll or .pyd file on Windows), or a directory of the same name for package imports. Imports may also load more exotic things such as ZIP file components, Java classes under the Jython version of Python, .NET components under IronPython, and statically linked C extensions that have no files present at all. With import hooks, imports can load anything.
A:	A namespace is a self-contained package of variables, which are known as the attributes of the namespace object. A module's namespace contains all the names assigned by code at the top level of the module file (i.e., not nested in def or class statements). Technically, a module's global scope morphs into the module object's attributes namespace. A module's namespace may also be altered by assignments from other files that import it, though this is frowned upon (see Chapter 16 for more on this).

 	 	30.4.2. Module Coding Basics

			Now that we've looked at the larger ideas behind modules, let's turn to a simple example of modules in action. Python modules are easy to create; they're just files of Python program code created with a text editor. You don't need to write special syntax to tell Python you're making a module; almost any text file will do. Because Python handles all the details of finding and loading modules, modules are also easy to use; clients simply import a module, or specific names a module defines, and use the objects they reference.
			30.4.2.1 Module Creation

To define a module, simply use your text editor to type some Python code into a text file, and save it with a ".py" extension; any such file is automatically considered a Python module. All the names assigned at the top level of the module become its attributes (names associated with the module object), and are exported for clients to use.

For instance, if you type the following def into a file called module1.py and import it, you create a module object with one attribute—the name printer, which happens to be a reference to a function object:


def printer(x):                   # Module attribute
    print x


Before we go on, I should say a few more words about module filenames. You can call modules just about anything you like, but module filenames should end in a .py suffix if you plan to import them. The .py is technically optional for top-level files that will be run but not imported, but adding it in all cases makes your files' types more obvious and allows you to import any of your files in the future.

Because module names become variable names inside a Python program (without the .py), they should also follow the normal variable name rules outlined in Chapter 11. For instance, you can create a module file named if.py, but you cannot import it because if is a reserved word—when you try to run import if, you'll get a syntax error. In fact, both the names of module files and the names of directories used in package imports (discussed in the next chapter) must conform to the rules for variable names presented in Chapter 11; they may, for instance, contain only letters, digits, and underscores. Package directories also cannot contain platform-specific syntax such as spaces in their names.

When a module is imported, Python maps the internal module name to an external filename by adding directory paths in the module search path to the front, and a .py or other extension at the end. For instance, a module named M ultimately maps to some external file <directory>\M.<extension> that contains the module's code.

As mentioned in the preceding chapter, it is also possible to create a Python module by writing code in an external language such as C or C++ (or Java, in the Jython implementation of the language). Such modules are called extension modules, and they are generally used to wrap up external libraries for use in Python scripts. When imported by Python code, extension modules look and feel the same as modules coded as Python source code files—they are accessed with import statements, and provide functions and objects as module attributes. Extension modules are beyond the scope of this book; see Python's standard manuals, or advanced texts such as Programming Python for more details.

			30.4.2.2 Module Usage

Clients can use the simple module file we just wrote by running import or from statements. Both statements find, compile, and run a module file's code, if it hasn't yet been loaded. The chief difference is that import fetches the module as a whole, so you must qualify to fetch its names; in contrast, from fetches (or copies) specific names out of the module.

Let's see what this means in terms of code. All of the following examples wind up calling the printer function defined in the external module file module1.py, but in different ways.

				30.4.2.2.1 The import Statement

In the first example, the name module1 serves two different purposes—it identifies an external file to be loaded, and it becomes a variable in the script, which references the module object after the file is loaded:


>>> import module1  # Get module as a whole
>>> module1.printer('Hello world!')  # Qualify to get names
Hello world!


Because import gives a name that refers to the whole module object, we must go through the module name to fetch its attributes (e.g., module1.printer).

				30.4.2.2.2 The from statement

By contrast, because from also copies names from one file over to another scope, it allows us to use the copied names directly in the script without going through the module (e.g., printer):


>>> from module1 import printer  # Copy out one variable
>>> printer('Hello world!')  # No need to qualify name
Hello world!


This has the same effect as the prior example, but because the imported name is copied into the scope where the from statement appears, using that name in the script requires less typing: we can use it directly instead of naming the enclosing module.

As you'll see in more detail later, the from statement is really just a minor extension to the import statement—it imports the module file as usual, but adds an extra step that copies one or more names out of the file.

				30.4.2.2.3 The from * Statement

Finally, the next example uses a special form of from: when we use a *, we get copies of all the names assigned at the top level of the referenced module. Here again, we can then use the copied name printer in our script without going through the module name:


>>> from module1 import *  # Copy out all variables
>>> printer('Hello world!') 
Hello world!


Technically, both import and from statements invoke the same import operation; the from * form simply adds an extra step that copies all the names in the module into the importing scope. It essentially collapses one module's namespace into another; again, the net effect is less typing for us.

And that's it—modules really are simple to use. To give you a better understanding of what really happens when you define and use modules, though, let's move on to look at some of their properties in more detail.


				30.4.2.2.4 Imports Happen Only Once

One of the most common questions beginners seem to ask when using modules is, "Why won't my imports keep working?" They often report that the first import works fine, but later imports during an interactive session (or program run) seem to have no effect. In fact, they're not supposed to, and here's why.

Modules are loaded and run on the first import or from, and only the first. This is on purpose—because this is an expensive operation, by default, Python does it just once per file, per process. Later import operations simply fetch the already loaded module object.

As one consequence, because top-level code in a module file is usually executed only once, you can use it to initialize variables. Consider the file simple.py, for example:


print 'hello'
spam = 1                   # Initialize variable


In this example, the print and = statements run the first time the module is imported, and the variable spam is initialized at import time:


% python 
>>> import simple  # First import: loads and runs file's code
hello
>>> simple.spam  # Assignment makes an attribute
1


Second and later imports don't rerun the module's code; they just fetch the already created module object from Python's internal modules table. Thus, the variable spam is not reinitialized:


>>> simple.spam = 2  # Change attribute in module
>>> import simple  # Just fetches already loaded module
>>> simple.spam  # Code wasn't rerun: attribute unchanged
2


Of course, sometimes you really want a module's code to be rerun on a subsequent import. We'll see how to do this with the reload built-in function later in this chapter.

				30.4.2.2.5 import and from Are Assignments

Just like def, import and from are executable statements, not compile-time declarations. They may be nested in if tests, appear in function defs, and so on, and they are not resolved or run until Python reaches them while executing your program. In other words, imported modules and names are not available until their associated import or from statements run. Also, like def, import and from are implicit assignments:

    * import assigns an entire module object to a single name.
    * from assigns one or more names to objects of the same names in another module.

All the things we've already discussed about assignment apply to module access, too. For instance, names copied with a from become references to shared objects; as with function arguments, reassigning a fetched name has no effect on the module from which it was copied, but changing a fetched mutable object can change it in the module from which it was imported. To illustrate, consider the following file, small.py:


x = 1
y = [1, 2]

% python 
>>> from small import x, y  # Copy two names out
>>> x = 42  # Changes local x only
>>> y[0] = 42  # Changes shared mutable in-place


Here, x is not a shared mutable object, but y is. The name y in the importer and the importee reference the same list object, so changing it from one place changes it in the other:


>>> import small  # Get module name (from doesn't)
>>> small.x  # Small's x is not my x
1
>>> small.y  # But we share a changed mutable
[42, 2]


For a graphical picture of what from assignments do with references, flip back to Figure 16-2 (function argument passing), and mentally replace "caller" and "function" with "imported" and "importer." The effect is the same, except that here we're dealing with names in modules, not functions. Assignment works the same everywhere in Python.

				30.4.2.2.6 Cross-File Name Changes

Recall from the prior example that the assignment to x in the interactive session changed the name x in that scope only, not the x in the file—there is no link from a name copied with from back to the file it came from. To really change a global name in another file, you must use import:


% python 
>>> from small import x, y  # Copy two names out
>>> x = 42  # Changes my x only

>>> import small  # Get module name
>>> small.x = 42  # Changes x in other module


This phenomenon was introduced in Chapter 16. Because changing variables in other modules like this is a common source of confusion (and often a bad design choice), we'll revisit this technique again later in this part of the book. Note that the change to y[0] in the prior session is different; it changes an object, not a name.

				30.4.2.2.7 import and from Equivalence

Notice in the prior example that we have to execute an import statement after the from to access the small module name at all; from only copies names from one module to another, and does not assign the module name itself. At least conceptually, a from statement like this one:


from module import name1, name2     # Copy these two names out (only)


is equivalent to this statement sequence:


import module                       # Fetch the module object
name1 = module.name1                # Copy names out by assignment
name2 = module.name2
del module                          # Get rid of the module name


Like all assignments, the from statement creates new variables in the importer, which initially refer to objects of the same names in the imported file. Only the names are copied out, though, not the module itself. When we use the from * form of this statement (from module import *), the equivalence is the same, but all the top-level names in the module are copied over to the importing scope this way.

Notice that the first step of the from runs a normal import operation. Because of this, the from always imports the entire module into memory if it has not yet been imported, regardless of how many names it copies out of the file. There is no way to load just part of a module file (e.g., just one function), but because modules are byte code in Python instead of machine code, the performance implications are generally negligible.

				30.4.2.2.8 Potential Pitfalls of the from Statement

Because the from statement makes the location of a variable more implicit and obscure (name is less meaningful to the reader than module.name), some Python users recommend using import instead of from most of the time. I'm not sure this advice is warranted, though; from is commonly and widely used, without too many dire consequences. In practice, in realistic programs, it's often convenient not to have to type a module's name every time you wish to use one of its tools. This is especially true for large modules that provide many attributes—the standard library's Tkinter GUI module, for example.

It is true that the from statement has the potential to corrupt namespaces, at least in principle—if you use it to import variables that happen to have the same names as existing variables in your scope, your variables will be silently overwritten. This problem doesn't occur with the simple import statement because you must always go through a module's name to get to its contents (module.attr will not clash with a variable named attr in your scope). As long as you understand and expect that this can happen when using from, though, this isn't a major concern in practice, especially if you list the imported names explicitly (e.g., from module import x, y, z).

On the other hand, the from statement has more serious issues when used in conjunction with the reload call, as imported names might reference prior versions of objects. Moreover, the from module import * form really can corrupt namespaces and make names difficult to understand, especially when applied to more than one file—in this case, there is no way to tell which module a name came from, short of searching the external source files. In effect, the from * form collapses one namespace into another, and so defeats the namespace partitioning feature of modules. We will explore these issues in more detail in the "Section 21.8" section at the end of this part of the book (see Chapter 21).

Probably the best real-world advice here is to generally prefer import to from for simple modules, to explicitly list the variables you want in most from statements, and to limit the from * form to just one import per file. That way, any undefined names can be assumed to live in the module referenced with the from *. Some care is required when using the from statement, but armed with a little knowledge, most programmers find it to be a convenient way to access modules.

					30.4.2.2.8.1 When import is required

The only time you really must use import instead of from is when you must use the same name defined in two different modules. For example, if two files define the same name differently:


# M.py

def func(  ):
    ...do something...

# N.py

def func(  ):
    ...do something else...


and you must use both versions of the name in your program, the from statement will fail—you can only have one assignment to the name in your scope:


# O.py

from M import func
from N import func        # This overwites the one we got from M
func(  )                  # Calls N.func only


An import will work here, though, because including the name of the enclosing module makes the two names unique:


# O.py

import M, N                # Get the whole modules, not their names
M.func(  )                 # We can call both names now
N.func(  )                 # The module names make them unique


This case is unusual enough that you're unlikely to encounter it very often in practice.

			30.4.2.3 Module Namespaces

Modules are probably best understood as simply packages of names—i.e., places to define names you want to make visible to the rest of a system. Technically, modules usually correspond to files, and Python creates a module object to contain all the names assigned in a module file. But, in simple terms, modules are just namespaces (places where names are created), and the names that live in a module are called its attributes. We'll explore how all this works in this section.

				30.4.2.3.1 Files Generate Namespaces

So, how do files morph into namespaces? The short story is that every name that is assigned a value at the top level of a module file (i.e., not nested in a function or class body) becomes an attribute of that module.

For instance, given an assignment statement such as X = 1 at the top level of a module file M.py, the name X becomes an attribute of M, which we can refer to from outside the module as M.X. The name X also becomes a global variable to other code inside M.py, but we need to explain the notion of module loading and scopes a bit more formally to understand why:

    * Module statements run on the first import. The first time a module is imported anywhere in a system, Python creates an empty module object, and executes the statements in the module file one after another, from the top of the file to the bottom.
    * Top-level assignments create module attributes. During an import, statements at the top level of the file not nested in a def or class that assign names (e.g., =, def) create attributes of the module object; assigned names are stored in the module's namespace.
    * Module namespaces can be accessed via the attribute __dict__ or dir(M). Module namespaces created by imports are dictionaries; they may be accessed through the built-in __dict__ attribute associated with module objects and may be inspected with the dir function. The dir function is roughly equivalent to the sorted keys list of an object's __dict__ attribute, but it includes inherited names for classes, may not be complete, and is prone to changing from release to release.
    * Modules are a single scope (local is global). As we saw in Chapter 16, names at the top level of a module follow the same reference/assignment rules as names in a function, but the local and global scopes are the same (more formally, they follow the LEGB scope rule we met in Chapter 16, but without the L and E lookup layers). But, in modules, the module scope becomes an attribute dictionary of a module object after the module has been loaded. Unlike with functions (where the local namespace exists only while the function runs), a module file's scope becomes a module object's attribute namespace and lives on after the import.

Here's a demonstration of these ideas. Suppose we create the following module file in a text editor and call it module2.py:


print 'starting to load...'
import sys
name = 42

def func(  ): pass

class klass: pass

print 'done loading.'


The first time this module is imported (or run as a program), Python executes its statements from top to bottom. Some statements create names in the module's namespace as a side effect, but others may do actual work while the import is going on. For instance, the two print statements in this file execute at import time:


>>> import module2
starting to load...
done loading.


But once the module is loaded, its scope becomes an attribute namespace in the module object we get back from import. We can then access attributes in this namespace by qualifying them with the name of the enclosing module:


>>> module2.sys
<module 'sys' (built-in)>

>>> module2.name
42

>>> module2.func
<function func at 0x012B1830>

>>> module2.klass
<class module2.klass at 0x011C0BA0>


Here, sys, name, func, and klass were all assigned while the module's statements were being run, so they are attributes after the import. We'll talk about classes in Part 6, but notice the sys attribute—import statements really assign module objects to names, and any type of assignment to a name at the top level of a file generates a module attribute.

Internally, module namespaces are stored as dictionary objects. These are just normal dictionary objects with the usual methods. We can access a module's namespace dictionary through the module's __dict__ attribute:


>>> module2.__dict__.keys(  )
['__file__', 'name', '__name__', 'sys', '__doc__', '__builtins__',
'klass', 'func']


The names we assigned in the module file become dictionary keys internally, so most of the names here reflect top-level assignments in our file. However, Python also adds some names in the module's namespace for us; for instance, __file__ gives the name of the file the module was loaded from, and __name__ gives its name as known to importers (without the .py extension and directory path).

				30.4.2.3.2 Attribute Name Qualification

Now that you're becoming more familiar with modules, we should look at the notion of name qualification in more depth. In Python, you can access the attributes of any object that has attributes using the qualification syntax object.attribute.

Qualification is really an expression that returns the value assigned to an attribute name associated with an object. For example, the expression module2.sys in the previous example fetches the value assigned to sys in module2. Similarly, if we have a built-in list object L, L.append returns the append method object associated with that list.

So, what does attribute qualification do to the scope rules we studied in Chapter 16? Nothing, really: it's an independent concept. When you use qualification to access names, you give Python an explicit object from which to fetch the specified names. The LEGB rule applies only to bare, unqualified names. Here are the rules:


Simple variables 
    X means search for the name X in the current scopes (following the LEGB rule).

Qualification 
    X.Y means find X in the current scopes, then search for the attribute Y in the object X (not in scopes).

Qualification paths 
    X.Y.Z means look up the name Y in the object X, then look up Z in the object X.Y.

Generality 
    Qualification works on all objects with attributes: modules, classes, C extension types, etc.

In Part 6, we'll see that qualification means a bit more for classes (it's also the place where something called inheritance happens), but, in general, the rules outlined here apply to all names in Python.

				30.4.2.3.3 Imports Versus Scopes

As we've learned, it is never possible to access names defined in another module file without first importing that file. That is, you never automatically get to see names in another file, regardless of the structure of imports or function calls in your program. A variable's meaning is always determined by the locations of assignments in your source code, and attributes are always requested of an object explicitly.

For example, consider the following two simple modules. The first, moda.py, defines a variable X global to code in its file only, along with a function that changes the global X in this file:


X = 88                        # My X: global to this file only
def f(  ):
    global X                  # Change this file's X
    X = 99                    # Cannot see names in other modules


The second module, modb.py, defines its own global variable X, and imports and calls the function in the first module:


X = 11                         # My X: global to this file only
import moda                   # Gain access to names in moda
moda.f(  )                         # Sets moda.X, not this file's X
print X, moda.X


When run, moda.f changes the X in moda, not the X in modb. The global scope for moda.f is always the file enclosing it, regardless of which module it is ultimately called from:


% python modb.py
11 99


In other words, import operations never give upward visibility to code in imported files—an imported file cannot see names in the importing file. More formally:

    * Functions can never see names in other functions, unless they are physically enclosing.
    * Module code can never see names in other modules, unless they are explicitly imported.

Such behavior is part of the lexical scoping notion—in Python, the scopes surrounding a piece of code are completely determined by the code's physical position in your file. Scopes are never influenced by function calls or module imports.[55]

    [55] * Some languages act differently and provide for dynamic scoping, where scopes really may depend on runtime calls. This tends to make code trickier, though, because the meaning of a variable can differ over time.

				30.4.2.3.4 Namespace Nesting

In some sense, although imports do not nest namespaces upward, they do nest downward. Using attribute qualification paths, it's possible to descend into arbitrarily nested modules and access their attributes. For example, consider the next three files. mod3.py defines a single global name and attribute by assignment:


X = 3


mod2.py in turn defines its own X, then imports mod3 and uses qualification to access the imported module's attribute:


X = 2
import mod3

print X,                      # My global X
print mod3.X                  # mod3's X


mod1.py also defines its own X, then imports mod2, and fetches attributes in both the first and second files:


X = 1
import mod2

print X,                      # My global X
print mod2.X,                 # mod2's X
print mod2.mod3.X             # Nested mod3's X


Really, when mod1 imports mod2 here, it sets up a two-level namespace nesting. By using the path of names mod2.mod3.X, it can descend into mod3, which is nested in the imported mod2. The net effect is that mod1 can see the Xs in all three files, and hence has access to all three global scopes:


% python mod1.py
2 3
1 2 3


The reverse, however, is not true: mod3 cannot see names in mod2, and mod2 cannot see names in mod1. This example may be easier to grasp if you don't think in terms of namespaces and scopes, but instead focus on the objects involved. Within mod1, mod2 is just a name that refers to an object with attributes, some of which may refer to other objects with attributes (import is an assignment). For paths like mod2.mod3.X, Python simply evaluates from left to right, fetching attributes from objects along the way.

Note that mod1 can say import mod2, and then mod2.mod3.X, but it cannot say import mod2.mod3—this syntax invokes something called package (directory) imports, described in the next chapter. Package imports also create module namespace nesting, but their import statements are taken to reflect directory trees, not simple import chains.

			30.4.2.4 Reloading Modules
As we’ve seen, a module’s code is run only once per process by default. To force a
module’s code to be reloaded and rerun, you need to ask Python to do so explicitly
by calling the reload built-in function. In this section, we’ll explore how to use
reloads to make your systems more dynamic. In a nutshell:
• Imports (via both import and from statements) load and run a module’s code
only the first time the module is imported in a process.
• Later imports use the already loaded module object without reloading or rerunning the file’s code.
• The reload function forces an already loaded module’s code to be reloaded
and rerun. Assignments in the file’s new code change the existing module
object in-place.
Why all the fuss about reloading modules? The reload function allows parts of a program to be changed without stopping the whole program. With reload, therefore,
the effects of changes in components can be observed immediately. Reloadingdoesn't help in every situation, but where it does, it makes for a much shorter development cycle. For instance, imagine a database program that must connect to a server on startup; because program changes or customizations can be tested immediately after reloads, you need to connect only once while debugging.

Because Python is interpreted (more or less), it already gets rid of the compile/link steps you need to go through to get a C program to run: modules are loaded dynamically when imported by a running program. Reloading offers a further performance advantage by allowing you to also change parts of running programs without stopping. Note that reload currently only works on modules written in Python; compiled extension modules coded in a language such as C can be dynamically loaded at runtime, too, but they can't be reloaded. 
				30.4.2.4.1 reload Basics

Unlike import and from:

    * reload is a built-in function in Python, not a statement.
    * reload is passed an existing module object, not a name.

Because reload expects an object, a module must have been previously imported successfully before you can reload it (if the import was unsuccessful, due to a syntax or other error, you may need to repeat it before you can reload the module). Furthermore, the syntax of import statements and reload calls differs: reloads require parentheses, but imports do not. Reloading looks like this:


import module                     # Initial import
...use module.attributes...
...                               # Now, go change the module file
...
reload(module)                    # Get updated exports
...use module.attributes...


The typical usage pattern is that you import a module, then change its source code in a text editor, and then reload it. When you call reload, Python rereads the module file's source code, and reruns its top-level statements. Perhaps the most important thing to know about reload is that it changes a module object in-place; it does not delete and re-create the module object. Because of that, every reference to a module object anywhere in your program is automatically affected by a reload. Here are the details:

    * reloadruns a module file's new code in the module's current namespace. Rerunning a module file's code overwrites its existing namespace, rather than deleting and re-creating it.
    * Top-level assignments in the file replace names with new values. For instance, rerunning a def statement replaces the prior version of the function in the module's namespace by reassigning the function name.
    * Reloads impact all clients that useimportto fetch modules. Because clients that use import qualify to fetch attributes, they'll find new values in the module object after a reload.
    * Reloads impact futurefromclients only. Clients that used from to fetch attributes in the past won't be affected by a reload; they'll still have references to the old objects fetched before the reload.

				30.4.2.4.2 reload Example

Here's a more concrete example of reload in action. In the following example, we'll change and reload a module file without stopping the interactive Python session. Reloads are used in many other scenarios, too (see the sidebar "Why You Will Care: Module Reloads"), but we'll keep things simple for illustration here. First, in the text editor of your choice, write a module file named changer.py with the following contents:


message = "First version"
def printer(  ):
    print message


This module creates and exports two names—one bound to a string, and another to a function. Now, start the Python interpreter, import the module, and call the function it exports. The function will print the value of the global message variable:


% python
>>> import changer
>>> changer.printer(  )
First version


Keeping the interpreter active, now edit the module file in another window:

...modify changer.py without stopping Python... 
% vi changer.py 


Change the global message variable, as well as the printer function body:


message = "After editing"
def printer(  ):
    print 'reloaded:', message


Then, return to the Python window, and reload the module to fetch the new code. Notice in the following interaction that importing the module again has no effect; we get the original message, even though the file's been changed. We have to call reload in order to get the new version:

...back to the Python interpreter/program... 
>>> import changer 
>>> changer.printer(  )  # No effect: uses loaded module
First version
>>> reload(changer)  # Forces new code to load/run
<module 'changer'>
>>> changer.printer(  )  # Runs the new version now
reloaded: After editing


Notice that reload actually returns the module object for us—its result is usually ignored, but because expression results are printed at the interactive prompt, Python shows a default <module 'name'> representation.

Why You Will Care: Module Reloads

Besides allowing you to reload (and hence rerun) modules at the interactive prompt, module reloads are also useful in larger systems, especially when the cost of restarting the entire application is prohibitive. For instance, systems that must connect to servers over a network on startup are prime candidates for dynamic reloads.

They're also useful in GUI work (a widget's callback action can be changed while the GUI remains active), and when Python is used as an embedded language in a C or C++ program (the enclosing program can request a reload of the Python code it runs, without having to stop). See Programming Python for more on reloading GUI callbacks and embedded Python code.

More generally, reloads allow programs to provide highly dynamic interfaces. For instance, Python is often used as a customization language for larger systems—users can customize products by coding bits of Python code on-site, without having to recompile the entire product (or even having its source code at all). In such worlds, the Python code already adds a dynamic flavor by itself.

To be even more dynamic, though, such systems can automatically reload the Python customization code periodically at runtime. That way, users' changes are picked up while the system is running; there is no need to stop and restart each time the Python code is modified. Not all systems require such a dynamic approach, but for those that do, module reloads provide an easy-to-use dynamic customization tool.

			30.4.2.5 Chapter Summary

This chapter delved into the basics of module coding tools—the import and from statements, and the reload call. We learned how the from statement simply adds an extra step that copies names out of a file after it has been imported, and how reload forces a file to be imported again without stopping and restarting Python. We also surveyed namespace concepts, saw what happens when imports are nested, explored the way files become module namespaces, and learned about some potential pitfalls of the from statement.

Although we've already seen enough to handle module files in our programs, the next chapter extends our coverage of the import model by presenting package imports—a way for our import statements to specify part of the directory path leading to the desired module. As we'll see, package imports give us a hierarchy that is useful in larger systems, and allow us to break conflicts between same-named modules. Before we move on, though, here's a quick quiz on the concepts presented here.

BRAIN BUILDER

Chapter Quiz

Q:	How do you make a module?
Q:	How is the from statement related to the import statement?
Q:	How is the reload function related to imports?
Q:	When must you use import instead of from?
Q:	Name three potential pitfalls of the from statement.
Q:	What is the airspeed velocity of an unladen swallow?


Quiz Answers

A:	To create a module, you just write a text file containing Python statements; every source code file is automatically a module, and there is no syntax for declaring one. Import operations load module files into module objects in memory. You can also make a module by writing code in an external language like C or Java, but such extension modules are beyond the scope of this book.
A:	The from statement imports an entire module, like the import statement, but as an extra step, it also copies one or more variables from the imported module into the scope where the from appears. This enables you to use the imported names directly (name) instead of having to go through the module (module.name).
A:	By default, a module is imported only once per process. The reload function forces a module to be imported again. It is mostly used to pick up new versions of a module's source code during development, and in dynamic customization scenarios.
A:	You must use import instead of from only when you need to access the same name in two different modules; because you'll have to specify the names of the enclosing modules, the two names will be unique.
A:	The from statement can obscure the meaning of a variable (which module it is defined in), can have problems with the reload call (names may reference prior versions of objects), and can corrupt namespaces (it might silently overwrite names you are using in your scope). The from * form is worse in most regards—it can seriously corrupt namespaces, and obscure the meaning of variables; it is probably best used sparingly.
A:	What do you mean? An African or European swallow?


 	 	30.4.3. Module Packages
So far, when we’ve imported modules, we’ve been loading files. This represents typical module usage, and is probably the technique you’ll use for most imports you’ll code early on in your Python career. However, the module import story is a bit richer than I have thus far implied.
In addition to a module name, an import can name a directory path. A directory of Python code is said to be a package, so such imports are known as package imports.
In effect, a package import turns a directory on your computer into another Python namespace, with attributes corresponding to the subdirectories and module files that the directory contains.
This is a somewhat advanced feature, but the hierarchy it provides turns out to be handy for organizing the files in a large system, and tends to simplify module search path settings. As we’ll see, package imports are also sometimes required to resolve import ambiguities when multiple program files of the same name are installed on a single machine.

			30.4.3.1 Package Import Basics
So, how do package imports work? In the place where you have been naming a simple file in your import statements, you can instead list a path of names separated by periods:


import dir1.dir2.mod


The same goes for from statements:


from dir1.dir2.mod import x


The "dotted" path in these statements is assumed to correspond to a path through the directory hierarchy on your machine, leading to the file mod.py (or similar; the extension may vary). That is, the preceding statements indicate that on your machine there is a directory dir1, which has a subdirectory dir2, which contains a module file mod.py (or similar).

Furthermore, these imports imply that dir1 resides within some container directory dir0, which is accessible on the Python module search path. In other words, the two import statements imply a directory structure that looks something like this (shown with DOS backslash separators):


dir0\dir1\dir2\mod.py             # Or mod.pyc, mod.so, etc.


The container directory dir0 needs to be added to your module search path (unless it's the home directory of the top-level file), exactly as if dir1 were a module file. From there down, the import statements in your script give the directory paths leading to the modules explicitly.

				30.4.3.1.1 Packages and Search Path Settings

If you use this feature, keep in mind that the directory paths in your import statements can only be variables separated by periods. You cannot use any platform-specific path syntax in your import statements, such as C:\dir1, My Documents.dir2, or ../dir1—these do not work syntactically. Instead, use platform-specific syntax in your module search path settings to name the container directories.

For instance, in the prior example, dir0—the directory name you add to your module search path—can be an arbitrarily long and platform-specific directory path leading up to dir1. Instead of using an invalid statement like this:


import C:\mycode\dir1\dir2\mod      # Error: illegal syntax


add C:\mycode to your PYTHONPATH variable or a .pth file (assuming it is not the program's home directory, in which case this step is not necessary), and say this:


import dir1.dir2.mod


In effect, entries on the module search path provide platform-specific directory path prefixes, which lead to the leftmost names in import statements. Import statements provide directory path tails in a platform-neutral fashion.[56]

    [56] * The dot path syntax was chosen partly for platform neutrality, but also because paths in import statements become real nested object paths. This syntax also means that you get odd error messages if you forget to omit the .py in your import statements. For example, import mod.py is assumed to be a directory path import—it loads mod.py, then tries to load a mod\py.py, and ultimately issues a potentially confusing error message.

				30.4.3.1.2 Package __init__.py Files

If you choose to use package imports, there is one more constraint you must follow: each directory named within the path of a package import statement must contain a file named __init__.py, or your package imports will fail. That is, in the example we've been using, both dir1 and dir2 must contain a file called __init__.py; the container directory dir0 does not require such a file because it's not listed in the import statement itself. More formally, for a directory structure such as this:


dir0\dir1\dir2\mod.py


and an import statement of the form:


import dir1.dir2.mod


the following rules apply:

    * dir1 and dir2 both must contain an __init__.py file.
    * dir0, the container, does not require an __init__.py file; this file will simply be ignored if present.
    * dir0, not dir0\dir1, must be listed on the module search path (i.e., it must be the home directory, or be listed in your PYTHONPATH, etc.).

The net effect is that this example's directory structure should be as follows, with indentation designating directory nesting:

dir0 \                        # Container on module search path
    dir1\
        __init__.py
        dir2\
            __init__.py
            mod.py


The __init__.py files can contain Python code, just like normal module files. They are partly present as a declaration to Python, however, and can be completely empty. As declarations, these files serve to prevent directories with common names from unintentionally hiding true modules that appear later on the module search path. Without this safeguard, Python might pick a directory that has nothing to do with your code, just because it appears in an earlier directory on the search path.

More generally, the __init__.py file serves as a hook for package initialization-time actions, generates a module namespace for a directory, and implements the behavior of from * (i.e., from .. import *) statements when used with directory imports:


Package initialization

    The first time Python imports through a directory, it automatically runs all the code in the directory's __init__.py file. Because of that, these files are a natural place to put code to initialize the state required by files in the package. For instance, a package might use its initialization file to create required data files, open connections to databases, and so on. Typically, __init__.py files are not meant to be useful if executed directly; they are run automatically when a package is first accessed.

Module namespace initialization

    In the package import model, the directory paths in your script become real nested object paths after an import. For instance, in the preceding example, after the import, the expression dir1.dir2 works and returns a module object whose namespace contains all the names assigned by dir2's __init__.py file. Such files provide a namespace for module objects created for directories, which have no real associated module files.

from * statement behavior

    As an advanced feature, you can use __all__ lists in __init__.py files to define what is exported when a directory is imported with the from * statement form. (We'll meet __all__ in Chapter 21.) In an __init__.py file, the __all__ list is taken to be the list of submodule names that should be imported when from * is used on the package (directory) name. If __all__ is not set, the from * statement does not automatically load submodules nested in the directory; instead, it loads just names defined by assignments in the directory's __init__.py file, including any submodules explicitly imported by code in this file. For instance, the statement from submodule import X in a directory's __init__.py makes the name X available in that directory's namespace.

You can also simply leave these files empty, if their roles are beyond your needs. They must exist, though, for your directory imports to work at all.

			30.4.3.2 Package Import Example
Let's actually code the example we've been talking about to show how initialization files and paths come into play. The following three files are coded in a directory dir1, and its subdirectory dir2:


# File: dir1\__init__.py
print 'dir1 init'
x = 1

# File: dir1\dir2\__init__.py
print 'dir2 init'
y = 2

# File: dir1\dir2\mod.py
print 'in mod.py'
z = 3


Here, dir1 will either be a subdirectory of the one we're working in (i.e., the home directory), or a subdirectory of a directory that is listed on the module search path (technically, on sys.path). Either way, dir1's container does not need an __init__.py file.

import statements run each directory's initialization file the first time that directory is traversed, as Python descends the path; print statements are included here to trace their execution. Also, as with module files, an already imported directory may be passed to reload to force re-execution of that single item. As shown here, reload accepts a dotted path name to reload nested directories and files:


% python 
>>> import dir1.dir2.mod  # First imports run init files
dir1 init
dir2 init
in mod.py
>>>
>>> import dir1.dir2.mod  # Later imports do not
>>>
>>> reload(dir1) 
dir1 init
<module 'dir1' from 'dir1\__init__.pyc'>
>>>
>>> reload(dir1.dir2) 
dir2 init
<module 'dir1.dir2' from 'dir1\dir2\__init__.pyc'>


Once imported, the path in your import statement becomes a nested object path in your script. Here, mod is an object nested in the object dir2, which in turn is nested in the object dir1:


>>> dir1
<module 'dir1' from 'dir1\__init__.pyc'>
>>> dir1.dir2
<module 'dir1.dir2' from 'dir1\dir2\__init__.pyc'>
>>> dir1.dir2.mod
<module 'dir1.dir2.mod' from 'dir1\dir2\mod.pyc'>


In fact, each directory name in the path becomes a variable assigned to a module object whose namespace is initialized by all the assignments in that directory's __init__.py file. dir1.x refers to the variable x assigned in dir1\__init__.py, much as mod.z refers to the variable z assigned in mod.py:


>>> dir1.x
1
>>> dir1.dir2.y
2
>>> dir1.dir2.mod.z
3

				30.4.3.2.1 from Versus import with Packages

import statements can be somewhat inconvenient to use with packages because you may have to retype the paths frequently in your program. In the prior section's example, for instance, you must retype and rerun the full path from dir1 each time you want to reach z. If you try to access dir2 or mod directly, you'll get an error:


>>> dir2.mod
NameError: name 'dir2' is not defined
>>> mod.z
NameError: name 'mod' is not defined


It's often more convenient, therefore, to use the from statement with packages to avoid retyping the paths at each access. Perhaps more importantly, if you ever restructure your directory tree, the from statement requires just one path update in your code, whereas the import may require many. The import as extension, discussed in the next chapter, can also help here by providing a shorter synonym for the full path:


% python 
>>> from dir1.dir2 import mod  # Code path here only
dir1 init
dir2 init
in mod.py
>>> mod.z  # Don't repeat path
3
>>> from dir1.dir2.mod import z 
>>> z 
3
>>> import dir1.dir2.mod as mod  # Use shorter name
>>> mod.z 
3
			30.4.3.3 Why Use Package Imports?
If you're new to Python, make sure that you've mastered simple modules before stepping up to packages, as they are a somewhat advanced feature. They do serve useful roles, though, especially in larger programs: they make imports more informative, serve as an organizational tool, simplify your module search path, and can resolve ambiguities.

First of all, because package imports give some directory information in program files, they both make it easier to locate your files and serve as an organizational tool. Without package paths, you must often resort to consulting the module search path to find files. Moreover, if you organize your files into subdirectories for functional areas, package imports make it more obvious what role a module plays, and so make your code more readable. For example, a normal import of a file in a directory somewhere on the module search path, like this:


import utilities


offers much less information than an import that includes the path:


import database.client.utilities


Package imports can also greatly simplify your PYTHONPATH and .pth file search path settings. In fact, if you use package imports for all your cross-directory imports, and you make those package imports relative to a common root directory, where all your Python code is stored, you really only need a single entry on your search path: the common root. Finally, package imports serve to resolve ambiguities by making explicit exactly which files you want to import. The next section explores this role in more detail.

				30.4.3.3.1 A Tale of Three Systems

The only time package imports are actually required is to resolve ambiguities that may arise when multiple programs with same-named files are installed on a single machine. This is something of an install issue, but it can also become a concern in general practice. Let's turn to a hypothetical scenario to illustrate.

Suppose that a programmer develops a Python program that contains a file called utilities.py for common utility code, and a top-level file named main.py that users launch to start the program. All over this program, its files say import utilities to load and use the common code. When the program is shipped, it arrives as a single .tar or .zip file containing all the program's files, and when it is installed, it unpacks all its files into a single directory named system1 on the target machine:


system1\
    utilities.py        # Common utility functions, classes
    main.py             # Launch this to start the program
    other.py            # Import utilities to load my tools


Now, suppose that a second programmer develops a different program with files also called utilities.py and main.py, and again uses import utilities throughout the program to load the common code file. When this second system is fetched and installed on the same computer as the first system, its files will unpack into a new directory called system2 somewhere on the receiving machine so that they do not overwrite same-named files from the first system:


system2\
    utilities.py        # Common utilities
    main.py             # Launch this to run
    other.py            # Imports utilities


So far, there's no problem: both systems can coexist and run on the same machine. In fact, you won't even need to configure the module search path to use these programs on your computer—because Python always searches the home directory first (that is, the directory containing the top-level file), imports in either system's files will automatically see all the files in that system's directory. For instance, if you click on system1\main.py, all imports will search system1 first. Similarly, if you launch system2\main.py, system2 will be searched first instead. Remember, module search path settings are only needed to import across directory boundaries.

However, suppose that after you've installed these two programs on your machine, you decide that you'd like to use some of the code in each of the utilities.py files in a system of your own. It's common utility code, after all, and Python code by nature wants to be reused. In this case, you want to be able to say the following from code that you're writing in a third directory to load one of the two files:


import utilities
utilities.func('spam')


Now the problem starts to materialize. To make this work at all, you'll have to set the module search path to include the directories containing the utilities.py files. But which directory do you put first in the path—system1 or system2?

The problem is the linear nature of the search path. It is always scanned from left to right, so no matter how long you ponder this dilemma, you will always get utilities.py from the directory listed first (leftmost) on the search path. As is, you'll never be able to import it from the other directory at all. You could try changing sys.path within your script before each import operation, but that's both extra work, and highly error-prone. By default, you're stuck.

This is the issue that packages actually fix. Rather than installing programs as flat lists of files in standalone directories, you can package and install them as subdirectories under a common root. For instance, you might organize all the code in this example as an install hierarchy that looks like this:


root\
    system1\
        __init__.py
        utilities.py
        main.py
        other.py
    system2\
        __init__.py
        utilities.py
        main.py
        other.py
    system3\                    # Here or elsewhere
        __init__.py          # Your new code here
        myfile.py


Now, add just the common root directory to your search path. If your code's imports are all relative to this common root, you can import either system's utility file with a package import—the enclosing directory name makes the path (and hence, the module reference) unique. In fact, you can import both utility files in the same module, as long as you use an import statement, and repeat the full path each time you reference the utility modules:


import system1.utilities
import system2.utilities
system1.utilities.function('spam')
system2.utilities.function('eggs')


The name of the enclosing directory here makes the module references unique.

Note that you have to use import instead of from with packages only if you need to access the same attribute in two or more paths. If the name of the called function here was different in each path, from statements could be used to avoid repeating the full package path whenever you call one of the functions, as described earlier.

Also, notice in the install hierarchy shown earlier that __init__.py files were added to the system1 and system2 directories to make this work, but not to the root directory. Only directories listed within import statements in your code require these files; as you'll recall, they are run automatically the first time the Python process imports through a package directory.

Technically, in this case, the system3 directory doesn't have to be under root—just the packages of code from which you will import. However, because you never know when your own modules might be useful in other programs, you might as well place them under the common root directory to avoid similar name-collision problems in the future.

Finally, notice that both of the two original systems' imports will keep working unchanged. Because their home directories are searched first, the addition of the common root on the search path is irrelevant to code in system1 and system2; they can keep saying just import utilities, and expect to find their own files. Moreover, if you're careful to unpack all your Python systems under a common root like this, path configuration becomes simple: you'll only need to add the common root directory, once.

Why You Will Care: Module Packages

Now that packages are a standard part of Python, it's common to see larger third-party extensions shipped as a set of package directories, rather than a flat list of modules. The win32all Windows extensions package for Python, for instance, was one of the first to jump on the package bandwagon. Many of its utility modules reside in packages imported with paths. For instance, to load client-side COM tools, you use a statement like this:


from win32com.client import constants, Dispatch


This line fetches names from the client module of the win32com package (an install subdirectory).

Package imports are also pervasive in code run under the Jython Java-based implementation of Python because Java libraries are organized into hierarchies as well. In recent Python releases, the email and XML tools are likewise organized into packaged subdirectories in the standard library. Whether you create package directories, you will probably import from them eventually.

			30.4.3.4 Chapter Summary
This chapter introduced Python's package import model—an optional but useful way to explicitly list part of the directory path leading up to your modules. Package imports are still relative to a directory on your module import search path, but rather than relying on Python to traverse the search path manually, your script gives the rest of the path to the module explicitly.

As we've seen, packages not only make imports more meaningful in larger systems, but also simplify import search path settings (if all cross-directory imports are relative to a common root directory), and resolve ambiguities when there is more than one module of the same name (including the name of the enclosing directory in a package import helps distinguish between them).

In the next chapter, we will survey a handful of more advanced module-related topics, such as relative import syntax, and the __name__ usage mode variable. As usual, though, we'll close out this chapter with a short quiz to test what you've learned here.

BRAIN BUILDER

Chapter Quiz

Q:	What is the purpose of an __init__.py file in a module package directory?
Q:	How can you avoid repeating the full package path every time you reference a package's content?
Q:	Which directories require __init__.py files?
Q:	When must you use import instead of from with packages?


Quiz Answers

A:	The __init__.py file serves to declare and initialize a module package; Python automatically runs its code the first time you import through a directory in a process. Its assigned variables become the attributes of the module object created in memory to correspond to that directory. It is also not optional—you can't import through a directory with package syntax, unless it contains this file.
A:	Use the from statement with a package to copy names out of the package directly, or use the as extension with the import statement to rename the path to a shorter synonym. In both cases, the path is listed in only one place, in the from or import statement.
A:	Each directory listed in an import or from statement must contain an __init__.py file. Other directories, including the directory containing the leftmost component of a package path, do not need to include this file.
A:	You must use import instead of from with packages only if you need to access the same name defined in more than one path. With import, the path makes the references unique, but from allows only one version of any given name.

 	 	30.4.4. Advanced Module Topics

This chapter concludes Part 5 with a collection of more advanced module-related topics—relative import syntax, data hiding, the __future__ module, the __name__ variable, sys.path changes, and so on—along with the standard set of gotchas and exercises related to what we've covered in this part of the book. Like functions, modules are more effective when their interfaces are well defined, so this chapter also briefly reviews module design concepts, some of which we have explored in prior chapters.

Despite the word "advanced" in this chapter's title, some of the topics discussed here (such as the __name__ trick) are widely used, so be sure you take a look before moving on to classes in the next part of the book.

			30.4.4.1 . Data Hiding in Modules

As we've seen, a Python module exports all the names assigned at the top level of its file. There is no notion of declaring which names should and shouldn't be visible outside the module. In fact, there's no way to prevent a client from changing names inside a module if it wants to.

In Python, data hiding in modules is a convention, not a syntactical constraint. If you want to break a module by trashing its names, you can, but fortunately, I've yet to meet a programmer who would. Some purists object to this liberal attitude toward data hiding, and claim that it means Python can't implement encapsulation. However, encapsulation in Python is more about packaging than about restricting.

				30.4.4.1.1 Minimizing from * Damage: _X and __all__

As a special case, you can prefix names with a single underscore (e.g., _X) to prevent them from being copied out when a client imports a module's names with a from * statement. This really is intended only to minimize namespace pollution; because from * copies out all names, the importer may get more than it's bargained for (including names that overwrite names in the importer). Underscores aren't "private" declarations: you can still see and change such names with other import forms, such as the import statement.

Alternatively, you can achieve a hiding effect similar to the _X naming convention by assigning a list of variable name strings to the variable __all__ at the top level of the module. For example:


__all__ = ["Error", "encode", "decode"]     # Export these only


When this feature is used, the from * statement will copy out only those names listed in the __all__ list. In effect, this is the converse of the _X convention: __all__ identifies names to be copied, while _X identifies names not to be copied. Python looks for an __all__ list in the module first; if one is not defined, from * copies all names without a single leading underscore.

Like the _X convention, the __all__ list has meaning only to the from * statement form, and does not amount to a privacy declaration. Module writers can use either trick to implement modules that are well behaved when used with from *. (See also the discussion of __all__ lists in package __init__.py files in Chapter 20; there, these lists declare submodules to be loaded for a from *.)

			30.4.4.2 . Enabling Future Language Features
Changes to the language that may potentially break existing code are introduced gradually. Initially, they appear as optional extensions, which are disabled by default. To turn on such extensions, use a special import statement of this form:


from __future__ import featurename


This statement should generally appear at the top of a module file (possibly after a docstring) because it enables special compilation of code on a per-module basis. It's also possible to submit this statement at the interactive prompt to experiment with upcoming language changes; the feature will then be available for the rest of the interactive session.

For example, in prior editions of this book, we had to use this statement form to demonstrate generator functions, which required a keyword that was not yet enabled by default (they use a featurename of generators). We also used this statement to activate true division for numbers in Chapter 5, and we'll use it again in this chapter to turn on absolute imports and again later in Part 7 to demonstrate context managers.

All of these changes have the potential to break existing code, and so are being phased in gradually as optional features presently enabled with this special import.

			30.4.4.3 . Mixed Usage Modes: __name__ and __main__
Here's a special module-related trick that lets you import a file as a module, and run it as a standalone program. Each module has a built-in attribute called __name__, which Python sets automatically as follows:

    * If the file is being run as a top-level program file, __name__ is set to the string "__main__" when it starts.
    * If the file is being imported, __name__ is instead set to the module's name as known by its clients.

The upshot is that a module can test its own __name__ to determine whether it's being run or imported. For example, suppose we create the following module file, named runme.py, to export a single function called tester:


def tester(  ):
    print "It's Christmas in Heaven..."

if __name__ == '__main__':         # Only when run
    tester(  )                        # Not when imported


This module defines a function for clients to import and use as usual:


% python
>>> import runme
>>> runme.tester(  )
It's Christmas in Heaven...


But, the module also includes code at the bottom that is set up to call the function when this file is run as a program:


% python runme.py
It's Christmas in Heaven...


Perhaps the most common place you'll see the __name__ test applied is for self-test code. In short, you can package code that tests a module's exports in the module itself by wrapping it in a __name__ test at the bottom of the file. This way, you can use the file in clients by importing it, and test its logic by running it from the system shell, or via another launching scheme. In practice, self-test code at the bottom of a file under the __name__ test is probably the most common and simplest unit-testing protocol in Python. (Chapter 29 will discuss other commonly used options for testing Python code—as you'll see, the unittest and doctest standard library modules provide more advanced testing tools.)

The __name__ trick is also commonly used when writing files that can be used both as command-line utilities, and as tool libraries. For instance, suppose you write a file finder script in Python. You can get more mileage out of your code if you package it in functions, and add a __name__ test in the file to automatically call those functions when the file is run standalone. That way, the script's code becomes reusable in other programs.

				30.4.4.3.1 Unit Tests with __name__

In fact, we've already seen a prime example in this book of an instance where the __name__ check could be useful. In the section on arguments in Chapter 16, we coded a script that computed the minimum value from the set of arguments sent in:


def minmax(test, *args):
    res = args[0]
    for arg in args[1:]:
        if test(arg, res):
            res = arg
    return res

def lessthan(x, y): return x < y
def grtrthan(x, y): return x > y

print minmax(lessthan, 4, 2, 1, 5, 6, 3)         # Self-test code
print minmax(grtrthan, 4, 2, 1, 5, 6, 3)


This script includes self-test code at the bottom, so we can test it without having to retype everything at the interactive command line each time we run it. The problem with the way it is currently coded, however, is that the output of the self-test call will appear every time this file is imported from another file to be used as a tool—not exactly a user-friendly feature! To improve it, we can wrap up the self-test call in a __name__ check, so that it will be launched only when the file is run as a top-level script, not when it is imported:


print 'I am:', __name__

def minmax(test, *args):
    res = args[0]
    for arg in args[1:]:
        if test(arg, res):
            res = arg
    return res

def lessthan(x, y): return x < y
def grtrthan(x, y): return x > y

if __name__ == '__main__':
    print minmax(lessthan, 4, 2, 1, 5, 6, 3)     # Self-test code
    print minmax(grtrthan, 4, 2, 1, 5, 6, 3)


We're also printing the value of __name__ at the top here to trace its value. Python creates and assigns this usage-mode variable as soon as it starts loading a file. When we run this file as a top-level script, its name is set to __main__, so its self-test code kicks in automatically:


% python min.py
I am: __main__
1
6


But, if we import the file, its name is not __main__, so we must explicitly call the function to make it run:


>>> import min
I am: min
>>> min.minmax(min.lessthan, 's', 'p', 'a', 'm')
'a'


Again, regardless of whether this is used for testing, the net effect is that we get to use our code in two different roles—as a library module of tools, or as an executable program.

			30.4.4.4 . Changing the Module Search Path
In Chapter 18, we learned that the module search path is a list of directories that can be customized via the environment variable PYTHONPATH, and possibly .pth path files. What I haven't shown you until now is how a Python program itself can actually change the search path by changing a built-in list called sys.path (the path attribute in the built-in sys module). sys.path is initialized on startup, but thereafter, you can delete, append, and reset its components however you like:


>>> import sys 
>>> sys.path 
['', 'D:\\PP3ECD-Partial\\Examples', 'C:\\Python25', ...more deleted... ]

>>> sys.path.append('C:\\sourcedir')  # Extend module search path
>>> import string  # All imports search the new dir


Once you've made such a change, it will impact future imports anywhere in the Python program, as all imports and all files share the single sys.path list. In fact, this list may be changed arbitrarily:


>>> sys.path = [r'd:\temp']  # Change module search path
>>> sys.path.append('c:\\lp3e\\examples')  # For this process only
>>> sys.path 
['d:\\temp', 'c:\\lp3e\\examples']

>>> import string 
Traceback (most recent call last):
  File "<stdin>", line 1, in ?
ImportError: No module named string


Thus, you can use this technique to dynamically configure a search path inside a Python program. Be careful, though: if you delete a critical directory from the path, you may lose access to critical utilities. In the prior example, for instance, we no longer have access to the string module because we deleted the Python source library's directory from the path.

Also, remember that such sys.path settings only endure for the Python session or program (technically, process) that made them; they are not retained after Python exits. PYTHONPATH and .pth file path configurations live in the operating system instead of a running Python program, and so are more global: they are picked up by every program on your machine, and live on after a program completes.

			30.4.4.5 . The import as Extension
Both the import and from statements have been extended to allow a module to be given a different name in your script. The following import statement:


import longmodulename as name


is equivalent to:


import longmodulename
name = longmodulename
del longmodulename                           # Don't keep original name


After such an import, you can (and in fact must) use the name listed after the as to refer to the module. This works in a from statement, too, to assign a name imported from a file to a different name in your script:


from module import longname as name


This extension is commonly used to provide short synonyms for longer names, and to avoid name clashes when you are already using a name in your script that would otherwise be overwritten by a normal import statement. It also comes in handy for providing a short, simple name for an entire directory path when using the package import feature described in Chapter 20.

			30.4.4.6 . Relative Import Syntax
Python 2.5 modifies the import search path semantics of some from statements when they are applied to the module packages we studied in the previous chapter. Some aspects of this change will not become apparent until a later Python release (currently this is planned for version 2.7 and version 3.0), though some are already present today.

In short, from statements can now use dots (".") to specify that they prefer modules located within the same package (known as package-relative imports) to modules located elsewhere on the module import search path (called absolute imports). That is:

    * Today, you can use dots to indicate that imports should be relative to the containing package—such imports will prefer modules located inside the package to same-named modules located elsewhere on the import search path, sys.path.
    * Normal imports in a package's code (without dots) currently default to a relative-then-absolute search path order. However, in the future, Python will make imports absolute by default—in the absence of any special dot syntax, imports will skip the containing package itself and look elsewhere on the sys.path search path.

For example, presently, a statement of the form:
from .spam import name


means "from a module named spam located in the same package that this statement is contained in, import the variable name." A similar statement without the leading dot will still default to the current relative-then-absolute search path order, unless a statement of the following form is included in the importing file:
from __future__ import  absolute_import   # Required until 2.7?


If present, this statement enables the future absolute default path change. It causes all imports without extra dots to skip the relative components of the module import search path, and look instead in the absolute directories that sys.path contains. For instance, when absolute imports are thus enabled, a statement of the following form will always find the standard library's string module, instead of a module of the same name in the package:
import string                             # Always finds standard lib's version

Without the from __future__ statement, if there's a string module in the package, it will be imported instead. To get the same behavior when the future absolute import change is enabled, run a statement of the following form (which also works in Python today) to force a relative import:


from . import string                      # Searches this package first


Note that leading dots can only be used with the from statement, not the import statement. The import modname statement form still performs relative imports today, but these will become absolute in Python 2.7.

Other dot-based relative reference patterns are possible, too. Given a package named mypkg, the following alternative import forms used by code within the package work as described:

from .string import name1, name2             # Imports names from mypkg.string
from . import string                        # Imports mypkg.string
from .. import string                         # Imports string from parent directory

To understand these latter forms better, we need to understand the rationale behind this change.

				30.4.4.6.1 Why Relative Imports?

This feature is designed to allow scripts to resolve ambiguities that can arise when a same-named file appears in multiple places on the module search path. Consider the following package directory:


mypkg\
    __init__.py
    main.py
    string.py


This defines a package named mypkg containing modules named mypkg.main and mypkg.string. Now, suppose that the main module tries to import a module named string. In Python 2.4 and earlier, Python will first look in the mypkg directory to perform a relative import. It will find and import the string.py file located there, assigning it to the name string in the mypkg.main module's namespace.

It could be, though, that the intent of this import was to load the Python standard library's string module instead. Unfortunately, in these versions of Python, there's no straightforward way to ignore mypkg.string and look for the standard library's string module located further to the right on the module search path. We cannot depend on any extra package directory structure above the standard library being present on every machine.

In other words, imports in packages can be ambiguous—within a package, it's not clear whether an import spam statement refers to a module within or outside the package. More accurately, a local module or package can hide another hanging directly off of sys.path, whether intentionally or not.

In practice, Python users can avoid reusing the names of standard library modules they need for modules of their own (if you need the standard string, don't name a new module string). But this doesn't help if a package accidentally hides a standard module; moreover, Python might add a new standard library module in the future that has the same name as a module of your own. Code that relies on relative imports is also less easy to understand because the reader may be confused about which module is intended to be used. It's better if the resolution can be made explicit in code.

In Python 2.5, we can control the behavior of imports, forcing them to be absolute by using the from __future__ import directive listed earlier. Again, bear in mind that this absolute-import behavior will become the default in a future version (planned currently for Python 2.7). When absolute imports are enabled, a statement of the following form in our example file mypkg/main.py will always find the standard library's version of string, via an absolute import search:


import string                          # Imports standard lib string


You should get used to using absolute imports now, so you're prepared when the change takes effect. That is, if you really want to import a module from your package, to make this explicit and absolute, you should begin writing statements like this in your code (mypkg will be found in an absolute directory on sys.path):


from mypkg import string               # Imports mypkg.string (absolute)


Relative imports are still possible by using the dot pattern in the from statement:


from . import string                   # Imports mypkg.string (relative)


This form imports the string module relative to the current package, and is the relative equivalent to the prior example's absolute form (the package's directory is automatically searched first).

We can also copy specific names from a module with relative syntax:


from .string import name1, name2       # Imports names from mypkg.string


This statement again refers to the string module relative to the current package. If this code appears in our mypkg.main module, for example, it will import name1 and name2 from mypkg.string. An additional leading dot performs the relative import starting from the parent of the current package. For example:


from .. import spam                    # Imports a sibling of mypkg


will load a sibling of mypkg—i.e., the spam module located in the parent directory, next to mypkg. More generally, code located in some module A.B.C can do any of these:


from . import D                        # Imports A.B.D
from .. import E                       # Imports A.E
from ..F import G                      # Imports A.F.G


Relative import syntax and the proposed absolute-by-default imports change are advanced concepts, and these features are still only partially present in Python 2.5. Because of that, we'll omit further details here; see Python's standard manual set for more information.

			30.4.4.7 . Module Design Concepts
Like functions, modules present design tradeoffs: you have to think about which functions go in which modules, module communication mechanisms, and so on. All of this will become clearer when you start writing bigger Python systems, but here are a few general ideas to keep in mind:

    * You're always in a module in Python. There's no way to write code that doesn't live in some module. In fact, code typed at the interactive prompt really goes in a built-in module called __main__; the only unique things about the interactive prompt are that code runs and is discarded immediately, and expression results are printed automatically.
    * Minimize module coupling: global variables. Like functions, modules work best if they're written to be closed boxes. As a rule of thumb, they should be as independent of global names in other modules as possible.
    * Maximize module cohesion: unified purpose. You can minimize a module's couplings by maximizing its cohesion; if all the components of a module share a general purpose, you're less likely to depend on external names.
    * Modules should rarely change other modules' variables. We illustrated this with code in Chapter 16, but it's worth repeating here: it's perfectly OK to use globals defined in another module (that's how clients import services, after all), but changing globals in another module is often a symptom of a design problem. There are exceptions, of course, but you should try to communicate results through devices such as function argument return values, not cross-module changes. Otherwise, your globals' values become dependent on the order of arbitrarily remote assignments in other files, and your modules become harder to understand and reuse.

As a summary, Figure 21-1 sketches the environment in which modules operate. Modules contain variables, functions, classes, and other modules (if imported). Functions have local variables of their own. You'll meet classes—other objects that live within modules—in Chapter 22.

Figure 21-1. Module execution environment. Modules are imported, but modules also import and use other modules, which may be coded in Python or another language such as C. Modules in turn contain variables, functions, and classes to do their work, and their functions and classes may contain variables and other items of their own. At the top, though, programs are just a set of modules.

				30.4.4.7.1 Modules Are Objects: Metaprograms

Because modules expose most of their interesting properties as built-in attributes, it's easy to write programs that manage other programs. We usually call such manager programs metaprograms because they work on top of other systems. This is also referred to as introspection because programs can see and process object internals. Introspection is an advanced feature, but it can be useful for building programming tools.

For instance, to get to an attribute called name in a module called M, we can use qualification, or index the module's attribute dictionary (exposed in the built-in __dict__ attribute). Python also exports the list of all loaded modules as the sys.modules dictionary (that is, the modules attribute of the sys module) and provides a built-in called getattr that lets us fetch attributes from their string names (it's like saying object.attr, but attr is a runtime string). Because of that, all the following expressions reach the same attribute and object:


M.name                            # Qualify object
M.__dict__['name']                 # Index namespace dictionary manually
sys.modules['M'].name            # Index loaded-modules table manually
getattr(M, 'name')               # Call built-in fetch function


By exposing module internals like this, Python helps you build programs about programs.[57] For example, here is a module named mydir.py that puts these ideas to work to implement a customized version of the built-in dir function. It defines and exports a function called listing, which takes a module object as an argument, and prints a formatted listing of the module's namespace:

    [57] * As we saw in Chapter 16, because a function can access its enclosing module by going through the sys.modules table like this, it's possible to emulate the effect of the global statement. For instance, the effect of global X; X=0 can be simulated (albeit with much more typing!) by saying this inside a function: import sys; glob=sys.modules[__name__]; glob.X=0. Remember, each module gets a __name__ attribute for free; it's visible as a global name inside the functions within the module. This trick provides another way to change both local and global variables of the same name inside a function.

# A module that lists the namespaces of other modules

verbose = 1

def listing(module):
    if verbose:
        print "-"*30
        print "name:", module.__name__, "file:", module.__file__
        print "-"*30

    count = 0
    for attr in module.__dict__.keys(  ):       # Scan namespace
        print "%02d) %s" % (count, attr),
        if attr[0:2] == "__":
            print "<built-in name>"           # Skip __file__, etc.
        else:
            print getattr(module, attr)       # Same as .__dict__[attr]
        count = count+1

    if verbose:
        print "-"*30
        print module.__name__, "has %d names" % count
        print "-"*30

if __name__ == "__main__":
    import mydir
    listing(mydir)                            # Self-test code: list myself

					  


We've also provided self-test logic at the bottom of this module, which narcissistically imports and lists itself. Here's the sort of output produced:


C:\python> python mydir.py
------------------------------
name: mydir file: mydir.py
------------------------------
00) __file__ <built-in name>
01) __name__ <built-in name>
02) listing <function listing at 885450>
03) __doc__ <built-in name>
04) __builtins__ <built-in name>
05) verbose 1
------------------------------
mydir has 6 names
------------------------------


We'll meet getattr and its relatives again later. The point to notice here is that mydir is a program that lets you browse other programs. Because Python exposes its internals, you can process objects generically.[58]

    [58] * Tools such as mydir.listing can be preloaded into the interactive namespace by importing them in the file referenced by the PYTHONSTARTUP environment variable. Because code in the startup file runs in the interactive namespace (module __main__), importing common tools in the startup file can save you some typing. See Appendix A for more details.

			30.4.4.8 . Module Gotchas
In this section, we'll take a look at the usual collection of boundary cases that make life interesting for Python beginners. Some are so obscure that it was hard to come up with examples, but most illustrate something important about the language.

				30.4.4.8.1 Statement Order Matters in Top-Level Code

When a module is first imported (or reloaded), Python executes its statements one by one, from the top of the file to the bottom. This has a few subtle implications regarding forward references that are worth underscoring here:

    * Code at the top level of a module file (not nested in a function) runs as soon as Python reaches it during an import; because of that, it can't reference names assigned lower in the file.
    * Code inside a function body doesn't run until the function is called; because names in a function aren't resolved until the function actually runs, they can usually reference names anywhere in the file.

Generally, forward references are only a concern in top-level module code that executes immediately; functions can reference names arbitrarily. Here's an example that illustrates forward reference:


func1(  )                           # Error: "func1" not yet assigned

def func1(  ):
    print func2(  )                  # OK:  "func2" looked up later

func1(  )                           # Error: "func2" not yet assigned

def func2(  ):
    return "Hello"

func1(  )                           # Okay:  "func1" and "func2" assigned


When this file is imported (or run as a standalone program), Python executes its statements from top to bottom. The first call to func1 fails because the func1 def hasn't run yet. The call to func2 inside func1 works as long as func2's def has been reached by the time func1 is called (it hasn't when the second top-level func1 call is run). The last call to func1 at the bottom of the file works because func1 and func2 have both been assigned.

Mixing defs with top-level code is not only hard to read, it's dependent on statement ordering. As a rule of thumb, if you need to mix immediate code with defs, put your defs at the top of the file, and top-level code at the bottom. That way, your functions are guaranteed to be defined and assigned by the time code that uses them runs.

				30.4.4.8.2 Importing Modules by Name String

The module name in an import or from statement is a hardcoded variable name. Sometimes, though, your program will get the name of a module to be imported as a string at runtime (e.g., if a user selects a module name from within a GUI). Unfortunately, you can't use import statements directly to load a module given its name as a string—Python expects a variable name here, not a string. For instance:


>>> import "string"
  File "<stdin>", line 1
    import "string"
                  ^
SyntaxError: invalid syntax


It also won't work to simply assign the string to a variable name:


x = "string"
import x


Here, Python will try to import a file x.py, not the string module.

To get around this, you need to use special tools to load a module dynamically from a string that is generated at runtime. The most general approach is to construct an import statement as a string of Python code, and pass it to the exec statement to run:


>>> modname = "string" 
>>> exec "import " + modname  # Run a string of code
>>> string  # Imported in this namespace
<module 'string'>


The exec statement (and its cousin for expressions, the eval function) compiles a string of code, and passes it to the Python interpreter to be executed. In Python, the byte code compiler is available at runtime, so you can write programs that construct and run other programs like this. By default, exec runs the code in the current scope, but you can get more specific by passing in optional namespace dictionaries.

The only real drawback to exec is that it must compile the import statement each time it runs; if it runs many times, your code may run quicker if it uses the built-in __import__ function to load from a name string instead. The effect is similar, but __import__ returns the module object, so assign it to a name here to keep it:


>>> modname = "string"
>>> string = __import__(modname)
>>> string
<module 'string'>

				30.4.4.8.3 from Copies Names but Doesn't Link

Although it's commonly used, the from statement is the source of a variety of potential gotchas in Python. The from statement is really an assignment to names in the importer's scope—a name-copy operation, not a name aliasing. The implications of this are the same as for all assignments in Python, but subtle, especially given that the code that shares the objects lives in different files. For instance, suppose we define the following module (nested1.py):


X = 99
def printer(  ): print X


If we import its two names using from in another module (nested2.py), we get copies of those names, not links to them. Changing a name in the importer resets only the binding of the local version of that name, not the name in nested1.py:


from nested1 import X, printer     # Copy names out
X = 88                             # Changes my "X" only!
printer(  )                              # nested1's X is still 99
% python nested2.py 
99


If we use import to get the whole module, and then assign to a qualified name, however, we change the name in nested1.py. Qualification directs Python to a name in the module object, rather than a name in the importer (nested3.py):


import nested1                     # Get module as a whole
nested1.X = 88                     # OK: change nested1's X
nested1.printer(  )

% python nested3.py 
88


				30.4.4.8.4 from * Can Obscure the Meaning of Variables

I mentioned this in Chapter 19, but saved the details for here. Because you don't list the variables you want when using the from module import * statement form, it can accidentally overwrite names you're already using in your scope. Worse, it can make it difficult to determine where a variable comes from. This is especially true if the from * form is used on more than one imported file.

For example, if you use from * on three modules, you'll have no way of knowing what a raw function call really means, short of searching all three external module files (all of which may be in other directories):


>>> from module1 import *  # Bad: may overwrite my names silently
>>> from module2 import *  # Worse: no way to tell what we get!
>>> from module3 import * 
>>> . . .

>>> func(  )  # Huh???


The solution again is not to do this: try to explicitly list the attributes you want in your from statements, and restrict the from * form to at most one imported module per file. That way, any undefined names must by deduction be in the module named in the single from *. You can avoid the issue altogether if you always use import instead of from, but that advice is too harsh; like much else in programming, from is a convenient tool if used wisely.

				30.4.4.8.5 reload May Not Impact from Imports

Here's another from-related gotcha: as discussed previously, because from copies (assigns) names when run, there's no link back to the module where the names came from. Names imported with from simply become references to objects, which happen to have been referenced by the same names in the importee when the from ran.

Because of this behavior, reloading the importee has no effect on clients that import its names using from. That is, the client's names will still reference the original objects fetched with from, even if the names in the original module are later reset:


from module import X          # X may not reflect any module reloads!
 . . .
reload(module)                # Changes module, but not my names
X                             # Still references old object


To make reloads more effective, use import and name qualification instead of from. Because qualifications always go back to the module, they will find the new bindings of module names after reloading:


import module                 # Get module, not names
 . . .
reload(module)                # Changes module in-place
module.X                      # Get current X: reflects module reloads

				30.4.4.8.6 reload, from, and Interactive Testing

Chapter 3 warned that it's usually better not to launch programs with imports and reloads because of the complexities involved. Things get even worse when from is brought into the mix. Python beginners often encounter the gotcha described here. After opening a module file in a text edit window, say you launch an interactive session to load and test your module with from:


from module import function
function(1, 2, 3)


Finding a bug, you jump back to the edit window, make a change, and try to reload the module this way:


reload(module)


But this doesn't work—the from statement assigned the name function, not module. To refer to the module in a reload, you have to first load it with an import statement at least once:


import module
reload(module)
function(1, 2, 3)


However, this doesn't quite work either—reload updates the module object, but as discussed in the preceding section, names like function that were copied out of the module in the past still refer to the old objects (in this instance, the original version of the function). To really get the new function, you must call it module.function after the reload, or rerun the from:


import module
reload(module)
from module import function
function(1, 2, 3)


Now, the new version of the function will finally run.

As you can see, there are problems inherent in using reload with from: not only do you have to remember to reload after imports, but you also have to remember to rerun your from statements after reloads. This is complex enough to trip up even an expert once in a while.

You should not expect reload and from to play together nicely. The best policy is not to combine them at all—use reload with import, or launch your programs other ways, as suggested in Chapter 3 (e.g., using the Run → Run Module menu option in IDLE, file icon clicks, or system command lines).

				30.4.4.8.7 reload Isn't Applied Transitively

When you reload a module, Python only reloads that particular module's file; it doesn't automatically reload modules that the file being reloaded happens to import. For example, if you reload some module A, and A imports modules B and C, the reload applies only to A, not to B and C. The statements inside A that import B and C are rerun during the reload, but they just fetch the already loaded B and C module objects (assuming they've been imported before). In actual code, here's the file A.py:


import B                    # Not reloaded when A is
import C                    # Just an import of an already loaded module

% python 
>>> . . .
>>> reload(A) 


Don't depend on transitive module reloads—instead, use multiple reload calls to update subcomponents independently. If desired, you can design your systems to reload their subcomponents automatically by adding reload calls in parent modules like A.

Better still, you could write a general tool to do transitive reloads automatically by scanning modules' __dict__ attributes (see "Section 21.7.1" earlier in this chapter) and checking each item's type (see Chapter 9) to find nested modules to reload recursively. Such a utility function could call itself recursively to navigate arbitrarily shaped import dependency chains.

For example, the module reloadall.py listed below has a reload_all function that automatically reloads a module, every module that the module imports, and so on, all the way to the bottom of each import chain. It uses a dictionary to keep track of already reloaded modules; recursion to walk the import chains; and the standard library's types module (introduced at the end of Chapter 9), which simply predefines type results for built-in types.

To use this utility, import its reload_all function, and pass it the name of an already loaded module (like you would the built-in reload function). When the file runs standalone, its self-test code will test itself—it has to import itself because its own name is not defined in the file without an import. I encourage you to study and experiment with this example on your own:


import types

def status(module):
    print 'reloading', module.__name__

def transitive_reload(module, visited):
    if not visited.has_key(module):                   # Trap cycles, dups
        status(module)                                # Reload this module
        reload(module)                               # And visit children
        visited[module] = None
        for attrobj in module.__dict__.values(  ):     # For all attrs
            if type(attrobj) == types.ModuleType:    # Recur if module
                transitive_reload(attrobj, visited)

def reload_all(*args):
    visited = {  }
    for arg in args:
        if type(arg) == types.ModuleType:
            transitive_reload(arg, visited)

if __name__ == '__main__':
    import reloadall                      # Test code: reload myself
    reload_all(reloadall)                 # Should reload this, types


				30.4.4.8.8 Recursive from Imports May Not Work

I saved the most bizarre (and, thankfully, obscure) gotcha for last. Because imports execute a file's statements from top to bottom, you need to be careful when using modules that import each other (known as recursive imports). Because the statements in a module may not all have been run when it imports another module, some of its names may not yet exist.

If you use import to fetch the module as a whole, this may or may not matter; the module's names won't be accessed until you later use qualification to fetch their values. But, if you use from to fetch specific names, you must bear in mind that you will only have access to names in that module that have already been assigned.

For instance, take the following modules, recur1 and recur2. recur1 assigns a name X, and then imports recur2 before assigning the name Y. At this point, recur2 can fetch recur1 as a whole with an import (it already exists in Python's internal modules table), but if it uses from, it will be able to see only the name X; the name Y, which is assigned below the import in recur1, doesn't yet exist, so you get an error:
Code View: Scroll / Show All


# File: recur1.py
X = 1
import recur2                              # Run recur2 now if it doesn't exist
Y = 2

# File: recur2.py
from recur1 import X                       # OK: "X" already assigned
from recur1 import Y                       # Error: "Y" not yet assigned
>>> import recur1 
Traceback (innermost last):
  File "<stdin>", line 1, in ?
  File "recur1.py", line 2, in ?
    import recur2
  File "recur2.py", line 2, in ?
    from recur1 import Y                   # Error: "Y" not yet assigned
ImportError: cannot import name Y

					  


Python avoids rerunning recur1's statements when they are imported recursively from recur2 (or else the imports would send the script into an infinite loop), but recur1's namespace is incomplete when imported by recur2.

The solution? Don't use from in recursive imports (no, really!). Python won't get stuck in a cycle if you do, but your programs will once again be dependent on the order of the statements in the modules.

There are two ways out of this gotcha:

    * You can usually eliminate import cycles like this by careful design—maximizing cohesion, and minimizing coupling are good first steps.
    * If you can't break the cycles completely, postpone module name accesses by using import and qualification (instead of from), or by running your froms either inside functions (instead of at the top level of the module), or near the bottom of your file to defer their execution.

			30.4.4.9 . Chapter Summary
This chapter surveyed some more advanced module-related concepts. We studied data hiding techniques, enabling new language features with the __future__ module, the __name__ usage mode variable, package-relative import syntax, and more. We also explored and summarized module design issues, and looked at common mistakes related to modules to help you avoid them in your code.

The next chapter begins our look at Python's object-oriented programming tool, the class. Much of what we've covered in the last few chapters will apply there, too—classes live in modules, and are namespaces as well, but they add an extra component to attribute lookup called "inheritance search." As this is the last chapter in this part of the book, however, before we dive into that topic, be sure to work through this part's set of lab exercises. And, before that, here is this chapter's quiz to review the topics covered here.

BRAIN BUILDER

Chapter Quiz

Q:	What is significant about variables at the top level of a module whose names begin with a single underscore?
Q:	What does it mean when a module's __name__ variable is the string "__main__"?
Q:	What is the difference between from mypkg import spam and from . import spam?
Q:	If the user interactively types the name of a module to test, how can you import it?
Q:	How is changing sys.path different from setting PYTHONPATH to modify the module search path?
Q:	If the module __future__ allows us to import from the future, can we also import from the past?


Quiz Answers

A:	Variables at the top level of a module whose names begin with a single underscore are not copied out to the importing scope when the from * statement form is used. They can still be accessed by an import, or the normal from statement form, though.
A:	If a module's __name__ variable is the string "__main__", it means that the file is being executed as a top-level script, instead of being imported from another file in the program. That is, the file is being used as a program, not a library.
A:	from mypkg import spam is an absolute import—mypkg is located in an absolute directory in sys.path. from . import spam, on the other hand, is a relative import—spam is looked up relative to the package in which this statement is contained before sys.path is searched.
A:	User input usually comes into a script as a string; to import the referenced module given its string name, you can build and run an import statement with exec, or pass the string name in a call to the __import__ function.
A:	Changing sys.path only affects one running program, and is temporary—the change goes away when the program ends. PYTHONPATH settings live in the operating system—they are picked up globally by all programs on a machine, and changes to these settings endure after programs exit.
A:	No, we can't import from the past in Python. We can install (or stubbornly use) an older version of the language, but the latest Python is generally the best Python.

			30.4.4.10. BRAIN BUILDER
|   1.  Import basics. Write a program that counts the lines and characters in a file (similar in spirit to wc on Unix). With your text editor, code a Python module called mymod.py that exports three top-level names:
|          * A countLines(name) function that reads an input file and counts the number of lines in it (hint: file.readlines does most of the work for you, and len does the rest).
|          * A countChars(name) function that reads an input file and counts the number of characters in it (hint: file.read returns a single string).
|          * A test(name) function that calls both counting functions with a given input filename. Such a filename generally might be passed in, hardcoded, input with raw_input, or pulled from a command line via the sys.argv list; for now, assume it's a passed-in function argument.
|
|      All three mymod functions should expect a filename string to be passed in. If you type more than two or three lines per function, you're working much too hard—use the hints I just gave!
|
|      Next, test your module interactively, using import and name qualification to fetch your exports. Does your PYTHONPATH need to include the directory where you created mymod.py? Try running your module on itself: e.g., test("mymod.py"). Note that test opens the file twice; if you're feeling ambitious, you may be able to improve this by passing an open file object into the two count functions (hint: file.seek(0) is a file rewind).

My answer:
>>> sys.path
['',
 '/usr/lib/python26.zip',
 ...
 '/usr/lib/python2.6/site-packages/gtk-2.0']
>>> os.listdir("/cygdrive/c")
['$Recycle.Bin',
 'ADOBEPATH',
 ...
 'work']
>>> sys.path.append("/cygdrive/c/work/scripts/python/learn/learning_python/")
>>> import wc_api
>>> os.listdir("/cygdrive/c/work/scripts/python/learn/learning_python/")
['adder.py', 'timerseqs.py', 'wc_api.py', 'wc_api.pyc']
>>> wc_api.test("/cygdrive/c/work/scripts/python/learn/learning_python/adder.py")
# lines in /cygdrive/c/work/scripts/python/learn/learning_python/adder.py is 50
# chars in /cygdrive/c/work/scripts/python/learn/learning_python/adder.py is 1117
>>> wc_api.test("/cygdrive/c/work/scripts/python/learn/learning_python/wc_api.py")
# lines in /cygdrive/c/work/scripts/python/learn/learning_python/wc_api.py is 14
# chars in /cygdrive/c/work/scripts/python/learn/learning_python/wc_api.py is 349


|   2.  from/from *. Test your mymod module from exercise 1 interactively by using from to load the exports directly, first by name, then using the from * variant to fetch everything.

My answer:
>>> from wc_api import test
>>> test("/cygdrive/c/work/scripts/python/learn/learning_python/wc_api.py")
# lines in /cygdrive/c/work/scripts/python/learn/learning_python/wc_api.py is 14
# chars in /cygdrive/c/work/scripts/python/learn/learning_python/wc_api.py is 349
>>> from wc_api import *
>>> count
countChars(  countLines(  
>>> countChars("/cygdrive/c/work/scripts/python/learn/learning_python/wc_api.py")
349

|   3.  __main__. Add a line in your mymod module that calls the test function automatically only when the module is run as a script, not when it is imported. The line you add will probably test the value of __name__ for the string "__main__" as shown in this chapter. Try running your module from the system command line; then, import the module and test its functions interactively. Does it still work in both modes?
:!python /cygdrive/c/work/scripts/python/learn/learning_python/wc_api.py
# lines in /cygdrive/c/work/scripts/python/learn/learning_python/wc_api.py is 15
# chars in /cygdrive/c/work/scripts/python/learn/learning_python/wc_api.py is 449

|   4.  Nested imports. Write a second module, myclient.py, that imports mymod, and tests its functions; then, run myclient from the system command line. If myclient uses from to fetch from mymod, will mymod's functions be accessible from the top level of myclient? What if it imports with import instead? Try coding both variations in myclient and test interactively by importing myclient and inspecting its __dict__ attribute.
|   5.  Package imports. Import your file from a package. Create a subdirectory called mypkg nested in a directory on your module import search path, move the mymod.py module file you created in exercise 1 or 3 into the new directory, and try to import it with a package import of the form import mypkg.mymod.
|
|      You'll need to add an __init__.py file in the directory your module was moved to make this go, but it should work on all major Python platforms (that's part of the reason Python uses "." as a path separator). The package directory you create can be simply a subdirectory of the one you're working in; if it is, it will be found via the home directory component of the search path, and you won't have to configure your path. Add some code to your __init__.py, and see if it runs on each import.
|   6.  Reloads. Experiment with module reloads: perform the tests in Chapter 19's changer.py example, changing the called function's message, and/or behavior repeatedly, without stopping the Python interpreter. Depending on your system, you might be able to edit changer in another window, or suspend the Python interpreter, and edit in the same window (on Unix, a Ctrl-Z key combination usually suspends the current process, and an fg command later resumes it).
|   7.  Circular imports.[59] In the section on recursive import gotchas, importing recur1 raised an error. But, if you restart Python, and import recur2 interactively, the error doesn't occur—test and see this for yourself. Why do you think it works to import recur2, but not recur1? (Hint: Python stores new modules in the built-in sys.modules table (a dictionary) before running their code; later imports fetch the module from this table first, whether the module is "complete" yet or not.) Now, try running recur1 as a top-level script file: python recur1.py. Do you get the same error that occurs when recur1 is imported interactively? Why? (Hint: when modules are run as programs, they aren't imported, so this case has the same effect as importing recur2 interactively; recur2 is the first module imported.) What happens when you run recur2 as a script?
|
|          [59] * Note that circular imports are extremely rare in practice. In fact, this author has never coded or come across a circular import in a decade of Python coding. On the other hand, if you can understand why they are a potential problem, you know a lot about Python's import semantics.	

---------------------------------------------------
ANSWERS
---------------------------------------------------
|   1.  Import basics. This one is simpler than you may think. When you're done, your file (mymod.py) and interaction should look similar to the following; remember that Python can read a whole file into a list of line strings, and the len built-in returns the length of strings and lists.


      def countLines(name):
          file = open(name)
          return len(file.readlines(  ))

      def countChars(name):
          return len(open(name).read(  ))

      def test(name):                                   # Or pass file object
          return countLines(name), countChars(name)     # Or return a dictionary

      % python 
      >>> import mymod 
      >>> mymod.test('mymod.py') 
      (10, 291)


      Note that these functions load the entire file in memory all at once, and so won't work for pathologically large files too big for your machine's memory. To be more robust, you could read line by line with iterators instead and count as you go:


      def countLines(name):
          tot = 0
          for line in open(name): tot += 1
          return tot

      def countChars(name):
          tot = 0
          for line in open(name): tot += len(line)
          return tot


      On Unix, you can verify your output with a wc command; on Windows, right-click on your file to view its properties. But note that your script may report fewer characters than Windows does—for portability, Python converts Windows \r\n line-end markers to \n, thereby dropping one byte (character) per line. To match byte counts with Windows exactly, you have to open in binary mode ('rb'), or add the number of bytes corresponding to the number of lines.

      Incidentally, to do the "ambitious" part of this exercise (passing in a file object, so you only open the file once), you'll probably need to use the seek method of the built-in file object. We didn't cover it in the text, but it works just like C's fseek call (and calls it behind the scenes): seek resets the current position in the file to a passed-in offset. After a seek, future input/output operations are relative to the new position. To rewind to the start of a file without closing and reopening it, call file.seek(0); the file read methods all pick up at the current position in the file, so you need to rewind to reread. Here's what this tweak would look like:

      def countLines(file):
          file.seek(0)                                  # Rewind to start of file
          return len(file.readlines(  ))

      def countChars(file):
          file.seek(0)                                  # Ditto (rewind if needed)
          return len(file.read(  ))

      def test(name):
          file = open(name)                             # Pass file object
          return countLines(file), countChars(file)     # Open file only once

      >>> import mymod2 
      >>> mymod2.test("mymod2.py") 
      (11, 392)

      					  


|   2.  from/from *. Here's the from * part; replace * with countChars to do the rest:


      % python
      >>> from mymod import *
      >>> countChars("mymod.py")
      291


|   3.  __main__. If you code it properly, it works in either mode (program run or module import):


      def countLines(name):
          file = open(name)
          return len(file.readlines(  ))

      def countChars(name):
          return len(open(name).read(  ))

      def test(name):                                   # Or pass file object
          return countLines(name), countChars(name)     # Or return a dictionary

      if __name__ == '__main__':
          print test('mymod.py')

      % python mymod.py 
      (13, 346)


|   4.  Nested imports. Here is my solution (file myclient.py):


      from mymod import countLines, countChars
      print countLines('mymod.py'), countChars('mymod.py')

      % python myclient.py
      13 346


      As for the rest of this one, mymod's functions are accessible (that is, importable) from the top level of myclient, since from simply assigns to names in the importer (it works almost as though mymod's defs appeared in myclient). For example, another file can say this:


      import myclient
      myclient.countLines(...)

      from myclient import countChars
      countChars(...)


      If myclient used import instead of from, you'd need to use a path to get to the functions in mymod through myclient:


      import myclient
      myclient.mymod.countLines(...)

      from myclient import mymod
      mymod.countChars(...)


      In general, you can define collector modules that import all the names from other modules so they're available in a single convenience module. Using the following code, you wind up with three different copies of the name somename (mod1.somename, collector.somename, and __main__.somename); all three share the same integer object initially, and only the name somename exists at the interactive prompt as is:

      # File: mod1.py
      somename = 42

      # File: collector.py
      from mod1 import *                                # Collect lots of names here
      from mod2 import *                                # from assigns to my names
      from mod3 import *

      >>> from collector import somename 

      					  


|   5.  Package imports. For this, I put the mymod.py solution file listed for exercise 3 into a directory package. The following is what I did to set up the directory and its required __init__.py file in a Windows console interface; you'll need to interpolate for other platforms (e.g., use mv and vi instead of move and edit). This works in any directory (I just happened to run my commands in Python's install directory), and you can do some of this from a file explorer GUI, too.

      When I was done, I had a mypkg subdirectory that contained the files __init__.py and mymod.py. You need an __init__.py in the mypkg directory, but not in its parent; mypkg is located in the home directory component of the module search path. Notice how a print statement coded in the directory's initialization file fires only the first time it is imported, not the second:


      C:\python25> mkdir mypkg 
      C:\Python25> move mymod.py mypkg\mymod.py 
      C:\Python25> edit mypkg\__init__.py ...coded a print statement... 
      C:\Python25> python 
      >>> import mypkg.mymod 
      initializing mypkg
      >>> mypkg.mymod.countLines('mypkg\mymod.py') 
      13
      >>> from mypkg.mymod import countChars 
      >>> countChars('mypkg\mymod.py') 
      346


|   6.  Reloads. This exercise just asks you to experiment with changing the changer.py example in the book, so there's nothing to show here.
|   7.  Circular imports. The short story is that importing recur2 first works because the recursive import then happens at the import in recur1, not at a from in recur2.

      The long story goes like this: importing recur2 first works because the recursive import from recur1 to recur2 fetches recur2 as a whole, instead of getting specific names. recur2 is incomplete when imported from recur1, but because it uses import instead of from, you're safe: Python finds and returns the already created recur2 module object, and continues to run the rest of recur1 without a glitch. When the recur2 import resumes, the second from finds the name Y in recur1 (it's been run completely), so no error is reported. Running a file as a script is not the same as importing it as a module; these cases are the same as running the first import or from in the script interactively. For instance, running recur1 as a script is the same as importing recur2 interactively, as recur2 is the first module imported in recur1.

	30.5. Classes and OOP
So far in this book, we've been using the term "object" generically. Really, the code written up to this point has been object-based—we've passed objects around our scripts, used them in expressions, called their methods, and so on. For our code to qualify as being truly object-oriented (OO), though, our objects will generally need to also participate in something called an inheritance hierarchy.

This chapter begins our exploration of the Python class—a device used to implement new kinds of objects in Python that support inheritance. Classes are Python's main object-oriented programming (OOP) tool, so we'll also look at OOP basics along the way in this part of the book. OOP offers a different and often more effective way of looking at programming, in which we factor code to minimize redundancy, and write new programs by customizing existing code instead of changing it in-place.

In Python, classes are created with a new statement: the class statement. As you'll see, the objects defined with classes can look a lot like the built-in types we studied earlier in the book. In fact, classes really just apply and extend the ideas we've already covered; roughly, they are packages of functions that largely use and process built-in object types. Classes, though, are designed to create and manage new objects, and they also support inheritance—a mechanism of code customization and reuse above and beyond anything we've seen so far.

One note up front: in Python, OOP is entirely optional, and you don't need to use classes just to get started. In fact, you can get plenty of work done with simpler constructs such as functions, or even simple top-level script code. Because using classes well requires some up-front planning, they tend to be of more interest to people who work in strategic mode (doing long-term product development) than to people who work in tactical mode (where time is in very short supply).

Still, as you'll see in this part of the book, classes turn out to be one of the most useful tools Python provides. When used well, classes can actually cut development time radically. They're also employed in popular Python tools like the Tkinter GUI API, so most Python programmers will usually find at least a working knowledge of class basics helpful.

		30.5.1. OOP: The Big Picture

			30.5.1.1 Why Use Classes?

Remember when I told you that programs "do things with stuff"? In simple terms, classes are just a way to define new sorts of stuff, reflecting real objects in a program's domain. For instance, suppose we decide to implement that hypothetical pizza-making robot we used as an example in Chapter 15. If we implement it using classes, we can model more of its real-world structure and relationships. Two aspects of OOP prove useful here:


Inheritance

    Pizza-making robots are kinds of robots, so they possess the usual robot-y properties. In OOP terms, we say they "inherit" properties from the general category of all robots. These common properties need to be implemented only once for the general case, and can be reused by all types of robots we may build in the future.

Composition

    Pizza-making robots are really collections of components that work together as a team. For instance, for our robot to be successful, it might need arms to roll dough, motors to maneuver to the oven, and so on. In OOP parlance, our robot is an example of composition; it contains other objects that it activates to do its bidding. Each component might be coded as a class, which defines its own behavior and relationships.

			30.5.1.2 OOP from 30,000 Feet 

Before we see what this all means in terms of code, I'd like to say a few words about the general ideas behind OOP. If you've never done anything object-oriented in your life before now, some of the terminology in this chapter may seem a bit perplexing on the first pass. Moreover, the motivation for these terms may be elusive until you've had a chance to study the ways that programmers apply them in larger systems. OOP is as much an experience as a technology.

				30.5.1.2.1 Attribute Inheritance Search

The good news is that OOP is much simpler to understand and use in Python than in other languages such as C++ or Java. As a dynamically typed scripting language, Python removes much of the syntactic clutter and complexity that clouds OOP in other tools. In fact, most of the OOP story in Python boils down to this expression:

			30.5.1.3 Chapter Summary

We took an abstract look at classes and OOP in this chapter, taking in the big picture before we dive into syntax details. As we've seen, OOP is mostly about looking up attributes in trees of linked objects; we call this lookup an inheritance search. Objects at the bottom of the tree inherit attributes from objects higher up in the tree—a feature that enables us to program by customizing code, rather than changing it, or starting from scratch. When used well, this model of programming can cut development time radically.

The next chapter will begin to fill in the coding details behind the picture painted here. As we get deeper into Python classes, though, keep in mind that the OOP model in Python is very simple; as I've already stated, it's really just about looking up attributes in object trees. Before we move on, here's a quick quiz to review what we've covered here.

		30.5.2. Class Coding Basis
Now that we've talked about OOP in the abstract, it's time to see how this translates to actual code. This chapter and the next will fill in the syntax details behind the class model in Python.

If you've never been exposed to OOP in the past, classes can seem somewhat complicated if taken in a single dose. To make class coding easier to absorb, we'll begin our detailed exploration of OOP by taking a first look at some basic classes in action in this chapter. We'll expand on the details introduced here in later chapters of this part of the book, but in their basic form, Python classes are easy to understand.

Classes have three primary distinctions. At a base level, they are mostly just namespaces, much like the modules we studied in Part 5. But, unlike modules, classes also have support for generating multiple objects, for namespace inheritance, and for operator overloading. Let's begin our class statement tour by exploring each of these three distinctions in turn.

			30.5.2.1 Classes Generate Multiple Instance Objects
To understand how the multiple objects idea works, you have to first understand that there are two kinds of objects in Python's OOP model: class objects, and instance objects. Class objects provide default behavior, and serve as factories for instance objects. Instance objects are the real objects your programs process—each is a namespace in its own right, but inherits (i.e., has automatic access to) names in the class from which it was created. Class objects come from statements, and instances from calls; each time you call a class, you get a new instance of that class.

This object-generation concept is very different from any of the other program constructs we've seen so far in this book. In effect, classes are factories for generating multiple instances. By contrast, only one copy of each module is ever imported into a single program (in fact, one reason that we have to call reload is to update the single module object so that changes are reflected once they've been made).

The following is a quick summary of the bare essentials of Python OOP. As you'll see, Python classes are in some ways similar to both defs and modules, but they may be quite different from what you're used to in other languages.

				30.5.2.1.1 Class Objects Provide Default Behavior

When we run a class statement, we get a class object. Here's a rundown of the main properties of Python classes:

    * The classs tatement creates a class object and assigns it a name. Just like the function def statement, the Python class statement is an executable statement. When reached and run, it generates a new class object, and assigns it to the name in the class header. Also, like defs, class statements typically run when the files they are coded in are first imported.
    * Assignments inside classs tatements make class attributes. Just like in module files, top-level assignments within a class statement (not nested in a def) generate attributes in a class object. Technically, the class statement scope morphs into the attribute namespace of the class object, just like a module's global scope. After running a class statement, class attributes are accessed by name qualification: object.name.
    * Class attributes provide object state and behavior. Attributes of a class object record state information and behavior to be shared by all instances created from the class; function def statements nested inside a class generate methods, which process instances.

				30.5.2.1.2 Instance Objects Are Concrete Items

When we call a class object, we get an instance object. Here's an overview of the key points behind class instances:

    * Calling a class object like a function makes a new instance object. Each time a class is called, it creates and returns a new instance object. Instances represent concrete items in your program's domain.
    * Each instance object inherits class attributes and gets its own namespace. Instance objects created from classes are new namespaces; they start out empty, but inherit attributes that live in the class objects from which they were generated.
    * Assignments to attributes of self in methods make per-instance attributes. Inside class method functions, the first argument (called self by convention) references the instance object being processed; assignments to attributes of self create or change data in the instance, not the class.

				30.5.2.1.3 A First Example

Let's turn to a real example to show how these ideas work in practice. To begin, let's define a class named FirstClass by running a Python class statement interactively:


>>> class FirstClass:  # Define a class object
...     def setdata(self, value):  # Define class methods
...         self.data = value  # self is the instance
...     def display(self): 
...         print self.data  # self.data: per instance
...


We're working interactively here, but typically, such a statement would be run when the module file it is coded in is imported. Like functions created with defs, this class won't even exist until Python reaches and runs this statement.

Like all compound statements, the class starts with a header line that lists the class name, followed by a body of one or more nested and (usually) indented statements. Here, the nested statements are defs; they define functions that implement the behavior the class means to export. As we've learned, def is really an assignment; here, it assigns function objects to the names set data and display in the class statement's scope, and so generates attributes attached to the class: FirstClass.setdata, and FirstClass.display. In fact, any name assigned at the top level of the class's nested block becomes an attribute of the class.

Functions inside a class are usually called methods. They're normal defs, and they support everything we've learned about functions already (they can have defaults, return values, and so on). But, in a method function, the first argument automatically receives an implied instance object when called—the subject of the call. We need to create a couple of instances to see how this works:


>>> x = FirstClass(  )  # Make two instances
>>> y = FirstClass(  )  # Each is a new namespace


By calling the class this way (notice the parentheses), we generate instance objects, which are just namespaces that have access to their class' attributes. Properly speaking, at this point, we have three objects—two instances, and a class. Really, we have three linked namespaces, as sketched in Figure 23-1. In OOP terms, we say that x "is a" FirstClass, as is y.

Figure 23-1. Classes and instances are linked namespace objects in a class tree that is searched by inheritance. Here, the "data" attribute is found in instances, but "setdata" and "display" are in the class above them.


The two instances start empty, but have links back to the class from which they were generated. If we qualify an instance with the name of an attribute that lives in the class object, Python fetches the name from the class by inheritance search (unless it also lives in the instance):


>>> x.setdata("King Arthur")  # Call methods: self is x
>>> y.setdata(3.14159)  # Runs: FirstClass.setdata(y, 3.14159)


Neither x nor y has a setdata attribute of its own, so to find it, Python follows the link from instance to class. And that's about all there is to inheritance in Python: it happens at attribute qualification time, and it just involves looking up names in linked objects (e.g., by following the is-a links in Figure 23-1).

In the setdata function inside FirstClass, the value passed in is assigned to self.data. Within a method, self—the name given to the leftmost argument by convention—automatically refers to the instance being processed (x or y), so the assignments store values in the instances' namespaces, not the class' (that's how the data names in Figure 23-1 are created).

Because classes generate multiple instances, methods must go through the self argument to get to the instance to be processed. When we call the class' display method to print self.data, we see that it's different in each instance; on the other hand, the name display itself is the same in x and y, as it comes (is inherited) from the class:


>>> x.display(  )  # self.data differs in each instance
King Arthur
>>> y.display(  ) 
3.14159


Notice that we stored different object types in the data member in each instance (a string, and a floating point). As with everything else in Python, there are no declarations for instance attributes (sometimes called members); they spring into existence the first time they are assigned values, just like simple variables. In fact, if we were to call display on one of our instances before calling setdata, we would trigger an undefined name error—the attribute named data doesn't even exist in memory until it is assigned within the setdata method.

As another way to appreciate how dynamic this model is, consider that we can change instance attributes in the class itself, by assigning to self in methods, or outside the class, by assigning to an explicit instance object:


>>> x.data = "New value"  # Can get/set attributes
>>> x.display(  )  # Outside the class too
New value


Although less common, we could even generate a brand new attribute in the instance's namespace by assigning to its name outside the class's method functions:


>>> x.anothername = "spam"  # Can set new attributes here too


This would attach a new attribute called anothername, which may or may not be used by any of the class' methods to the instance object x. Classes usually create all of the instance's attributes by assignment to the self argument, but they don't have to; programs can fetch, change, or create attributes on any objects to which they have references.

			30.5.2.2 Classes Are Customized by Inheritance
Besides serving as factories for generating multiple instance objects, classes also allow us to make changes by introducing new components (called subclasses), instead of changing existing components in-place. Instance objects generated from a class inherit the class' attributes. Python also allows classes to inherit from other classes, opening the door to coding hierarchies of classes that specialize behavior by overriding existing attributes lower in the hierarchy. Here, too, there is no parallel with modules: their attributes live in a single, flat namespace.

In Python, instances inherit from classes, and classes inherit from superclasses. Here are the key ideas behind the machinery of attribute inheritance:

    * Superclasses are listed in parentheses in aclassheader. To inherit attributes from another class, just list the class in parentheses in a class statement's header. The class that inherits is called a subclass, and the class that is inherited from is its superclass.
    * Classes inherit attributes from their superclasses. Just as instances inherit the attribute names defined in their classes, classes inherit all the attribute names defined in their superclasses; Python finds them automatically when they're accessed, if they don't exist in the subclasses.
    * Instances inherit attributes from all accessible classes. Each instance gets names from the class it's generated from, as well as all of that class' superclasses. When looking for a name, Python checks the instance, then its class, then all superclasses.
    * Each object.attribute reference invokes a new, independent search. Python performs an independent search of the class tree for each attribute fetch expression. This includes references to instances and classes made outside class statements (e.g., X.attr), as well as references to attributes of the self instance argument in class method functions. Each self.attr expression in a method invokes a new search for attr in self and above.
    * Logic changes are made by subclassing, not by changing superclasses. By redefining superclass names in subclasses lower in the hierarchy (tree), subclasses replace and thus customize inherited behavior.

The net effect, and the main purpose of all this searching, is that classes support factoring and customization of code better than any other language tool we've seen so far. On the one hand, they allow us to minimize code redundancy (and so reduce maintenance costs) by factoring operations into a single, shared implementation; on the other, they allow us to program by customizing what already exists, rather than changing it in-place or starting from scratch.

				30.5.2.2.1 A Second Example

This next example builds on the previous one. First, we'll define a new class, SecondClass, that inherits all of FirstClass' names and provides one of its own:


>>> class SecondClass(FirstClass):  # Inherits setdata
...     def display(self):  # Changes display
...         print 'Current value = "%s"' % self.data 
...


SecondClass defines the display method to print with a different format. By defining an attribute with the same name as an attribute in FirstClass, SecondClass effectively replaces the display attribute in its superclass.

Recall that inheritance searches proceed upward from instances, to subclasses, to superclasses, stopping at the first appearance of the attribute name that it finds. In this case, since the display name in SecondClass will be found before the one in FirstClass, we say that SecondClass overrides FirstClass's display. Sometimes we call this act of replacing attributes by redefining them lower in the tree overloading.

The net effect here is that SecondClass specializes FirstClass, by changing the behavior of the display method. On the other hand, SecondClass (and any instances created from it) still inherits the setdata method in FirstClass verbatim. Let's make an instance to demonstrate:


>>> z = SecondClass(  ) 
>>> z.setdata(42)  # Finds setdata in FirstClass
>>> z.display(  )  # Finds overridden method in SecondClass
Current value = "42"


As before, we make a SecondClass instance object by calling it. The setdata call still runs the version in FirstClass, but this time the display attribute comes from SecondClass and prints a custom message. Figure 23-2 sketches the namespaces involved.

Figure 23-2. Specialization by overriding inherited names by redefining them in extensions lower in the class tree. Here, SecondClass redefines and so customizes the "display" method for its instances.


Here's a very important thing to notice about OOP: the specialization introduced in SecondClass is completely external to FirstClass. That is, it doesn't affect existing or future FirstClass objects, like the x from the prior example:


>>> x.display(  )  # x is still a FirstClass instance (old message)
New value


Rather than changing FirstClass, we customized it. Naturally, this is an artificial example, but as a rule, because inheritance allows us to make changes like this in external components (i.e., in subclasses), classes often support extension and reuse better than functions or modules can.

				30.5.2.2.2 Classes Are Attributes in Modules

Before we move on, remember that there's nothing magic about a class name. It's just a variable assigned to an object when the class statement runs, and the object can be referenced with any normal expression. For instance, if our FirstClass was coded in a module file instead of being typed interactively, we could import it and use its name normally in a class header line:


from modulename import FirstClass           # Copy name into my scope
class SecondClass(FirstClass):              # Use class name directly
    def display(self): ...


Or, equivalently:


import modulename                           # Access the whole module
class SecondClass(modulename.FirstClass):   # Qualify to reference
    def display(self): ...


Like everything else, class names always live within a module, and so must follow all the rules we studied in Part 5. For example, more than one class can be coded in a single module file—like other statements in a module, class statements are run during imports to define names, and these names become distinct module attributes. More generally, each module may arbitrarily mix any number of variables, functions, and classes, and all names in a module behave the same way. The file food.py demonstrates:


# food.py
var = 1                                       # food.var
def func(  ):                                 # food.func
    ...
class spam:                                 # food.spam
    ...
class ham:                                  # food.ham
    ...
class eggs:                                 # food.eggs
    ...


This holds true even if the module and class happen to have the same name. For example, given the following file, person.py:


class person:
   ...


we need to go through the module to fetch the class as usual:


import person                                 # Import module
x = person.person(  )                         # Class within module


Although this path may look redundant, it's required: person.person refers to the person class inside the person module. Saying just person gets the module, not the class, unless the from statement is used:


from person import person                     # Get class from module
x = person(  )                                # Use class name


Like any other variable, we can never see a class in a file without first importing and somehow fetching it from its enclosing file. If this seems confusing, don't use the same name for a module and a class within it.

Also, keep in mind that although classes and modules are both namespaces for attaching attributes, they correspond to very different source code structures: a module reflects an entire file, but a class is a statement within a file. We'll say more about such distinctions later in this part of the book.

			30.5.2.3 Classes Can Intercept Python Operators
Now, let's take a look at the third major difference between classes and modules: operator overloading. In simple terms, operator overloading lets objects coded with classes intercept and respond to operations that work on built-in types: addition, slicing, printing, qualification, and so on. It's mostly just an automatic dispatch mechanism: expressions and other built-in operations route control to implementations in classes. Here, too, there is nothing similar in modules: modules can implement function calls, but not the behavior of expressions.

Although we could implement all class behavior as method functions, operator overloading lets objects be more tightly integrated with Python's object model. Moreover, because operator overloading makes our own objects act like built-ins, it tends to foster object interfaces that are more consistent and easier to learn, and it allows class-based objects to be processed by code written to expect a built-in type's interface. Here is a quick rundown of the main ideas behind overloading operators:

    * Methods named with double underscores (__X__) are special hooks. Python operator overloading is implemented by providing specially named methods to intercept operations. The Python language defines a fixed and unchangeable mapping from each of these operations to a specially named method.
    * Such methods are called automatically when instances appear in built-in operations. For instance, if an instance object inherits an __add__ method, that method is called whenever the object appears in a + expression. The method's return value becomes the result of the corresponding expression.
    * Classes may override most built-in type operations. There are dozens of special operator overloading method names for intercepting and implementing nearly every operation available for built-in types. This includes expressions, but also basic operations like printing and object creation.
    * There are no defaults for operator overloading methods, and none are required. If a class does not define or inherit an operator overloading method, it just means that the corresponding operation is not supported for the class' instances. If there is no __add__, for example, + expressions raise exceptions.
    * Operators allow classes to integrate with Python's object model. By overloading type operations, user-defined objects implemented with classes act just like built-ins, and so provide consistency as well as compatibility with expected interfaces.

Operator overloading is an optional feature; it's used primarily by people developing tools for other Python programmers, not by application developers. And, candidly, you probably shouldn't try to use it just because it seems "cool." Unless a class needs to mimic built-in type interfaces, it should usually stick to simpler named methods. Why would an employee database application support expressions like * and +, for example? Named methods like giveRaise and promote would usually make more sense.

Because of this, we won't go into details on every operator overloading method available in Python in this book. Still, there is one operator overloading method you are likely to see in almost every realistic Python class: the __init__ method, which is known as the constructor method, and is used to initialize objects' state. You should pay special attention to this method, because __init__, along with the self argument, turns out to be one of the keys to understanding OOP code in Python.

				30.5.2.3.1 A Third Example

On to another example. This time, we'll define a subclass of SecondClass that implements three specially named attributes that Python will call automatically: __init__ is called when a new instance object is being constructed (self is the new ThirdClass object), and __add__ and __mul__ are called when a ThirdClass instance appears in a + or * expression, respectively. Here's our new subclass:


>>> class ThirdClass(SecondClass):  # Is a SecondClass
...     def __init__(self, value):  # On "ThirdClass(value)"
...         self.data = value 
...     def __add__(self, other):  # On "self + other"
...         return ThirdClass(self.data + other) 
...     def __mul__(self, other): 
...         self.data = self.data * other  # On "self * other"
...
>>> a = ThirdClass("abc")  # New __init__ called
>>> a.display(  )  # Inherited method
Current value = "abc"

>>> b = a + 'xyz'  # New __add__: makes a new instance
>>> b.display(  ) 
Current value = "abcxyz"

>>> a * 3  # New __mul__: changes instance in-place
>>> a.display(  ) 
Current value = "abcabcabc"


ThirdClass "is a" SecondClass, so its instances inherit the display method from SecondClass. But, ThirdClass generation calls pass an argument now (e.g., "abc"); it's passed to the value argument in the __init__ constructor and assigned to self.data there. Further, ThirdClass objects can show up in + and * expressions; Python passes the instance object on the left to the self argument, and the value on the right to other, as illustrated in Figure 23-3.

Figure 23-3. In operator overloading, expression operators and other built-on operations performed on class instances are mapped back to specially named methods in the class. These special methods are optional, and may be inherited as usual. Here, a "*" expression triggers the "_mul_" method.


Specially named methods such as __init__ and __add__ are inherited by subclasses and instances, just like any other names assigned in a class. If the methods are not coded in a class, Python looks for such names in all its superclasses, as usual. Operator overloading method names are also not built-in or reserved words; they are just attributes that Python looks for when objects appear in various contexts. Python usually calls them automatically, but they may occasionally be called by your code as well (more on this later; the __init__ method, for example, is often called manually to trigger superclass constructors).

Notice that the __add__ method makes and returns a new instance object of its class (by calling ThirdClass with the result value), but __mul__ changes the current instance object in-place (by reassigning the self attribute). This is different from the behavior of built-in types such as numbers and strings, which always make new objects for the * operator. Because operator overloading is really just an expression-to-method dispatch mechanism, you can interpret operators any way you like in your own class objects.[63]

    [63] * But you probably shouldn't. Common practice dictates that overloaded operators should work the same way the built-in operator implementations do. In this case, that means our __mul__ method should also return a new object as its result, rather than changing the instance (self) in place; for in-place changes, a mul method call may be better style than a * overload here (e.g., a.mul(3) instead of a * 3).

				30.5.2.3.2 Why Use Operator Overloading?

As a class designer, you can choose to use operator overloading or not. Your choice simply depends on how much you want your object to look and feel like a built-in type. As mentioned earlier, if you omit an operator overloading method, and do not inherit it from a superclass, the corresponding operation will not be supported for your instances; if it's attempted, an exception will be thrown (or a standard default will be used).

Frankly, many operator overloading methods tend to be used only when implementing objects that are mathematical in nature; a vector or matrix class may overload the addition operator, for example, but an employee class likely would not. For simpler classes, you might not use overloading at all, and would rely instead on explicit method calls to implement your objects' behavior.

On the other hand, you might decide to use operator overloading if you need to pass a user-defined object to a function that was coded to expect the operators available on a built-in type like a list or a dictionary. Implementing the same operator set in your class will ensure that your objects support the same expected object interface, and so are compatible with the function.

One overloading method seems to show up in almost every realistic class: the __init__ constructor method. Because it allows classes to fill out the attributes in their newly created instances immediately, the constructor is useful for almost every kind of class you might code. In fact, even though instance attributes are not declared in Python, you can usually find out which attributes an instance will have by inspecting its class' __init__ method code. We won't go into too much detail on this advanced feature in this book, but we'll see some additional inheritance and operator overloading techniques in action in Chapter 24.

			30.5.2.4 The World's Simplest Python Class
We've begun studying class statement syntax in detail in this chapter, but I'd again like to remind you that the basic inheritance model that classes produce is very simple—all it really involves is searching for attributes in trees of linked objects. In fact, we can create a class with nothing in it at all. The following statement makes a class with no attributes attached (an empty namespace object):


>>> class rec: pass  # Empty namespace object


We need the no-operation pass statement (discussed in Chapter 13) here because we don't have any methods to code. After we make the class by running this statement interactively, we can start attaching attributes to the class by assigning names to it completely outside of the original class statement:


>>> rec.name = 'Bob'  # Just objects with attributes
>>> rec.age  = 40 


And, after we've created these attributes by assignment, we can fetch them with the usual syntax. When used this way, a class is roughly similar to a "struct" in C, or a "record" in Pascal—an object with field names attached to it (we can do similar work with dictionary keys, but it requires extra characters):


>>> print rec.name  # Like a C struct or a record
Bob


Notice that this works even though there are no instances of the class yet; classes are objects in their own right, even without instances. In fact, they are just self-contained namespaces, so as long as we have a reference to a class, we can set or change its attributes anytime we wish. Watch what happens when we do create two instances, though:


>>> x = rec(  )  # Instances inherit class names
>>> y = rec(  ) 


These instances begin their lives as completely empty namespace objects. Because they remember the class they were made from, though, they will obtain the attributes we attached to the class by inheritance:


>>> x.name, y.name  # Name is stored on the class only here
('Bob', 'Bob')


Really, these instances have no attributes of their own; they simply fetch the name attribute from the class object where it is stored. If we do assign an attribute to an instance, though, it creates (or changes) the attribute in that object, and no other—attribute references kick off inheritance searches, but attribute assignments affect only the objects in which the attributes are assigned. Here, x gets its own name, but y still inherits the name attached to the class above it:


>>> x.name = 'Sue'  # But assignment changes x only
>>> rec.name, x.name, y.name 
('Bob', 'Sue', 'Bob')


In fact, as we'll explore in more detail in the next chapter, the attributes of a namespace object are usually implemented as dictionaries, and class inheritance trees are (generally speaking) just dictionaries with links to other dictionaries. If you know where to look, you can see this explicitly. The __dict__ attribute is the namespace dictionary for most class-based objects:


>>> rec.__dict__.keys(  )
['age', '__module__', '__doc__', 'name']

>>> x.__dict__.keys(  )
['name']

>>> y.__dict__.keys(  )
[]


Here, the class' dictionary shows the name and age attributes we assigned to it, x has its own name, and y is still empty. Each instance has a link to its class for inheritance, though—it's called __class__, if you want to inspect it:


>>> x.__class__
<class __main__.rec at 0x00BAFF60>


Classes also have a __bases__ attribute, which is a tuple of their superclasses; these two attributes are how class trees are literally represented in memory by Python.

The main point to take away from this look under the hood is that Python's class model is extremely dynamic. Classes and instances are just namespace objects, with attributes created on the fly by assignment. Those assignments usually happen within the class statements you code, but they can occur anywhere you have a reference to one of the objects in the tree.

Even methods, normally created by a def nested in a class, can be created completely independent of any class object. The following, for example, defines a simple function outside of any class that takes one argument:


>>> def upperName(self): 
...     return self.name.upper(  )  # Still needs a self


There is nothing about a class here yet—it's a simple function, and it can be called as such at this point, provided we pass in an object with a name attribute (the name self does not make this special in any way). If we assign this simple function to an attribute of our class, though, it becomes a method, callable through any instance (as well as through the class name itself, as long as we pass in an instance manually):[64]

    [64] * In fact, this is one of the reasons the self argument must always be explicit in Python methods—because methods can be created as simple functions independent of a class, they need to make the implied instance argument explicit. Python cannot otherwise guess that a simple function might eventually become a class method. The main reason for the explicit self argument, though, is to make the meanings of names more obvious: names not referenced through self are simple variables, while names referenced through self are obviously instance attributes.


>>> rec.method = upperName 

>>> x.method(  )  # Run  method to process x
'SUE'

>>> y.method(  )  # Same, but  pass y to self
'BOB'

>>> rec.method(x)  # Can call through instance or class
'SUE'


Normally, classes are filled out by class statements, and instance attributes are created by assignments to self attributes in method functions. The point again, though, is that they don't have to be; OOP in Python really is mostly about looking up attributes in linked namespace objects.

			30.5.2.5 Chapter Summary
This chapter introduced the basics of coding classes in Python. We studied the syntax of the class statement, and saw how to use it to build up a class inheritance tree. We also studied how Python automatically fills in the first argument in method functions, how attributes are attached to objects in a class tree by simple assignment, and how specially named operator overloading methods intercept and implement built-in operations for our instances (e.g., expressions and printing).

In the next chapter, we'll continue our look at class coding, taking a second pass over the model to fill in some of the details that were omitted here to keep things simple. We'll also start to explore some larger and more realistic classes. First, though, let's work through a quiz to review the basics we've covered so far.

BRAIN BUILDER

Chapter Quiz

Q:	How are classes related to modules?
Q:	How are instances and classes created?
Q:	Where and how are class attributes created?
Q:	Where and how are instance attributes created?
Q:	What does self mean in a Python class?
Q:	How is operator overloading coded in a Python class?
Q:	When might you want to support operator overloading in your classes?
Q:	Which operator overloading method is most commonly used?
Q:	What are the two most important concepts in Python OOP code?


Quiz Answers

A:	Classes are always nested inside a module; they are attributes of a module object. Classes and modules are both namespaces, but classes correspond to statements (not entire files), and support the OOP notions of multiple instances, inheritance, and operator overloading. In a sense, a module is like a single-instance class without inheritance that corresponds to an entire file of code.
A:	Classes are made by running class statements; instances are created by calling a class as though it were a function.
A:	Class attributes are created by assigning attributes to a class object. They are normally generated by top-level assignments nested in a class statement—each name assigned in the class statement block becomes an attribute of the class object (technically, the class statement scope morphs into the class object's attribute namespace). Class attributes can also be created, though, by assigning attributes to the class anywhere a reference to the class object exists—i.e., even outside the class statement.
A:	Instance attributes are created by assigning attributes to an instance object. They are normally created within class method functions inside the class statement by assigning attributes to the self argument (which is always the implied instance). Again, though, they may be created by assignment anywhere a reference to the instance appears, even outside the class statement. Normally, all instance attributes are initialized in the __init__ constructor method; that way, later method calls can assume the attributes already exist.
A:	self is the name commonly given to the first (leftmost) argument in a class method function; Python automatically fills it in with the instance object that is the implied subject of the method call. This argument need not be called self; its position is what is significant. (Ex-C++ or Java programmers might prefer to call it this because in those languages that name reflects the same idea; in Python, though, this argument must always be explicit.)
A:	Operator overloading is coded in a Python class with specially named methods; they all begin and end with double underscores to make them unique. These are not built-in or reserved names; Python just runs them automatically when an instance appears in the corresponding operation. Python defines the mappings from operations to special method names.
A:	Operator overloading is useful to implement objects that resemble built-in types (e.g., sequences or numeric objects such as matrixes), and to mimic the built-in type interface expected by a piece of code. Mimicking built-in type interfaces enables you to pass in class instances that also have state information—i.e., attributes that remember data between operation calls. You shouldn't use operator overloading when a simple named method will suffice, though.
A:	The __init__ constructor method is the most commonly used; almost every class uses this method to set initial values for instance attributes and perform other startup tasks.
A:	The special self argument in method functions and the __init__ constructor method are the two cornerstones of OOP code in Python.

		30.5.3. Class Coding Details
If you did not understand all of Chapter 23, don't worry; now that we've had a quick tour, we're going to dig a bit deeper and study the concepts introduced earlier in further detail. In this chapter, we'll take another look at classes and methods, inheritance, and operator overloading, formalizing and expanding on some of the class coding ideas introduced in Chapter 23. Because the class is our last namespace tool, we'll summarize the concepts of namespaces in Python here as well. This chapter will also present some larger and more realistic classes than those we have seen so far, including a final example that ties together much of what we've learned about OOP.
			30.5.3.1 The class Statement
Although the Python class statement may seem similar to tools in other OOP languages on the surface, on closer inspection, it is quite different from what some programmers are used to. For example, as in C++, the class statement is Python's main OOP tool, but unlike in C++, Python's class is not a declaration. Like a def, a class statement is an object builder, and an implicit assignment—when run, it generates a class object, and stores a reference to it in the name used in the header. Also, like a def, a class statement is true executable code—your class doesn't exist until Python reaches and runs the class statement that defines it (typically while importing the module it is coded in, but not before).

				30.5.3.1.1 General Form

class is a compound statement, with a body of indented statements typically appearing under the header. In the header, superclasses are listed in parentheses after the class name, separated by commas. Listing more than one superclass leads to multiple inheritance (which we'll discuss further in the next chapter). Here is the statement's general form:


class <name>(superclass,...):         # Assign to name
    data = value                            # Shared class data
    def method(self,...):                    # Methods
        self.member = value                    # Per-instance data


Within the class statement, any assignments generate class attributes, and specially named methods overload operators; for instance, a function called __init__ is called at instance object construction time, if defined.

				30.5.3.1.2 Example

As we've seen, classes are mostly just namespaces—that is, tools for defining names (i.e., attributes) that export data and logic to clients. So, how do you get from the class statement to a namespace?

Here's how. Just like in a module file, the statements nested in a class statement body create its attributes. When Python executes a class statement (not a call to a class), it runs all the statements in its body, from top to bottom. Assignments that happen during this process create names in the class' local scope, which become attributes in the associated class object. Because of this, classes resemble both modules and functions:

    * Like functions, class statements are local scopes where names created by nested assignments live.
    * Like names in a module, names assigned in a class statement become attributes in a class object.

The main distinction for classes is that their namespaces are also the basis of inheritance in Python; reference attributes that are not found in a class or instance object are fetched from other classes.

Because class is a compound statement, any sort of statement can be nested inside its body—print, =, if, def, and so on. All the statements inside the class statement run when the class statement itself runs (not when the class is later called to make an instance). Assigning names inside the class statement makes class attributes, and nested defs make class methods, but other assignments make attributes, too.

For example, assignments of simple nonfunction objects to class attributes produce data attributes, shared by all instances:


>>> class SharedData: 
...     spam = 42  # Generates a class data attribute
...
>>> x = SharedData(  )  # Make two instances
>>> y = SharedData(  ) 
>>> x.spam, y.spam  # They inherit and share spam
(42, 42)


Here, because the name spam is assigned at the top level of a class statement, it is attached to the class, and so will be shared by all instances. We can change it by going through the class name, and refer to it through either instances or the class.[65]

    [65] * If you've used C++ you may recognize this as similar to the notion of C++'s "static" data members—members that are stored in the class, independent of instances. In Python, it's nothing special: all class attributes are just names assigned in the class statement, whether they happen to reference functions (C++'s "methods") or something else (C++'s "members").


>>> SharedData.spam = 99
>>> x.spam, y.spam, SharedData.spam
(99, 99, 99)


Such class attributes can be used to manage information that spans all the instances—a counter of the number of instances generated, for example (we'll expand on this idea in Chapter 26). Now, watch what happens if we assign the name spam through an instance instead of the class:


>>> x.spam = 88
>>> x.spam, y.spam, SharedData.spam
(88, 99, 99)


Assignments to instance attributes create or change the names in the instance, rather than in the shared class. More generally, inheritance searches occur only on attribute references, not on assignment: assigning to an object's attribute always changes that object, and no other.[66] For example, y.spam is looked up in the class by inheritance, but the assignment to x.spam attaches a name to x itself.

    [66] † Unless the class has redefined the attribute assignment operation to do something unique with the __setattr__ operator overloading method.

Here's a more comprehensive example of this behavior that stores the same name in two places. Suppose we run the following class:


class MixedNames:                            # Define class
    data = 'spam'                            # Assign class attr
    def __init__(self, value):             # Assign method name
        self.data = value                    # Assign instance attr
    def display(self):
        print self.data, MixedNames.data     # Instance attr, class attr


This class contains two defs, which bind class attributes to method functions. It also contains an = assignment statement; because this assignment assigns the name data inside the class, it lives in the class' local scope, and becomes an attribute of the class object. Like all class attributes, this data is inherited, and shared by all instances of the class that don't have data attributes of their own.

When we make instances of this class, the name data is attached to those instances by the assignment to self.data in the constructor method:
Code View: Scroll / Show All


>>> x = MixedNames(1)  # Make two instance objects
>>> y = MixedNames(2)  # Each has its own data
>>> x.display(  ); y.display(  )  # self.data differs, MixedNames.data is the same
1 spam
2 spam

					  


The net result is that data lives in two places: in the instance objects (created by the self.data assignment in __init__), and in the class from which they inherit names (created by the data assignment in the class). The class' display method prints both versions, by first qualifying the self instance, and then the class.

By using these techniques to store attributes in different objects, we determine their scope of visibility. When attached to classes, names are shared; in instances, names record per-instance data, not shared behavior or data. Although inheritance searches look up names for us, we can always get to an attribute anywhere in a tree by accessing the desired object directly.

In the preceding example, for instance, specifying x.data or self.data will return an instance name, which normally hides the same name in the class; however, MixedNames.data grabs the class name explicitly. We'll see various roles for such coding patterns later; the next section describes one of the most common.

			30.5.3.2 Methods
Because you already know about functions, you also know about methods in classes. Methods are just function objects created by def statements nested in a class statement's body. From an abstract perspective, methods provide behavior for instance objects to inherit. From a programming perspective, methods work in exactly the same way as simple functions, with one crucial exception: a method's first argument always receives the instance object that is the implied subject of the method call.

In other words, Python automatically maps instance method calls to class method functions as follows. Method calls made through an instance, like this:


instance.method(args...)


are automatically translated to class method function calls of this form:


class.method(instance, args...)


where the class is determined by locating the method name using Python's inheritance search procedure. In fact, both call forms are valid in Python.

Besides the normal inheritance of method attribute names, the special first argument is the only real magic behind method calls. In a class method, the first argument is usually called self by convention (technically, only its position is significant, not its name). This argument provides methods with a hook back to the instance that is the subject of the call—because classes generate many instance objects, they need to use this argument to manage data that varies per instance.

C++ programmers may recognize Python's self argument as being similar to C++'s this pointer. In Python, though, self is always explicit in your code: methods must always go through self to fetch or change attributes of the instance being processed by the current method call. This explicit nature of self is by design—the presence of this name makes it obvious that you are using instance attribute names in your script, not names in the local or global scope.

				30.5.3.2.1 Example

To clarify these concepts, let's turn to an example. Suppose we define the following class:


class NextClass:                            # Define class
    def printer(self, text):                # Define method
        self.message = text                 # Change instance
        print self.message                  # Access instance


The name printer references a function object; because it's assigned in the class statement's scope, it becomes a class object attribute, and is inherited by every instance made from the class. Normally, because methods like printer are designed to process instances, we call them through instances:


>>> x = NextClass(  )  # Make instance

>>> x.printer('instance call')  # Call its method
instance call

>>> x.message  # Instance changed
'instance call'


When we call it by qualifying an instance like this, printer is first located by inheritance, and then its self argument is automatically assigned the instance object (x); the text argument gets the string passed at the call ('instance call'). Notice that because Python automatically passes the first argument to self for us, we only actually have to pass in one argument. Inside printer, the name self is used to access or set per-instance data because it refers back to the instance currently being processed.

Methods may be called in one of two ways—through an instance, or through the class itself. For example, we can also call printer by going through the class name, provided we pass an instance to the self argument explicitly:


>>> NextClass.printer(x, 'class call')  # Direct class call
class call

>>> x.message  # Instance changed again
'class call'


Calls routed through the instance and the class have the exact same effect, as long as we pass the same instance object ourselves in the class form. By default, in fact, you get an error message if you try to call a method without any instance:
Code View: Scroll / Show All


>>> NextClass.printer('bad call')
TypeError: unbound method printer(  ) must be called with NextClass instance...

					  


				30.5.3.2.2 Calling Superclass Constructors

Methods are normally called through instances. Calls to methods through the class, though, do show up in a variety of special roles. One common scenario involves the constructor method. The __init__ method, like all attributes, is looked up by inheritance. This means that at construction time, Python locates and calls just one __init__. If subclass constructors need to guarantee that superclass construction-time logic runs, too, they generally must call the superclass's __init__ method explicitly through the class:
Code View: Scroll / Show All


class Super:
    def __init__(self, x):
        ...default code...

class Sub(Super):
    def __init__(self, x, y):
        Super.__init__(self, x)                                        # Run
superclass __init__
        ...custom code...                   # Do my init actions

I = Sub(1, 2)

					  


This is one of the few contexts in which your code is likely to call an operator overloading method directly. Naturally, you should only call the superclass constructor this way if you really want it to run—without the call, the subclass replaces it completely. For a more realistic illustration of this technique in action, stay tuned for the final example in this chapter.[67]

    [67] * On a somewhat related note, you can also code multiple __init__ methods within the same class, but only the last definition will be used; see Chapter 25 for more details.

				30.5.3.2.3 Other Method Call Possibilities

This pattern of calling methods through a class is the general basis of extending (instead of completely replacing) inherited method behavior. In Chapter 26, we'll also meet a new option added in Python 2.2, static methods, that allow you to code methods that do not expect instance objects in their first arguments. Such methods can act like simple instanceless functions, with names that are local to the classes in which they are coded. This is an advanced and optional extension, though; normally, you must always pass an instance to a method, whether it is called through an instance or a class.


			30.5.3.3 Inheritance
The whole point of a namespace tool like the class statement is to support name inheritance. This section expands on some of the mechanisms and roles of attribute inheritance in Python.

In Python, inheritance happens when an object is qualified, and it involves searching an attribute definition tree (one or more namespaces). Every time you use an expression of the form object.attr (where object is an instance or class object), Python searches the namespace tree from bottom to top, beginning with object, looking for the first attr it can find. This includes references to self attributes in your methods. Because lower definitions in the tree override higher ones, inheritance forms the basis of specialization.

				30.5.3.3.1 Attribute Tree Construction

Figure 24-1 summarizes the way namespace trees are constructed and populated with names. Generally:

    * Instance attributes are generated by assignments to self attributes in methods.
    * Class attributes are created by statements (assignments) in class statements.
    * Superclass links are made by listing classes in parentheses in a class statement header.

Figure 24-1. Program code creates a tree of objects in memory to be searched by attribute inheritance. Calling a class creates a new instance that remembers its class, running a class statement creates a new class, and superclasses are listed in parentheses in the class statement header. Each attribute reference triggers a new bottom-up tree search—even self attributes within a class' methods


The net result is a tree of attribute namespaces that leads from an instance, to the class it was generated from, to all the superclasses listed in the class header. Python searches upward in this tree, from instances to superclasses, each time you use qualification to fetch an attribute name from an instance object.[68]

    [68] * This description isn't 100 percent complete because we can also create instance and class attributes by assigning to objects outside class statements—but that's a much less common and sometimes more error-prone approach (changes aren't isolated to class statements). In Python, all attributes are always accessible by default; we'll talk more about name privacy in Chapter 26

				30.5.3.3.2 Specializing Inherited Methods

The tree-searching model of inheritance just described turns out to be a great way to specialize systems. Because inheritance finds names in subclasses before it checks superclasses, subclasses can replace default behavior by redefining their superclasses' attributes. In fact, you can build entire systems as hierarchies of classes, which are extended by adding new external subclasses rather than changing existing logic in-place.

The idea of redefining inherited names leads to a variety of specialization techniques. For instance, subclasses may replace inherited attributes completely, provide attributes that a superclass expects to find, and extend superclass methods by calling back to the superclass from an overridden method. We've already seen replacement in action. Here's an example that shows how extension works:


>>> class Super: 
...     def method(self): 
...         print 'in Super.method' 
...
>>> class Sub(Super): 
...     def method(self):  # Override method
...         print 'starting Sub.method'  # Add actions here
...         Super.method(self)  # Run default action
...         print 'ending Sub.method' 
...


Direct superclass method calls are the crux of the matter here. The Sub class replaces Super's method function with its own specialized version. But, within the replacement, Sub calls back to the version exported by Super to carry out the default behavior. In other words, Sub.method just extends Super.method's behavior, rather than replacing it completely:


>>> x = Super(  )  # Make a Super instance
>>> x.method(  )  # Runs Super.method
in Super.method

>>> x = Sub(  )  # Make a Sub instance
>>> x.method(  )  # Runs Sub.method, which calls Super.method
starting Sub.method
in Super.method
ending Sub.method


This extension coding pattern is also commonly used with constructors; see the earlier section, "Section 24.2," for an example.

				30.5.3.3.3 Class Interface Techniques

Extension is only one way to interface with a superclass. The file shown below, specialize.py, defines multiple classes that illustrate a variety of common techniques:


Super

    Defines a method function and a delegate that expects an action in a subclass.

Inheritor

    Doesn't provide any new names, so it gets everything defined in Super.

Replacer

    Overrides Super's method with a version of its own.

Extender

    Customizes Super's method by overriding and calling back to run the default.

Provider

    Implements the action method expected by Super's delegate method.

Study each of these subclasses to get a feel for the various ways they customize their common superclass. Here's the file:
Code View: Scroll / Show All


class Super:
    def method(self):
        print 'in Super.method'             # Default behavior
    def delegate(self):
        self.action(  )                     # Expected to be defined

class Inheritor(Super):                    # Inherit method verbatim
    pass

class Replacer(Super):                     # Replace method completely
    def method(self):
        print 'in Replacer.method'

class Extender(Super):                     # Extend method behavior
    def method(self):
        print 'starting Extender.method'
        Super.method(self)
        print 'ending Extender.method'

class Provider(Super):                     # Fill in a required method
    def action(self):
        print 'in Provider.action'

if __name__ == '__main__':
    for klass in (Inheritor, Replacer, Extender):
        print '\n' + klass.__name__ + '...'
        klass(  ).method(  )
    print '\nProvider...'
    x = Provider(  )
    x.delegate(  )

					  


A few things are worth pointing out here. First, the self-test code at the end of this example creates instances of three different classes in a for loop. Because classes are objects, you can put them in a tuple, and create instances generically (more on this idea later). Classes also have the special __name__ attribute, like modules; it's preset to a string containing the name in the class header. Here's what happens when we run the file:


% python specialize.py

Inheritor...
in Super.method

Replacer...
in Replacer.method

Extender...
starting Extender.method
in Super.method
ending Extender.method

Provider...
in Provider.action

				30.5.3.3.4 Abstract Superclasses

Notice how the Provider class in the prior example works. When we call the delegate method through a Provider instance, two independent inheritance searches occur:

   a.  On the initial x.delegate call, Python finds the delegate method in Super by searching the Provider instance and above. The instance x is passed into the method's self argument as usual.
   b.  Inside the Super.delegate method, self.action invokes a new, independent inheritance search of self and above. Because self references a Provider instance, the action method is located in the Provider subclass.

This "filling in the blanks" sort of coding structure is typical of OOP frameworks. At least in terms of the delegate method, the superclass in this example is what is sometimes called an abstract superclass—a class that expects parts of its behavior to be provided by its subclasses. If an expected method is not defined in a subclass, Python raises an undefined name exception when the inheritance search fails. Class coders sometimes make such subclass requirements more obvious with assert statements, or by raising the built-in NotImplementedError exception:


class Super:
    def method(self):
        print 'in Super.method'
    def delegate(self):
        self.action(  )
    def action(self):
        assert 0, 'action must be defined!'


We'll meet assert in Chapter 27; in short, if its expression evaluates to false, it raises an exception with an error message. Here, the expression is always false (0) so as to trigger an error message if a method is not redefined, and inheritance locates the version here. Alternatively, some classes simply raise a NotImplemented exception directly in such method stubs. We'll study the raise statement in Chapter 27.

For a somewhat more realistic example of this section's concepts in action, see exercise 8 at the end of Chapter 26, and its solution in "Section B.6" (in Appendix B). Such taxonomies are a traditional way to introduce OOP, but they're a bit removed from most developers' job descriptions.

			30.5.3.4 Operator Overloading

We looked briefly at operator overloading in the prior chapter; here, we'll fill in more details, and look at a few commonly used overloading methods. Here's a review of the key ideas behind overloading:

    * Operator overloading lets classes intercept normal Python operations.
    * Classes can overload all Python expression operators.
    * Classes can also overload operations such as printing, function calls, attribute qualifications, etc.
    * Overloading makes class instances act more like built-in types.
    * Overloading is implemented by providing specially named class methods.

Let's look at a simple example of overloading at work. If certain specially named methods are provided in a class, Python automatically calls them when instances of the class appear in expressions related to the associated operations. For instance, the Number class in the following file, number.py, provides a method to intercept instance construction (__init__), as well as one for catching subtraction expressions (__sub__). Special methods such as these are the hooks that let you tie into built-in operations:


class Number:
    def __init__(self, start):                 # On Number(start)
        self.data = start
    def __sub__(self, other):                  # On instance - other
        return Number(self.data - other)     # Result is a new instance

>>> from number import Number  # Fetch class from module
>>> X = Number(5)  # Number.__init__(X, 5)
>>> Y = X - 2  # Number.__sub__(X, 2)
>>> Y.data  # Y is new Number instance
3


As discussed previously, the __init__ constructor method seen in this code is the most commonly used operator overloading method in Python; it's present in most classes. In this section, we will sample some of the other tools available in this domain, and look at example code that applies them in common use cases.

				30.5.3.4.1 Common Operator Overloading Methods

Just about everything you can do to built-in objects such as integers and lists has a corresponding specially named method for overloading in classes. Table 24-1 lists a few of the most common; there are many more. In fact, many overloading methods come in multiple versions (e.g., __add__, __radd__, and __iadd__ for addition). See other Python books, or the Python language reference manual, for an exhaustive list of the special method names available.

Table 24-1. Common operator overloading methods
Method 	Overloads 	Called for
__init__ 	Constructor 	Object creation: X = Class( )
__del__ 	Destructor 	Object reclamation
__add__ 	Operator + 	X + Y, X += Y
__or__ 	Operator | (bitwise OR) 	X | Y, X |= Y
__repr__,__str__ 	Printing, conversions 	print X, repr(X), str(X)
__call__ 	Function calls 	X( )
__getattr__ 	Qualification 	X.undefined
__setattr__ 	Attribute assignment 	X.any = value
__getitem__ 	Indexing 	X[key], for loops and other iterations if no __iter__
__setitem__ 	Index assignment 	X[key] = value
__len__ 	Length 	len(X), truth tests
__cmp__ 	Comparison 	X == Y, X < Y
__lt__ 	Specific comparison 	X < Y (or else __cmp__)
__eq__ 	Specific comparison 	X == Y (or else __cmp__)
__radd__ 	Right-side operator + 	Noninstance + X
__iadd__ 	In-place (augmented) addition 	X += Y (or else __add__)
__iter__ 	Iteration contexts 	for loops, in tests, list comprehensions, map, others

All overloading methods have names that start and end with two underscores to keep them distinct from other names you define in your classes. The mappings from special method names to expressions or operations are predefined by the Python language (and documented in the standard language manual). For example, the name __add__ always maps to + expressions by Python language definition, regardless of what an __add__ method's code actually does.

All operator overloading methods are optional—if you don't code one, that operation is simply unsupported by your class (and may raise an exception if attempted). Most overloading methods are used only in advanced programs that require objects to behave like built-ins; the __init__ constructor tends to appear in most classes, however. We've already met the __init__ initialization-time constructor method, and a few of the others in Table 24-1. Let's explore some of the additional methods in the table by example.

				30.5.3.4.2 __getitem__ Intercepts Index References

The __getitem__ method intercepts instance-indexing operations. When an instance X appears in an indexing expression like X[i], Python calls the __getitem__ method inherited by the instance (if any), passing X to the first argument, and the index in brackets to the second argument. For instance, the following class returns the square of an index value:


>>> class indexer: 
...     def __getitem__(self, index): 
...         return index ** 2 
...
>>> X = indexer(  ) 
>>> X[2]  # X[i] calls __getitem__(X, i).
4
>>> for i in range(5): 

...     print X[i], 
...
0 1 4 9 16


				30.5.3.4.3 __getitem__ and __iter__ Implement Iteration

Here's a trick that isn't always obvious to beginners, but turns out to be incredibly useful. The for statement works by repeatedly indexing a sequence from zero to higher indexes, until an out-of-bounds exception is detected. Because of that, __getitem__ also turns out to be one way to overload iteration in Python—if this method is defined, for loops call the class' __getitem__ each time through, with successively higher offsets. It's a case of "buy one, get one free"—any built-in or user-defined object that responds to indexing also responds to iteration:


>>> class stepper: 
...     def __getitem__(self, i): 
...         return self.data[i] 
...
>>> X = stepper(  )  # X is a stepper object
>>> X.data = "Spam" 
>>>
>>> X[1]  # Indexing calls __getitem__
'p'
>>> for item in X:  # for loops call __getitem__
...     print item,  # for indexes items 0..N
...
S p a m


In fact, it's really a case of "buy one, get a bunch free." Any class that supports for loops automatically supports all iteration contexts in Python, many of which we've seen in earlier chapters (see Chapter 13 for other iteration contexts). For example, the in membership test, list comprehensions, the map built-in, list and tuple assignments, and type constructors will also call __getitem__ automatically, if it's defined:


>>> 'p' in X  # All call __getitem__ too
True

>>> [c for c in X]  # List comprehension
['S', 'p', 'a', 'm']

>>> map(None, X)  # map calls
['S', 'p', 'a', 'm']

>>> (a, b, c, d) = X  # Sequence assignments
>>> a, c, d 
('S', 'a', 'm')

>>> list(X), tuple(X), ''.join(X) 
(['S', 'p', 'a', 'm'], ('S', 'p', 'a', 'm'), 'Spam')

>>> X 
<__main__.stepper instance at 0x00A8D5D0>


In practice, this technique can be used to create objects that provide a sequence interface and to add logic to built-in sequence type operations; we'll revisit this idea when extending built-in types in Chapter 26.

				30.5.3.4.4 User-Defined Iterators

Today, all iteration contexts in Python will try the __iter__ method first, before trying __getitem__. That is, they prefer the iteration protocol we learned about in Chapter 13 to repeatedly indexing an object; if the object does not support the iteration protocol, indexing is attempted instead.

Technically, iteration contexts work by calling the iter built-in function to try to find an __iter__ method, which is expected to return an iterator object. If it's provided, Python then repeatedly calls this iterator object's next method to produce items until a StopIteration exception is raised. If no such __iter__ method is found, Python falls back on the __getitem__ scheme, and repeatedly indexes by offsets as before, until an IndexError exception is raised.

In the new scheme, classes implement user-defined iterators by simply implementing the iterator protocol introduced in Chapter 13 and Chapter 17 (refer back to those chapters for more background details on iterators). For example, the following file, iters.py, defines a user-defined iterator class that generates squares:
Code View: Scroll / Show All


class Squares:
    def __init__(self, start, stop):   # Save state when created
        self.value = start - 1
        self.stop  = stop
    def __iter__(self):                  # Get iterator object on iter(  )
        return self
    def next(self):                    # Return a square on each iteration
        if self.value == self.stop:
            raise StopIteration
        self.value += 1
        return self.value ** 2

% python 
>>> from iters import Squares 
>>> for i in Squares(1, 5):  # for calls iter(  ), which calls __iter__(  )
...     print i,  # Each iteration calls next(  )
...
1 4 9 16 25

					  


Here, the iterator object is simply the instance self because the next method is part of this class. In more complex scenarios, the iterator object may be defined as a separate class and object with its own state information to support multiple active iterations over the same data (we'll see an example of this in a moment). The end of the iteration is signaled with a Python raise statement (more on raising exceptions in the next part of this book).

An equivalent coding with __getitem__ might be less natural because the for would then iterate through all offsets zero and higher; the offsets passed in would be only indirectly related to the range of values produced (0..N would need to map to start..stop). Because __iter__ objects retain explicitly managed state between next calls, they can be more general than __getitem__.

On the other hand, __iter__-based iterators can sometimes be more complex and less convenient than __getitem__. They are really designed for iteration, not random indexing—in fact, they don't overload the indexing expression at all:


>>> X = Squares(1, 5)
>>> X[1]
AttributeError: Squares instance has no attribute '__getitem__'


The __iter__ scheme is also the implementation for all the other iteration contexts we saw in action for __getitem__ (membership tests, type constructors, sequence assignment, and so on). However, unlike __getitem__, __iter__ is designed for a single traversal, not many. For example, the Squares class is a one-shot iteration; once iterated, it's empty. You need to make a new iterator object for each new iteration:


>>> X = Squares(1, 5) 
>>> [n for n in X]  # Exhausts items
[1, 4, 9, 16, 25]
>>> [n for n in X]  # Now it's empty
[]
>>> [n for n in Squares(1, 5)]  # Make a new iterator object
[1, 4, 9, 16, 25]
>>> list(Squares(1, 3)) 
[1, 4, 9]


Notice that this example would probably be simpler if coded with generator functions (a topic introduced in Chapter 17 and related to iterators):
Code View: Scroll / Show All


>>> from __future__ import generators  # Needed in Python 2.2, but not later
>>>
>>> def gsquares(start, stop): 
...     for i in range(start, stop+1): 
...         yield i ** 2 
...
>>> for i in gsquares(1, 5): 
...     print i, 
...
1 4 9 16 25

					  


Unlike the class, the function automatically saves its state between iterations. Of course, for this artificial example, you could, in fact, skip both techniques and simply use a for loop, map, or list comprehension to build the list all at once. The best and fastest way to accomplish a task in Python is often also the simplest:


>>> [x ** 2 for x in range(1, 6)]
[1, 4, 9, 16, 25]


However, classes may be better at modeling more complex iterations, especially when they can benefit from state information and inheritance hierarchies. The next section explores one such use case.

					30.5.3.4.4.1 Multiple iterators on one object

Earlier, I mentioned that the iterator object may be defined as a separate class with its own state information to support multiple active iterations over the same data. Consider what happens when we step across a built-in type like a string:


>>> S = 'ace'
>>> for x in S:
...     for y in S:
...         print x + y,
...
aa ac ae ca cc ce ea ec ee


Here, the outer loop grabs an iterator from the string by calling iter, and each nested loop does the same to get an independent iterator. Because each active iterator has its own state information, each loop can maintain its own position in the string, regardless of any other active loops. To achieve the same effect with user-defined iterators, __iter__ simply needs to define a new stateful object for the iterator, instead of returning self.

The following, for instance, defines an iterator class that skips every other item on iterations; because the iterator object is created anew for each iteration, it supports multiple active loops:
Code View: Scroll / Show All


class SkipIterator:
    def __init__(self, wrapped):
        self.wrapped = wrapped                    # Iterator state information
        self.offset  = 0
    def next(self):
        if self.offset >= len(self.wrapped):      # Terminate iterations
            raise StopIteration
        else:
            item = self.wrapped[self.offset]      # else return and skip
            self.offset += 2
            return item

class SkipObject:
    def __init__(self, wrapped):                    # Save item to be used
        self.wrapped = wrapped
    def __iter__(self):
        return SkipIterator(self.wrapped)         # New iterator each time

if __name__ == '__main__':
    alpha = 'abcdef'
    skipper = SkipObject(alpha)                   # Make container object
    I = iter(skipper)                             # Make an iterator on it
    print I.next(), I.next(  ), I.next(  )              # Visit offsets 0, 2, 4

    for x in skipper:               # for calls __iter__ automatically
        for y in skipper:           # Nested fors call __iter__ again each time
            print x + y,            # Each iterator has its own state, offset

					  


When run, this example works like the nested loops with built-in strings—each active loop has its own position in the string because each obtains an independent iterator object that records its own state information:


% python skipper.py
a c e
aa ac ae ca cc ce ea ec ee


By contrast, our earlier Squares example supports just one active iteration, unless we call Squares again in nested loops to obtain new objects. Here, there is just one SkipObject, with multiple iterator objects created from it.

As before, we could achieve similar results with built-in tools—for example, slicing with a third bound to skip items:


>>> S = 'abcdef' 
>>> for x in S[::2]: 
...     for y in S[::2]:  # New objects on each iteration
...         print x + y, 
...
aa ac ae ca cc ce ea ec ee


This isn't quite the same, though, for two reasons. First, each slice expression here will physically store the result list all at once in memory; iterators, on the other hand, produce just one value at a time, which can save substantial space for large result lists. Second, slices produce new objects, so we're not really iterating over the same object in multiple places here. To be closer to the class, we would need to make a single object to step across by slicing ahead of time:


>>> S = 'abcdef' 
>>> S = S[::2] 
>>> S 
'ace'
>>> for x in S: 
...     for y in S:  # Same object, new iterators
...         print x + y, 
...
aa ac ae ca cc ce ea ec ee


This is more similar to our class-based solution, but it still stores the slice result in memory all at once (there is no generator form of slicing today), and it's only equivalent for this particular case of skipping every other item.

Because iterators can do anything a class can do, they are much more general than this example may imply. Whether our applications require such generality, user-defined iterators are a powerful tool—they allow us to make arbitrary objects look and feel like the other sequences and iterables we have met in this book. We could use this technique with a database object, for example, to make iterations to database fetches, with multiple cursors into the same query result.

				30.5.3.4.5 __getattr__ and __setattr__ Catch Attribute References

The __getattr__ method intercepts attribute qualifications. More specifically, it's called with the attribute name as a string whenever you try to qualify an instance with an undefined (nonexistent) attribute name. It is not called if Python can find the attribute using its inheritance tree search procedure. Because of its behavior, __getattr__ is useful as a hook for responding to attribute requests in a generic fashion. For example:


>>> class empty: 
...     def __getattr__(self, attrname): 
...         if attrname == "age": 
...             return 40 
...         else: 
...             raise AttributeError, attrname 
...
>>> X = empty(  ) 
>>> X.age 
40
>>> X.name ...error text omitted... 
AttributeError: name


Here, the empty class and its instance X have no real attributes of their own, so the access to X.age gets routed to the __getattr__ method; self is assigned the instance (X), and attrname is assigned the undefined attribute name string ("age"). The class makes age look like a real attribute by returning a real value as the result of the X.age qualification expression (40). In effect, age becomes a dynamically computed attribute.

For attributes that the class doesn't know how to handle, this __getattr__ raises the built-in AttributeError exception to tell Python that these are bona fide undefined names; asking for X.name triggers the error. You'll see __getattr__ again when we see delegation and properties at work in the next two chapters, and I'll say more about exceptions in Part 7.

A related overloading method, __setattr__, intercepts all attribute assignments. If this method is defined, self.attr = value becomes self.__setattr__('attr', value). This is a bit trickier to use because assigning to any self attributes within __setattr__ calls __setattr__ again, causing an infinite recursion loop (and eventually, a stack overflow exception!). If you want to use this method, be sure that it assigns any instance attributes by indexing the attribute dictionary, discussed in the next section. Use self.__dict__['name'] = x, not self.name = x:


>>> class accesscontrol: 
...     def __setattr__(self, attr, value): 
...         if attr == 'age' :
...             self.__dict__[attr] = value 
...         else: 
...             raise AttributeError, attr + ' not allowed' 
...
>>> X = accesscontrol(  ) 
>>> X.age = 40  # Calls __setattr__
>>> X.age 
40
>>> X.name = 'mel' ...text omitted... 
AttributeError: name not allowed


These two attribute-access overloading methods allow you to control or specialize access to attributes in your objects. They tend to play highly specialized roles, some of which we'll explore later in this book.

				30.5.3.4.6 Emulating Privacy for Instance Attributes

The following code generalizes the previous example, to allow each subclass to have its own list of private names that cannot be assigned to its instances:
Code View: Scroll / Show All


class PrivateExc(Exception): pass                   # More on exceptions later

class Privacy:
    def __setattr__(self, attrname, value):      # On self.attrname = value
        if attrname in self.privates:
            raise PrivateExc(attrname, self)
        else:
            self.__dict__[attrname] = value      # Self.attrname = value loops!
class Test1(Privacy):
    privates = ['age']

class Test2(Privacy):
    privates = ['name', 'pay']
    def __init__(self):
        self.__dict__['name'] = 'Tom'

x = Test1(  )
y = Test2(  )

x.name = 'Bob'
y.name = 'Sue'   # <== fails

y.age  = 30
x.age  = 40      # <== fails

					  


In fact, this is first-cut solution for an implementation of attribute privacy in Python (i.e., disallowing changes to attribute names outside a class). Although Python doesn't support private declarations per se, techniques like this can emulate much of their purpose. This is a partial solution, though; to make it more effective, it must be augmented to allow subclasses to set private attributes too and to use __getattr__ and a wrapper (sometimes called a proxy) class to check for private attribute fetches.

I'll leave the complete solution as a suggested exercise, because even though privacy can be emulated this way, it almost never is in practice. Python programmers are able to write large OOP frameworks and applications without private declarations—an interesting finding about access controls in general that is beyond the scope of our purposes here.

Catching attribute references and assignments is generally a useful technique; it supports delegation, a design technique that allows controller objects to wrap up embedded objects, add new behaviors, and route other operations back to the wrapped objects (more on delegation and wrapper classes in the next chapter).

				30.5.3.4.7 __repr__ and __str__ Return String Representations

The next example exercises the __init__ constructor, and the __add__ overload method we've already seen, but also defines a __repr__ method that returns a string representation for instances. String formatting is used to convert the managed self.data object to a string. If defined, __repr__ (or its sibling, __str__) is called automatically when class instances are printed or converted to strings. These methods allow you to define a better display format for your objects than the default instance display:


>>> class adder: 
...     def __init__(self, value=0): 
...         self.data = value  # Initialize data
...     def __add__(self, other): 
...         self.data += other  # Add other in-place
...
>>> class addrepr(adder):  # Inherit __init__, __add__
...     def __repr__(self):  # Add string representation
...         return 'addrepr(%s)' % self.data  # Convert to string as code
...
>>> x = addrepr(2)  # Runs __init__
>>> x + 1  # Runs __add__
>>> x  # Runs __repr__
addrepr(3)
>>> print x  # Runs __repr__
addrepr(3)
>>> str(x), repr(x)  # Runs __repr__
('addrepr(3)', 'addrepr(3)')


So why two display methods? Roughly, __str__ is tried first for user-friendly displays, such as the print statement, and the str built-in function. The __repr__ method should in principle return a string that could be used as executable code to re-create the object; it's used for interactive prompt echoes, and the repr function. If no __str__ is present, Python falls back on __repr__ (but not vice versa):


>>> class addstr(adder): 
...     def __str__(self):  # __str__ but no __repr__
...         return '[Value: %s]' % self.data  # Convert to nice string
...
>>> x = addstr(3) 
>>> x + 1 
>>> x  # Default repr
<__main__.addstr instance at 0x00B35EF0>
>>> print x  # Runs __str__
[Value: 4]
>>> str(x), repr(x) 
('[Value: 4]', '<__main__.addstr instance at 0x00B35EF0>')


Because of this, __repr__ may be best if you want a single display for all contexts. By defining both methods, though, you can support different displays in different contexts—for example, an end-user display with __str__, and a low-level display for programmers to use during development with __repr__:


>>> class addboth(adder): 
...     def __str__(self): 
...         return '[Value: %s]' % self.data  # User-friendly string
...     def __repr__(self): 
...         return 'addboth(%s)' % self.data  # As-code string
...
>>> x = addboth(4) 
>>> x + 1 
>>> x  # Runs __repr__
addboth(5)
>>> print x  # Runs __str__
[Value: 5]
>>> str(x), repr(x) 
('[Value: 5]', 'addboth(5)')


In practice, __str__ (or its low-level relative, __repr__) seems to be the second most commonly used operator overloading method in Python scripts, behind __init__; any time you can print an object and see a custom display, one of these two tools is probably in use.

				30.5.3.4.8 __radd__ Handles Right-Side Addition

Technically, the __add__ method that appeared in the prior example does not support the use of instance objects on the right side of the + operator. To implement such expressions, and hence support commutative-style operators, code the __radd__ method as well. Python calls __radd__ only when the object on the right side of the + is your class instance, but the object on the left is not an instance of your class. The __add__ method for the object on the left is called instead in all other cases:


>>> class Commuter: 
...     def __init__(self, val): 
...         self.val = val 
...     def __add__(self, other): 
...         print 'add', self.val, other 
...     def __radd__(self, other): 
...         print 'radd', self.val, other 
...
>>> x = Commuter(88) 
>>> y = Commuter(99) 
>>> x + 1  # __add__: instance + noninstance
add 88 1
>>> 1 + y  # __radd__: noninstance + instance
radd 99 1
>>> x + y  # __add__: instance + instance
add 88 <__main__.Commuter instance at 0x0086C3D8>


Notice how the order is reversed in __radd__: self is really on the right of the +, and other is on the left. Every binary operator has a similar right-side overloading method (e.g., __mul__ and __rmul__). Typically, a right-side method like __radd__ just converts if needed, and reruns a + to trigger __add__, where the main logic is coded. Also, note that x and y are instances of the same class here; when instances of different classes appear mixed in an expression, Python prefers the class of the one on the left.

Right-side methods are an advanced topic, and tend to be fairly rarely used in practice; you only code them when you need operators to be commutative, and then only if you need to support operators at all. For instance, a Vector class may use these tools, but an Employee or Button class probably would not.

				30.5.3.4.9 __call__ Intercepts Calls

The __call__ method is called when your instance is called. No, this isn't a circular definition—if defined, Python runs a __call__ method for function call expressions applied to your instances. This allows class instances to emulate the look and feel of things like functions:


>>> class Prod:
...     def __init__(self, value):
...         self.value = value
...     def __call__(self, other):
...         return self.value * other
...
>>> x = Prod(2)
>>> x(3)
6
>>> x(4)
8


In this example, the __call__ may seem a bit gratuitous. A simple method provides similar utility:


>>> class Prod:
...     def __init__(self, value):
...         self.value = value
...     def comp(self, other):
...         return self.value * other
...
>>> x = Prod(3)
>>> x.comp(3)
9
>>> x.comp(4)
12


However, __call__ can become more useful when interfacing with APIs that expect functions—it allows us to code objects that conform to an expected function call interface, but also retain state information. In fact, it's probably the third most commonly used operator overloading method, behind the __init__ constructor, and the __str__ and __repr__ display-format alternatives.

				30.5.3.4.10 Function Interfaces and Callback-Based Code

As an example, the Tkinter GUI toolkit, which we'll meet later in this book, allows you to register functions as event handlers (a.k.a. callbacks); when events occur, Tkinter calls the registered objects. If you want an event handler to retain state between events, you can register either a class' bound method, or an instance that conforms to the expected interface with __call__. In this section's code, both x.comp from the second example, and x from the first, can pass as function-like objects this way.

I'll have more to say about bound methods in the next chapter, but for now, here's a hypothetical example of __call__ applied to the GUI domain. The following class defines an object that supports a function-call interface, but also has state information that remembers the color a button should change to when it is later pressed:
Code View: Scroll / Show All


class Callback:
    def __init__(self, color):               # Function + state information
        self.color = color
    def __call__(self):                      # Support calls with no arguments
        print 'turn', self.color

					  


Now, in the context of a GUI, we can register instances of this class as event handlers for buttons, even though the GUI expects to be able to invoke event handlers as simple functions with no arguments:


cb1 = Callback('blue')                       # 'Remember' blue
cb2 = Callback('green')

B1 = Button(command=cb1)                     # Register handlers
B2 = Button(command=cb2)                     # Register handlers


When the button is later pressed, the instance object is called as a simple function, exactly like in the following calls. Because it retains state as instance attributes, though, it remembers what to do:


cb1(  )                                        # On events: prints 'blue'
cb2(  )                                        # Prints 'green'


In fact, this is probably the best way to retain state information in the Python language—better than the techniques discussed earlier for functions (global variables, enclosing-function scope references, and default mutable arguments). With OOP, the state remembered is made explicit with attribute assignments.

Before we move on, there are two other ways that Python programmers sometimes tie information to a callback function like this. One option is to use default arguments in lambda functions:


cb3 = (lambda color='red': 'turn ' + color)  # Or: defaults
print cb3(  )


The other is to use bound methods of a class—a kind of object that remembers the self instance and the referenced function, such that it may be called as a simple function without an instance later:
Code View: Scroll / Show All


class Callback:
    def __init__(self, color):                # Class with state information
        self.color = color
    def changeColor(self):                   # A normal named method
        print 'turn', self.color

cb1 = Callback('blue')
cb2 = Callback('yellow')

B1 = Button(command=cb1.changeColor)         # Reference, but don't call
B2 = Button(command=cb2.changeColor)         # Remembers function+self

					  


When this button is later pressed, it's as if the GUI does this, which invokes the changeColor method to process the object's state information:


object = Callback('blue')
cb = object.changeColor                        # Registered event handler
cb(  )                                         # On event prints 'blue'


This technique is simpler, but less general than overloading calls with __call__; again, watch for more about bound methods in the next chapter.

You'll also see another __call__ example in Chapter 26, where we will use it to implement something known as a function decorator—a callable object that adds a layer of logic on top of an embedded function. Because __call__ allows us to attach state information to a callable object, it's a natural implementation technique for a function that must remember and call another function.

				30.5.3.4.11 __del__ Is a Destructor

The __init__ constructor is called whenever an instance is generated. Its counterpart, the destructor method __del__, is run automatically when an instance's space is being reclaimed (i.e., at "garbage collection" time):


>>> class Life:
...     def __init__(self, name='unknown'):
...         print 'Hello', name
...         self.name = name
...     def __del__(self):
...         print 'Goodbye', self.name
...
>>> brian = Life('Brian')
Hello Brian
>>> brian = 'loretta'
Goodbye Brian


Here, when brian is assigned a string, we lose the last reference to the Life instance, and so trigger its destructor method. This works, and it may be useful for implementing some cleanup activities (such as terminating server connections). However, destructors are not as commonly used in Python as in some OOP languages, for a number of reasons.

For one thing, because Python automatically reclaims all space held by an instance when the instance is reclaimed, destructors are not necessary for space management.[69] For another, because you cannot always easily predict when an instance will be reclaimed, it's often better to code termination activities in an explicitly called method (or try/finally statement, described in the next part of the book); in some cases, there may be lingering references to your objects in system tables that prevent destructors from running.

    [69] * In the current C implementation of Python, you also don't need to close file objects held by the instance in destructors because they are automatically closed when reclaimed. However, as mentioned in Chapter 9, it's better to explicitly call file close methods because auto-close-on-reclaim is a feature of the implementation, not of the language itself (this behavior can vary under Jython).

That's as many overloading examples as we have space for here. Most of the other operator overloading methods work similarly to the ones we've explored, and all are just hooks for intercepting built-in type operations; some overloading methods, for example, have unique argument lists or return values. You'll see a few others in action later in the book, but for complete coverage, I'll defer to other documentation sources.

			30.5.3.5 Namespaces: The Whole Story
Now that we've examined class and instance objects, the Python namespace story is complete. For reference, I'll quickly summarize all the rules used to resolve names here. The first things you need to remember are that qualified and unqualified names are treated differently, and that some scopes serve to initialize object namespaces:

    * Unqualified names (e.g., X) deal with scopes.
    * Qualified attribute names (e.g., object.X) use object namespaces.
    * Some scopes initialize object namespaces (for modules and classes).

				30.5.3.5.1 Simple Names: Global Unless Assigned

Unqualified simple names follow the LEGB lexical scoping rule outlined for functions in Chapter 16:


Assignment (X = value)

    Makes names local: creates or changes the name X in the current local scope, unless declared global.

Reference (X)

    Looks for the name X in the current local scope, then any and all enclosing functions, then the current global scope, then the built-in scope.

				30.5.3.5.2 Attribute Names: Object Namespaces

Qualified attribute names refer to attributes of specific objects, and obey the rules for modules and classes. For class and instance objects, the reference rules are augmented to include the inheritance search procedure:


Assignment (object.X = value)

    Creates or alters the attribute name X in the namespace of the object being qualified, and none other. Inheritance-tree climbing happens only on attribute reference, not on attribute assignment.

Reference (object.X)

    For class-based objects, searches for the attribute name X in object, then in all accessible classes above it, using the inheritance search procedure. For nonclass objects such as modules, fetches X from object directly.

				30.5.3.5.3 The "Zen" of Python Namespaces: Assignments Classify Names

With distinct search procedures for qualified and unqualified names, and multiple lookup layers for both, it can sometimes be difficult to tell where a name will wind up going. In Python, the place where you assign a name is crucial—it fully determines the scope or object in which a name will reside. The file manynames.py illustrates how this principle translates to code, and summarizes the namespace ideas we have seen throughout this book:
Code View: Scroll / Show All


# manynames.py

X = 11                     # Global (module) name/attribute (X, or manynames.X)

def f(  ):
    print X                # Access global X (11)

def g(  ):
    X = 22                 # Local (function) variable (X, hides module X)
    print X

class C:
    X = 33                 # Class attribute (C.X)
    def m(self):
        X = 44             # Local variable in method (X)
        self.X = 55        # Instance attribute (instance.X)

					  


This file assigns the same name, X, five times. Because this name is assigned in five different locations, though, all five Xs in this program are completely different variables. From top to bottom, the assignments to X here generate: a module attribute (11), a local variable in a function (22), a class attribute (33), a local variable in a method (44), and an instance attribute (55). Although all five are named X, the fact that they are all assigned at different places in the source code or to different objects makes all of these unique variables.

You should take the time to study this example carefully because it collects ideas we've been exploring throughout the last few parts of this book. When it makes sense to you, you will have achieved a sort of Python namespace nirvana. Of course, an alternative route to nirvana is to simply run the program and see what happens. Here's the remainder of this source file, which makes an instance, and prints all the Xs that it can fetch:


# manynames.py, continued

if __name__ == '__main__':
    print X                  # 11: module (a.k.a. manynames.X outside file)
    f(  )                    # 11: global
    g(  )                    # 22: local
    print X                  # 11: module name unchanged

    obj = C(  )              # Make instance
    print obj.X              # 33: class name inherited by instance

    obj.m(  )                    # Attach attribute name X to instance now
    print obj.X            # 55: instance
    print C.X              # 33: class (a.k.a. obj.X if no X in instance)

    #print C.m.X           # FAILS: only visible in method
    #print f.X             # FAILS: only visible in function


The outputs that are printed when the file is run are noted in the comments in the code; trace through them to see which variable named X is being accessed each time. Notice in particular that we can go through the class to fetch its attribute (C.X), but we can never fetch local variables in functions, or methods from outside their def statements. Locals are only visible to other code within the def, and, in fact, only live in memory while a call to the function or method is executing.

Some of the names defined by this file are visible outside the file to other modules, but recall that we must always import before we can access names in another file—that is the main point of modules, after all:


# otherfile.py

import manynames

X = 66
print X                    # 66: the global here
print manynames.X          # 11: globals become attributes after imports

manynames.f(  )              # 11: manynames's X, not the one here!
manynames.g(  )              # 22: local in other file's function

print manynames.C.X        # 33: attribute of class in other module
I = manynames.C(  )
print I.X                  # 33: still from class here
I.m(  )
print I.X                  # 55: now from instance!


Notice here how manynames.f( ) prints the X in manynames, not the X assigned in this file—scopes are always determined by the position of assignments in your source code (i.e., lexically), and are never influenced by what imports what, or who imports whom. Also, notice that the instance's own X is not created until we call I.m( )—attributes, like all variables, spring into existence when assigned, and not before. Normally we create instance attributes by assigning them in class __init__ constructor methods, but this isn't the only option.

You generally shouldn't use the same name for every variable in your script, of course! But as this example demonstrates, even if you do, Python's namespaces will work to keep names used in one context from accidentally clashing with those used in another.

				30.5.3.5.4 Namespace Dictionaries

In Chapter 19, we learned that module namespaces are actually implemented as dictionaries, and exposed with the built-in __dict__ attribute. The same holds for class and instance objects: attribute qualification is really a dictionary indexing operation internally, and attribute inheritance is just a matter of searching linked dictionaries. In fact, instance and class objects are mostly just dictionaries with links inside Python. Python exposes these dictionaries, as well as the links between them, for use in advanced roles (e.g., for coding tools).

To help you understand how attributes work internally, let's work through an interactive session that traces the way namespace dictionaries grow when classes are involved. First, let's define a superclass and a subclass with methods that will store data in their instances:


>>> class super:
...     def hello(self):
...         self.data1 = 'spam'
...
>>> class sub(super):
...     def hola(self):
...         self.data2 = 'eggs'
...


When we make an instance of the subclass, the instance starts out with an empty namespace dictionary, but has links back to the class for the inheritance search to follow. In fact, the inheritance tree is explicitly available in special attributes, which you can inspect. Instances have a __class__ attribute that links to their class, and classes have a __bases__ attribute that is a tuple containing links to higher superclasses:


>>> X = sub(  )
>>> X.__dict__
{  }

>>> X.__class__
<class __main__.sub at 0x00A48448>

>>> sub.__bases__
(<class __main__.super at 0x00A3E1C8>,)

>>> super.__bases__
(  )


As classes assign to self attributes, they populate the instance objects—that is, attributes wind up in the instances' attribute namespace dictionaries, not in the classes'. An instance object's namespace records data that can vary from instance to instance, and self is a hook into that namespace:
Code View: Scroll / Show All


>>> Y = sub(  )

>>> X.hello(  )
>>> X.__dict__
{'data1': 'spam'}

>>> X.hola(  )
>>> X.__dict__
{'data1': 'spam', 'data2': 'eggs'}

>>> sub.__dict__
{'__module__': '__main__', '__doc__': None, 'hola': <function hola at
 0x00A47048>}
>>> super.__dict__
{'__module__': '__main__', 'hello': <function hello at 0x00A3C5A8>,
 '__doc__': None}

>>> sub.__dict__.keys(  ), super.__dict__.keys(  )
(['__module__', '__doc__', 'hola'], ['__module__', 'hello', '__doc__'])

>>> Y.__dict__
{  }

					  


Notice the extra underscore names in the class dictionaries; Python sets these automatically. Most are not used in typical programs, but there are tools that use some of them (e.g., __doc__ holds the docstrings discussed in Chapter 14).

Also, observe that Y, a second instance made at the start of this series, still has an empty namespace dictionary at the end, even though X's dictionary has been populated by assignments in methods. Again, each instance has an independent namespace dictionary, which starts out empty, and can record completely different attributes than those recorded by the namespace dictionaries of other instances of the same class.

Because attributes are actually dictionary keys inside Python, there are really two ways to fetch and assign their values—by qualification, or by key indexing:


>>> X.data1, X.__dict__['data1']
('spam', 'spam')

>>> X.data3 = 'toast'
>>> X.__dict__
{'data1': 'spam', 'data3': 'toast', 'data2': 'eggs'}

>>> X.__dict__['data3'] = 'ham'
>>> X.data3
'ham'


This equivalence applies only to attributes actually attached to the instance, though. Because attribute qualification also performs an inheritance search, it can access attributes that namespace dictionary indexing cannot. The inherited attribute X.hello, for instance, cannot be accessed by X.__dict__['hello'].

Finally, here is the built-in dir function we met in Chapter 4 and Chapter 14 at work on class and instance objects. This function works on anything with attributes: dir(object) is similar to an object.__dict__.keys( ) call. Notice, though, that dir sorts its list and includes some system attributes—as of Python 2.2, dir also collects inherited attributes automatically:[70]

    [70] * The contents of attribute dictionaries and dir call results may change over time. For example, because Python now allows built-in types to be subclassed like classes, the contents of dir results for built-in types have expanded to include operator overloading methods. In general, attribute names with leading and trailing double underscores are interpreter-specific. Type subclasses will be discussed further in Chapter 26.


>>> X.__dict__
{'data1': 'spam', 'data3': 'ham', 'data2': 'eggs'}
>>> X.__dict__.keys(  )
['data1', 'data3', 'data2']

>>>> dir(X)
['__doc__', '__module__', 'data1', 'data2', 'data3', 'hello', 'hola']
>>> dir(sub)
['__doc__', '__module__', 'hello', 'hola']
>>> dir(super)
['__doc__', '__module__', 'hello']


Experiment with these special attributes on your own to get a better feel for how namespaces actually do their attribute business. Even if you will never use these in the kinds of programs you write, seeing that they are just normal dictionaries will help demystify the notion of namespaces in general.

				30.5.3.5.5 Namespace Links

The prior section introduced the special __class__ and __bases__ instance and class attributes, without really explaining why you might care about them. In short, these attributes allow you to inspect inheritance hierarchies within your own code. For example, they can be used to display a class tree, as in the following example:


# classtree.py

def classtree(cls, indent):
    print '.'*indent, cls.__name__        # Print class name here
    for supercls in cls.__bases__:        # Recur to all superclasses
        classtree(supercls, indent+3)         # May visit super > once

def instancetree(inst):
    print 'Tree of', inst                     # Show instance
    classtree(inst.__class__, 3)          # Climb to its class

def selftest(  ):
    class A: pass
    class B(A): pass
    class C(A): pass
    class D(B,C): pass
    class E: pass
    class F(D,E): pass
    instancetree(B(  ))
    instancetree(F(  ))

if __name__ == '__main__': selftest(  )


The classtree function in this script is recursive—it prints a class' name using __name__, and then climbs up to the superclasses by calling itself. This allows the function to traverse arbitrarily shaped class trees; the recursion climbs to the top, and stops at root superclasses that have empty __bases__ attributes. Most of this file is self-test code; when run standalone, it builds an empty class tree, makes two instances from it, and prints their class tree structures:


% python classtree.py
Tree of <__main__.B instance at 0x00ACB438>
... B
...... A
Tree of <__main__.F instance at 0x00AC4DA8>
... F
...... D
......... B
............ A
......... C
............ A
...... E


Here, indentation marked by periods is used to denote class tree height. Of course, we could improve on this output format, and perhaps even sketch it in a GUI display.

We can import these functions anywhere we want a quick class tree display:


>>> class Emp: pass
...
>>> class Person(Emp): pass
...
>>> bob = Person(  )
>>> import classtree
>>> classtree.instancetree(bob)
Tree of <__main__.Person instance at 0x00AD34E8>
... Person
...... Emp


Whether you will ever code or use such tools, this example demonstrates one of the many ways that you can make use of special attributes that expose interpreter internals. You'll see another when we code a general-purpose attribute-listing class in the "Section 25.6" section of Chapter 25.

			30.5.3.6 A More Realistic Example

Most of the examples we've looked at so far have been artificial and self-contained to help you focus on the basics. However, we'll close out this chapter with a larger example that pulls together much of what we've studied here. I'm including this mostly as a self-study exercise—try to trace through this example's code to see how method calls are resolved.

In short, the following module, person.py, defines three classes:

    * GenericDisplay is a mix-in class that provides a generic __str__ method; for any class that inherits from it, this method returns a string giving the name of the class from which the instance was created, as well as "name=value" pairs for every attribute in the instance. It uses the __dict__ attribute namespace dictionary to build up the list of "name=value" pairs for each attribute in the class instance and the built-in __name__ of an instance's built-in __class__ to determine the class name. Because the print statement triggers __str__, this class' result is the custom print format displayed for all instances that derive from the class. It's a generic tool.
    * Person records general information about people, and provides two processing methods to use and change instance object state information; it also inherits the custom print format logic from its superclass. A person object has two attributes and two methods managed by this class.
    * Employee is a customization of Person that inherits the last-name extraction and custom print format, but adds a new method for giving a raise, and redefines the birthday operation to customize it (apparently, employees age faster than other people). Notice how the superclass constructor is invoked manually; we need to run the superclass version above in order to fill out the name and age.

As you study this module's code, you'll see that each instance has its own state information. Notice how inheritance is used to mix in and customize behavior, and how operator overloading is used to initialize and print instances:
Code View: Scroll / Show All


# person.py

class GenericDisplay:
    def gatherAttrs(self):
        attrs = '\n'
        for key in self.__dict__:
            attrs += '\t%s=%s\n' % (key, self.__dict__[key])
        return attrs
    def __str__(self):
        return '<%s: %s>' % (self.__class__.__name__, self.gatherAttrs(  ))

class Person(GenericDisplay):
    def __init__(self, name, age):
        self.name = name
        self.age  = age
    def lastName(self):
        return self.name.split(  )[-1]
    def birthDay(self):
        self.age += 1

class Employee(Person):
    def __init__(self, name, age, job=None, pay=0):
        Person.__init__(self, name, age)
        self.job  = job
        self.pay  = pay
    def birthDay(self):
        self.age += 2
    def giveRaise(self, percent):
        self.pay *= (1.0 + percent)

if __name__ == '__main__':
    bob = Person('Bob Smith', 40)
    print bob
    print bob.lastName(  )
    bob.birthDay(  )
    print bob

    sue = Employee('Sue Jones', 44, job='dev', pay=100000)
    print sue
    print sue.lastName(  )
    sue.birthDay(  )
    sue.giveRaise(.10)
    print sue

					  


To test the code, we can import the module and make instances interactively. Here, for example, is the Person class in action. Creating an instance triggers __init__, calling a named method uses or changes instance state information (attributes), and printing an instance invokes the inherited __str__ to print all attributes generically:


>>> from person import Person
>>> ann = Person('Ann Smith', 45)
>>> ann.lastName(  )
'Smith'
>>> ann.birthDay(  )
>>> ann.age
46
>>> print ann
<Person:
    age=46
    name=Ann Smith
>


Finally, here is the output of the file's self-test logic (the code at the bottom, under the __name__ test), which creates a person and an employee, and changes each of them. As usual, this self-test code is run only when the file is run as a top-level script, not when it is being imported as a library module. Notice how employees inherit print formats and last-name extraction, have more state information, have an extra method for getting a raise, and run a customized version of the birthday method (they age by two!):
Code View: Scroll / Show All


% python person.py
<Person:
    age=40
    name=Bob Smith
>
Smith
<Person:
    age=41
    name=Bob Smith
>
<Employee:
    job=dev
    pay=100000
    age=44
    name=Sue Jones
>
Jones
<Employee:
    job=dev
    pay=110000.0
    age=46
    name=Sue Jones
>

					  


Trace through the code in this example to see how this output reflects method calls; it summarizes most of the ideas behind the mechanisms of OOP in Python.

Now that you know about Python classes, you can probably appreciate the fact that the classes used here are not much more than packages of functions, which embed and manage built-in objects attached to instance attributes as state information. When the lastName method splits and indexes, for example, it is simply applying built-in string and list processing operations to an object managed by the class.

Operator overloading and inheritance—the automatic lookup of attributes in the implied class tree—are the main tools OOP adds to the picture. Ultimately, this allows the Employee class at the bottom of the tree to obtain quite a bit of behavior "for free"—which is, at the end of the day, the main idea behind OOP.

			30.5.3.7 Chapter Summary
This chapter took us on a second, more in-depth tour of the OOP mechanisms of the Python language. We learned more about classes and methods, inheritance, and additional operator overloading methods; we also wrapped up the namespace story in Python by extending it to cover its application to classes. Along the way, we looked at some more advanced concepts, such as abstract superclasses, class data attributes, and manual calls to superclass methods and constructors. Finally, we studied a larger example that tied together much of what we've learned about OOP so far.

Now that we've learned all about the mechanics of coding classes in Python, the next chapter turns to common design patterns—some of the ways that classes are commonly used and combined to optimize code reuse. Some of the material in the next chapter is not specific to the Python language, but is important for using classes well. Before you read on, though, be sure to work though the usual chapter quiz to review what we've covered here.

BRAIN BUILDER

Chapter Quiz

Q:	What is an abstract superclass?
Q:	What two operator overloading methods can you use to support iteration in your classes?
Q:	What happens when a simple assignment statement appears at the top level of a class statement?
Q:	Why might a class need to manually call the __init__ method in a superclass?
Q:	How can you augment, instead of completely replacing, an inherited method?
Q:	In this chapter's final example, what methods are run when the sue Employee instance is printed?
Q:	What . . . was the capital of Assyria?


Quiz Answers

A:	An abstract superclass is a class that calls a method, but does not inherit or define it—it expects the method to be filled in by a subclass. This is often used as a way to generalize classes when behavior cannot be predicted until a more specific subclass is coded. OOP frameworks also use this as a way to dispatch to client-defined, customizable operations.
A:	Classes can support iteration by defining (or inheriting) __getitem__ or __iter__. In all iteration contexts, Python tries to use __iter__ (which returns an object that supports the iteration protocol with a next method) first: if no __iter__ is found by inheritance search, Python falls back on the __getitem__ indexing method (which is called repeatedly, with successively higher indexes).
A:	When a simple assignment statement (X = Y) appears at the top level of a class statement, it attaches a data attribute to the class (Class.X). Like all class attributes, this will be shared by all instances; data attributes are not callable method functions, though.
A:	A class must manually call the __init__ method in a superclass if it defines an __init__ constructor of its own, but must still kick off the superclass' construction code. Python itself automatically runs just one constructor—the lowest one in the tree. Superclass constructors are called through the class name, passing in the self instance manually: Superclass.__init__(self, ...).
A:	To augment instead of completely replacing an inherited method, redefine it in a subclass, but call back to the superclass' version of the method manually from the new version of the method in the subclass. That is, pass the self instance to the superclass' version of the method manually: Superclass.method(self, ...).
A:	Printing sue ultimately runs the GenericDisplay.__str__ method and the GenericDisplay.gatherAttrs method it calls. In more detail, to print sue, the print statement converts her to her user-friendly display string by passing her to the built-in str function. In a class, this means look for a __str__ operator overloading method by inheritance search, and run it if it is found. sue's class, Employee, does not have a __str__ method; Person is searched next, and eventually __str__ is found in the GenericDisplay class.
A:	Ashur (or Qalat Sherqat), Calah (or Nimrud), the short-lived Dur Sharrukin (or Khorsabad), and finally Nineveh.

		30.5.4. Designing with Classes
So far in this part of the book, we've concentrated on using Python's OOP tool, the class. But OOP is also about design issues—i.e., how to use classes to model useful objects. This chapter will touch on a few core OOP ideas, and present some additional examples that are more realistic than those shown so far. Many of the design terms mentioned here (delegation, composition, factories, and more) require more explanation than I can provide in this book; if this material sparks your curiosity, I suggest exploring a text on OOP design, or design patterns as a next step.

			30.5.4.1  Python and OOP

Python's implementation of OOP can be summarized by three ideas:


Inheritance

    Inheritance is based on attribute lookup in Python (in X.name expressions).

Polymorphism

    In method, the meaning of method depends on the type (class) of X.

Encapsulation

    Methods and operators implement behavior; data hiding is a convention by default.

By now, you should have a good feel for what inheritance is all about in Python. We've also talked about Python's polymorphism a few times already; it flows from Python's lack of type declarations. Because attributes are always resolved at runtime, objects that implement the same interfaces are interchangeable; clients don't need to know what sorts of objects are implementing the methods they call.

Encapsulation means packaging in Python—that is, hiding implementation details behind an object's interface. It does not mean enforced privacy, as you'll see in Chapter 26. Encapsulation allows the implementation of an object's interface to be changed without impacting the users of that object.

				30.5.4.1.1  Overloading by Call Signatures (or Not)

Some OOP languages also define polymorphism to mean overloading functions based on the type signatures of their arguments. But because there are no type declarations in Python, this concept doesn't really apply; polymorphism in Python is based on object interfaces, not types.

You can try to overload methods by their argument lists, like this:


class C:
    def meth(self, x):
        ...
    def meth(self, x, y, z):
        ...


This code will run, but because the def simply assigns an object to a name in the class' scope, the last definition of the method function is the only one that will be retained (it's just as if you say X = 1, and then X = 2; X will be 2).

Type-based selections can always be coded using the type testing ideas we met in Chapter 4 and Chapter 9, or the argument list tools in Chapter 16:


class C:
    def meth(self, *args):
        if len(args) == 1:
            ...
        elif type(arg[0]) == int:
            ...


You normally shouldn't do this, though—as described in Chapter 15, you should write your code to expect an object interface, not a specific data type. That way, it will be useful for a broader category of types and applications, both now and in the future:


class C:
    def meth(self, x):
        x.operation(  )        # Assume x does the right thing


It's also generally considered better to use distinct method names for distinct operations, rather than relying on call signatures (no matter what language you code in).

			30.5.4.2  Classes As Records

Chapter 8 showed how to use dictionaries to record properties of entities in our programs. Let's explore this in more detail. Here is the example for dictionary-based records used earlier:


>>> rec = {}
>>> rec['name'] = 'mel'
>>> rec['age']  = 40
>>> rec['job']  = 'trainer/writer'
>>>
>>> print rec['name']
mel


This code emulates tools like records and structs in other languages. As we saw in Chapter 23, though, there are also multiple ways to do the same with classes. Perhaps the simplest is this:


>>> class rec: pass
...
>>> rec.name = 'mel'
>>> rec.age  = 40
>>> rec.job  = 'trainer/writer'
>>>
>>> print rec.age
40


This code has substantially less syntax than the dictionary equivalent. It uses an empty class statement to generate an empty namespace object (notice the pass statement—we need a statement syntactically even though there is no logic to code in this case). Once we make the empty class, we fill it out by assigning to class attributes over time.

This works, but a new class statement will be required for each distinct record we will need. Perhaps more typically, we can instead generate instances of an empty class to represent each distinct entity:


>>> class rec: pass
...
>>> pers1 = rec(  )
>>> pers1.name = 'mel'
>>> pers1.job  = 'trainer'
>>> pers1.age   = 40
>>>
>>> pers2 = rec(  )
>>> pers2.name = 'dave'
>>> pers2.job  = 'developer'
>>>
>>> pers1.name, pers2.name
('mel', 'dave')


Here, we make two records from the same class—instances start out life empty, just like classes. We then fill in the records by assigning to attributes. This time, though, there are two separate objects, and hence, two separate name attributes. In fact, instances of the same class don't even have to have the same set of attribute names; in this example, one has a unique age name. Instances really are distinct namespaces: each has a distinct attribute dictionary. Although they are normally filled out consistently by class methods, they are more flexible than you might expect.

Finally, we might instead code a more full-blown class to implement the record:


>>> class Person:
...     def __init__(self, name, job):
...         self.name = name
...         self.job  = job
...     def info(self):
...         return (self.name, self.job)
...
>>> mark = Person('ml', 'trainer')
>>> dave = Person('da', 'developer')
>>>
>>> mark.job, dave.info(  )
('trainer', ('da', 'developer'))


This scheme also makes multiple instances, but the class is not empty this time: we've added logic (methods) to initialize instances at construction time and collect attributes into a tuple. The constructor imposes some consistency on instances here by always setting name and job attributes.

We could further extend this code by adding logic to compute salaries, parse names, and so on (see the end of Chapter 24 for an example that does this). Ultimately, we might link the class into a larger hierarchy to inherit an existing set of methods via the automatic attribute search of classes, or perhaps even store instances of the class in a file with Python object pickling to make them persistent (more on pickling and persistence in the sidebar "Why You Will Care: Classes and Persistence," and again later in the book). In the end, although types like dictionaries are flexible, classes allow us to add behavior to objects in ways that built-in types and simple functions do not directly support.

			30.5.4.3  OOP and Inheritance: "Is-a" Relationships

We've explored the mechanics of inheritance in depth already, but I'd like to show you an example of how it can be used to model real-world relationships. From a programmer's point of view, inheritance is kicked off by attribute qualifications, which trigger searches for names in instances, their classes, and then any superclasses. From a designer's point of view, inheritance is a way to specify set membership: a class defines a set of properties that may be inherited and customized by more specific sets (i.e., subclasses).

To illustrate, let's put that pizza-making robot we talked about at the start of this part of the book to work. Suppose we've decided to explore alternative career paths and open a pizza restaurant. One of the first things we'll need to do is hire employees to serve customers, prepare the food, and so on. Being engineers at heart, we've decided to build a robot to make the pizzas; but being politically and cybernetically correct, we've also decided to make our robot a full-fledged employee with a salary.

Our pizza shop team can be defined by the four classes in the example file, employees.py. The most general class, Employee, provides common behavior such as bumping up salaries (giveRaise) and printing (__repr__). There are two kinds of employees, and so two subclasses of Employee: Chef and Server. Both override the inherited work method to print more specific messages. Finally, our pizza robot is modeled by an even more specific class: PizzaRobot is a kind of Chef, which is a kind of Employee. In OOP terms, we call these relationships "is-a" links: a robot is a chef, which is a(n) employee. Here's the employees.py file:
Code View: Scroll / Show All


class Employee:
    def __init__(self, name, salary=0):
        self.name   = name
        self.salary = salary
    def giveRaise(self, percent):
        self.salary = self.salary + (self.salary * percent)
    def work(self):
        print self.name, "does stuff"
    def __repr__(self):
        return "<Employee: name=%s, salary=%s>" % (self.name, self.salary)

class Chef(Employee):
    def __init__(self, name):
        Employee.__init__(self, name, 50000)
    def work(self):
        print self.name, "makes food"

class Server(Employee):
    def __init__(self, name):
        Employee.__init__(self, name, 40000)
    def work(self):
        print self.name, "interfaces with customer"

class PizzaRobot(Chef):
    def __init__(self, name):
        Chef.__init__(self, name)
    def work(self):
        print self.name, "makes pizza"

if __name__ == "__main__":
    bob = PizzaRobot('bob')       # Make a robot named bob
    print bob                     # Run inherited __repr__
    bob.work(  )                        # Run type-specific action
    bob.giveRaise(0.20)           # Give bob a 20% raise
    print bob; print

    for klass in Employee, Chef, Server, PizzaRobot:
        obj = klass(klass.__name__)
        obj.work(  )

					  


When we run the self-test code included in this module, we create a pizza-making robot named bob, which inherits names from three classes: PizzaRobot, Chef, and Employee. For instance, printing bob runs the Employee.__repr__ method, and giving bob a raise invokes Employee.giveRaise because that's where the inheritance search finds that method:


C:\python\examples> python employees.py
<Employee: name=bob, salary=50000>
bob makes pizza
<Employee: name=bob, salary=60000.0>
Employee does stuff
Chef makes food
Server interfaces with customer
PizzaRobot makes pizza


In a class hierarchy like this, you can usually make instances of any of the classes, not just the ones at the bottom. For instance, the for loop in this module's self-test code creates instances of all four classes; each responds differently when asked to work because the work method is different in each. Really, these classes just simulate real-world objects; work prints a message for the time being, but it could be expanded to do real work later.

			30.5.4.4  OOP and Composition: "Has-a" Relationships

The notion of composition was introduced in Chapter 22. From a programmer's point of view, composition involves embedding other objects in a container object, and activating them to implement container methods. To a designer, composition is another way to represent relationships in a problem domain. But, rather than set membership, composition has to do with components—parts of a whole.

Composition also reflects the relationships between parts, which is usually called a "has-a" relationship. Some OOP design texts refer to composition as aggregation (or distinguish between the two terms by using aggregation to describe a weaker dependency between container and contained); in this text, a "composition" simply refers to a collection of embedded objects. The composite class generally provides an interface all its own, and implements it by directing the embedded objects.

Now that we've implemented our employees, let's put them in the pizza shop and let them get busy. Our pizza shop is a composite object: it has an oven, and it has employees like servers and chefs. When a customer enters and places an order, the components of the shop spring into action—the server takes the order, the chef makes the pizza, and so on. The following example (the file pizzashop.py) simulates all the objects and relationships in this scenario:
Code View: Scroll / Show All


from employees import PizzaRobot, Server

class Customer:
    def __init__(self, name):
        self.name = name
    def order(self, server):
        print self.name, "orders from", server
    def pay(self, server):
        print self.name, "pays for item to", server

class Oven:
    def bake(self):
        print "oven bakes"
class PizzaShop:
    def __init__(self):
        self.server = Server('Pat')         # Embed other objects
        self.chef   = PizzaRobot('Bob')     # A robot named bob
        self.oven   = Oven(  )

    def order(self, name):
        customer = Customer(name)           # Activate other objects
        customer.order(self.server)         # Customer orders from server
        self.chef.work(  )
        self.oven.bake(  )
        customer.pay(self.server)

if __name__ == "__main__":
    scene = PizzaShop(  )                         # Make the composite
    scene.order('Homer')                    # Simulate Homer's order
    print '...'
    scene.order('Shaggy')                   # Simulate Shaggy's order

					  


The PizzaShop class is a container and controller; its constructor makes and embeds instances of the employee classes we wrote in the last section, as well as an Oven class defined here. When this module's self-test code calls the PizzaShop order method, the embedded objects are asked to carry out their actions in turn. Notice that we make a new Customer object for each order, and we pass on the embedded Server object to Customer methods; customers come and go, but the server is part of the pizza shop composite. Also, notice that employees are still involved in an inheritance relationship; composition and inheritance are complementary tools. When we run this module, our pizza shop handles two orders—one from Homer, and then one from Shaggy:


C:\python\examples> python pizzashop.py
Homer orders from <Employee: name=Pat, salary=40000>
Bob makes pizza
oven bakes
Homer pays for item to <Employee: name=Pat, salary=40000>
...
Shaggy orders from <Employee: name=Pat, salary=40000>
Bob makes pizza
oven bakes
Shaggy pays for item to <Employee: name=Pat, salary=40000>


Again, this is mostly just a toy simulation, but the objects and interactions are representative of composites at work. As a rule of thumb, classes can represent just about any objects and relationships you can express in a sentence; just replace nouns with classes, and verbs with methods, and you'll have a first cut at a design.

				30.5.4.4.1  Stream Processors Revisited

For a more realistic composition example, recall the generic data stream processor function we partially coded in the introduction to OOP in Chapter 22:


def processor(reader, converter, writer):
    while 1:
        data = reader.read(  )
        if not data: break
        data = converter(data)
        writer.write(data)


Rather than using a simple function here, we might code this as a class that uses composition to do its work to provide more structure and support inheritance. The following file, streams.py, demonstrates one way to code the class:


class Processor:
    def __init__(self, reader, writer):
        self.reader = reader
        self.writer = writer
    def process(self):
        while 1:
            data = self.reader.readline(  )
            if not data: break
            data = self.converter(data)
            self.writer.write(data)
    def converter(self, data):
        assert 0, 'converter must be defined'


Coded this way, reader and writer objects are embedded within the class instance (composition), and we supply the converter logic in a subclass rather than passing in a converter function (inheritance). The file converters.py shows how:


from streams import Processor

class Uppercase(Processor):
    def converter(self, data):
        return data.upper(  )

if __name__ == '__main__':
    import sys
    Uppercase(open('spam.txt'), sys.stdout).process(  )


Here, the Uppercase class inherits the stream-processing loop logic (and anything else that may be coded in its superclasses). It needs to define only the thing that is unique about it—the data conversion logic. When this file is run, it makes and runs an instance, which reads from the file spam.txt, and writes the uppercase equivalent of that file to the stdout stream:


C:\lp3e> type spam.txt
spam
Spam
SPAM!

C:\lp3e> python converters.py
SPAM
SPAM
SPAM!


To process different sorts of streams, pass in different sorts of objects to the class construction call. Here, we use an output file instead of a stream:


C:\lp3e> python
>>> import converters
>>> prog = converters.Uppercase(open('spam.txt'), open('spamup.txt', 'w'))
>>> prog.process(  )

C:\lp3e> type spamup.txt
SPAM
SPAM
SPAM!


But, as suggested earlier, we could also pass in arbitrary objects wrapped up in classes that define the required input and output method interfaces. Here's a simple example that passes in a writer class that wraps up the text inside HTML tags:


C:\lp3e> python
>>> from converters import Uppercase
>>>
>>> class HTMLize:
...      def write(self, line):
...         print '<PRE>%s</PRE>' % line[:−1]
...
>>> Uppercase(open('spam.txt'), HTMLize(  )).process(  )
<PRE>SPAM</PRE>
<PRE>SPAM</PRE>
<PRE>SPAM!</PRE>


If you trace through this example's control flow, you'll see that we get both uppercase conversion (by inheritance), and HTML formatting (by composition), even though the core processing logic in the original Processor superclass knows nothing about either step. The processing code only cares that writers have a write method, and that a method named convert is defined; it doesn't care what those calls do. Such polymorphism and encapsulation of logic is behind much of the power of classes.

As is, the Processor superclass only provides a file-scanning loop. In more real work, we might extend it to support additional programming tools for its subclasses, and, in the process, turn it into a full-blown framework. Coding such a tool once in a superclass enables you to reuse it in all of your programs. Even in this simple example, because so much is packaged and inherited with classes, all we had to code was the HTML formatting step; the rest was free.

For another example of composition at work, see exercise 9 at the end of Chapter 26 and its solution in Appendix B; it's similar to the pizza shop example. We've focused on inheritance in this book because that is the main tool that the Python language itself provides for OOP. But, in practice, composition is used as much as inheritance as a way to structure classes, especially in larger systems. As we've seen, inheritance and composition are often complementary (and sometimes alternative) techniques. Because composition is a design issue outside the scope of the Python language and this book, though, I'll defer to other resources for more on this topic.

Why You Will Care: Classes and Persistence

I've mentioned pickling a few times in this part of the book because it works especially well with class instances. For example, besides allowing us to simulate real-world interactions, the pizza shop classes developed here could also be used as the basis of a persistent restaurant database. Instances of classes can be stored away on disk in a single step using Python's pickle or shelve modules. The object pickling interface is remarkably easy to use:


import pickle
object = someClass(  )
file   = open(filename, 'wb')     # Create external file
pickle.dump(object, file)         # Save object in file

import pickle
file   = open(filename, 'rb')
object = pickle.load(file)        # Fetch it back later


Pickling converts in-memory objects to serialized byte streams, which may be stored in files, sent across a network, and so on; unpickling converts back from byte streams to identical in-memory objects. Shelves are similar, but they automatically pickle objects to an access-by-key database, which exports a dictionary-like interface:


import shelve
object = someClass(  )
dbase  = shelve.open('filename')
dbase['key'] = object             # Save under key

import shelve
dbase  = shelve.open('filename')
object = dbase['key']             # Fetch it back later


In our example, using classes to model employees means we can get a simple database of employees and shops with little extra work: pickling such instance objects to a file makes them persistent across Python program executions. See the standard library manual and later examples for more on pickling.

			30.5.4.5  OOP and Delegation

Object-oriented programmers often also talk about something called delegation, which usually implies controller objects that embed other objects to which they pass off operation requests. The controllers can take care of administrative activities, such as keeping track of accesses, and so on. In Python, delegation is often implemented with the __getattr__ method hook; because it intercepts accesses to nonexistent attributes, a wrapper class (sometimes called a proxy class) can use __getattr__ to route arbitrary accesses to a wrapped object. The wrapper class retains the interface of the wrapped object, and may add additional operations of its own.

Consider the file trace.py, for instance:


class wrapper:
    def __init__(self, object):
        self.wrapped = object                    # Save object
    def __getattr__(self, attrname):
        print 'Trace:', attrname                 # Trace fetch
        return getattr(self.wrapped, attrname)   # Delegate fetch


Recall from Chapter 24 that __getattr__ gets the attribute name as a string. This code makes use of the getattr built-in function to fetch an attribute from the wrapped object by name string—getattr(X,N) is like X.N, except that N is an expression that evaluates to a string at runtime, not a variable. In fact, getattr(X,N) is similar to X.__dict__[N], but the former also performs an inheritance search, like X.N, while the latter does not (see "Section 24.5.4" in Chapter 24 for more on the __dict__ attribute).

You can use the approach of this module's wrapper class to manage access to any object with attributes—lists, dictionaries, and even classes and instances. Here, the wrapper class simply prints a trace message on each attribute access, and delegates the attribute request to the embedded wrapped object:


>>> from trace import wrapper 
>>> x = wrapper([1,2,3])  # Wrap a list
>>> x.append(4)  # Delegate to list method
Trace: append
>>> x.wrapped  # Print my member
[1, 2, 3, 4]

>>> x = wrapper({"a": 1, "b": 2})  # Wrap a dictionary
>>> x.keys(  )  # Delegate to dictionary method
Trace: keys
['a', 'b']


The net effect is to augment the entire interface of the wrapped object, with additional code in the wrapper class. We can use this to log our method calls, route method calls to extra or custom logic, and so on.

We'll revive the notions of wrapped objects and delegated operations as one way to extend built-in types in Chapter 26. If you are interested in the delegation design pattern, also watch for the discussion of function decorators in Chapter 26—this is a strongly related concept, designed to augment a specific function or method call, rather than the entire interface of an object.

			30.5.4.6  Multiple Inheritance

In a class statement, more than one superclass can be listed in parentheses in the header line. When you do this, you use something called multiple inheritance—the class and its instances inherit names from all listed superclasses.

When searching for an attribute, Python searches superclasses in the class header from left to right until a match is found. Technically, the search proceeds depth-first all the way to the top of the inheritance tree, and then from left to right, as any of the superclasses may have superclasses of their own.

In general, multiple inheritance is good for modeling objects that belong to more than one set. For instance, a person may be an engineer, a writer, a musician, and so on, and inherit properties from all such sets.

Perhaps the most common way multiple inheritance is used is to "mix in" general-purpose methods from superclasses. Such superclasses are usually called mix-in classes—they provide methods you add to application classes by inheritance. For instance, Python's default way to print a class instance object isn't incredibly useful:


>>> class Spam: 
...     def __init__(self):  # No __repr__
...         self.data1 = "food" 
...
>>> X = Spam(  ) 
>>> print X  # Default: class, address
<__main__.Spam instance at 0x00864818>


As seen in the previous section on operator overloading, you can provide a __repr__ method to implement a custom string representation of your own. But, rather than coding a __repr__ in each and every class you wish to print, why not code it once in a general-purpose tool class and inherit it in all your classes?

That's what mix-ins are for. The following file, mytools.py, defines a mix-in class called Lister that overloads the __repr__ method for each class that includes Lister in its header line. It simply scans the instance's attribute dictionary (remember, it's exported in __dict__) to build up a string showing the names and values of all instance attributes. Because classes are objects, Lister's formatting logic can be used for instances of any subclass; it's a generic tool.[71]

    [71] * For an alternative way to do this, see the person.py module example at the end of Chapter 24. It also scans attribute namespace dictionaries, but assumes there are no double-underscore names to be skipped.

Lister uses two special tricks to extract the instance's class name and address. Each instance has a built-in __class__ attribute that references the class from which it was created, and each class has a __name__ attribute that references the name in the header, so self.__class__.__name__ fetches the name of an instance's class. You get the instance's memory address by calling the built-in id function, which returns any object's address (by definition, a unique object identifier):
Code View: Scroll / Show All


###########################################
# Lister can be mixed into any class to
# provide a formatted print of instances
# via inheritance of __repr__ coded here;
# self is the instance of the lowest class.
###########################################

class Lister:
   def __repr__(self):
       return ("<Instance of %s, address %s:\n%s>" %
                         (self.__class__.__name__,     # My class's name
                          id(self),                        # My address
                          self.attrnames(  )) )            # name=value list
   def attrnames(self):
       result = ''
       for attr in self.__dict__.keys(  ):               # Instance
namespace dict
           if attr[:2] == '__':
               result = result + "\tname %s=<built-in>\n" % attr
           else:
               result = result + "\tname %s=%s\n" % (attr, self.__dict__ [attr])
       return result

					  


Instances derived from this class display their attributes automatically when printed, giving a bit more information than a simple address:


>>> from mytools import Lister
>>> class Spam(Lister):
...     def __init__(self):
...         self.data1 = 'food'
...
>>> x = Spam(  )
>>> x
<Instance of Spam, address 8821568:
        name data1=food
>


The Lister class is useful for any classes you write—even classes that already have a superclass. This is where multiple inheritance comes in handy: by adding Lister to the list of superclasses in a class header (mixing it in), you get its __repr__ for free while still inheriting from the existing superclass. The file testmixin.py demonstrates:


from mytools import Lister            # Get tool class

class Super:
    def __init__(self):               # superclass __init__
        self.data1 = "spam"

class Sub(Super, Lister):             # Mix in a __repr__
    def __init__(self):                 # Lister has access to self
        Super.__init__(self)
        self.data2 = "eggs"           # More instance attrs
        self.data3 = 42
if __name__ == "__main__":
    X = Sub(  )
    print X                           # Mixed-in repr


Here, Sub inherits names from both Super and Lister; it's a composite of its own names and names in both its superclasses. When you make a Sub instance and print it, you automatically get the custom representation mixed in from Lister:


C:\lp3e> python testmixin.py
<Instance of Sub, address 7833392:
        name data3=42
        name data2=eggs
        name data1=spam
>


Lister works in any class it's mixed into because self refers to an instance of the subclass that pulls Lister in, whatever that may be. If you later decide to extend Lister's __repr__ to also print all the class attributes that an instance inherits, you're safe; because it's an inherited method, changing Lister.__repr__ automatically updates the display of each subclass that imports the class and mixes it in.[72]

    [72] * If you're curious how, flip back to "Section 24.5.4" in Chapter 24 for hints. We saw there that each class has a built-in attribute called __bases__, which is a tuple of the class' superclass objects. A general-purpose class hierarchy lister or browser can traverse the inheritance tree from an instance's __class__ to its class, and then from the class' __bases__ to all superclasses recursively, much like the classtree.py example shown earlier. In Python 2.2 and later, it's even simpler, as the built-in dir function now includes inherited attribute names automatically. If you don't care about displaying the tree structure, you can just scan the dir list instead of the dictionary keys list, and use getattr to fetch attributes by name string instead of dictionary-key indexing. We'll rehash this idea in one of this part's closing exercises.

In a sense, mix-in classes are the class equivalent of modules—packages of methods useful in a variety of clients. Here is Lister working again in single-inheritance mode on a different class's instances:


>>> from mytools import Lister
>>> class x(Lister):
...     pass
...
>>> t = x(  )
>>> t.a = 1; t.b = 2; t.c = 3
>>> t
<Instance of x, address 7797696:
        name b=2
        name a=1
        name c=3
>


OOP is all about code reuse, and mix-in classes are a powerful tool. Like almost everything else in programming, multiple inheritance can be a useful device when applied well; however, in practice, it is an advanced feature and can become complicated if used carelessly or excessively. We'll revisit this topic as a gotcha at the end of the next chapter. In that chapter, we'll also meet an option (new-style classes) that modifies the search order for one special multiple inheritance case.

			30.5.4.7  Classes Are Objects: Generic Object Factories

Because classes are objects, it's easy to pass them around a program, store them in data structures, and so on. You can also pass classes to functions that generate arbitrary kinds of objects; such functions are sometimes called factories in OOP design circles. They are a major undertaking in a strongly typed language such as C++, but almost trivial to implement in Python. The apply function and newer alternative syntax we met in Chapter 17 can call any class with any number of constructor arguments in one step to generate any sort of instance:[73]

    [73] * Actually, apply can call any callable object, including functions, classes, and methods. The factory function here can also run any callable object, not just a class (despite the argument name). Also, note that in recent Python versions, the aClass(*args) call syntax is generally preferred to the apply(aClass, args) built-in call.

Code View: Scroll / Show All


def factory(aClass, *args):                  # varargs tuple
    return apply(aClass, args)               # Call aClass, or: aClass(*args)

class Spam:
    def doit(self, message):
        print message

class Person:
    def __init__(self, name, job):
        self.name = name
        self.job  = job

object1 = factory(Spam)                      # Make a Spam object
object2 = factory(Person, "Guido", "guru")   # Make a Person object

					  


In this code, we define an object generator function called factory. It expects to be passed a class object (any class will do) along with one or more arguments for the class' constructor. The function uses apply to call the function and return an instance.

The rest of the example simply defines two classes, and generates instances of both by passing them to the factory function. And that's the only factory function you'll ever need to write in Python; it works for any class, and any constructor arguments.

One possible improvement worth noting is that to support keyword arguments in constructor calls, the factory can collect them with a **args argument, and pass them as a third argument to apply:


def factory(aClass, *args, **kwargs):        # +kwargs dict
    return apply(aClass, args, kwargs)       # Call aClass


By now, you should know that everything is an "object" in Python, including things like classes, which are just compiler input in languages like C++. However, as mentioned at the start of Part 6, only objects derived from classes are OOP objects in Python.

				30.5.4.7.1  Why Factories?

So what good is the factory function (besides providing an excuse to illustrate class objects in this book)? Unfortunately, it's difficult to show applications of this design pattern without listing much more code than we have space for here. In general, though, such a factory might allow code to be insulated from the details of dynamically configured object construction.

For example, recall the processor example presented in the abstract in Chapter 22, and then again as a has-a composition example in this chapter. It accepted reader and writer objects for processing arbitrary data streams.

The original version of this example manually passed in instances of specialized classes like FileWriter and SocketReader to customize the data streams being processed; later, we passed in hardcoded file, stream, and formatter objects. In a more dynamic scenario, external devices such as configuration files or GUIs might be used to configure the streams.

In such a dynamic world, we might not be able to hardcode the creation of stream interface objects in our script, but might instead create them at runtime according to the contents of a configuration file.

For instance, the file might simply give the string name of a stream class to be imported from a module, plus an optional constructor call argument. Factory-style functions or code might come in handy here because they would allow us to fetch and pass in classes that are not hardcoded in our program ahead of time. Indeed, those classes might not even have existed at all when we wrote our code:


classname = ...parse from config file...
classarg  = ...parse from config file...

import streamtypes                           # Customizable code
aclass = getattr(streamtypes, classname)     # Fetch from module
reader = factory(aclass, classarg)           # Or aclass(classarg)
processor(reader, ...)


Here, the getattr built-in is again used to fetch a module attribute given a string name (it's like saying obj.attr, but attr is a string). Because this code snippet assumes a single constructor argument, it doesn't strictly need factory or apply—we could make an instance with just aclass(classarg). They may prove more useful in the presence of unknown argument lists, however, and the general factory coding pattern can improve the code's flexibility. For more details on this topic, please consult books that cover OOP design and design patterns.

			30.5.4.8  Methods Are Objects: Bound or Unbound

Methods are a kind of object, much like functions. Class methods can be accessed from an instance or a class, and hence, they actually come in two flavors in Python:


Unbound class method objects: no self

    Accessing a function attribute of a class by qualifying a class returns an unbound method object. To call the method, you must provide an instance object explicitly as the first argument.

Bound instance method objects:self + function pairs

    Accessing a function attribute of a class by qualifying an instance returns a bound method object. Python automatically packages the instance with the function in the bound method object, so you don't need to pass an instance to call the method.

Both kinds of methods are full-fledged objects; they can be passed around, stored in lists, and so on. Both also require an instance in their first argument when run (i.e., a value for self). This is why we had to pass in an instance explicitly when calling superclass methods from subclass methods in the previous chapter; technically, such calls produce unbound method objects.

When calling a bound method object, Python provides an instance for you automatically—the instance used to create the bound method object. This means that bound method objects are usually interchangeable with simple function objects, and makes them especially useful for interfaces originally written for functions (see the sidebar "Why You Will Care: Bound Methods and Callbacks" for a realistic example).

To illustrate, suppose we define the following class:


class Spam:
    def doit(self, message):
        print message


Now, in normal operation, we make an instance, and call its method in a single step to print the passed-in argument:


object1 = Spam(  )
object1.doit('hello world')


Really, though, a bound method object is generated along the way, just before the method call's parentheses. In fact, we can fetch a bound method without actually calling it. An object.name qualification is an object expression. In the following, it returns a bound method object that packages the instance (object1) with the method function (Spam.doit). We can assign this bound method to another name, and then call it as though it were a simple function:


object1 = Spam(  )
x = object1.doit        # Bound method object: instance+function
x('hello world')        # Same effect as object1.doit('...')


On the other hand, if we qualify the class to get to doit, we get back an unbound method object, which is simply a reference to the function object. To call this type of method, we must pass in an instance as the leftmost argument:


object1 = Spam(  )
t = Spam.doit           # Unbound method object
t(object1, 'howdy')     # Pass in instance


By extension, the same rules apply within a class' method if we reference self attributes that refer to functions in the class. A self.method expression is a bound method object because self is an instance object:


class Eggs:
    def m1(self, n):
        print n
    def m2(self):
        x = self.m1     # Another bound method object
        x(42)           # Looks like a simple function

Eggs(  ).m2(  )              # Prints 42


Most of the time, you call methods immediately after fetching them with qualification, so you don't always notice the method objects generated along the way. But if you start writing code that calls objects generically, you need to be careful to treat unbound methods specially—they normally require an explicit instance object to be passed in.[74]

    [74] * See the discussion of static and class methods in Chapter 26 for an optional exception to this rule. Like bound methods, both of these can masquerade as basic functions, too, because they do not expect instances when called.

Now that you understand the method object model, for other examples of bound methods at work, see this chapter's sidebar "Why You Will Care: Bound Methods and Callbacks," and the prior chapter's discussion of callback handlers in "Section 24.4.9."

			30.5.4.9  Documentation Strings Revisited

Docstrings, which we covered in detail in Chapter 14, are string literals that show up at the top of various structures, and are automatically saved by Python in the corresponding objects' __doc__ attributes. This works for module files, function defs, and classes and methods. Now that we know more about classes and methods, the file docstr.py provides a quick but comprehensive example that summarizes the places where docstrings can show up in your code. All of these can be triple-quoted blocks:


"I am: docstr.__doc__"

class spam:
    "I am: spam.__doc__ or docstr.spam.__doc__"
    def method(self, arg):
        "I am: spam.method.__doc__ or self.method.__doc__"
        pass

def func(args):
    "I am: docstr.func.__doc__"
    pass


Why You Will Care: Bound Methods and Callbacks

Because bound methods automatically pair an instance with a class method function, you can use them anywhere a simple function is expected. One of the most common places you'll see this idea put to work is in code that registers methods as event callback handlers in the Tkinter GUI interface. Here's the simple case:


def handler(  ):
    ...use globals for state...
...
widget = Button(text='spam', command=handler)


To register a handler for button click events, we usually pass a callable object that takes no arguments to the command keyword argument. Function names (and lambdas) work here, and so do class methods, as long as they are bound methods:


class MyWidget:
    def handler(self):
        ...use self.attr for state...
    def makewidgets(self):
        b = Button(text='spam', command=self.handler)


Here, the event handler is self.handler—a bound method object that remembers both self and MyGui.handler. Because self will refer to the original instance when handler is later invoked on events, the method will have access to instance attributes that can retain state between events. With simple functions, state normally must be retained in global variables instead. See also the discussion of __call__ operator overloading in Chapter 24 for another way to make classes compatible with function-based APIs.


The main advantage of documentation strings is that they stick around at runtime. Thus, if it's been coded as a docstring, you can qualify an object with its __doc__ attribute to fetch its documentation:


>>> import docstr
>>> docstr.__doc__
'I am: docstr.__doc__'

>>> docstr.spam.__doc__
'I am: spam.__doc__ or docstr.spam.__doc__'

>>> docstr.spam.method.__doc__
'I am: spam.method.__doc__ or self.method.__doc__'
>>> docstr.func.__doc__
'I am: docstr.func.__doc__'


A discussion of the PyDoc tool, which knows how to format all these strings in reports, appears in Chapter 14.

Documentation strings are available at runtime, but they are less flexible syntactically than # comments (which can appear anywhere in a program). Both forms are useful tools, and any program documentation is good (as long as it's accurate).

			30.5.4.10 Classes Versus Modules

We'll wrap up this chapter by briefly comparing the topics of this book's last two parts: modules and classes. Because they're both about namespaces, the distinction can be confusing. In short:


Modules

        Are data/logic packages.

        Are created by writing Python files or C extensions.

        Are used by being imported.


Classes

        Implement new objects.

        Are created by class statements.

        Are used by being called.

        Always live within a module.

Classes also support extra features that modules don't, such as operator overloading, multiple instance generation, and inheritance. Although both classes and modules are namespaces, you should be able to tell by now that they are very different things.

			30.5.4.11 Chapter Summary
In this chapter, we sampled common ways to use and combine classes to optimize their reusability and factoring benefits—what are usually considered design issues that are often independent of any particular programming language (though Python can make them easier to implement). We studied delegation (wrapping objects in proxy classes), composition (controlling embedded objects), inheritance (acquiring behavior from other classes), and some more esoteric concepts such as multiple inheritance, bound methods, and factories.

The next chapter ends our look at classes and OOP by surveying more advanced class-related topics; some of its material may be of more interest to tool writers than application programmers, but it still merits a review by most people who will do OOP in Python. First, though, another quick chapter quiz.

BRAIN BUILDER

Chapter Quiz

Q:	What is multiple inheritance?
Q:	What is delegation?
Q:	What is composition?
Q:	What are bound methods?



Quiz Answers

A:	Multiple inheritance occurs when a class inherits from more than one superclass; it's useful for mixing together multiple packages of class-based code.
A:	Delegation involves wrapping an object in a proxy class, which adds extra behavior, and passes other operations to the wrapped object. The proxy retains the interface of the wrapped object.
A:	Composition is a technique whereby a controller class embeds and directs a number of objects, and provides an interface all its own; it's a way to build up larger structures with classes.
A:	Bound methods combine an instance and a method function; you can call them without passing in an instance object explicitly because the original instance is still available.

		30.5.5. Advanced Class Topics
This chapter concludes Part 6 and our look at OOP in Python by presenting a few more advanced class-related topics: we will survey subclassing built-in types, pseudoprivate attributes, new-style classes, static methods, function decorators, and more.

As we've seen, Python's OOP model is, at its core, very simple, and some of the topics presented in this chapter are so advanced and optional that you may not encounter them very often in your Python applications-programming career. In the interest of completeness, though, we'll round out our discussion of classes with a brief look at these advanced tools for advanced OOP work.

As usual, because this is the last chapter in this part of the book, it ends with a section on class-related gotchas, and the set of lab exercises for this part. I encourage you to work through the exercises to help cement the ideas we've studied here. I also suggest working on or studying larger OOP Python projects as a supplement to this book. As with much in computing, the benefits of OOP tend to become more apparent with practice.

			30.5.5.1 Extending Built-in Types

Besides implementing new kinds of objects, classes are sometimes used to extend the functionality of Python's built-in types to support more exotic data structures. For instance, to add queue insert and delete methods to lists, you can code classes that wrap (embed) a list object, and export insert and delete methods that process the list specially, like the delegation technique studied in Chapter 25. As of Python 2.2, you can also use inheritance to specialize built-in types. The next two sections show both techniques in action.

				30.5.5.1.1 Extending Types by Embedding

Remember those set functions we wrote in Part 4 Here's what they look like brought back to life as a Python class. The following example (the file setwrapper.py) implements a new set object type by moving some of the set functions to methods, and adding some basic operator overloading. For the most part, this class just wraps a Python list with extra set operations. Because it's a class, it also supports multiple instances and customization by inheritance in subclasses:
Code View: Scroll / Show All


class Set:
   def __init__(self, value = []):      # Constructor
       self.data = []                 # Manages a list
       self.concat(value)

   def intersect(self, other):        # other is any sequence
       res = []                       # self is the subject
       for x in self.data:
           if x in other:             # Pick common items
               res.append(x)
       return Set(res)                # Return a new Set

   def union(self, other):            # other is any sequence
       res = self.data[:]             # Copy of my list
       for x in other:                # Add items in other
           if not x in res:
               res.append(x)
       return Set(res)

   def concat(self, value):           # value: list, Set...
       for x in value:                # Removes duplicates
          if not x in self.data:
               self.data.append(x)

   def __len__(self):          return len(self.data)        # len(self)
   def __getitem__(self, key): return self.data[key]        # self[i]
   def __and__(self, other):   return self.intersect(other) # self & other
   def __or__(self, other):    return self.union(other)     # self | other
   def __repr__(self):         return 'Set:' + 'self.data'  # Print

					  


Overloading indexing enables instances of our Set class to masquerade as real lists. Because you will interact with and extend this class in an exercise at the end of this chapter, I won't say much more about this code until Appendix B.

				30.5.5.1.2 Extending Types by Subclassing

Beginning with Python 2.2, all the built-in types can now be subclassed directly. Type-conversion functions such as list, str, dict, and tuple have become built-in type names—although transparent to your script, a type-conversion call (e.g., list('spam')) is now really an invocation of a type's object constructor.

This change allows you to customize or extend the behavior of built-in types with user-defined class statements: simply subclass the new type names to customize them. Instances of your type subclasses can be used anywhere that the original built-in type can appear. For example, suppose you have trouble getting used to the fact that Python list offsets begin at 0 instead of 1. Not to worry—you can always code your own subclass that customizes this core behavior of lists. The file typesubclass.py shows how:


# Subclass built-in list type/class.
# Map 1..N to 0..N-1; call back to built-in version.

class MyList(list):
    def __getitem__(self, offset):
        print '(indexing %s at %s)' % (self, offset)
        return list.__getitem__(self, offset - 1)

if __name__ == '__main__':
    print list('abc')
    x = MyList('abc')               # __init__ inherited from list
    print x                         # __repr__ inherited from list

    print x[1]                      # MyList.__getitem__
    print x[3]                      # Customizes list superclass method

    x.append('spam'); print x       # Attributes from list superclass
    x.reverse(  );      print x


In this file, the MyList subclass extends the built-in list's __getitem__ indexing method only to map indexes 1 to N back to the required 0 to N−1. All it really does is decrement the index submitted, and call back to the superclass' version of indexing, but it's enough to do the trick:


% python typesubclass.py
['a', 'b', 'c']
['a', 'b', 'c']
(indexing ['a', 'b', 'c'] at 1)
a
(indexing ['a', 'b', 'c'] at 3)
c
['a', 'b', 'c', 'spam']
['spam', 'c', 'b', 'a']


This output also includes tracing text the class prints on indexing. Whether changing indexing this way is a good idea in general is another issue—users of your MyList class may very well be confused by such a core departure from Python sequence behavior. The fact that you can customize built-in types this way can be a powerful tool though, in general.

For instance, this coding pattern gives rise to an alternative way to code sets—as a subclass of the built-in list type, rather than a standalone class that manages an embedded list object. The following class, coded in the file setsubclass.py, customizes lists to add just methods and operators related to set processing. Because all other behavior is inherited from the built-in list superclass, this makes for a shorter and simpler alternative:
Code View: Scroll / Show All


class Set(list):
    def __init__(self, value = []):        # Constructor
        list.__init__([])                  # Customizes list
        self.concat(value)               # Copies mutable defaults

    def intersect(self, other):          # other is any sequence
        res = []                         # self is the subject
        for x in self:
            if x in other:               # Pick common items
                res.append(x)
        return Set(res)                  # Return a new Set

    def union(self, other):              # other is any sequence
        res = Set(self)                  # Copy me and my list
        res.concat(other)
        return res

    def concat(self, value):             # value: list, Set . . .
        for x in value:                  # Removes duplicates
            if not x in self:
                self.append(x)

    def __and__(self, other): return self.intersect(other)
    def __or__(self, other):  return self.union(other)
    def __repr__(self):       return 'Set:' + list.__repr__(self)

if __name__ == '__main__':
    x = Set([1,3,5,7])
    y = Set([2,1,4,5,6])
    print x, y, len(x)
    print x.intersect(y), y.union(x)
    print x & y, x | y
    x.reverse(  ); print x

					  


Here is the output of the self-test code at the end of this file. Because subclassing core types is an advanced feature, I'll omit further details here, but I invite you to trace through these results in the code to study its behavior:


% python setsubclass.py
Set:[1, 3, 5, 7] Set:[2, 1, 4, 5, 6] 4
Set:[1, 5] Set:[2, 1, 4, 5, 6, 3, 7]
Set:[1, 5] Set:[1, 3, 5, 7, 2, 4, 6]
Set:[7, 5, 3, 1]


There are more efficient ways to implement sets with dictionaries in Python, which replace the linear scans in the set implementations shown here with dictionary index operations (hashing), and so run much quicker. (For more details, see Programming Python.) If you're interested in sets, also take another look at the set object type we explored in Chapter 5; this type provides set operations as built-in tools. Set implementations are fun to experiment with, but they are no longer strictly required in Python today.

For another type subclassing example, see the implementation of the new bool type in Python 2.3: as mentioned earlier in the book, bool is a subclass of int with two instances (True and False) that behave like the integers 1 and 0, but inherit custom string-representation methods that display their names.

			30.5.5.2 Pseudoprivate Class Attributes
In Part 4, we learned that every name assigned at the top level of a module file is exported. By default, the same holds for classes—data hiding is a convention, and clients may fetch or change any class or instance attribute they like. In fact, attributes are all "public" and "virtual" in C++ terms; they're all accessible everywhere, and looked up dynamically at runtime.[75]

    [75] * This tends to scare C++ people unnecessarily. In Python, it's even possible to change or completely delete a class method at runtime. On the other hand, almost nobody ever does in practical programs. As a scripting language, Python is more about enabling than restricting. Also, recall from our discussion of operator overloading in Chapter 24 that __getattr__ and __setattr__ can be used to emulate privacy, but are generally not used for this in practice.

That's still true today. However, Python also supports the notion of name "mangling" (i.e., expansion) to localize some names in classes. Mangled names are sometimes misleadingly called "private attributes," but really this is just a way to localize a name to the class that created it—name mangling does not prevent access by code outside the class. This feature is mostly intended to avoid namespace collisions in instances, not to restrict access to names in general; mangled names are therefore better called "pseudoprivate" than "private."

Pseudoprivate names are an advanced and entirely optional feature, and you probably won't find them very useful until you start writing large class hierarchies in multi-programmer projects. But, because you may see this feature in other people's code, you need to be somewhat aware of it, even if you don't use it yourself.

				30.5.5.2.1 Name Mangling Overview

Here's how name mangling works: names inside a class statement that start with two underscores, but don't end with two underscores, are automatically expanded to include the name of the enclosing class. For instance, a name like __X within a class named Spam is changed to _Spam__X automatically: the original name is prefixed with a single underscore, and the enclosing class' name. Because the modified name contains the name of the enclosing class, it's somewhat unique; it won't clash with similar names created by other classes in a hierarchy.

Name mangling happens only in class statements, and only for names that begin with two leading underscores. However, it happens for every name preceded with double underscores, including method names and instance attribute names (for example, in our Spam class, the instance attribute reference self.__X would be transformed to self._Spam__X). Because more than one class may add attributes to an instance, this mangling helps avoid clashes—but we need to move on to an example to see how.

				30.5.5.2.2 Why Use Pseudoprivate Attributes?

The problem that the pseudoprivate attribute feature is meant to alleviate has to do with the way instance attributes are stored. In Python, all instance attributes wind up in the single instance object at the bottom of the class tree. This is very different from the C++ model, where each class gets its own space for data members it defines.

Within a class method in Python, whenever a method assigns to a self attribute (e.g., self.attr = value), it changes or creates an attribute in the instance (inheritance searches only happen on reference, not on assignment). Because this is true even if multiple classes in a hierarchy assign to the same attribute, collisions are possible.

For example, suppose that when a programmer codes a class, she assumes that she owns the attribute name X in the instance. In this class' methods, the name is set, and later fetched:


class C1:
    def meth1(self): self.X = 88         # Assume X is mine
    def meth2(self): print self.X


Suppose further that another programmer, working in isolation, makes the same assumption in a class that she codes:


class C2:
    def metha(self): self.X = 99         # Me too
    def methb(self): print self.X


Both of these classes work by themselves. The problem arises if the two classes are ever mixed together in the same class tree:


class C3(C1, C2): ...
I = C3(  )                                 # Only 1 X in I!


Now, the value that each class gets back when it says self.X will depend on which class assigned it last. Because all assignments to self.X refer to the same single instance, there is only one X attribute—I.X—no matter how many classes use that attribute name.

To guarantee that an attribute belongs to the class that uses it, prefix the name with double underscores everywhere it is used in the class, as in this file, private.py:


class C1:
    def meth1(self): self.__X = 88       # Now X is mine
    def meth2(self): print self.__X      # Becomes _C1__X in I
class C2:
    def metha(self): self.__X = 99       # Me too
    def methb(self): print self.__X      # Becomes _C2__X in I

class C3(C1, C2): pass
I = C3(  )                                 # Two X names in I

I.meth1(  ); I.metha(  )
print I.__dict__
I.meth2(  ); I.methb(  )


When thus prefixed, the X attributes will be expanded to include the names of their classes before being added to the instance. If you run a dir call on I, or inspect its namespace dictionary after the attributes have been assigned, you'll see the expanded names, _C1__X and _C2__X, but not X. Because the expansion makes the names unique within the instance, the class coders can safely assume that they truly own any names that they prefix with two underscores:


% python private.py
{'_C2__X': 99, '_C1__X': 88}
88
99


This trick can avoid potential name collisions in the instance, but note that it does not amount to true privacy. If you know the name of the enclosing class, you can still access either of these attributes anywhere you have a reference to the instance by using the fully expanded name (e.g., I._C1__X = 77). On the other hand, this feature makes it less likely that you will accidentally step on a class' names.

Again, I should note that this feature tends to be more useful for larger, multi-programmer projects, and then only for selected names. Don't be tempted to clutter your code unnecessarily; only use this feature for names that truly need to be controlled by a single class. For simpler programs, it's probably overkill.

	

Also, see the emulation of private instance attributes sketched in Chapter 24, in the __getattr__ section. Although it's possible to emulate access controls in Python classes, this is rarely done in practice, even for large systems.

			30.5.5.3 New-Style Classes
In Release 2.2, Python introduced a new flavor of classes, known as "new-style" classes; the classes covered so far in this part of the book are known as "classic classes" when comparing them to the new kind.

New-style classes are only slightly different from classic classes, and the ways in which they differ are completely irrelevant to the vast majority of Python users. Moreover, the classic class model, which has been with Python for some 15 years, still works exactly as I've described previously.

New-style classes are almost completely backward compatible with classic classes in syntax and behavior; they mostly just add a few advanced new features. However, because they modify one special case of inheritance, they had to be introduced as a distinct tool so as to avoid impacting any existing code that depends on the prior behavior.

New-style classes are coded with all the normal class syntax we have studied. The chief coding difference is that you subclass from a built-in type (e.g., list) to produce a new-style class. A new built-in name, object, is provided to serve as a superclass for new-style classes if no other built-in type is appropriate to use:


class newstyle(object):
    ...normal code...


More generally, any class derived from object, or any other built-in type, is automatically treated as a new-style class. (By derived, I mean that this includes subclasses of object, subclasses of subclasses of object, and so on—as long as a built-in is somewhere in the superclass tree, the new class will be treated as a new-style class.) Classes not derived from built-ins are considered classic.

	

Per Python creator Guido van Rossum, in Python 3.0, all classes will automatically be new-style, so the requirement of being derived from a built-in superclass will no longer exist. Because even standalone classes will be considered new-style, and because new-style classes are almost completely backward compatible with classic classes, for most programmers, the change will be transparent.

In the past, there was some concern that top-level classes might need to derive from object in Python 3.0, but Guido has recently stated that this won't be required. To most programmers, all classes in 3.0 will work as described in this book, but with the additional new-style features available. I can't predict the future completely, though, so be sure to check the 3.0 release notes for more on this front.

				30.5.5.3.1 Diamond Inheritance Change

Perhaps the most visible change in new-style classes is their slightly different treatment of inheritance for the so-called diamond pattern of multiple inheritance trees, where more than one superclass leads to the same higher superclass further above. The diamond pattern is an advanced design concept that we have not even discussed for normal classes.

In short, with classic classes, the inheritance search procedure is strictly depth first, and then left to right—Python climbs all the way to the top, hugging the left side of the tree, before it backs up, and begins to look further to the right. In new-style classes, the search is more breadth-first in such cases—Python first looks in any superclasses to the right of the first one searched before ascending all the way to the common superclass at the top. Because of this change, lower superclasses can overload attributes of higher superclasses, regardless of the sort of multiple inheritance trees they are mixed into.

					30.5.5.3.1.1 Diamond inheritance example

To illustrate, consider this simplistic incarnation of the diamond inheritance pattern for classic classes:


>>> class A:      attr = 1  # Classic
>>> class B(A):   pass 
>>> class C(A):   attr = 2 
>>> class D(B,C): pass  # Tries A before C
>>> x = D(  ) 
>>> x.attr 
1


The attribute here was found in superclass A, because with classic classes, the inheritance search climbs as high as it can before backing up and moving right—Python will search D, B, A, and then C (but will stop when attr is found in A, above B). With the new-style classes derived from a built-in like object, though, Python looks in C (to the right of B) before A (above B)—that is, it searches D, B, C, and then A (and in this case, stops in C):


>>> class A(object): attr = 1  # New style
>>> class B(A):      pass 
>>> class C(A):      attr = 2 
>>> class D(B,C):    pass  # Tries C before A
>>> x = D(  ) 
>>> x.attr 
2


This change in the inheritance search procedure is based upon the assumption that if you mix in C lower in the tree, you probably intend to grab its attributes in preference to A's. It also assumes that C is always intended to override A's attributes, which is probably true when it's used standalone, but may not be when it's mixed into a diamond with classic classes—you might not even know that C may be mixed in like this when you code it.
					30.5.5.3.1.2 Explicit conflict resolution

Of course, the problem with assumptions is that they assume things. If this search order deviation seems too subtle to remember, or if you want more control over the search process, you can always force the selection of an attribute from anywhere in the tree by assigning or otherwise naming the one you want at the place where the classes are mixed together:


>>> class A:      attr = 1  # Classic
>>> class B(A):   pass 
>>> class C(A):   attr = 2 
>>> class D(B,C): attr = C.attr  # Choose C, to the right
>>> x = D(  ) 
>>> x.attr  # Works like new style
2


Here, a tree of classic classes is emulating the search order of new-style classes: the assignment to the attribute in D picks the version in C, thereby subverting the normal inheritance search path (D.attr will be lowest in the tree). New-style classes can similarly emulate classic classes by choosing the attribute above at the place where the classes are mixed together:


>>> class A(object): attr = 1  # New style
>>> class B(A):      pass 
>>> class C(A):      attr = 2 
>>> class D(B,C):    attr = B.attr  # Choose A.attr, above
>>> x = D(  ) 
>>> x.attr  # Works like classic
1


If you are willing to always resolve conflicts like this, you can largely ignore the search order difference, and not rely on assumptions about what you meant when you coded your classes. Naturally, attributes picked this way can also be method functions—methods are normal, assignable objects:


>>> class A: 
...    def meth(s): print 'A.meth' 
>>> class C(A): 
...     def meth(s): print 'C.meth' 
>>> class B(A): 
...     pass 
>>> class D(B,C): pass  # Use default search order
>>> x = D(  )  # Will vary per class type
>>> x.meth(  )  # Defaults to classic order
A.meth

>>> class D(B,C): meth = C.meth  # Pick C's method: new style
>>> x = D(  ) 
>>> x.meth(  ) 
C.meth

>>> class D(B,C): meth = B.meth  # Pick B's method: classic
>>> x = D(  ) 
>>> x.meth(  ) 
A.meth


Here, we select methods by explicitly assigning to names lower in the tree. We might also simply call the desired class explicitly; in practice, this pattern might be more common, especially for things like constructors:


class D(B,C):
    def meth(self):                   # Redefine lower
        ...
        C.meth(self)                  # Pick C's method by calling


Such selections by assignment or call at mix-in points can effectively insulate your code from this difference in class flavors. Explicitly resolving the conflicts this way ensures that your code won't vary per Python version in the future (apart from perhaps needing to derive classes from a built-in type for the new style).[76]

    [76] * Even without the classic/new divergence, this technique may sometimes come in handy in multiple inheritance scenarios in general. If you want part of a superclass on the left and part of a superclass on the right, you might need to tell Python which same-named attributes to choose by using explicit assignments in subclasses. We'll revisit this notion in a gotcha at the end of this chapter. Also note that diamond inheritance patterns can be more problematic in some cases than I've implied here (e.g., what if B and C both have required constructors that call to A's?), but this is beyond this book's scope.

In sum, by default, the diamond pattern is searched differently for classic and new-style classes, and this is a nonbackward-compatible change. However, keep in mind that this change only affects diamond pattern cases; new-style class inheritance works unchanged for all other inheritance tree structures. Further, it's not impossible that this entire issue may be of more theoretical than practical importance—because it wasn't significant enough to change until 2.2, it seems unlikely to impact much Python code.
				30.5.5.3.2 Other New-Style Class Extensions

Beyond this change in the diamond inheritance search pattern (which is itself too obscure to matter to most readers of this book), new-style classes open up a handful of even more advanced possibilities. Here's a brief look at each.

					30.5.5.3.2.1 Static and class methods

As of Python 2.2, it is possible to define methods within a class that can be called without an instance: static methods work roughly like simple instanceless functions inside a class, and class methods are passed a class instead of an instance. Special built-in functions called staticmethod and classmethod must be called within the class to enable these method modes. Although this feature was added in conjunction with new-style classes, static and class methods work for classic classes, too. Because of that, we'll save this topic for the next section.

					30.5.5.3.2.2 Instance slots

By assigning a list of string attribute names to a special __slots__ class attribute, it is possible for a new-style class to limit the set of legal attributes that instances of the class will have. This special attribute is typically set by assigning to the variable __slots__ at the top level of a class statement: only those names in the __slots__ list can be assigned as instance attributes. However, like all names in Python, instance attribute names must still be assigned before they can be referenced, even if listed in __slots__. Here's an example to illustrate:


>>> class limiter(object): 
...     __slots__ = ['age', 'name', 'job'] 
...
>>> x = limiter(  ) 
>>> x.age  # Must assign before use
AttributeError: age

>>> x.age = 40 
>>> x.age 
40
>>> x.ape = 1000  # Illegal: not in slots
AttributeError: 'limiter' object has no attribute 'ape'


This feature was envisioned as a way to catch "typo" errors (assignments to illegal attribute names not in __slots__ are detected), and as an optimization mechanism (slot attributes may be stored in a tuple instead of a dictionary for quicker lookup).

Slots are something of a break with Python's dynamic nature, which dictates that any name may be created by assignment. They also have additional constraints and implications that are far too complex for us to discuss here. For example, some instances with slots may not have a __dict__ attribute dictionary, which can make some of the metaprograms we've coded in this book more complex; tools that generically list attributes, for instance, may have to inspect two sources instead of one. See the Python 2.2 release documents and Python's standard manual set for more details.

					30.5.5.3.2.3 Class properties

A mechanism known as properties provides another way for new-style classes to define automatically called methods for access or assignment to instance attributes. This feature is an alternative to many current uses of the __getattr__ and __setattr__ overloading methods we studied in Chapter 24. Properties have a similar effect to these two methods, but they incur an extra method call only for accesses to names that require dynamic computation. Properties (and slots) are based on a new notion of attribute descriptors, which is too advanced for us to cover here.

In short, properties are a type of object assigned to class attribute names. They are generated by calling a property built-in with three methods (handlers for get, set, and delete operations), as well as a docstring; if any argument is passed as None or omitted, that operation is not supported. Properties are typically assigned at the top level of a class statement [e.g., name = property(...)]. When thus assigned, accesses to the class attribute itself (e.g., obj.name) are automatically routed to one of the accessor methods passed into the property. For example, the __getattr__ method allows classes to intercept undefined attribute references:


>>> class classic: 
...     def __getattr__(self, name): 
...         if name == 'age': 
...             return 40 
...         else: 
...             raise AttributeError 
...
>>> x = classic(  ) 
>>> x.age  # Runs __getattr__
40
>>> x.name  # Runs __getattr__
AttributeError


Here is the same example, coded with properties instead:


>>> class newprops(object): 
...     def getage(self): 
...         return 40 
...     age = property(getage, None, None, None)  # get,set,del,docs
...
>>> x = newprops(  ) 
>>> x.age  # Runs getage
40
>>> x.name  # Normal fetch
AttributeError: newprops instance has no attribute 'name'


For some coding tasks, properties can be less complex and quicker to run than the traditional techniques. For example, when we add attribute assignment support, properties become more attractive—there's less code to type, and no extra method calls are incurred for assignments to attributes we don't wish to compute dynamically:


>>> class newprops(object): 
...     def getage(self): 
...         return 40 
...     def setage(self, value): 
...         print 'set age:', value 
...         self._age = value 
...     age = property(getage, setage, None, None) 
...
>>> x = newprops(  ) 
>>> x.age  # Runs getage
40
>>> x.age = 42  # Runs setage
set age: 42
>>> x._age  # Normal fetch; no getage call
42
>>> x.job = 'trainer'  # Normal assign; no setage call
>>> x.job  # Normal fetch; no getage call
'trainer'


The equivalent classic class might trigger extra method calls, and may need to route attribute assignments through the attribute dictionary to avoid loops:
Code View: Scroll / Show All


>>> class classic: 
...     def __getattr__(self, name):  # On undefined reference
...         if name == 'age': 
...             return 40 
...         else: 
...             raise AttributeError 
...     def __setattr__(self, name, value):  # On all assignments
...         print 'set:', name, value 
...         if name == 'age': 
...             self.__dict__['_age'] = value 
...         else: 
...             self.__dict__[name] = value 
...
>>> x = classic(  ) 
>>> x.age  # Runs __getattr__
40
>>> x.age = 41  # Runs __setattr__
set: age 41
>>> x._age  # Defined:
no __getattr__ call
41
>>> x.job = 'trainer'  # Runs __setattr__ again
>>> x.job  # Defined:
no __getattr__ call

					  


Properties seem like a win for this simple example. However, some applications of __getattr__ and __setattr__ may still require more dynamic or generic interfaces than properties directly provide. For example, in many cases, the set of attributes to be supported cannot be determined when the class is coded, and may not even exist in any tangible form (e.g., when delegating arbitrary method references to a wrapped/embedded object generically). In such cases, a generic __getattr__ or a __setattr__ attribute handler with a passed-in attribute name may be preferable. Because such generic handlers can also handle simpler cases, properties are largely an optional extension.

					30.5.5.3.2.4 New __getattribute__ overloading method

The __getattribute__ method, available for new-style classes only, allows a class to intercept all attribute references, not just undefined references (like __getattr__). It is also substantially trickier to use than __getattr__ and __setattr__ (it is prone to loops). I'll defer to Python's standard documentation for more details on this method.

Besides all these feature additions, new-style classes integrate with the notion of subclassable types mentioned earlier in this chapter; subclassable types and new-style classes were introduced in conjunction with a merging of the type/class dichotomy in Python 2.2 and beyond.

Because new-style class features are all advanced topics, we won't go into further details in this introductory text. Please see the Python 2.2 release documentation and the language reference for more information.

			30.5.5.4 Static and Class Methods
In Python releases prior to 2.2, class method functions can never be called without an instance. In Python 2.2 and later, this is also the default behavior, but it can be modified with a new optional feature known as static methods—simple functions with no self argument that appear nested in a class, and are designed to work on class attributes instead of instance attributes. Such methods usually keep track of information that spans all instances (e.g., the number created), rather than providing behavior for instances.

In the prior chapter, we talked about unbound methods: when we fetch a method function by qualifying a class (instead of an instance), we get an unbound method object. Even though they are defined with def statements, unbound method objects are not simple functions; they cannot be called without an instance.

For example, suppose we want to use class attributes to count how many instances are generated from a class (as in the following file, spam.py). Remember, class attributes are shared by all instances, so we can store the counter in the class object itself:


class Spam:
    numInstances = 0
    def __init__(self):
        Spam.numInstances = Spam.numInstances + 1
    def printNumInstances(  ):
        print "Number of instances created: ", Spam.numInstances


But this won't work—the printNumInstances method still expects an instance to be passed in when called because the function is associated with a class (even though there are no arguments in the def header):


>>> from spam import *
>>> a = Spam(  )
>>> b = Spam(  )
>>> c = Spam(  )
>>> Spam.printNumInstances(  )
Traceback (innermost last):
  File "<stdin>", line 1, in ?
TypeError: unbound method must be called with class instance 1st argument


The problem here is that unbound instance methods aren't exactly the same as simple functions. This is mostly a knowledge issue, but if you want to call functions that access class members without an instance, probably the most straightforward idea is to just make them simple functions, not class methods. This way, an instance isn't expected in the call:


def printNumInstances(  ):
    print "Number of instances created: ", Spam.numInstances

class Spam:
    numInstances = 0
    def __init__(self):
        Spam.numInstances = Spam.numInstances + 1

>>> import spam
>>> a = spam.Spam(  )
>>> b = spam.Spam(  )
>>> c = spam.Spam(  )
>>> spam.printNumInstances(  )
Number of instances created:  3
>>> spam.Spam.numInstances
3


Because the class name is accessible to the simple function as a global variable, this works fine. Also, note that the name of the function becomes global, but only to this single module; it will not clash with names in other files of the program.

We can also make this work by calling the function through an instance, as usual, although this can be inconvenient if making an instance changes the class data:


class Spam:
    numInstances = 0
    def __init__(self):
        Spam.numInstances = Spam.numInstances + 1
    def printNumInstances(self):
        print "Number of instances created: ", Spam.numInstances

>>> from spam import Spam
>>> a, b, c = Spam(), Spam(  ), Spam(  )
>>> a.printNumInstances(  )
Number of instances created:  3
>>> b.printNumInstances(  )
Number of instances created:  3
>>> Spam(  ).printNumInstances(  )
Number of instances created:  4


Prior to Python 2.2's static method extension, some language theorists claimed that the availability of this technique meant that Python didn't have class methods, only instance methods. I suspect they really meant that Python classes don't work the same as classes in some other languages. What Python really has are bound and unbound method objects, with well-defined semantics; qualifying a class gets you an unbound method, which is a special kind of function. Python does have class attributes, but functions in classes expect an instance argument.

Moreover, because Python already provides modules as a namespace-partitioning tool, there's usually no need to package functions in classes unless they implement object behavior. Simple functions within modules usually do most of what instanceless class methods could. For example, in the first code sample in this section, printNumInstances is already associated with the class because it lives in the same module. The only lost functionality is that the function name has a broader scope—the entire module, rather than the class.

				30.5.5.4.1 Using Static and Class Methods

Today, there is another option for coding simple functions associated with a class. As of Python 2.2, you can code classes with static and class methods, neither of which requires an instance argument to be passed in when they are invoked. To designate such methods, classes call the built-in functions staticmethod and classmethod, as hinted in the earlier discussion of new-style classes. For example:


class Multi:
    def imeth(self, x):          # Normal instance method
        print self, x
    def smeth(x):                # Static: no instance passed
        print x
    def cmeth(cls, x):           # Class: gets class, not instance
        print cls, x
    smeth = staticmethod(smeth)  # Make smeth a static method
    cmeth = classmethod(cmeth)   # Make cmeth a class method.


Notice how the last two assignments in this code simply reassign the method names smeth and cmeth. Attributes are created and changed by any assignment in a class statement, so these final assignments overwrite the assignments made earlier by the defs.

Technically, Python now supports three kinds of class-related methods: instance, static, and class. Instance methods are the normal (and default) case that we've seen in this book. You must always call an instance method with an instance object. When you call it through an instance, Python passes the instance to the first (leftmost) argument automatically; when you call it through a class, you pass along the instance manually:


>>> obj = Multi(  )  # Make an instance
>>> obj.imeth(1)  # Normal call, through instance
<__main__.Multi instance...> 1
>>> Multi.imeth(obj, 2)  # Normal call, through class
<__main__.Multi instance...> 2


By contrast, static methods are called without an instance argument; their names are local to the scopes of the classes in which they are defined, and may be looked up by inheritance. Mostly, they work like simple functions that happen to be coded inside a class:


>>> Multi.smeth(3)  # Static call, through class
3
>>> obj.smeth(4)  # Static call, through instance
4


Class methods are similar, but Python automatically passes the class (not an instance) in to a class method's first (leftmost) argument:


>>> Multi.cmeth(5)  # Class call, through class
__main__.Multi 5
>>> obj.cmeth(6)  # Class call, through instance
__main__.Multi 6


Static and class methods are new and advanced features of the language, with highly specialized roles that we don't have space to document fully here. Static methods are commonly used in conjunction with class attributes to manage information that spans all instances generated from the class. For example, to keep track of the number of instances generated from a class (as in the earlier example), we could use static methods to manage a counter attached as a class attribute. Because such a count has nothing to do with any particular instance, it would be inconvenient to have to access methods that process it through an instance (especially since making an instance to access the counter might change the counter). Moreover, static methods' proximity to the class provides a more natural solution than coding class-oriented functions outside the class.

Here is the static method equivalent of this section's original example:


class Spam:
    numInstances = 0
    def __init__(self):
        Spam.numInstances += 1
    def printNumInstances(  ):
        print "Number of instances:", Spam.numInstances
    printNumInstances = staticmethod(printNumInstances)

>>> a = Spam(  )
>>> b = Spam(  )
>>> c = Spam(  )
>>> Spam.printNumInstances(  )
Number of instances: 3
>>> a.printNumInstances(  )
Number of instances: 3


Compared to simply moving printNumInstances outside the class, as prescribed earlier, this version requires an extra staticmethod call; however, it localizes the function name in the class scope (it won't clash with other names in the module) and moves the function code closer to where it is used (inside the class statement). You should judge for yourself whether this is a net improvement or not.

In recent Python versions, the static method designation has become even simpler; the next section explains how.

			30.5.5.5 Function Decorators
Because the staticmethod call technique described in the prior section seemed obscure to some users, a feature was added to make the operation simpler. Function decorators provide a way to specify special operation modes for functions, by wrapping them in an extra layer of logic implemented as another function.

Function decorators turn out to be general tools: they are useful for adding many types of logic to functions besides the static method use case. For instance, they may be used to augment functions with code that logs calls made to them, checks the types of passed arguments during debugging, and so on. In some ways, function decorators are similar to the delegation design pattern we explored in Chapter 25, but they are designed to augment a specific function or method call, not an entire object interface.

Python provides some built-in function decorators for operations such as marking static methods, but programmers can also code arbitrary decorators of their own. Although they are not strictly tied to classes, user-defined function decorators often are coded as classes to save the original functions, along with other data, as state information.

Syntactically, a function decorator is a sort of runtime declaration about the function that follows. A function decorator is coded on a line just before the def statement that defines a function or method and consists of the @ symbol, followed by what we call a metafunction—a function (or other callable object) that manages another function. Static methods today, for example, may be coded with decorator syntax like this:


class C:
   @staticmethod
   def meth(  ):
       ...


Internally, this syntax has the same effect as the following (passing the function through the decorator and assigning the result back to the original name):


class C:
   def meth(  ):
       ...
   meth = staticmethod(meth)                     # Rebind name


The net effect is that calling the method function's name actually triggers the result of its staticmethod decorator first. Because a decorator can return any sort of object, this allows the decorator to insert a layer of logic to be run on every call. The decorator function is free to return the original function itself or a new object that saves the original function passed to the decorator to be invoked indirectly after the extra logic layer runs.

In fact, decorator syntax supports adding multiple layers of wrapper logic to a decorated function or method. A decorator line of this form:


@A @B @C
def f(  ):
    ...


runs the same as the following:


def f(  ):
    ...
f = A(B(C(f)))


It passes the original function through three different decorators, and assigns the result back to the original name. Again, the net effect is that when the original function name is called, three layers of logic can be invoked to augment the original function.

				30.5.5.5.1 Decorator Example

Here is an example of a user-defined decorator at work. Recall from Chapter 24 that the __call__ operator overloading method implements a function-call interface for class instances. The following code uses this to define a class that saves the decorated function in the instance and catches calls to the original name. Because this is a class, it also has state information (a counter of calls made):
Code View: Scroll / Show All


class tracer:
    def __init__(self, func):
        self.calls = 0
        self.func  = func
    def __call__(self, *args):
        self.calls += 1
        print 'call %s to %s' % (self.calls, self.func.__name__)
        self.func(*args)

@tracer
def spam(a, b, c):            # Wrap spam in a decorator object
    print a, b, c

spam(1, 2, 3)                  # Really calls the tracer wrapper object
spam('a', 'b', 'c')           # Invokes __call__ in class
spam(4, 5, 6)                 # __call__ adds logic and runs original object

					  


Because the spam function is run through the tracer decorator, when the original spam name is called, it actually triggers the __call__ method in the class. This method counts and logs the call, and then dispatches to the original wrapped function. Note how the *name argument syntax is used to pack and unpack arguments passed in; because of this, this decorator can be used to wrap any function with any number of arguments.

The net effect, again, is to add a layer of logic to the original spam function. Here is the script's output—the first line comes from the tracer class, and the second comes from the spam function:


call 1 to spam
1 2 3
call 2 to spam
a b c
call 3 to spam
4 5 6


Trace through this example's code for more insight.

Although they are a general mechanism, function decorators are an advanced feature of interest primarily to tool writers, not application programmers, so I'll again defer to Python's standard manual set for more details on this subject.

			30.5.5.6 Class Gotchas

Most class issues can usually be boiled down to namespace issues (which makes sense, given that classes are just namespaces with a few extra tricks). Some of the topics we'll cover in this section are more like case studies of advanced class usage than problems, and one or two of these gotchas have been eased by recent Python releases.

				30.5.5.6.1 Changing Class Attributes Can Have Side Effects

Theoretically speaking, classes (and class instances) are mutable objects. Like built-in lists and dictionaries, they can be changed in-place by assigning to their attributes—and, as with lists and dictionaries, this means that changing a class or instance object may impact multiple references to it.

That's usually what we want (and is how objects change their state in general), but this becomes especially critical to know when changing class attributes. Because all instances generated from a class share the class' namespace, any changes at the class level are reflected in all instances unless they have their own versions of the changed class attributes.

Because classes, modules, and instances are all just objects with attribute namespaces, you can normally change their attributes at runtime by assignments. Consider the following class. Inside the class body, the assignment to the name a generates an attribute X.a, which lives in the class object at runtime, and will be inherited by all of X's instances:


>>> class X: 
...     a = 1  # Class attribute
...
>>> I = X(  ) 
>>> I.a  # Inherited by instance
1
>>> X.a 
1


So far, so good—this is the normal case. But notice what happens when we change the class attribute dynamically outside the class statement: it also changes the attribute in every object that inherits from the class. Moreover, new instances created from the class during this session or program run get the dynamically set value, regardless of what the class' source code says:


>>> X.a = 2  # May change more than X
>>> I.a  # I changes too
2
>>> J = X(  )  # J inherits from X's runtime values
>>> J.a  # (but assigning to J.a changes a in J, not X or I)
2


Is this a useful feature or a dangerous trap? You be the judge. You can actually get work done by changing class attributes without ever making a single instance; this technique can simulate "records" or "structs" in other languages. As a refresher, consider the following unusual but legal Python program:


class X: pass                       # Make a few attribute namespaces
class Y: pass

X.a = 1                             # Use class attributes as variables
X.b = 2                             # No instances anywhere to be found
X.c = 3
Y.a = X.a + X.b + X.c

for X.i in range(Y.a): print X.i    # Prints 0..5


Here, the classes X and Y work like "fileless" modules—namespaces for storing variables we don't want to clash. This is a perfectly legal Python programming trick, but it's less appropriate when applied to classes written by others; you can't always be sure that class attributes you change aren't critical to the class' internal behavior. If you're out to simulate a C struct, you may be better off changing instances than classes, as that way, only one object is affected:


class Record: pass
X = Record(  )
X.name = 'bob'
X.job  = 'Pizza maker'


				30.5.5.6.2 Multiple Inheritance: Order Matters

This may be obvious, but it's worth underscoring: if you use multiple inheritance, the order in which superclasses are listed in the class statement header can be critical. Python always searches superclasses from left to right, according to their order in the header line.

For instance, in the multiple inheritance example we saw in Chapter 25, suppose that the Super class implemented a __repr__ method, too; would we want to inherit Lister's or Super's? We would get it from whichever class is listed first in Sub's class header, as inheritance searches proceed from left to right. Presumably, we would list Lister first because its whole purpose is its custom __repr__:
Code View: Scroll / Show All


class Lister:
    def __repr__(self): ...

class Super:
    def __repr__(self): ...

class Sub(Lister, Super):    # Get Lister's __repr__ by listing it first

					  


But now suppose Super and Lister have their own versions of other same-named attributes, too. If we want one name from Super, and another from Lister, the order in which we list them in the class header won't help—we will have to override inheritance by manually assigning to the attribute name in the Sub class:
Code View: Scroll / Show All


class Lister:
    def __repr__(self): ...
    def other(self): ...

class Super:
    def __repr__(self): ...
    def other(self): ...

class Sub(Lister, Super):    # Get Lister's __repr__ by listing it first
    other = Super.other      # But explicitly pick Super's version of other
    def __init__(self):
        ...

x = Sub(  )                    # Inheritance searches Sub before Super/Lister

					  


Here, the assignment to other within the Sub class creates Sub.other—a reference back to the Super.other object. Because it is lower in the tree, Sub.other effectively hides Lister.other, the attribute that the inheritance search would normally find. Similarly, if we listed Super first in the class header to pick up its other, we would need to select Lister's method explicitly:
Code View: Scroll / Show All


class Sub(Super, Lister):                 # Get Super's other by order
    __repr__ = Lister.__repr__    # Explicitly pick Lister.__repr__

					  


Multiple inheritance is an advanced tool. Even if you understood the last paragraph, it's still a good idea to use it sparingly and carefully. Otherwise, the meaning of a name may come to depend on the order in which classes are mixed in an arbitrarily far-removed subclass. (For another example of the technique shown here in action, see the discussion of explicit conflict resolution in "Section 26.3" earlier in this chapter.)

As a rule of thumb, multiple inheritance works best when your mix-in classes are as self-contained as possible—because they may be used in a variety of contexts, they should not make assumptions about names related to other classes in a tree. The pseudoprivate attributes feature we studied earlier can help by localizing names that a class relies on owning, and limiting the names that your mix-in classes add to the mix. In this example, for instance, if Lister only means to export its custom __repr__, it can name its other method __other to avoid clashing with other classes.

				30.5.5.6.3 Methods, Classes, and Nested Scopes

This gotcha went away in Python 2.2, with the introduction of nested function scopes, but I've retained it here for historical perspective, for readers working with older Python releases, and because it demonstrates what happens to the new nested function scope rules when one layer of the nesting is a class.

Classes introduce local scopes, just as functions do, so the same sorts of scope behavior can happen in a class statement body. Moreover, methods are further nested functions, so the same issues apply. Confusion seems to be especially common when classes are nested.

In the following example (the file nester.py), the generate function returns an instance of the nested Spam class. Within its code, the class name Spam is assigned in the generate function's local scope. But, in versions of Python prior to 2.2, within the class' method function, the class name Spam is not visible—method has access only to its own local scope, the module surrounding generate, and built-in names:
Code View: Scroll / Show All


def generate(  ):
    class Spam:
        count = 1
        def method(self):         # Name Spam not visible:
            print Spam.count      # Not local(def), global(module), built-in
    return Spam(  )

generate(  ).method(  )

C:\python\examples> python nester.py 
Traceback (innermost last):
  File "nester.py", line 8, in ?
    generate(  ).method(  )
  File "nester.py", line 5, in method
    print Spam.count              # Not local(def), global(module), built-in
NameError: Spam

					  


This example works in Python 2.2 and later because the local scopes of all enclosing function defs are automatically visible to nested defs (including nested method defs, as in this example). But, it doesn't work before 2.2 (see below for some possible solutions).

Note that even in 2.2, method defs cannot see the local scope of the enclosing class; they can only see the local scopes of enclosing defs. That's why methods must go through the self instance or the class name to reference methods and other attributes defined in the enclosing class statement. For example, code in the method must use self.count, or Spam.count, not just count.

If you're using a release prior to 2.2, there are a variety of ways to get the preceding example to work. One of the simplest is to move the name Spam out to the enclosing module's scope with a global declaration. Because method sees global names in the enclosing module, references to Spam will work:


def generate(  ):
    global Spam                 # Force Spam to module scope
    class Spam:
        count = 1
        def method(self):
            print Spam.count    # Works: in global (enclosing module)
    return Spam(  )

generate(  ).method(  )             # Prints 1


A better alternative would be to restructure the code such that the class Spam is defined at the top level of the module by virtue of its nesting level, rather than using global declarations. The nested method function and the top-level generate will then find Spam in their global scopes:


def generate(  ):
    return Spam(  )

class Spam:                    # Define at top level of module
    count = 1
    def method(self):
        print Spam.count       # Works: in global (enclosing module)

generate(  ).method(  )


In fact, this approach is recommended for all Python releases—code tends to be simpler in general if you avoid nesting classes and functions.

If you want to get complicated and tricky, you can also get rid of the Spam reference in method altogether by using the special __class__ attribute, which returns an instance's class object:
Code View: Scroll / Show All


def generate(  ):
    class Spam:
        count = 1
        def method(self):
            print self.__class__.count       # Works: qualify to get class
    return Spam(  )

generate(  ).method(  )

					  


				30.5.5.6.4 "Overwrapping-itis"

When used well, the code reuse features of OOP make it excel at cutting development time. Sometimes, though, OOP's abstraction potential can be abused to the point of making code difficult to understand. If classes are layered too deeply, code can become obscure; you may have to search through many classes to discover what an operation does.

For example, I once worked in a C++ shop with thousands of classes (some machine-generated), and up to 15 levels of inheritance. Deciphering method calls in such a complex system was often a monumental task: multiple classes had to be consulted for even the most basic of operations. In fact, the logic of the system was so deeply wrapped that understanding a piece of code in some cases required days of wading through related files.

The most general rule of thumb of Python programming applies here, too: don't make things complicated unless they truly must be. Wrapping your code in multiple layers of classes to the point of incomprehensibility is always a bad idea. Abstraction is the basis of polymorphism and encapsulation, and it can be a very effective tool when used well. However, you'll simplify debugging and aid maintainability if you make your class interfaces intuitive, avoid making your code overly abstract, and keep your class hierarchies short and flat unless there is a good reason to do otherwise.

			30.5.5.7 Chapter Summary
This chapter presented a handful of advanced class-related topics, including subclassing built-in types, pseudoprivate attributes, new-style classes, static methods, and function decorators. Most of these are optional extensions to the OOP model in Python, but they may become more useful as you start writing larger object-oriented programs.

This is the end of the class part of this book, so you'll find the usual lab exercises at the end of the chapter—be sure to work through them to get some practice coding real classes. In the next chapter, we'll begin our look at our last core language topic, exceptions. Exceptions are Python's mechanism for communicating errors and other conditions to your code. This is a relatively lightweight topic, but I've saved it for last because exceptions are supposed to be coded as classes today. Before we tackle that final subject, though, take a look at this chapter's quiz and the lab exercises.

BRAIN BUILDER

Chapter Quiz

Q:	Name two ways to extend a built-in object type.
Q:	What are function decorators used for?
Q:	How do you code a new-style class?
Q:	How are new-style and classic classes different?
Q:	How are normal and static methods different?
Q:	How long should you wait before lobbing a "Holy Hand Grenade"?



Quiz Answers

A:	You can embed a built-in object in a wrapper class, or subclass the built-in type directly. The latter of these tends to be simpler, as most original behavior is automatically inherited.
A:	Function decorators are generally used to add to an existing function a layer of logic that is run each time the function is called. They can be used to log or count calls to a function, check its argument types, and so on. They are also used to "declare" static methods—simple functions in a class that are not passed an instance.
A:	New-style classes are coded by inheriting from the object built-in class (or any other built-in type). In Python 3.0, this will likely not be required; all classes will be new-style classes by default.
A:	New-style classes search the diamond pattern of multiple inheritance trees differently—they essentially search breadth-first (across), instead of depth-first (up). New-style classes also support a set of advanced extra tools, including properties and a __slots__ instance attributes list.
A:	Normal (instance) methods receive a self argument (the implied instance), but static methods do not. Static methods are simple functions nested in a class object. To make a method static, it must be run through a special built-in function, or decorated with decorator syntax.
A:	Three seconds. (Or, more accurately: "And the Lord spake, saying, 'First shalt thou take out the Holy Pin. Then, shalt thou count to three, no more, no less. Three shalt be the number thou shalt count, and the number of the counting shall be three. Four shalt thou not count, nor either count thou two, excepting that thou then proceed to three. Five is right out. Once the number three, being the third number, be reached, then lobbest thou thy Holy Hand Grenade of Antioch towards thy foe, who, being naughty in my sight, shall snuff it.'")[77]

    [77] * This quote is from Monty Python and the Holy Grail.

			30.5.5.8 BRAIN BUILDER 

				30.5.5.8.1 Exercises

These exercises ask you to write a few classes, and experiment with some existing code. Of course, the problem with existing code is that it must be existing. To work with the set class in exercise 5, either pull the class source code off the Internet (see the Preface), or type it up by hand (it's fairly brief). These programs are starting to get more sophisticated, so be sure to check the solutions at the end of the book for pointers. You'll find them in in Appendix B, in "Section B.6."

					30.5.5.8.1.1 Inheritance. 
					Write a class called Adder that exports a method add(self, x, y) that prints a "Not Implemented" message. Then, define two subclasses of Adder that implement the add method:


    ListAdder

        With an add method that returns the concatenation of its two list arguments.

    DictAdder

        With an add method that returns a new dictionary containing the items in both its two dictionary arguments (any definition of addition will do).

    Experiment by making instances of all three of your classes interactively and calling their add methods.

    Now, extend your Adder superclass to save an object in the instance with a constructor (e.g., assign self.data a list or a dictionary), and overload the + operator with an __add__ method to automatically dispatch to your add methods (e.g., X + Y triggers X.add(X.data,Y)). Where is the best place to put the constructors and operator overloading methods (i.e., in which classes)? What sorts of objects can you add to your class instances?

    In practice, you might find it easier to code your add methods to accept just one real argument (e.g., add(self,y)), and add that one argument to the instance's current data (e.g., self.data + y). Does this make more sense than passing two arguments to add? Would you say this makes your classes more "object-oriented"?

					30.5.5.8.1.2 Operator overloading. 
					Write a class called Mylist that shadows ("wraps") a Python list: it should overload most list operators and operations, including +, indexing, iteration, slicing, and list methods such as append and sort. See the Python reference manual for a list of all possible methods to support. Also, provide a constructor for your class that takes an existing list (or a Mylist instance) and copies its components into an instance member. Experiment with your class interactively. Things to explore:

        Why is copying the initial value important here?

        Can you use an empty slice (e.g., start[:]) to copy the initial value if it's a Mylist instance?

        Is there a general way to route list method calls to the wrapped list?

        Can you add a Mylist and a regular list? How about a list and a Mylist instance?

        What type of object should operations like + and slicing return? What about indexing operations?

        If you are working with a more recent Python release (version 2.2 or later), you may implement this sort of wrapper class by embedding a real list in a standalone class, or by extending the built-in list type with a subclass. Which is easier, and why?

					30.5.5.8.1.3 Subclassing. 
					Make a subclass of Mylist from exercise 2 called MylistSub, which extends Mylist to print a message to stdout before each overloaded operation is called, and counts the number of calls. MylistSub should inherit basic method behavior from Mylist. Adding a sequence to a MylistSub should print a message, increment the counter for + calls, and perform the superclass' method. Also, introduce a new method that prints the operation counters to stdout, and experiment with your class interactively. Do your counters count calls per instance, or per class (for all instances of the class)? How would you program both of these? (Hint: it depends on which object the count members are assigned to: class members are shared by instances, but self members are per-instance data.)

					30.5.5.8.1.4 Metaclass methods. 
					Write a class called Meta with methods that intercept every attribute qualification (both fetches and assignments), and print messages listing their arguments to stdout. Create a Meta instance, and experiment with qualifying it interactively. What happens when you try to use the instance in expressions? Try adding, indexing, and slicing the instance of your class.

					30.5.5.8.1.5 Set objects. 
					Experiment with the set class described in "Section 26.1.1." Run commands to do the following sorts of operations:

        Create two sets of integers, and compute their intersection and union by using & and | operator expressions.

        Create a set from a string, and experiment with indexing your set. Which methods in the class are called?

        Try iterating through the items in your string set using a for loop. Which methods run this time?

        Try computing the intersection and union of your string set, and a simple Python string. Does it work?

        Now, extend your set by subclassing to handle arbitrarily many operands using the *args argument form. (Hint: see the function versions of these algorithms in Chapter 16.) Compute intersections and unions of multiple operands with your set subclass. How can you intersect three or more sets, given that & has only two sides?

        How would you go about emulating other list operations in the set class? (Hints: __add__ can catch concatenation, and __getattr__ can pass most list method calls off to the wrapped list.)

					30.5.5.8.1.6 Class tree links. 
					In "Section 24.5" in Chapter 24, and "Section 25.6" in Chapter 25, I mentioned that classes have a __bases__ attribute that returns a tuple of their superclass objects (the ones in parentheses in the class header). Use __bases__ to extend the Lister mix-in class (see Chapter 25) so that it prints the names of the immediate superclasses of the instance's class. When you're done, the first line of the string representation should look like this (your address may vary):


        <Instance of Sub(Super, Lister), address 7841200:


    How would you go about listing inherited class attributes, too? (Hint: classes have a __dict__ attribute.) Try extending your Lister class to display all accessible superclasses and their attributes as well; for hints on climbing class trees, see Chapter 24's classtree.py example, and the footnote about using dir and getattr in Python 2.2 in the "Section 25.6" section of Chapter 25.

					30.5.5.8.1.7 Composition. 
					Simulate a fast-food ordering scenario by defining four classes:


    Lunch

        A container and controller class.

    Customer

        The actor who buys food.

    Employee

        The actor from whom a customer orders.

    Food

        What the customer buys.

    To get you started, here are the classes and methods you'll be defining:
    Code View: Scroll / Show All


        class Lunch:
            def __init__(self)                 # Make/embed Customer and Employee
            def order(self, foodName)        # Start a Customer order simulation
            def result(self)                 # Ask the Customer what kind of Food it has

        class Customer:
            def __init__(self)                        # Initialize my food to None
            def placeOrder(self, foodName, employee)      # Place order with an Employee
            def printFood(self)                           # Print the name of my food

        class Employee:
            def takeOrder(self, foodName)    # Return a Food, with requested name

        class Food:
            def __init__(self, name)           # Store food name

    					  


    The order simulation works as follows:

        The Lunch class' constructor should make and embed an instance of Customer and an instance of Employee, and should export a method called order. When called, this order method should ask the Customer to place an order by calling its placeOrder method. The Customer's placeOrder method should in turn ask the Employee object for a new Food object by calling Employee's takeOrder method.

        Food objects should store a food name string (e.g., "burritos"), passed down from Lunch.order, to Customer.placeOrder, to Employee.takeOrder, and finally, to Food's constructor. The top-level Lunch class should also export a method called result, which asks the customer to print the name of the food it received from the Employee via the order (this can be used to test your simulation).

    Note that Lunch needs to pass either the Employee or itself to the Customer to allow the Customer to call Employee methods.

    Experiment with your classes interactively by importing the Lunch class, calling its order method to run an interaction, and then calling its result method to verify that the Customer got what he or she ordered. If you prefer, you can also simply code test cases as self-test code in the file where your classes are defined, using the module __name__ trick in Chapter 21. In this simulation, the Customer is the active agent; how would your classes change if Employee were the object that initiated customer/employee interaction instead?

					30.5.5.8.1.8 Zoo animal hierarchy. 
					Consider the class tree shown in Figure 26-1. Code a set of six class statements to model this taxonomy with Python inheritance. Then, add a speak method to each of your classes that prints a unique message, and a reply method in your top-level Animal superclass that simply calls self.speak to invoke the category-specific message printer in a subclass below (this will kick off an independent inheritance search from self). Finally, remove the speak method from your Hacker class so that it picks up the default above it. When you're finished, your classes should work this way:

    Figure 26-1. A zoo hierarchy composed of classes linked into a tree to be searched by attribute inheritance. Animal has a common "reply" method, but each class may have its own custom "speak" method called by "reply."


        % python 
        >>> from zoo import Cat, Hacker 
        >>> spot = Cat(  ) 
        >>> spot.reply(  )  # Animal.reply; calls Cat.speak
        meow
        >>> data = Hacker(  )  # Animal.reply; calls Primate.speak
        >>> data.reply(  ) 
        Hello world!


					30.5.5.8.1.9 The Dead Parrot Sketch. 
					Consider the object embedding structure captured in Figure 26-2. Code a set of Python classes to implement this structure with composition. Code your Scene object to define an action method, and embed instances of Customer, Clerk, and Parrot classes—all three of which should define a line method that prints a unique message. The embedded objects may inherit from a common superclass that defines line and simply provide message text, or define line themselves. In the end, your classes should operate like this:


        % python 
        >>> import parrot 
        >>> parrot.Scene(  ).action(  )  # Activate nested objects
        customer: "that's one ex-bird!"
        clerk: "no it isn't..."
        parrot: None


Figure 26-2. A scene composite with a controller class (Scene) that embeds and directs instances of three other classes (Customer, Clerk, Parrot). The embedded instance's classes may also participate in an inheritance hierarchy; composition and inheritance are often equally useful ways to structure classes for code reuse.


Why You Will Care: OOP by the Masters

When I teach Python classes, I invariably find that about halfway through the class, people who have used OOP in the past are following along intensely, while people who have not are beginning to glaze over (or nod off completely). The point behind the technology just isn't apparent.

In a book like this, I have the luxury of including material like the new Big Picture overview in Chapter 22—in fact, you should probably review that section if you're starting to feel like OOP is just some computer science mumbo-jumbo.

In real classes, however, to help get the newcomers on board (and keep them awake), I have been known to stop and ask the experts in the audience why they use OOP. The answers they've given might help shed some light on the purpose of OOP, if you are new to the subject.

Here, then, with only a few embellishments, are the most common reasons to use OOP, as cited by my students over the years:


Code reuse

    This one's easy (and is the main reason for using OOP). By supporting inheritance, classes allow you to program by customization, instead of starting each project from scratch.

Encapsulation

    Wrapping up implementation details behind object interfaces insulates users of a class from code changes.

Structure

    Classes provide new local scopes, which minimizes name clashes. They also provide a natural place to write and look for implementation code, and to manage object state.

Maintenance

    Classes naturally promote code factoring, which allows us to minimize redundancy. Thanks both to the structure and code reuse support of classes, usually only one copy of the code needs to be changed.

Consistency

    Classes and inheritance allow you to implement common interfaces, and hence, a common look and feel in your code; this eases debugging, comprehension, and maintenance.

Polymorphism

    This is more a property of OOP than a reason for using it, but by supporting code generality, polymorphism makes code more flexible and widely applicable, and hence, more reusable.

Other

    And, of course, the number one reason students gave for using OOP: it looks good on a résumé! (OK, I threw this one in as a joke, but it is important to be familiar with OOP if you plan to work in the software field today.)

Finally, keep in mind what I said at the beginning of Part 6: you won't fully appreciate OOP until you've used it for awhile. Pick a project, study larger examples, work through the exercises—do whatever it takes to get your feet wet with OO code; it's worth the effort.

				30.5.5.8.2 Answers

					30.5.5.8.2.1 Inheritance. 
					Here's the solution code for this exercise (file adder.py), along with some interactive tests. The __add__ overload has to appear only once, in the superclass, as it invokes type-specific add methods in subclasses:
Code View: Scroll / Show All


class Adder:
    def add(self, x, y):
        print 'not implemented!'
    def __init__(self, start=[]):
        self.data = start
    def __add__(self, other):                 # Or in subclasses?
        return self.add(self.data, other)         # Or return type?

class ListAdder(Adder):
    def add(self, x, y):
        return x + y

class DictAdder(Adder):
    def add(self, x, y):
        new = {}
        for k in x.keys(  ): new[k] = x[k]
        for k in y.keys(  ): new[k] = y[k]
        return new

% python 
>>> from adder import * 
>>> x = Adder(  ) 
>>> x.add(1, 2) 
not implemented!
>>> x = ListAdder(  ) 
>>> x.add([1], [2]) 
[1, 2]
>>> x = DictAdder(  ) 
>>> x.add({1:1}, {2:2}) 
{1: 1, 2: 2}

>>> x = Adder([1]) 
>>> x + [2] 
not implemented!
>>>
>>> x = ListAdder([1]) 
>>> x + [2] 
[1, 2]
>>> [2] + x 
Traceback (innermost last):
  File "<stdin>", line 1, in ?
TypeError: __add__ nor __radd__ defined for these operands

					  


Notice in the last test that you get an error for expressions where a class instance appears on the right of a +; if you want to fix this, use __radd__ methods, as described in "Section 24.4" in Chapter 24.

If you are saving a value in the instance anyhow, you might as well rewrite the add method to take just one argument, in the spirit of other examples in Part 6:
Code View: Scroll / Show All


class Adder:
    def __init__(self, start=[]):
        self.data = start
    def __add__(self, other):          # Pass a single argument
        return self.add(other)             # The left side is in self
    def add(self, y):
        print 'not implemented!'

class ListAdder(Adder):
    def add(self, y):
        return self.data + y

class DictAdder(Adder):
    def add(self, y):
        pass                           # Change me to use self.data instead of x

x = ListAdder([1, 2, 3])
y = x + [4, 5, 6]
print y                                # Prints [1, 2, 3, 4, 5, 6]

					  


Because values are attached to objects rather than passed around, this version is arguably more object-oriented. And, once you've gotten to this point, you'll probably find that you can get rid of add altogether, and simply define type-specific __add__ methods in the two subclasses.

					30.5.5.8.2.2 Operator overloading. 
					The solution code (file mylist.py) uses a few operator overloading methods that the text didn't say much about, but they should be straightforward to understand. Copying the initial value in the constructor is important because it may be mutable; you don't want to change or have a reference to an object that's possibly shared somewhere outside the class. The __getattr__ method routes calls to the wrapped list. For hints on an easier way to code this in Python 2.2 and later, see "Section 26.1.2" in Chapter 26:
Code View: Scroll / Show All


class MyList:
    def __init__(self, start):
        #self.wrapped = start[:]        # Copy start: no side effects
        self.wrapped = []               # Make sure it's a list here
        for x in start: self.wrapped.append(x)
    def __add__(self, other):
        return MyList(self.wrapped + other)
    def __mul__(self, time):
        return MyList(self.wrapped * time)
    def __getitem__(self, offset):
        return self.wrapped[offset]
    def __len__(self):
        return len(self.wrapped)
    def __getslice__(self, low, high):
        return MyList(self.wrapped[low:high])
    def append(self, node):
        self.wrapped.append(node)
    def __getattr__(self, name):        # Other members: sort/reverse/etc
        return getattr(self.wrapped, name)
    def __repr__(self):
        return repr(self.wrapped)

if __name__ == '__main__':
    x = MyList('spam')
    print x
    print x[2]
    print x[1:]
    print x + ['eggs']
    print x * 3
    x.append('a')
    x.sort(  )
    for c in x: print c,

% python mylist.py 
['s', 'p', 'a', 'm']
a
['p', 'a', 'm']
['s', 'p', 'a', 'm', 'eggs']
['s', 'p', 'a', 'm', 's', 'p', 'a', 'm', 's', 'p', 'a', 'm']
a a m p s

					  


Note that it's important to copy the start value by appending instead of slicing here because otherwise the result may not be a true list, and so will not respond to expected list methods, such as append (e.g., slicing a string returns another string, not a list). You would be able to copy a MyList start value by slicing because its class overloads the slicing operation, and provides the expected list interface; however, you need to avoid slice-based copying for objects such as strings. Also, note that sets are a built-in type in Python today, so this is largely just a coding exercise (see Chapter 5 for more on sets).

					30.5.5.8.2.3 Subclassing. 
					My solution (mysub.py) appears below. Your solution should be similar:
Code View: Scroll / Show All


from mylist import MyList

class MyListSub(MyList):
    calls = 0                                       # Shared by instances

    def __init__(self, start):
        self.adds = 0                               # Varies in each instance
        MyList.__init__(self, start)

    def __add__(self, other):
        MyListSub.calls = MyListSub.calls + 1       # Class-wide counter
        self.adds = self.adds + 1                   # Per-instance counts
        return MyList.__add__(self, other)

    def stats(self):
        return self.calls, self.adds                # All adds, my adds

if __name__ == '__main__':
    x = MyListSub('spam')
    y = MyListSub('foo')
    print x[2]
    print x[1:]
    print x + ['eggs']
    print x + ['toast']
    print y + ['bar']
    print x.stats(  )

% python mysub.py 
a
['p', 'a', 'm']
['s', 'p', 'a', 'm', 'eggs']
['s', 'p', 'a', 'm', 'toast']
['f', 'o', 'o', 'bar']
(3, 2)

					  


					30.5.5.8.2.4 Metaclass methods. 
					I worked through this exercise as follows. Notice that operators try to fetch attributes through __getattr__, too; you need to return a value to make them work:
Code View: Scroll / Show All


>>> class Meta:
...     def __getattr__(self, name):
...         print 'get', name
...     def __setattr__(self, name, value):
...         print 'set', name, value
...
>>> x = Meta(  )
>>> x.append
get append
>>> x.spam = "pork"
set spam pork
>>>
>>> x + 2
get __coerce__
Traceback (innermost last):
  File "<stdin>", line 1, in ?
TypeError: call of non-function
>>>
>>> x[1]
get __getitem__
Traceback (innermost last):
  File "<stdin>", line 1, in ?
TypeError: call of non-function

>>> x[1:5]
get __len__
Traceback (innermost last):
  File "<stdin>", line 1, in ?
TypeError: call of non-function

					  


					30.5.5.8.2.5 Set objects. 
					Here's the sort of interaction you should get. Comments explain which methods are called.


% python 
>>> from setwrapper import Set 
>>> x = Set([1, 2, 3, 4])  # Runs __init__
>>> y = Set([3, 4, 5]) 

>>> x & y  # __and__, intersect, then __repr__
Set:[3, 4]
>>> x | y  # __or__, union, then __repr__
Set:[1, 2, 3, 4, 5]

>>> z = Set("hello")  # __init__ removes duplicates
>>> z[0], z[-1]  # __getitem__
('h', 'o')

>>> for c in z: print c,  # __getitem__
...
h e l o
>>> len(z), z  # __len__, __repr__
(4, Set:['h', 'e', 'l', 'o'])

>>> z & "mello", z | "mello" 
(Set:['e', 'l', 'o'], Set:['h', 'e', 'l', 'o', 'm'])


My solution to the multiple-operand extension subclass looks like the class below (file multiset.py). It only needs to replace two methods in the original set. The class' documentation string explains how it works:
Code View: Scroll / Show All


from setwrapper import Set

class MultiSet(Set):
    """
    inherits all Set names, but extends intersect
    and union to support multiple operands; note
    that "self" is still the first argument (stored
    in the *args argument now); also note that the
    inherited & and | operators call the new methods
    here with 2 arguments, but processing more than
    2 requires a method call, not an expression:
    """

    def intersect(self, *others):
        res = []
        for x in self:                         # Scan first sequence
            for other in others:               # For all other args
                if x not in other: break       # Item in each one?
            else:                              # No: break out of loop
                res.append(x)                  # Yes: add item to end
        return Set(res)

    def union(*args):                          # Self is args[0]
        res = []
        for seq in args:                       # For all args
            for x in seq:                      # For all nodes
                if not x in res:
                    res.append(x)              # Add new items to result
        return Set(res)

					  


Your interaction with the extension will look something like the following. Note that you can intersect by using & or calling intersect, but you must call intersect for three or more operands; & is a binary (two-sided) operator. Also, note that we could have called MultiSet simply Set to make this change more transparent if we used setwrapper.Set to refer to the original within multiset:


>>> from multiset import * 
>>> x = MultiSet([1,2,3,4]) 
>>> y = MultiSet([3,4,5]) 
>>> z = MultiSet([0,1,2]) 

>>> x & y, x | y  # Two operands
(Set:[3, 4], Set:[1, 2, 3, 4, 5])

>>> x.intersect(y, z)  # Three operands
Set:[]
>>> x.union(y, z) 
Set:[1, 2, 3, 4, 5, 0]
>>> x.intersect([1,2,3], [2,3,4], [1,2,3])  # Four operands
Set:[2, 3]
>>> x.union(range(10))  # NonmultiSets work, too
Set:[1, 2, 3, 4, 0, 5, 6, 7, 8, 9]


					30.5.5.8.2.6 Class tree links. 
					Here is the way I changed the Lister class, and a rerun of the test to show its format. To display inherited class attributes, too, you'd need to do something like what the attrnames method currently does, but recursively, at each class reached by climbing __bases__ links. Because dir includes inherited attributes in Python 2.2, you might also simply loop through its result: say for x in dir(self) and use getattr(self,x). This won't directly help, though, if you wish to represent the class tree's structure in your display (as in the classtree.py example in Chapter 24):
Code View: Scroll / Show All


class Lister:
    def __repr__(self):
        return ("<Instance of %s(%s), address %s:\n%s>" %
                          (self.__class__.__name__,        # My class's name
                           self.supers(  ),                  # My class's supers
                           id(self),                         # My address
                           self.attrnames(  )) )             # name=value list
    def attrnames(self):
        ...unchanged ...
    def supers(self):
        result = ""
        first = 1
        for super in self.__class__.__bases__:           # One level up
from class
            if not first:
                result = result + ", "
            first = 0
            result = result + super.__name__             # name, not repr(super)
        return result

C:\python\examples> python testmixin.py 
<Instance of Sub(Super, Lister), address 7841200:
        name data3=42
        name data2=eggs
        name data1=spam
>

					  


					30.5.5.8.2.7 Composition. 
					My solution is below (file lunch.py), with comments from the description mixed in with the code. This is one case where it's probably easier to express a problem in Python than it is in English:
Code View: Scroll / Show All


class Lunch:
    def __init__(self):                       # Make/embed Customer and Employee
        self.cust = Customer(  )
        self.empl = Employee(  )
    def order(self, foodName):                 # Start a Customer order simulation
        self.cust.placeOrder(foodName, self.empl)
    def result(self):                          # Ask the Customer about its Food
        self.cust.printFood(  )
class Customer:
    def __init__(self):                         # Initialize my food to None
        self.food = None
    def placeOrder(self, foodName, employee):  # Place order with Employee
        self.food = employee.takeOrder(foodName)
    def printFood(self):                       # Print the name of my food
        print self.food.name

class Employee:
    def takeOrder(self, foodName):             # Return a Food, with requested name
        return Food(foodName)

class Food:
    def __init__(self, name):                  # Store food name
        self.name = name

if __name__ == '__main__':
    x = Lunch(  )                                # Self-test code
    x.order('burritos')                          # If run, not imported
    x.result(  )
    x.order('pizza')
    x.result(  )

% python lunch.py 
burritos
pizza

					  


					30.5.5.8.2.8 Zoo animal hierarchy. 
					Here is the way I coded the taxonomy on Python (file zoo.py); it's artificial, but the general coding pattern applies to many real structures, from GUIs to employee databases. Notice that the self.speak reference in Animal triggers an independent inheritance search, which finds speak in a subclass. Test this interactively per the exercise description. Try extending this hierarchy with new classes, and making instances of various classes in the tree:


class Animal:
    def reply(self):   self.speak(  )           # Back to subclass
    def speak(self):   print 'spam'             # Custom message

class Mammal(Animal):
    def speak(self):   print 'huh?'

class Cat(Mammal):
    def speak(self):   print 'meow'

class Dog(Mammal):
    def speak(self):   print 'bark'

class Primate(Mammal):
    def speak(self):   print 'Hello world!'

class Hacker(Primate): pass                   # Inherit from Primate


					30.5.5.8.2.9 The Dead Parrot Sketch. 
					Here's how I implemented this one (file parrot.py). Notice how the line method in the Actor superclass works: by accessing self attributes twice, it sends Python back to the instance twice, and hence invokes two inheritance searches—self.name and self.says( ) find information in the specific subclasses:


class Actor:
    def line(self): print self.name + ':', repr(self.says(  ))

class Customer(Actor):
    name = 'customer'
    def says(self): return "that's one ex-bird!"

class Clerk(Actor):
    name = 'clerk'
    def says(self): return "no it isn't..."

class Parrot(Actor):
    name = 'parrot'
    def says(self): return None

class Scene:
    def __init__(self):
        self.clerk    = Clerk(  )               # Embed some instances
        self.customer = Customer(  )            # Scene is a composite
        self.subject  = Parrot(  )

    def action(self):
        self.customer.line(  )                  # Delegate to embedded
        self.clerk.line(  )
        self.subject.line(  )

				30.5.5.8.3
	30.6. Exceptions and Tools

		30.6.1. Exception Basics
			30.6.1.1 Why Use Exceptions?
			30.6.1.2 Exception Handling: The Short Story
			30.6.1.3 The try/except/else Statement
			30.6.1.4 The try/finally Statement
			30.6.1.5 Unified try/except/finally
			30.6.1.6 The raise Statement
			30.6.1.7 The assert Statement
			30.6.1.8 with/as Context Managers
Context managers, with as, with_as

https://www.geeksforgeeks.org/context-manager-in-python/

Context Manager in Python
Managing Resources : In any programming language, the usage of resources like file operations or database connections is very common. But these resources are limited in supply. Therefore, the main problem lies in making sure to release these resources after usage. If they are not released then it will lead to resource leakage and may cause the system to either slow down or crash. It would be very helpful if user have a mechanism for the automatic setup and teardown of resources.In Python, it can be achieved by the usage of context managers which facilitate the proper handling of resources. The most common way of performing file operations is by using the withkeyword as shown below:
filter_none


brightness_4
# Python program showing  
# a use of with keyword 
  
with open("test.txt") as f:    
    data = f.read()
Let’s take the example of file management. When a file is opened, a file descriptor is consumed which is a limited resource. Only a certain number of files can be opened by a process at a time. The following program demonstrates it.
filter_none


brightness_4
file_descriptors = [] 
for x in range(100000): 
    file_descriptors.append(open('test.txt', 'w'))
Output:
Traceback (most recent call last):
  File "context.py", line 3, in 
OSError: [Errno 24] Too many open files: 'test.txt'
An error message saying that too many files are open. The above example is a case of file descriptor leakage. It happens because there are too many open files and they are not closed. There might be chances where a programmer may forget to close an opened file.
Managing Resources using context manager :
Suppose a block of code raises an exception or if it has a complex algorithm with multiple return paths, it becomes cumbersome to close a file in all the places. Generally in other languages when working with files try-except-finally is used to ensure that the file resource is closed after usage even if there is an exception.Python provides an easy way to manage resources: Context Managers. The with keyword is used. When it gets evaluated it should result in an object that performs context management. Context managers can be written using classes or functions(with decorators).
Creating a Context Manager :
When creating context managers using classes, user need to ensure that the class has the methods: __enter__()and __exit__(). The __enter__() returns the resource that needs to be managed and the __exit__() does not return anything but performs the cleanup operations. First, lets create a simple class called ContextManager to understand the basic structure of creating context managers using classes, as shown below:
filter_none


brightness_4
# Python program creating a 
# context manager 
  
class ContextManager(): 
    def __init__(self): 
        print('init method called') 
          
    def __enter__(self): 
        print('enter method called') 
        return self
      
    def __exit__(self, exc_type, exc_value, exc_traceback): 
        print('exit method called') 
  
  
with ContextManager() as manager: 
    print('with statement block')
Output:
init method called
enter method called
with statement block
exit method called
In this case a ContextManager object is created. This is assigned to the variable after the as keyword i.e manager. On running the above program, the following get executed in sequence:
* __init__()
* __enter__()
* statement body (code inside the with block)
* __exit__()[the parameters in this method are used to manage exceptions]
File management using context manager :
Let’s apply the above concept to create a class that helps in file resource management.The FileManager class helps in opening a file, writing/reading contents and then closing it.
filter_none


brightness_4
# Python program showing 
# file management using  
# context manager 
  
class FileManager(): 
    def __init__(self, filename, mode): 
        self.filename = filename 
        self.mode = mode 
        self.file = None
          
    def __enter__(self): 
        self.file = open(self.filename, self.mode) 
        return self.file
      
    def __exit__(self, exc_type, exc_value, exc_traceback): 
        self.file.close() 
  
# loading a file  
with FileManager('test.txt', 'w') as f: 
    f.write('Test') 
  
print(f.closed)
Output:
True
  File management using context manager and with statement :
On executing the with block, the following operations happen in sequence:
* A FileManager object is created with test.txt as the filename and w(write) as the mode when __init__ method is executed.
* The __enter__ method opens the test.txt file in write mode(setup operation) and returns the FileManager object to variable f.
* The text ‘Test’ is written into the file.
* The __exit__ method takes care of closing the file on exiting the with block(teardown operation). When print(f.closed) is run, the output is True as the FileManager has already taken care of closing the file which otherwise needed to be explicitly done.
Database connection management using context manager :
Let’s create a simple database connection management system. The number of database connections that can be opened at a time is also limited(just like file descriptors). Therefore context managers are helpful in managing connections to the database as there could be chances that the programmer may forget to close the connection.
filter_none


brightness_4
# Python program shows the 
# connection management  
# for MongoDB 
  
from pymongo import MongoClient 
  
class MongoDBConnectionManager(): 
    def __init__(self, hostname, port): 
        self.hostname = hostname 
        self.port = port 
        self.connection = None
  
    def __enter__(self): 
        self.connection = MongoClient(self.hostname, self.port) 
        return self
  
    def __exit__(self, exc_type, exc_value, exc_traceback): 
        self.connection.close() 
  
# connecting with a localhost 
with MongoDBConnectionManager('localhost', '27017') as mongo: 
    collection = mongo.connection.SampleDb.test 
    data = collection.find({'_id': 1}) 
    print(data.get('name'))
  Database connection management using context manager and with statement :
On executing the with block, the following operations happen in sequence:
* A MongoDBConnectionManager object is created with localhost as the hostnamename and 27017 as the port when __init__ method is executed.
* The __enter__ method opens the mongodb connection and returns the MongoDBConnectionManager object to variable mongo.
* The test collection in SampleDb database is accessed and the document with _id=1 is retrieved. The name field of the document is printed.
* The __exit__ method takes care of closing the connection on exiting the with block(teardown operation).
            
			30.6.1.9 Chapter Summary

		30.6.2. Exception Objects
			30.6.2.1 String-Based Exceptions
			30.6.2.2 Class-Based Exceptions
			30.6.2.3 General raise Statement Forms
			30.6.2.4 Chapter Summary

		30.6.3. Designing with Exceptions
			30.6.3.1 Nesting Exception Handlers
			30.6.3.2 Exception Idioms
			30.6.3.3 Exception Design Tips
			30.6.3.4 Exception Gotchas
			30.6.3.5 Core Language Summary
			30.6.3.6 Chapter Summary
			30.6.3.7 BRAIN BUILDER

		30.6.3.
	30.7. Appendixes

		30.7.1. Installation and Configuration
31. Python Pocket reference, 4th edition

	31.1 Common idioms
This section lists common Python coding tricks and general usage hints. Consult the Python Library Reference and Python Language Reference (http://www.python.org/doc/) for further information on topics mentioned here.
Core Language Hints

    * S[:] makes a top-level (shallow) copy of any sequence; copy.deepcopy(X) makes full copies; list(L) and D.copy() copy lists and dictionaries.
    * L[:0]=[X,Y,Z] inserts items at front of list L, in-place.
    * L[len(L):]=[X,Y,Z], L.extend([X,Y,Z]), and L += [X,Y,Z] all insert multiple items at the end of a list, in-place.
    * L.append(X) and X=L.pop() can be used to implement in-place stack operations, where the end of the list is the top of the stack.
    * Use for key in D.keys(): to iterate through dictionaries, or simply for key in D: in version 2.2 and later. In Python 3.0 these two forms are equivalent, since keys is an iterable view.
    * Use for key in sorted(D): to iterate over dictionary keys in sorted fashion in version 2.4 and later; the form K=D.keys(); K.sort(); for key in K: also works in Python 2.X but not Python 3.0, since keys results are view objects, not lists.
    * X=A or B or None assigns X to the first true object among A and B, or None if both are false (i.e., 0 or empty).
    * X,Y = Y,X swaps the values of X and Y.
    * red, green, blue = range(3) assigns integer series.
    * Use try/finally statements to ensure that arbitrary termination code is run; especially useful around locking calls (acquire before the try, release in the finally).
    * Use with/as statements to guarantee that object-specific termination code is run; for objects that support the context manager protocol (e.g., file auto-close, tread lock auto-release).
    * Wrap iterables in a list() call to view all their results interactively in Python 3; this includes range(), map(), zip(), filter(), dict.keys(), and more.

Environment Hints

    * Use if __name__ == '__main__': to add self-test code or a call to a main function at the bottom of module files; true only when file is run, not when it is imported as a library component.
    * To load file contents in a single expression, use data=open('filename').read().
    * To iterate through text files by lines, use for line in file: in version 2.2 and later (in older versions, use for line in file.readlines():).
    * To retrieve command-line arguments, use sys.argv.
    * To retrieve shell environment settings, use os.environ.
    * The standard streams are: sys.stdin, sys.stdout, and sys.stderror.
    * To return a list of files matching a given pattern, use: glob.glob('pattern').
    * To return a list of files and subdirectories on a path, use: os.listdir('.').
    * To walk an entire tree of directories, use os.walk in Python 3.0 and 2.6 (os.path.walk is also available in Python 2.6 only).
    * To run shell commands within Python scripts, you can use os.system('cmdline'), output=os.popen('cmdline', 'r').read(). The latter form reads the spawned programÂ¿s standard output, and may also be used to read line-by-line.
    * Other streams of a spawned command are available via the subprocess module in Python 3.0, and the os.popen2/3/4 calls in Python 2.X only. The os.fork/os.exec* calls have similar effect on Unix-like platforms.
    * To make a file an executable script on Unix-like platforms, add a line like #!/usr/bin/env python or #!/usr/local/bin/python at the top and give the file executable permissions with a chmod command. On Windows, files can be clicked and run directly due to the registry.
    * The dir([object]) function is useful for inspecting attribute namespaces; print(object.__doc__) often gives documentation.
    * The help([object]) function provides interactive help for modules, functions, types, and more; help(str) gives help on the str type; help("module") gives help on modules even if they have not yet been imported; and help("topic") gives help on keywords and other help topics (use "topics" for a list of help topics).
    * print() and input() (known as print and raw_input() in Python 2.X) use sys.stdout and sys.stdin streams: assign to file-like objects to redirect I/O internally, or use the print(..., file=F) form in Python 3.0 (or the print >> F, ... form in Python 2.X).

Usage Hints

    * Use from __future__ import featurename to enable experimental language features that might break existing code.
    * Intuition about performance in Python programs is usually wrong: always measure before optimizing or migrating to C. Use the profile and time modules (as well as cProfile and timeit).
    * See modules unittest (a.k.a. PyUnit) and doctest for automated testing tools shipped with the Python standard library; unittest is a class framework; doctest scans documentation strings for tests and outputs.
    * See the pydoc library module and script shipped with Python for extraction and display of documentation strings associated with modules, functions, classes, and methods.
    * See the section Section 1.15.4, as well as -W in Section 1.5, for details about turning off future-deprecation warnings emitted by the interpreter.
    * See Distutils, PyInstaller, py2exe, eggs, and other tools for Python program distribution options.
    * See PyInstaller and py2exe for turning Python programs into .exe files for Windows.
    * See NumPy, SciPy, and related packages for extensions that turn Python into a numeric-scientific-programming tool with vector objects, etc.
    * See ZODB and others for full-featured OODB support that allows Python native objects to be stored by key, and SQLObject, SQLAlchemy, and others for object relational mappers that allow classes to be used with relational tables.
    * See SWIG (among others) for a tool that can automatically generate glue code for using C and C++ libraries within Python scripts.
    * See IDLE for a development GUI shipped with Python, with syntax-coloring text editors, object browsers, debugging, etc.; see also PythonWin, Komodo, Eclipse, NetBeans, and others for additional IDE options.
    * See Emacs help for tips on editing/running code in the Emacs text editor. Most other editors support Python as well (e.g., auto-indenting, coloring), including VIM and IDLE; see the editorsÂ¿ page at www.python.org.
    * Porting to Python 3.0: use the Â¿3 command-line option in Python 2.6 to issue incompatibility warnings, and see the 2to3 script which automatically converts much 2.X code to run under 3.X Python.

Assorted Hints

    * Important websites to refer to:


      http://www.python.org

          The Python home page

      http://oreilly.com

          The publisherÂ¿s home page

      http://www.python.org/pypi

          Additional third-party Python tools

      http://www.rmi.net/~lutz

          The authorÂ¿s site

    * Python philosophy: import this.
    * You should say spam and eggs instead of foo and bar in Python examples.
    * Always look on the bright side of life.


32. Recipes 

	32.1 Sorting

		32.1.1 Sorting Mini-HOW TO

Original version by Andrew Dalke

Contents

|   1. Sorting Mini-HOW TO
|         1. Backward compatibility note
|         2. Sorting basic data types
|         3. Sorting by keys
|         4. Comparing classes
|         5. Topics to be covered
|         6. See Also

Python lists have a built-in sort() method that modifies the list in-place and a sorted() built-in function that builds a new sorted list from an iterable.

There are many ways to use them to sort data and there doesn't appear to be a single, central place in the various manuals describing them, so I'll do so here.

Backward compatibility note

Many constructs given in the HOWTO assume Python 2.4. Before that, there was no sorted() builtin and list.sort() took no keyword arguments.

Sorting basic data types

A simple ascending sort is very easy: just call the sorted() function. It returns a new sorted list:

>>> print sorted([5, 2, 3, 1, 4])
[1, 2, 3, 4, 5]

You can also use the sort() method of a list. It modifies the list in-place (and returns None to avoid confusion). Usually it's less convenient than sorted() - but if you don't need the original list, it's slightly more efficient.

>>> a = [5, 2, 3, 1, 4]
>>> a.sort()
>>> print a
[1, 2, 3, 4, 5]

Sort takes an optional function which can be called for doing the comparisons. The default sort routine is equivalent to using cmp:

>>> print sorted([5, 2, 3, 1, 4], cmp)
[1, 2, 3, 4, 5]

where cmp() is the built-in function that compares two objects, x and y, and returns a negative number, 0 or a positive number depending on whether x<y, x==y, or x>y. During the course of the sort the relationships must stay the same for the final list to make sense.

If you want, you can define your own function for the comparison. For integers we can do:

>>> def numeric_compare(x, y):
>>>    return x - y
>>>

Note that this does not work for numbers in general, as the comparison function must return integers.

For numbers in general, but a little more understandably:

>>> def numeric_compare(x, y):
>>>    if x > y:
>>>       return 1
>>>    elif x == y:
>>>       return 0
>>>    else:  #x < y
>>>       return -1
>>>
>>> a = [5, 2, 3, 1, 4]
>>> a.sort(numeric_compare)
>>> print a
[1, 2, 3, 4, 5]

By the way, the sort() function won't work if the result of the subtraction is out of range, as in sys.maxint - (-1).

Or, if you don't want to define a new named function you can create an anonymous one using lambda, as in:

>>> a = [5, 2, 3, 1, 4]
>>> a.sort(lambda x, y: x - y)
>>> print a
[1, 2, 3, 4, 5]

Python 2.4 adds three keyword arguments to sort() that simplify many common usages: cmp, key, and reverse. The cmp keyword is for providing a sorting function; the previous examples could be written as:

>>> a.sort(cmp=numeric_compare)
>>> a.sort(cmp=lambda x, y: x - y)

The reverse parameter is a Boolean value; if it's true, the list is sorted into reverse order.

>>> a = [5, 2, 3, 1, 4]
>>> a.sort(reverse=True)
>>> a
[5, 4, 3, 2, 1]

For Python versions before 2.4, you can reverse the sense of the comparison function:

>>> a = [5, 2, 3, 1, 4]
>>> def reverse_numeric(x, y):
>>>     return y - x
>>>
>>> a.sort(reverse_numeric)
>>> a
[5, 4, 3, 2, 1]

(a more general implementation could return cmp(y,x) or -cmp(x,y)).

However, it's faster if Python doesn't have to call a function for every comparison, so the most efficient solution is to do the forward sort first, then use the reverse() method.

>>> a = [5, 2, 3, 1, 4]
>>> a.sort()
>>> a.reverse()
>>> a
[5, 4, 3, 2, 1]

Sorting by keys

Python 2.4's key parameter lets you derive a sorting key for each element of the list, and then sort using the key.

For example, here's a case-insensitive string comparison:

>>> a = "This is a test string from Andrew".split()
>>> a.sort(key=str.lower)
>>> a
['a', 'Andrew', 'from', 'is', 'string', 'test', 'This']

The value of the key parameter should be a function that takes a single argument and returns a key to use for sorting purposes.

Often there's a built-in that will match your needs, such as str.lower(). The operator module contains a number of functions useful for this purpose. For example, you can sort tuples based on their second element using operator.itemgetter():

>>> import operator
>>> L = [('c', 2), ('d', 1), ('a', 4), ('b', 3)]
>>> map(operator.itemgetter(0), L)
['c', 'd', 'a', 'b']
>>> map(operator.itemgetter(1), L)
[2, 1, 4, 3]
>>> sorted(L, key=operator.itemgetter(1))
[('d', 1), ('c', 2), ('b', 3), ('a', 4)]

If the invocation of key returns a tuple, second and subsequent items in the tuple will be treated as sub-keys in the same way that Python generally sorts tuples:

>>> L = [('d', 2), ('a', 4), ('b', 3), ('c', 2)]
>>> sorted(L, key=lambda x:(x[1], x[0]))
[('c', 2), ('d', 2), ('b', 3), ('a', 4)]

To reverse sort based on the second item in each tuple, but forward sort based on the first item when a tie is met, then forward sort first and then reverse sort based on 2nd item:

>>> L = [('a', 2), ('d', 4), ('b', 3), ('c', 2)]
>>> L.sort(); L
[('a', 2), ('b', 3), ('c', 2), ('d', 4)]
>>> sorted(L, key=operator.itemgetter(1), reverse=True)
[('d', 4), ('b', 3), ('a', 2), ('c', 2)]

Versions of Python before 2.4 don't have the convenient key parameter of sort(), so you have to write a comparison function that embodies the key-generating logic:

>>> a = "This is a test string from Andrew".split()
>>> a.sort(lambda x, y: cmp(x.lower(), y.lower()))
>>> print a
['a', 'Andrew', 'from', 'is', 'string', 'test', 'This']

This goes through the overhead of converting a word to lower case every time it must be compared, roughly O(n lg n) times. Python 2.4's key parameter is called once for each item in the list, which is O(n) and therefore more efficient. You can manually perform the same optimization by computing the keys once and using those values to control the sort order:

>>> words = "This is a test string from Andrew.".split()
>>> deco = [ (word.lower(), i, word) for i, word in enumerate(words) ]
>>> deco.sort()
>>> new_words = [ word for _, _, word in deco ]
>>> print new_words
['a', 'Andrew.', 'from', 'is', 'string', 'test', 'This']

This idiom is called Decorate-Sort-Undecorate after its three steps:

    * First, the initial list is decorated with new values that control the sort order.
    * Second, the decorated list is sorted.
    * Finally, the decorations are removed, creating a list that contains only the initial values in the new order. 

This idiom works because tuples are compared lexicographically; the first items are compared; if they are the same then the second items are compared, and so on.

It is not strictly necessary in all cases to include the index i in the decorated list. Including it gives two benefits:

    * The sort is stable - if two items have the same key, their order will be preserved in the sorted list.
    *

      The original items do not have to be comparable because the ordering of the decorated tuples will be determined by at most the first two items. So for example the original list could contain complex numbers which cannot be sorted directly. 

Another name for this idiom is Schwartzian transform, after Randal L. Schwartz, who popularized it among Perl programmers.

For large lists and lists where the comparison information is expensive to calculate, and Python versions < 2.4, DSU is likely to be the fastest way to sort the list.

Comparing classes

The comparison for two basic data types, like ints to ints or string to string, is built into Python and makes sense. There is a default way to compare class instances, but the default manner isn't usually very useful. You can define your own comparison with the __cmp__ method, as in:

>>> class Spam:
>>>     def __init__(self, spam, eggs):
>>>         self.spam = spam
>>>         self.eggs = eggs
>>>     def __cmp__(self, other):
>>>         return cmp(self.spam+self.eggs, other.spam+other.eggs)
>>>     def __str__(self):
>>>         return str(self.spam + self.eggs)
>>>
>>> a = [Spam(1, 4), Spam(9, 3), Spam(4, 6)]
>>> a.sort()
>>> for spam in a:
>>>     print spam
5
10
12

Sometimes you may want to sort by a specific attribute of a class. If appropriate you should just define the __cmp__ method to compare those values, but you cannot do this if you want to compare between different attributes at different times.

Python 2.4 has an operator.attrgetter() function that makes this easy:

>>> import operator
>>> a = [Spam(1, 4), Spam(9, 3), Spam(4, 6)]
>>> a.sort(key=operator.attrgetter('eggs'))
>>> for spam in a:
>>>   print spam.eggs, spam
3 12
4 5
6 10

In Python 2.4 if you don't want to import the operator module you can:

>>> a = [Spam(1, 4), Spam(9, 3), Spam(4, 6)]
>>> a.sort(key=lambda obj: obj.eggs)
>>> for spam in a:
>>>   print spam.eggs, spam
3 12
4 5
6 10

Again, earlier Python version require you to go back to passing a comparison function to sort, as in:

>>> a = [Spam(1, 4), Spam(9, 3), Spam(4, 6)]
>>> a.sort(lambda x, y: cmp(x.eggs, y.eggs))
>>> for spam in a:
>>>   print spam.eggs, spam
3 12
4 5
6 10

If you want to compare two arbitrary attributes (and aren't overly concerned about performance) you can even define your own comparison function object. This uses the ability of a class instance to emulate a function by defining the __call__ method, as in:

>>> class CmpAttr:
>>>     def __init__(self, attr):
>>>         self.attr = attr
>>>     def __call__(self, x, y):
>>>         return cmp(getattr(x, self.attr), getattr(y, self.attr))
>>>
>>> a = [Spam(1, 4), Spam(9, 3), Spam(4,6)]
>>> a.sort(CmpAttr("spam"))  # sort by the "spam" attribute
>>> for spam in a:
>>>    print spam.spam(), spam.eggs(), spam
1 4 5
4 6 10
9 3 12

>>> a.sort(CmpAttr("eggs"))   # re-sort by the "eggs" attribute
>>> for spam in a:
>>>    print spam.spam(), spam.eggs(), spam
9 3 12
1 4 5
4 6 10

Of course, if you want a faster sort you can extract the attributes into an intermediate list and sort that list.

So, there you have it; about a half-dozen different ways to define how to sort a list:

|   1. sort using the default method
|   2. sort using a comparison function
|   3. reverse sort not using a comparison function
|   4. sort on an intermediate list (two forms)
|   5.
|
|      sort using class defined cmp method
|   6. sort using a sort function object 

Topics to be covered

    * Rich comparisons
          o With custom comparisons, it is possible to create

            insane combinations, such as ((x < y) and (x == y)) or ((x < y) and not (x <= y)).
          o

            The most important comparisons are eq (but be sure to update hash as well), and lt (which is used by the sorting algorithms in practice). 
    * Sorting stability
          o Python sorts are stable. Guido has indicated that this is a promise of the language. Therefore, if x == y, sorted ([x, y]) returns [x, y] but sorted([y, x]) returns [y, x]. 
    * The sorted() function
          o It takes any iterable, and returns a sorted version. If the items in the iterable are not sensibly compared, it will still return a canonical ordering, unless someone went out of their way to prevent one. 
    * Small info about locale aware sorting, eg:
          o

            >>> l = [u'\xc4\x85', u'a', u'z']
            >>> sorted(l, reverse=False)
            ['a', 'z', '\xc4\x85']
            >>> import locale
            >>> locale.setlocale(locale.LC_ALL, "pl_PL.UTF-8")
            >>> sorted(l, cmp=locale.strcoll, reverse=False)
            ['a', '\xc4\x85', 'z']

See Also

    *

      SortingListsOfDictionaries

		32.1.2 "Sort" Dictionaries

			32.1.2.1 Python 2.4
 
			# Example from PEP 265 - Sorting Dictionaries By Value
#    Counting occurences of letters

d = {'a':2, 'b':23, 'c':5, 'd':17, 'e':1}

# operator.itemgetter is new in Python 2.4
#  `itemgetter(index)(container)` is equivalent to `container[index]`
from operator import itemgetter

# Items sorted by key
#   The new builtin `sorted()` will return a sorted copy of the input iterable.
print sorted(d.items())

# Items sorted by key, in reverse order
#   The keyword argument `reverse` operates as one might expect
print sorted(d.items(), reverse=True)

# Items sorted by value
#    The keyword argument `key` allows easy selection of sorting criteria
print sorted(d.items(), key=itemgetter(1))

# In-place sort still works, and also has the same new features as sorted
items = d.items()
items.sort(key = itemgetter(1), reverse=True)
print items
 
			 
			 
			32.1.2.2
Dictionaries can't be sorted -- a mapping has no ordering! -- so, when you feel the need to sort one, you no doubt want to sort its keys (in a separate list). Sorting (key,value) pairs (items) is simplest, but not fastest. 

 (IMHO) the simplest approach:
def sortedDictValues1(adict):
    items = adict.items()
    items.sort()
    return [value for key, value in items]

# an alternative implementation, which
# happens to run a bit faster for large
# dictionaries on my machine:
def sortedDictValues2(adict):
    keys = adict.keys()
    keys.sort()
    return [dict[key] for key in keys]

# a further slight speed-up on my box
# is to map a bound-method:
def sortedDictValues3(adict):
    keys = adict.keys()
    keys.sort()
    return map(adict.get, keys)
			 
			 
The concept of 'sort' applies only to a collection which has _order_ -- a sequence; a mapping (e.g. a dictionary) has NO order, thus it cannot be sorted. Still, its keys can be extracted as a list, which can then be sorted. The example functions return the values in order of sorted key, which just happens to be the single most frequent actual need corresponding to user questions such as "how do I sort a dictionary":-)

The implementation choices are interesting. Since we are sorting key-value pairs by the key field, then returning the list of value fields, it seems clearest (conceptually simplest) to architect the solution as in the first example: .items, .sort, then a list comprehension to pick the value fields.

However (at least on my machine) this turns out not to be fastest: extracting just the keys, sorting them, then accessing the dictionary for each key in the resulting list comprehension, as in the second example, appears to be speedier.

Furthermore, it is subject to a further, obvious optimization: from the dictionary we can extract just once the bound-method adict.get, which will map each key to the corresponding value, then use builtin function map to build the list obtained by applying this callable to each item in the sorted list of keys. This does indeed provide a further speed-up (again, on my machine).

Simplicity is a great virtue, but the second and third examples aren't really more complicated (or complex) than the first -- just, perhaps, a little bit subtler. They're probably worth using to 'sort' any dictionary, even though their performance advantages are really only measurable for large ones -- because uniformity of idiom is also an important programming virtue!


		32.1.3 Sort lists that contain class objects


			32.1.3.1  Override cmp operator
By adding a __cmp__ method to class Movie, you can sort instance objects in list m1  with list method sort(). Following is an example of a __cmp__ overload in a Vector class that sorts on the x, y and z attributes:


    def __cmp__(self, other, epsilon=0.000001):
        x = abs(self.x-other.x)
        if x < epsilon:
            y = abs(self.y-other.y)
            if y < epsilon:
                return cmp(self.z, other.z)
            return cmp(self.y, other.y)
        return cmp(self.x, other.x)

			32.1.3.2 Pass comparison function to sort()
You can also define your own comparison function and pass it to list method sort(). Example using Point objects: 
pts = [p1,p2,p3,p4]
>>> pts
[Point(5.000000, 5.000000, 5.000000), Point(7.000000, 10.000000, 6.000000), Point(1.000000, 15.000000, 7.000000), Point(20.000000, 20.000000, 20.000000)]
>>> def comp(a, b):
... 	if a.x == b.x:
... 		if a.y == b.y:
... 			return cmp(a.z, b.z)
... 		return cmp(a.y, b.y)
... 	return cmp(a.x, b.x)
... 
>>> pts.sort(comp)
>>> pts
[Point(1.000000, 15.000000, 7.000000), Point(5.000000, 5.000000, 5.000000), Point(7.000000, 10.000000, 6.000000), Point(20.000000, 20.000000, 20.000000)]
>>>
		32.1.4
	32.2 Operator overloading

		32.2.1 equality operator, ==, comparison
http://www.learningpython.com/2008/06/21/operator-overload-learn-how-to-change-the-behavior-of-equality-operators/

Operator Overload! Learn how to change the behavior of equality operators.

 Share

By: Mark Mruss

Note: This article was first published the November 2007 issue of Python Magazine

While the equality operator works great on numbers and strings the fact the way it treats your custom objects really is not that useful. This article looks into overloading the equality operator so that you can easily compare your custom classes.

|   1. Introduction
|   2. Introducing the terms: operators and operator overloading
|   3. A Quick Example of the Default Equality Operator
|   4. Overloading the Equality Operator
|   5. Telling Python that the Comparison has Not Been Implemented
|   6. The Inequality Operator
|   7. Dangers
|   8. Conclusion

Introduction

In my experience as a professional programmer, testing for the equality between two instances of a class is a fairly common task. In other words, you are comparing the data that each class contains and checking whether the data in one class is identical to the data in the other class.

One of the nice features of Python is that it has a default equality operator defined for any custom objects that you create.
The unfortunate thing about this default equality operator is that it doesnt provide the functionality that you expect.
This is because the equality operator (==) actually performs an identity comparison, rather than an equivalence test.
If you were to run the following code:

if (object_one == object_two):

By default Python actually compares whether or not object_one is object_two (this is the same comparison that can be made using the is keyword) instead of determining whether or not object_one is equivalent to object_two.
Fortunately for us, overloading the default equality operator in Python is a relatively easy task.
There are, however, some gotchas and other interesting features of which one should be aware.
Introducing the terms: operators and operator overloading

An operator can be difficult to define,
and like many programming definitions,
sometimes the definition only serves to confuse the matter further.
In general though,
you can think of operators as being very similar to the operators that you encountered in Math class,
such as: 
the + operator,
the - operator,
and so forth.

In Python the following are operators[1]:

+	-	*		/	//	%	<>>	&
|	^	~	<>	< =	>=	==	!=	<>

In programming languages we generally encounter binary operators. This means that each operator takes two operands. An operand serves as input to an operator. For example, in the statement:

2 + 6

+ is a binary operator that takes two operands, 2 and 6 as inputs. Similarly, in this statement:

my_value - 6

- is an operator that takes two operands, my_value and 6 as inputs.

Operator overloading is a programming term that means taking the default behaviour of an operator and overloading it. That is, changing the default implementation of an operator for a given object. An example of this (although something that you should never do) would be to overload the + operator to actually perform subtraction instead when it is applied to your class.
A Quick Example of the Default Equality Operator

Now that the definitions are out of the way, let's look at an example where one might want to overload the equality operator. For this example I will bring back a favourite example from my Computer Science days: the Student class:

class Student(object):

	def __init__(self, name, student_number):
		self.name = name
		self.student_number = student_number

As you can see the Student class has two data members: 1) the student's name, and, 2) her student number.

If we run the following code:

mark = Student("Mark Mruss", 067213)
guido = Student("Guido van Rossum", 000001)
if (mark == guido):
	print "Equal"
else:
	print "Not Equal"

Not Equal will be printed out as you would expect since the two students are clearly not equivalent. But what about this code:

mark = Student("Mark Mruss", 067213)
mark_two = Student("Mark Mruss", 067213)
if (mark == mark_two):
	print "Equal"
else:
	print "Not Equal"

Here, as in the previous example, Not Equal will be printed out. This is because, as mentioned earlier, the default implementation of the equality operator is to perform an identity comparison. In other words, the default equality operator asks, is mark the same object as mark_two? In Python the equality comparison depends on the type of objects being compared. For custom classes that you or I will create, the equality comparison will perform an identity comparison by comparing the objects internal id. In other words, it will only result in True if the objects being compared actually are each other. For example:

student_one = Student("Mark Mruss", 067213)
student_two = student_one
if (student_one == student_two):
	print "Equal"
else:
	print "Not Equal"

Results in Equal being printed out, as would:


student_one = Student("Mark Mruss", 067213)
student_two = student_one
if (id(student_one) == id(student_two)):
	print "Equal"
else:
	print "Not Equal"

Note: The equality comparison for built-in objects and types like numbers, strings, lists, tuples, and mappings behave differently. Numbers are compared arithmetically. The numerical values of the characters within strings are compared arithmetically. The comparison of lists and tuples is simply a comparison of their inner values, while the comparison of mappings are comparisons of an ordered list of their values.[2]
Overloading the Equality Operator

Hopefully the above example illustrated a case where we might want to overload the equality operator to make it so that the following code:

student_one = Student("Mark Mruss", 067213)
student_two = Student("Mark Mruss", 067213)
if (student_one == student_two):
	print "Equal"
else:
	print "Not Equal"

Would result in Equal being printed out, i.e. a true equality comparison as opposed to an identity comparison. In order to do this we need to change to the default functionality of the equality operator. In other words we need to overload it.

In general, operator overloading in Python means adding a special function to your class that will perform the function of the operator it is meant to represent. There are two ways in which one can overload the equality operator in Python: 1) the first method is to use the __eq__ function, a so-called rich comparison function. Rich comparison functions are functions that overload specific comparison operators (i.e. __eq__ to overload ==). 2) The second is to use the __cmp__ function, which is used to overload all comparison operators if no rich comparison functions are present.

Since __cmp__ is used to override all comparison operators (==, !=, < , <=, >, >=), I would suggest using the rich comparison method unless you are using a version of Python that is earlier then version 2.1, or you are convinced that you know what < = means to our Student class. Lets forget about the __cmp__ operator for now and focus on using the rich comparison functions to overload the equality operator.

Rich comparison functions can return any value, but you should try to return a value that is, or can be, interpreted as a boolean value. This is important because these functions will often be used in situations where the return value will be used in a boolean comparison.

When using the rich comparison functions it is important to know which functions are being called internally. For example, when we run:

student_one == student_two

If __eq__ exists in the Student class, the following is actually being called:

student_one.__eq__(student_two)

When we run:

student_two == student_one

The following is actually called:

student_two.__eq__(student_one)

As you can see it is the operand on the left-hand side whose __eq__ function will be called. It is important to note that if the operand on the left-hand side lacks the __eq__ function while the operand on the right-hand side has one, the right-hand operands __eq__ function will not be called.

Lets start off with a simple, but incorrect, example (the reasons for its incorrectness will be explained below):

def __eq__(self, other):
	return ((self.name == other.name)
		and (self.student_number == other.student_number))

This is very straightforward. In the equality comparison, we simply compare the Student class two data members. This performs as expected when we run:

student_one = Student("Mark Mruss", 067213)
student_two = Student("Guido van Rossum", 000001)
student_three = Student("Mark Mruss", 000001)
print (student_one == student_two)
print (student_one == student_three)

You get:

False
True

But what happens when we introduce the Professor class and try the overloaded equality operator:

class Professor(object):

	def __init__(self, instructor, course):
		self.instructor = instructor
		self.course = course

As you can see, the Professor class lacks the name and student_number data members. What happens when we compare an instance of the Professor class with an instance of the Student class?

guido = Student("Guido van Rossum", 000001)
rob = Professor("Rob Ward", "74-300")
print (guido == rob)

It results in something like this:

File "operators.py", line 10, in __eq__
    return ((self.name == other.name)
AttributeError: 'Professor' object has no attribute 'name'

The way we are overriding the equality operator is not correct because it automatically assumes that the other object has the name and student_number data members. There are a number of methods to get around this problem, including: 1) using the hasattr function, or 2) using the isinstance function. Using the hasattr function determines if other has the attributes we are looking for before actually querying them. hasattr simply tells us if an object has a specific attribute or not. Here is a quick example illustrating how to do this:

def __eq__(self, other):
	if (hasattr(other, "name") and hasattr(other, "student_number")):
		return ((self.name == other.name)
			and (self.student_number == other.student_number))
	else:
		return False

First, we check to see if other has the name and student_number attributes. If it does, we proceed as normal. If it does not, we simply return false. When we compare the professor and the student we get False as expected.

Whats nice about this method is that we dont have to care what type other is. We only care whether or not it contains the attributes we need to compare. However, the drawback to this function is that you have to test for the existence of each attribute. Although this may not always be a big deal, if you are dealing with fifty data members in your classes this can quickly become a pain in the neck.

Another solution to the problem with our first overloading example is to use the isinstance function to make sure that other is an instance of our class type. This has the drawback of forcing other to be the same type as your class. In practice however, I believe this to be more of an advantage than a disadvantage.

def __eq__(self, other):
	if (isinstance(other, Student)):
		return ((self.name == other.name)
			and (self.student_number == other.student_number))
	else:
		return False

The first thing we do is check the variable other to make sure that it is an instance of the Student class. If it is, we then compare all of the data members in the Student class. If object is not an instance of the Student class, we return False.

In my opinion, this is the preferred method since knowing that the class is the correct type is often important. The hasattr method seems more appropriate for simple data containers like a rect or vector class where you are only interested in three or four data members.
Telling Python that the Comparison has Not Been Implemented

Up until this point in time we have been returning False when our __eq__ function does not support the type of object passed in as other. While this is acceptable and correct given the Python documentation, it seems to be proper to actually return NotImplemented.
According to the Python documentation, Numeric methods and rich comparison methods may return this value if they do not implement the operation for the operands provided.
(The interpreter will then try the reflected operation, or some other fallback, depending on the operator.)
	Lets forget abou In other words, if the left operand returns NotImplemented, Python will attempt to use the right hand operands equality operator.
	And if that does not exist, Python will fall back to the default equality operator.

We can return NotImplemted from our Student class if the operand passed in is not an instance of the Student:

def _eq__(self, other):
	if (isinstance(other, Student)):
		return ((self.name == other.name)
			and (self.student_number == other.student_number))
	else:
		return NotImplemented

Now if we perform the following comparison:

guido = Student("Guido van Rossum", 000001)
rob = Professor("Rob Ward", "74-300")
print guido == rob

The first step in the processing will be:

guido.__eq__(rob)

This returns NotImplemented. As a result, the reflected operation is attempted:

rob == guido

Because the Professor class does not have the equality operator overloaded, the default operation is executed and False is printed out just like we wanted.

NotImplemented is useful in because instead of returning False, which means that the two operand are not equivalent, you return a value that says that the comparison between the operands has not been implemented.
The Inequality Operator

Now that we know how to overload the equality operator, it stands to reason that we have the opposite operation, the inequality operator (!=) covered as well.
But not so fast.
In Python the inequality and equality operators are handled separately, meaning that inequality is not simply the opposite of equality.
This means that whenever you overload the equality operator, you have to be sure to overload the inequality operator as well.
If you dont you might get some strange results. For example, when we use the current code (without the inequality operator overloaded), the following:

guido = Student("Guido van Rossum", 000001)
guido_too = Student("Guido van Rossum", 000001)
print guido == guido_too
print guido != guido_too

Results in:

True
True

In the first comparison the overloaded equality operator is used, and results in True being printed. Because the inequality operator is not overloaded in the second comparison, the default inequality operator is used (the identity comparison). True is printed because guido and guido_too are not the same instances.

Thankfully once you have overloaded the equality operator, overloading the inequality operator is very easy.
As a general rule, you have to return the opposite of the equality operator, but because we are working with NotImplemented, we have to do a bit more processing to ensure that we dont return False when we really want to return NotImplemented.
Here is how we can overload the inequality operator in the Student class:

def __ne__(self, other):
	equal_result = self.__eq__(other)
	if (equal_result is not NotImplemented):
		return not equal_result
	return NotImplemented

First, we call self.__eq__ to test whether or not we are equal to other. We then check to make sure that equal_result is not NotImplemented. 
If it is not, we know that the equality test was implemented and we can safely return its opposite. If the result for the equality comparison was NotImplemented, we return NotImplemented for the inequality comparison.

Note: It is safe to use the is check on NotImplemented (rather than an isinstance check) because NotImplemented is a singleton, meaning that there is only ever one instance of NotImplemented at anytime.
Dangers

While it may seem like operator overloading should become part of every class that you write, a word of warning is necessary. There is a large school of thought that views operator overloading as a dangerous programming technique. They argue that overloading operators changes the default way that an operator works, and not always correctly. Moreover, instead of overriding the equality operator, one can simply add an is_equal_to function to perform the equality check.

The logic behind this criticism is that when someone is using a class or reading some code that you wrote, they will be unable to tell what the equality operator is doing. For example, if they see:

value = MyClass(10)
value_two = MyClass(10)
print value == value_two

What gets printed out? True or False? If MyClass overrode the equality operator then True will be printed. However, if the equality operator is not overloaded, the standard Python behaviour of equality will result with False being printed out.
Conclusion

While its true that overloading the equality operator does change the default way the Python functions, I feel that its generally a safe and beneficial addition to your classes. Especially since unless people know the ins and outs of the equality operator they will generally assume that should work the way it does when you overload it. Like all the decisions that you make when working with Python, context is key.

[1] http://docs.python.org/ref/operators.html
[2] http://docs.python.org/ref/comparisons.html
[3] http://docs.python.org/ref/customization.html
[4] http://docs.python.org/ref/types.html

	32.3


33. Environment, Custimization, IDLE

	33.1 .pythonrc

		33.1.1 Example 1
Every Python developer knows that the Python shell can speed up the work, but not anybody know how fast it can be.
In Bash you can browse the history of previously typed commands and have autocompletion with the tab key. Why not having the same features in the Python shell too?
History

On UNIX-like systems, Python is shipped with a really useful module in the standard library: readline that, among other things, can be used to enable the history. Its usage is very simple: it just needs a file to read and write the history, just like Bash. Here’s how it should be used:

>>> import os
>>> import readline
>>> history_file = os.path.expanduser('~/.python_history')
>>> readline.read_history_file(history_file)

Note that the history file is not written automatically at exit: you ought to call write_history_file() when you finish to use the interpreter or, better, register a function with atexit:

>>> import atexit
>>> atexit.register(readline.write_history_file, history_file)

Autocompletion

The readline module can be used for autocompletion too, but needs some help from a second module. In fact readline just binds the tab key, but the word completer itself is in rlcompleter.

>>> import rlcompleter
>>> readline.parse_and_bind('tab: complete')

RC file

We have understood how to enable both history and autocompletion, but does this mean that we should type the above commands every time? No, of course. When used in interactive mode, Python always looks into the PYTHONSTARTUP environment variable and executes the specified script, if any. So, you can put the code to enable history and autocompletion in a file (for example: ~/.pythonrc) and specify it in your PYTHONSTARTUP.

Using the RC file you also have the advantage that you can import the modules that you use frequently, so that you won’t have to import the manually in the interpreter. For example, here’s my own Python RC file:

import atexit
import os
import re
import readline
import rlcompleter
import socket
import _socket
import sys
import time
import timeit

history = os.path.expanduser('~/.python_history')
readline.read_history_file(history)
readline.parse_and_bind('tab: complete')
atexit.register(readline.write_history_file, history)

def t(*args):
    return timeit.Timer(*args).timeit()

-> Here's my .pythonrc as of Wed Mar  2 18:40:32 JST 2011

import atexit
import os
import re
import readline
import rlcompleter
import socket
import _socket
import sys
import time
import timeit

history = os.path.expanduser('~/.python_history')
readline.read_history_file(history)
readline.parse_and_bind('tab: complete')
atexit.register(readline.write_history_file, history)

def t(*args):
    return timeit.Timer(*args).timeit()

print "Welcome %s, time is %s. Have a good session :-) "%(os.environ['USER'], time.strftime('%X %x %Z') )

-> Here's my .pythonrc as of Thu Mar  3 11:35:56 JST 2011
import atexit
import os
import re
import readline
import rlcompleter
import socket
import _socket
import sys
import time
import timeit

history = os.path.expanduser('~/.python_history')
readline.read_history_file(history)
readline.parse_and_bind('tab: complete')
atexit.register(readline.write_history_file, history)

def t(*args):
    return timeit.Timer(*args).timeit()

# Color Support
class TermColors(dict):
    """Gives easy access to ANSI color codes. Attempts to fall back to no color
for certain TERM values. (Mostly stolen from IPython.)"""

    COLOR_TEMPLATES = (
        ("Black" , "0;30"),
        ("Red" , "0;31"),
        ("Green" , "0;32"),
        ("Brown" , "0;33"),
        ("Blue" , "0;34"),
        ("Purple" , "0;35"),
        ("Cyan" , "0;36"),
        ("LightGray" , "0;37"),
        ("DarkGray" , "1;30"), ("LightRed" , "1;31"),
        ("LightGreen" , "1;32"),
        ("Yellow" , "1;33"),
        ("LightBlue" , "1;34"),
        ("LightPurple" , "1;35"),
        ("LightCyan" , "1;36"),
        ("White" , "1;37"),
        ("Normal" , "0"),
    )

    NoColor = ''
    _base = '\001\033[%sm\002'

    def __init__(self):
        if os.environ.get('TERM') in ('xterm', 'xterm-color', 'xterm-256color', 'linux',
                                    'screen', 'screen-256color', 'screen-bce'):
            self.update(dict([(k, self._base % v) for k,v in self.COLOR_TEMPLATES]))
        else:
            self.update(dict([(k, self.NoColor) for k,v in self.COLOR_TEMPLATES]))
_c = TermColors()

# Enable Color Prompts
sys.ps1 = '%s>>> %s' % (_c['Green'], _c['Normal'])
sys.ps2 = '%s... %s' % (_c['Cyan'], _c['Normal'])

# Enable Pretty Printing for stdout
import pprint
def my_displayhook(value):
    if value is not None:
        try:
            import __builtin__
            __builtin__._ = value
        except ImportError:
            __builtins__._ = value

        pprint.pprint(value)

sys.displayhook = my_displayhook

##Set VI Editor for interactive console 
#from code import InteractiveConsole
#from tempfile import mkstemp
#
#EDITOR = os.environ.get('EDITOR', 'vim')
#EDIT_CMD = '\e'
#
#class EditableBufferInteractiveConsole(InteractiveConsole):
#    def __init__(self, *args, **kwargs):
#        self.last_buffer = [] # This holds the last executed statement
#        InteractiveConsole.__init__(self, *args, **kwargs)
#
#    def runsource(self, source, *args):
#        self.last_buffer = [ source.encode('latin-1') ]
#        return InteractiveConsole.runsource(self, source, *args)
#
#    def raw_input(self, *args):
#        line = InteractiveConsole.raw_input(self, *args)
#        if line == EDIT_CMD:
#            fd, tmpfl = mkstemp('.py')
#            os.write(fd, b'\n'.join(self.last_buffer))
#            os.close(fd)
#            os.system('%s %s' % (EDITOR, tmpfl))
#            line = open(tmpfl).read()
#            os.unlink(tmpfl)
#            tmpfl = ''
#            lines = line.split( '\n' )
#            for i in range(len(lines) - 1): self.push( lines[i] )
#            line = lines[-1]
#        return line
#
#c = EditableBufferInteractiveConsole(locals=locals())
#c.interact(banner=WELCOME)
#
## Exit the Python shell on exiting the InteractiveConsole
#sys.exit()

print "Welcome %s, time is %s. Have a good session :-) "%(os.environ['USER'], time.strftime('%X %x %Z') )
		33.1.2
	33.2

34. Tips, Tricks, Quick guides

	34.1 Python Tips, Tricks, and Hacks 
Want to write shorter, cleaner code? Have an unfortunate situation where you need to fit as much as you can in one expression? Prefer a quick dose of hacks to spending the rest of your life reading the docs? You've come to the right place. We start out with some quick tricks that you might have figured out if you've spent some time with Python, but I promise there's more crazy stuff towards the bottom.

I've tried to make all of the code snippets executable on their own. If you want, paste them into your Python shell and try them out. You'll notice that many examples contain a 'true example' and a 'false example', with one commented out. Feel free to switch the comment and see what happens.

You'll also see a few blank comments floating around in the code. These are to improve readability by adding line breaks while still allowing the Python interpreter to parse the code. These would not be necessary in a 'real' (non-pasted) program.

A quick distinction about true vs. True in this article: when I say an object is 'true', I mean that the object, if converted to a boolean in an if statement or elsewhere, would be converted to True and not False. I don't mean that the object necessarily is identical to or equal to True. In the same way, if I say an object is 'false', I mean that the object would be converted to False, not that it necessarily is equal to or identical to False.

If you prefer, there is a Russian translation of the first parts of this article.

		34.1.1 1   Quick Tricks

			34.1.1.1 Four Kinds of Quotes

Let's start with something quick that you probably know. If you're coming from a different language, you're probably used to using single quotes for one thing and double quotes for another. Python lets you use both, although not interchangeably (if you start with one, you have to end with the same one). Python also has a two more types of quotes. A triple quote, ''', is created by typing three single quotes. A triple-double quote, """, is created by typing three double quotes. So, you can have several layers of quoting before you need to worry about escaping your quotes. For example, this is valid Python:
# 's

1print """I wish that I'd never heard him say, '''She said, "He said, 'Give me five dollars'"'''"""

			34.1.1.2 Truthfulness of Various Objects

Unlike some programming languages (cough Javascript cough) Python types are false if empty, and true if not. That means you don't have to check, for example, that the length of a string, tuple, list, or dict is 0 or is equal to an empty one. It is enough to just check the truthfulness of the object.

As you would expect, the number zero is also false, while all other numbers are true.

For example, the following expressions are equivalent. Here, 'my_object' is a string, but it could easily be another Python type (with appropriate modifications to the equality test)
# 's

 1my_object = 'Test' # True example
 2# my_object = '' # False example
 3
 4if len(my_object) > 0:
 5    print 'my_object is not empty'
 6
 7if len(my_object):  # 0 will evaluate to False
 8    print 'my_object is not empty'
 9
10if my_object != '':
11    print 'my_object is not empty'
12
13if my_object: # an empty string will evaluate to False
14    print 'my_object is not empty'

In conclusion, there's really no need to check lengths or equality if you're only interested in if the object is empty or not.


"
			34.1.1.3 Checking if a String Contains a Substring

Here's a quick hint that might be obvious, but it took me about a year of Python programming to figure it out.

You probably know that you can test if a list, tuple, or dict contains an item by testing the expression 'item in list' or 'item not in list'. I never realized that this would work for strings as well. I was always writing code like:
# 's

1string = 'Hi there' # True example
2# string = 'Good bye' # False example
3if string.find('Hi') != -1:
4    print 'Success!'

That's some ugly code. It is completely equivalent to do 'if substring in string':
# 's

1string = 'Hi there' # True example
2# string = 'Good bye' # False example
3if 'Hi' in string:
4    print 'Success!'

Much cleaner and simpler. Might be obvious to 99% of the population, but I wish I'd known about it sooner.

			34.1.1.4 Pretty-Printing a List

Lists don't print nicely. It's of course obvious what the list is, but an average user doesn't want to see brackets around everything. There's a trivial solution to this, using a string's 'join' method:
# 's

1recent_presidents = ['George Bush', 'Bill Clinton', 'George W. Bush']
2print 'The three most recent presidents were: %s.' % ', '.join(recent_presidents)
3# prints 'The three most recent presidents were: George Bush, Bill Clinton, George W. Bush.

The join method turns the list into a string by casting each item into a string and connecting them with the string that join was called on. It's even smart enough to not put one after the last element.

As an added advantage, this is pretty fast, running in linear time. Don't ever create a string by '+'ing list items together in a for loop: not only is it ugly, but it takes much longer.

			34.1.1.5 Integer vs. Float Division

By default, if you divide one integer by another, the result will be truncated into an integer. For example, executing 5/2 returns 2.

There are two was to fix this. The first and simplest way is to just turn one of the integers into a float. If the values are static, you can just append a .0 to one to make it a float: 5.0/2 returns 2.5. Alternatively, you can just cast one of the values: float(5) / 2 returns 2.5.

The other way will result in cleaner code, but you must make sure none of your code is relying on this truncation. You can do a from __future__ import division to change Python to always return a float as the result of a division. After such an import, 5/2 will return 2.5. If you still need to use the truncating integer division somewhere, you can then use the // operator: 5//2 will always return 2.
# 's

|  15/2        # Returns 2
|  25.0/2      # Returns 2.5
|  3float(5)/2 # Returns 2.5
|  45//2       # Returns 2
|  5
|  6from __future__ import division
|  75/2        # Returns 2.5
|  85.0/2      # Returns 2.5
|  9float(5)/2 # Returns 2.5
| 105//2       # Returns 2

Note

At some point float division will be the default. If you want your code to be future-proof, use the // operator if you want truncating division, no matter if you are doing a from __future__ import division or not.

			34.1.1.6 Lambda Functions

Sometimes you need to pass a function as an argument, or you want to do a short but complex operation multiple times. You could define your function the normal way, or you could make a lambda function, a mini-function that returns the result of a single expression. The two definitions are completely identical:
# 's

1def add(a,b): return a+b
2
3add2 = lambda a,b: a+b

The advantage of the lambda function is that it is in itself an expression, and can be used inside another statement. Here's an example using the map function, which calls a function on every element in a list, and returns a list of the results. (I make a good case below in List Comprehensions that map is pretty useless. It does, however, presents a good one line example.)
# 's

1squares = map(lambda a: a*a, [1,2,3,4,5])
2# squares is now [1,4,9,16,25]

Without a lambda, you'd have to define the function separately. You've just saved a line of code and a variable name (for the function).

Syntax: Lambda Functions

A lambda function has the syntax: lambda variable(s) : expression
variable(s) 	a comma-separated list variable or variables that the function can receive. You can't use keywords, and you don't want these to be in parentheses (a mistake I started making for a couple of months and wondered why my lambdas never worked).
expression 	an inline python expression. Scope includes local scope and variable(s). This is what the function returns.
		34.1.2 Lists

			34.1.2.1 List Comprehensions

If you've used Python for very long, you've at least heard of list comprehensions. They're a way to fit a for loop, an if statement, and an assignment all in one line. In other words, you can map and filter a list in one expression.

			34.1.2.2 Mapping the List

We'll start with something really simple. Say you're trying to square every element in a list. A freshly-initiated Python programmer might write code like this:
# 's

1numbers = [1,2,3,4,5]
2squares = []
3for number in numbers:
4    squares.append(number*number)
5# Now, squares should have [1,4,9,16,25]

You've effectively 'mapped' one list to another list. You could also use the map function, and do something like this:
# 's

1numbers = [1,2,3,4,5]
2squares = map(lambda x: x*x, numbers)
3# Now, squares should have [1,4,9,16,25]

This code is definitely shorter (1 line instead of 3) but it's pretty ugly. It's hard to tell at a glance what the map function does (it accepts a function and a list, and applies the function to every element of that list). Plus, you have to give it a function of some sort which looks kind of messy. If only there were a cleaner way... perhaps a list comprehension:
# 's

1numbers = [1,2,3,4,5]
2squares = [number*number for number in numbers]
3# Now, squares should have [1,4,9,16,25]

This does the exact same thing as the previous two examples, but it's short (unlike the first example) and clean (unlike the second example). No one is going to have any problem determining what it does, even if they don't know Python.

			34.1.2.3 Filtering the List

What if you're more interested in filtering the list? Say you want to remove every element with a value equal to or greater than 4? (Okay, so the examples aren't very realistic. Whatever...) A Python neophyte might write:
# 's

1numbers = [1,2,3,4,5]
2numbers_under_4 = []
3for number in numbers:
4    if number < 4:
5        numbers_under_4.append(number)
6# Now, numbers_under_4 contains [1,4,9]

Pretty simple, right? But it took 4 lines, two degrees of nesting, and an append to do something completely trivial. You could reduce the size of the code with the filter function:
# 's

1numbers = [1,2,3,4,5]
2numbers_under_4 = filter(lambda x: x < 4, numbers)
3# Now, numbers_under_4 contains [1,2,3]

Similar to the map function we talked about above, this reduces code size but is really ugly. What the hell is going on? Like map, filter accepts a function and a list. It evaluates for every list element and if the function evaluates to true, that list element is included in the final list. Of course, we can do this with a list comprehension as well:
# 's

1numbers = [1,2,3,4,5]
2numbers_under_4 = [number for number in numbers if number < 4]
3# Now, numbers_under_4 contains [1,2,3]

Again, using a list comprehension gives us shorter, cleaner, and easier to understand code.

			34.1.2.4 Map and Filter at Once

Now we get to the true power of list comprehensions. If I haven't yet convinced you that map and filter are generally a waste of your time, hopefully this will.

Say I want to map and filter a list at the same time. In other words, I'd like to see the square of each element in the list where said element is under 4. Once more, the Python neophyte way:
# 's

1numbers = [1,2,3,4,5]
2squares = []
3for number in numbers:
4    if number < 4:
5        squares.append(number*number)
6# squares is now [1,4,9]

The code is starting to expand in the horizontal direction now! Alas, what could we possibly do to simplify the code? We could try using map and filter, but I don't have a good feeling about this...
# 's

1numbers = [1,2,3,4,5]
2squares = map(lambda x: x*x, filter(lambda x: x < 4, numbers))
3# squares is now [1,4,9]

While map and filter were ugly before, now they're just unreadable. Obviously this isn't a good idea. Once more, list comprehensions save the day:
# 's

1numbers = [1,2,3,4,5]
2squares = [number*number for number in numbers if number < 4]
3# square is now [1,4,9]

This is a bit longer than the earlier list comprehension examples, but in my opinion still very readable. It's definitely better than a for loop or using map and filter.

As you can see, a list comprehension filters then maps. If you absoulutely need to map then filter, things can get more complicated. You might even have to use nested list comprehensions, the map and filter commands, or a regular old for loop, depending on what is cleanest. That discussion, though, is outside the scope of this article.

			34.1.2.5 Generator Expressions

There is a downside to list comprehensions: the entire list has to be stored in memory at once. This isn't a problem for small lists like the ones in the above examples, or even of lists several orders of magnitude larger. But eventually this becomes pretty inefficient.

Generator expressions are newish in Python 2.4, and possibly the least publicized Cool Thing About Python ever. As in, I just found out about them. Generator expressions do not load the whole list into memory at once, but instead create a 'generator object' so only one list element has to be loaded at any time.

Of course, if you actually need to use the entire list for something, this doesn't really help much. But if you're just passing it off to something that takes any iterable object -- like a for loop -- you might as well use a generator function.

Generator expressions have the same syntax as list comprehensions, but with parentheses around the outside instead of brackets:
# 's

1numbers = (1,2,3,4,5) # Since we're going for efficiency, I'm using a tuple instead of a list ;)
2squares_under_10 = (number*number for number in numbers if number*number < 10)
3# squares_under_10 is now a generator object, from which each successive value can be gotten by calling .next()
4
5for square in squares_under_10:
6    print square,
7# prints '1 4 9'

This is ever so slightly more efficient than using a list comprehension.

So, you want to use generator expressions for large numbers of items. You want to always use list comprehensions if you need the entire list at once for some reason. If neither of these is true, just do whatever you want. It's probably good practice to use generator expressions unless there's some reason not to, but you're not going to see any real difference in efficiency unless the list is very large.

As a final note, generator expressions only need to be surrounded by one set of parentheses. So, if you're calling a function with only a generator expression, you only need one set of parentheses. This is valid Python: some_function(item for item in list).

			34.1.2.6 Nested 'for' Statements

List comprehensions and generator expressions can be used for more than just mapping and filtering; you can create rather complex lists of lists with them [1]. Not only can you map and filter, you can nest the for expressions. A python neophyte might write something like:
# 's

1for x in (0,1,2,3):
2    for y in (0,1,2,3):
3        if x < y:
4            print (x, y, x*y),
5
6# prints (0, 1, 0) (0, 2, 0) (0, 3, 0) (1, 2, 2) (1, 3, 3) (2, 3, 6)

You can see that this code is pretty crazy. With a list comprehension, though, you can do this more quickly:
# 's

1print [(x, y, x * y) for x in (0,1,2,3) for y in (0,1,2,3) if x < y]
2# prints [(0, 1, 0), (0, 2, 0), (0, 3, 0), (1, 2, 2), (1, 3, 3), (2, 3, 6)]

As you can see, this code iterates over four values of y, and for each of those values, iterates over four values of x and then filters and maps. Each list item then, is itself a list of x, y, x * y.

Note that xrange(4) is a bit cleaner than (0,1,2,3), especially for longer lists, but we haven't gotten there yet.

			34.1.2.7 Conclusion

I hate to say it, but we've only scratched the surface of what list comprehensions and generator expressions can do. You really do have the full power of a for loop and an if statement. You can do anything (I think) that you could do in either of those. You can operate on anything that you want to start as a list (or any other iterable) and end as a list (or a generator), including lists of lists.

Syntax: List Comprehensions and Generator Expressions

A list comprehension has the syntax: [ element for variable(s) in list if condition ]

A generator expression has the syntax: ( element for variable(s) in list if condition )
list 	anything that can be treated as a list or iterator
variable(s) 	variable or variables to assign the current list element to, just like in a regular for loop
condition 	an inline python expression. Scope again includes local scope and variable(s). If this evaluates to true, item will be included in result.
element 	an inline python expression. Scope includes the local scope and variable(s). This is the actual element that will be included in the result.

The for variable(s) in list bit can be repeated indefinitely.

			34.1.2.8 Reducing a List

Unfortunately, you can't yet write your entire program with list comprehensions. (I'm joking... of course you can.) Although they can map and filter, there isn't a simple way to use a list comprehension to reduce a list. By this I mean applying a function to the first two list elements, then to that result and the next list element, and so on until a single value is reached. For example, maybe you want find the product of all of the values in a list. You could make a for loop:
# 's

1numbers = [1,2,3,4,5]
2result = 1
3for number in numbers:
4    result *= number
5# result is now 120

Or you could use the built-in function reduce, which accepts a function that takes two arguments, and a list:
# 's

1numbers = [1,2,3,4,5]
2result = reduce(lambda a,b: a*b, numbers)
3# result is now 120

Now it's not as pretty as a list comprehension, but it is shorter than a for loop. Definitely worth keeping in mind.

			34.1.2.9 Iterating over a List: range, xrange and enumerate

Remember (or maybe not) when you programmed in C, and for loops counted through index numbers instead of elements? You probably already know how to replicate this behavior in Python, using range or xrange. Passing a value to range gives you a list of counting integers from 0 to the value - 1, inclusive. In other words, it gives you the index values of a list with that length. xrange does the same thing, except a bit more efficiently: it doesn't load the whole list into memory at once.

Here's an example:
# 's

1strings = ['a', 'b', 'c', 'd', 'e']
2for index in xrange(len(strings)):
3    print index,
4# prints '0 1 2 3 4'

The problem here is that usually you end up needing the list elements anyways. What's the use of just having the index values? Python has a really awesome built-in function called enumerate that will give you both. enumerate-ing a list will return an iterator of index, value pairs:
# 's

1strings = ['a', 'b', 'c', 'd', 'e']
2for index, string in enumerate(strings):
3    print index, string,
4# prints '0 a 1 b 2 c 3 d 4 e'

As an added advantage, enumerate is quite a bit cleaner and more readable than xrange(len()). Because of this, range and xrange are probably only useful if you need to create a list of values from scratch for some reason, instead of from an existing list.

			34.1.2.10 Checking a Condition on Any or Every List Element

Say you want to check to see if any element in a list satisfies a condition (say, it's below 10). Before Python 2.5, you could do something like this:
# 's

1numbers = [1,10,100,1000,10000]
2if [number for number in numbers if number < 10]:
3    print 'At least one element is over 10'
4# Output: 'At least one element is over 10'

If none of the elements satisfy the condition, the list comprehension will create an empty list which evaluates as false. Otherwise, a non-empty list will be created, which evaluates as true. Strictly, you don't need to evaluate every item in the list; you could bail after the first item that satisfies the condition. The method above, then, is less efficient, but might be your only choice if you can't commit to only Python 2.5 and need to squeeze all of this logic in one expression.

With the new built-in any function introduced in Python 2.5, you can do the same thing cleanly and efficiently. any is actually smart enough to bail and return True after the first item that satisfies the condition. Here, I use a generator expression that returns a True or False value for each element, and pass it to any. The generator expression only computes these values as they are needed, and any only requests the values it needs [2]:
# 's

1numbers = [1,10,100,1000,10000]
2if any(number < 10 for number in numbers):
3    print 'Success'
4# Output: 'Success!'

Similarly, you can check if every element satisfies a condition. Without Python 2.5, you'll have to do something like this:
# 's

1numbers = [1,2,3,4,5,6,7,8,9]
2if len(numbers) == len([number for number in numbers if number < 10]):
3    print 'Success!'
4# Output: 'Success!'

Here we filter with a list comprehension and check to see if we still have as many elements. If we do, then all of the elements satisfied the condition. Again, this is less efficient than it could be, because there is no need to keep checking after the first element that doesn't satisfy the condition. Also again, without Python 2.5 it might be your only choice for fitting all the logic in one expression.

With Python 2.5, there's of course an easier way: the built-in all function. As you might expect, it's smart enough to bail after the first element that doesn't match, returning False. This method works just like the any method described above.
# 's

1numbers = [1,2,3,4,5,6,7,8,9]
2if all(number < 10 for number in numbers):
3    print 'Success!'
4# Output: 'Success!'


			34.1.2.11 Combining Multiple Lists, Item by Item

The built-in zip function can be used, well, to zip lists together. It returns a list of tuples, where the nth tuple contains the nth item from each of the passed in lists. This might be a case where an example is the best explanation:
# 's

1letters = ['a', 'b', 'c']
2numbers = [1, 2, 3]
3squares = [1, 4, 9]
4
5zipped_list = zip(letters, numbers, squares)
6# zipped_list contains [('a', 1, 1), ('b', 2, 4), ('c', 3, 9)]

Often you'll use this sort of thing as the iterator a for loop, pulling out all three values at once ('for letter, number, squares in zipped_list').

			34.1.2.12 A Few More List Operators

The following are all built-in functions that can be called on any list or iterable.

max
    Returns the largest element in the list
min
    Returns the smallest element in the list
sum
    Returns the sum of all elements in the list. Accepts an optional second argument, the value to start with when summing (defaults to 0).


			34.1.2.13 Advanced Logic With Sets

Now, I realize a section on sets doesn't really belong in a section on lists. But while I never find myself using sets for much, I occasionally need to do some set logic on a list I have lying around. Sets differ from lists in that they enforce uniqueness (they can't contain more than one of the same item) and are unordered. Sets also support a myriad of different logical operations.

The most common thing I want to do is to make sure my list is unique. This is easy; I just have to convert it to a set and check if the length is the same:
# 's

1numbers = [1,2,3,3,4,1]
2set(numbers)
3# returns set([1,2,3,4])
4
5if len(numbers) == len(set(numbers)):
6    print 'List is unique!'
7# In this case, doesn't print anything

Of course, you can convert the set back into a list, but remember that ordering is not preserved. For more information about the many operations sets support, check out the Python Docs. You'll want to use one or another of these operations on your lists or sets in the future.
		34.1.3 Dictionaries

			34.1.3.1 Constructing Dictionaries with Keyword Arguments

When initially learning Python, I completely missed this alternate way to create dictionaries. Any keyword arguments you pass to the dict constructor are added to the newly created dictionary before returning. Of course, you are limited to the keys that can be made into keyword arguments: valid Python variable names. Here's an example:
# 's

1dict(a=1, b=2, c=3)
2# returns {'a': 1, 'b': 2, 'c': 3}

This might be a bit cleaner than a 'regular' dictionary creation depending on your code; there are less quotes floating around. I use it often.

			34.1.3.2 Dicts to Lists

Turning a dictionary into a list or an iterator is easy. To get a list of keys, you can just cast the dict into a list. It's cleaner, though to call .keys() on the dictionary to get a list of the keys, or .iterkeys() to get an iterator. Similarly, you can call .values() or .itervalues() to get a list or iterator of dictionary values. Remember though, that dicts are inherently unordered and so these values won't be in any meaningful order.

To preserve both keys and values, you can turn a dict into a list or iterator of 2-item tuples by using .items() or .iteritems(). This is something that you'll probably do a lot, and isn't very exciting:
# 's

1dictionary = {'a': 1, 'b': 2, 'c': 3}
2dict_as_list = dictionary.items()
3#dict_as_list now contains [('a', 1), ('b', 2), ('c', 3)]


			34.1.3.3 Lists to Dicts

You can reverse the process, turning a list of 2-element lists or tuples into a dict:
# 's

1dict_as_list = [['a', 1], ['b', 2], ['c', 3]]
2dictionary = dict(dict_as_list)
3# dictionary now contains {'a': 1, 'b': 2, 'c': 3}

You can also combine this with the 'keyword arguments' method of creating a dictionary discussed above:
# 's

1dict_as_list = [['a', 1], ['b', 2], ['c', 3]]
2dictionary = dict(dict_as_list, d=4, e=5)
3# dictionary now contains {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5}

Being able to convert a dict to a list is kind of handy, I guess. But what really makes it awesome is the next trick.

			34.1.3.4 'Dictionary Comprehensions'

Although Python doesn't have built-in dictionary comprehensions, you can do something pretty close with little mess or code. Just use .iteritems() to turn your dict into a list, throw it in a generator expression (or list comprehension), and then cast that list back into a dict.

For example, say I have a dictionary of name:email pairs, and I want to create a dictionary of name:is_email_at_a_dot_com pairs:
# 's

1emails = {'Dick': 'bob@example.com', 'Jane': 'jane@example.com', 'Stou': 'stou@example.net'}
2
3email_at_dotcom = dict( [name, '.com' in email] for name, email in emails.iteritems() )
4
5# email_at_dotcom now is {'Dick': True, 'Jane': True, 'Stou': False}

Damn straight. Of course, you don't have to start and end with a dict, you can throw some lists in there too.

While this is a little less readable than a straight list comprehension, I'd argue it's still better than a massive for loop.
		34.1.4 Selecting Values

			34.1.4.1 The Right Way

While writing this article, I stumbled upon the right way to select values inline, new in Python 2.5 (you'd think there would have been more fanfare!). Python now supports the syntax 'value_if_true if test else value_if_false'. So, you can do simple selection of values in one line, with no weird syntax or major caveats:
# 's

1test = True
2# test = False
3result = 'Test is True' if test else 'Test is False'
4# result is now 'Test is True'

Okay, it's a bit ugly still. Alas. You can also chain multiple tests in one line:
# 's

1test1 = False
2test2 = True
3result = 'Test1 is True' if test1 else 'Test1 is False, test2 is True' if test2 else 'Test1 and Test2 are both False'

The first if/else is evaluated first, and if test1 is false the second if/else is evaluated. You can do more complicated things too, especially if you throw in some parentheses.

Personal Note

This is pretty new on the field, and my reaction is mixed. It really is the Right Way, it's cleaner, and I like it... but it's still ugly especially if you have multiple nested if/else's.

Of course, the syntax for all of the value selection tricks is ugly.

I have soft spot for the and/or trick below, I actually find it very intuitive, now that I understand how it works. Also, it's not any less efficient than doing things the Right Way.

What do you think? Feel free to comment below.

Although the inline if/else is the new, more correct way, you'd better still check out the tricks below. Even if you only plan on programming in Python 2.5, you're still going to run into these in older code. Of course, if you need backwards compatibility or don't have Python 2.5, you'd really better check out the tricks below.

			34.1.4.2 The and/or Trick

In Python, 'and' and 'or' are complex creatures. and-ing two expressions together doesn't just return True if both are true and False if both are false. Instead, 'and' returns the first false value, or the last value if all are true. In other words, if the first value is false it is returned, otherwise the last value is returned. The result of this is something you would expect: if both are true, the last value is returned, which is true and will evaluate to True in a boolean test (eg, an 'if' statement). If one is false, that one is returned and will evaluate to False in a boolean test.

or-ing two expressions together is similar. 'or' returns the first true value, or the last value if all are false. In other words, if the first value is true it is returned, otherwise the last value is returned. So, if both are false, the last value is returned, which is false and will evaluate to False in a boolean test. If one is true, that one is returned and will evaluate to True in a boolean test.

This doesn't help you much when we're just testing for truthfulness. But you can use 'and' and 'or' for other purposes in Python; my favorite is to select between values in a manner akin to C's ternary conditional assignment operator 'test ? value_if_true : value_if_false':
# 's

1test = True
2# test = False
3result = test and 'Test is True' or 'Test is False'
4# result is now 'Test is True'

How does this work? If test is true, the and statement skips over it and returns its right half, here 'Test is True' or 'Test is False'. As processing continues left to right, the or statement returns the first true value, 'Test is True'.

If test is false, the and statement returns test. As processing continues left to right, the remaining statement is test or 'Test is False'. Since test is false, the or statement skips over it and returns its right half, 'Test is False'.

Warning

Be careful that the middle (if_true) value is never false. If it is, the 'or' statement will always skip over it and always return the rightmost (if_false) value, no matter what the test value is.

Having gotten used to this method, 'The Right Way' (above) actually seems less intuitive to me. If you're not worried about backwards compatibility, I suggest you try both and see which one you like better. It's pretty easy to nest and/or tricks or to throw on extra and's or or's once you understand the logic behind it. If you can't decide or don't feel like learning both, then don't use and/or. Do things the Right Way, and be done with it.

Of course, if you need to support Python versions under 2.5, 'The Right Way' won't work. (I was tempted to say that it 'is The Wrong Way'). In that case the and/or trick is definitely your best bet for most situations.

Hopefully this all makes sense; it's hard to explain. It might seem complicated now, but if you use it a few times and play with 'and' and 'or' it will shortly make sense and you'll be able to come up with more complicated 'and' and 'or' tricks on your own.

			34.1.4.3 Using True and False as Indexes

Another way to select values is to use True and False as list indexes, taking advantage of the fact that False == 0 and True == 1:
# 's

1test = True
2# test = False
3result = ['Test is False','Test is True'][test]
4# result is now 'Test is True'

This is more straightforward than the and/or trick, and free of the problem where the value_if_true must itself be true.

However, it also suffers from a significant flaw: both list items are evaluated before truthfulness is checked. For strings or other simple items, this is not a big deal. But if each item involves significant computation or I/O, you really don't want to do twice the work that you have to. For this reason I usually prefer the 'Right Way' or the and/or trick.

Also note that the index method only works when you know that test is False or True (or 0 or 1, but not any other integer or an arbitrary object). Otherwise you should write bool(test) instead of test to get the same behavior as the and/or trick expression above.
		34.1.5 Functions

			34.1.5.1 Default Argument Values are Only Evaluated Once

Let's start this section with a warning. Here's a problem that has confused many new Python writers, including myself, repeatedly, even after I figured out the problem... It's easy to be stupid about this (note that this isn't the world's best example, but it illustrates the point):
# 's

1def function(item, stuff = []):
2    stuff.append(item)
3    print stuff
4
5function(1)
6# prints '[1]'
7
8function(2)
9# prints '[1,2]' !!!

The default value for a function argument is only evaluated once, when the function is defined. Python simply assigns this value to the correct variable name when the function is called.

Python doesn't check if that value (that location in memory) was changed. It just continues to assign that value to any caller that needs it. So, if the value is changed, the change will persist across function calls. Above, when we appended a value to the list represented by stuff, we actually changed the default value for all eternity. When we called function again looking for a default value, the modified default was given to us.

The solution: don't use mutable objects as function defaults. You might be able to get away with it if you don't modify them, but it's still not a good idea.

A better way to write the above code would be:
# 's

 1def function(item, stuff = None):
 2    if stuff is None:
 3        stuff = []
 4    stuff.append(item)
 5    print stuff
 6
 7function(1)
 8# prints '[1]'
 9
10function(2)
11# prints '[2]', as expected

None is immutable (and we're not trying to change it anyways), so we're safe from accidently changing value of the default.

On the plus side, a clever programmer could probably turn this into a trick, in effect creating C-style 'static variables'.

			34.1.5.2 Force Default Arguments to be Evaluated Each Time

If you prefer less cluttered functions at the cost of some clarity, you can forcefully re-evaluate the default arguments before each function call. The following decorator stores the original values of the default arguments. It can be used to wrap a function and reset the default arguments before each call. [3]
# 's

1from copy import deepcopy
2
3def resetDefaults(f):
4    defaults = f.func_defaults
5    def resetter(*args, **kwds):
6        f.func_defaults = deepcopy(defaults)
7        return f(*args, **kwds)
8    resetter.__name__ = f.__name__
9    return resetter

Simply apply this decorator to your function to get the expected results.
# 's

 1@resetDefaults # This is how you apply a decorator
 2def function(item, stuff = []):
 3    stuff.append(item)
 4    print stuff
 5
 6function(1)
 7# prints '[1]'
 8
 9function(2)
10# prints '[2]', as expected


			34.1.5.3 Arbitrary Numbers of Arguments

Python lets you have arbitrary numbers of arguments in your functions. First define any required arguments (if any), then use a variable with a '*' prepended to it. Python will take the rest of the non-keyword arguments, put them in a list or tuple, and assign them to this variable:
# 's

1def do_something(a, b, c, *args):
2    print a, b, c, args
3
4do_something(1,2,3,4,5,6,7,8,9)
5# prints '1, 2, 3, (4, 5, 6, 7, 8, 9)'

Why would you want to do this? A common reason is that your function accepts a number of items and does the same thing with all of them (say, sums them up). You could force the user to pass a list: sum_all([1,2,3]) or you could allow them to use an arbitrary number of arguments, which makes for cleaner code: sum_all(1,2,3).

You can also have arbitrary numbers of keyword arguments. After you've defined all other arguments, use a variable with '**' prepended to it. Python will take the rest of the keyword arguments, put them in a dictionary, and assign them to this variable:
# 's

1def do_something_else(a, b, c, *args, **kwargs):
2    print a, b, c, args, kwargs
3
4do_something_else(1,2,3,4,5,6,7,8,9, timeout=1.5)
5# prints '1, 2, 3, (4, 5, 6, 7, 8, 9), {"timeout": 1.5}'

Why would you want to do this? I think the most common reason is if your function is a wrapper for some other function or functions, any keyword arguments that you use can be popped off the dictionary and the remainder of the keyword arguments can be passed to the other function(s) (see Passing a List or Dictionary as Arguments, below)

			34.1.5.4 Caveat

Passing both arbitrary non-keyword arguments and named (non-arbitrary) keyword arguments in one function is seemingly impossible. This is because named keyword arguments must be defined before the '*' parameter in the function definition, and are filled before that parameter is filled. For example, imagine a function:
# 's

1def do_something(a, b, c, actually_print = True, *args):
2    if actually_print:
3        print a, b, c, args

We now have a problem: there is no way to specify 'actually_print' as a named keyword argument while simultaneously providing arbitrary non-keyword arguments. Both of the following will error:
# 's

1do_something(1, 2, 3, 4, 5, actually_print = True)
2# actually_print is initially set to 4 (see why?) and then re-set,
3# causing a TypeError ('got multiple values for keyword argument')
4
5do_something(1, 2, 3, actually_print = True, 4, 5, 6)
6# This is not allowed as keyword arguments may not precede non-keyword arguments.  A SyntaxError is raised.

The only way to pass 'actually_print' in this situation is to pass it as a non-keyword argument:
# 's

1do_something(1, 2, 3, True, 4, 5, 6)
2# Result is '1, 2, 3, (4, 5, 6)'


			34.1.5.5 Passing a List or Dictionary as Arguments

Since you can receive arguments as a list or dictionary, it's not terribly surprising, I suppose, that you can send arguments to a function from a list or dictionary. The syntax is exactly the same as above.

To send a list as non-keyword arguments, just prepend it with a '*':
# 's

1args = [5,2]
2pow(*args)
3# returns pow(5,2), meaning 5^2 which is 25

And, of course, to send a dictionary as keyword arguments (this is probably more common), prepend it with '**':
# 's

 1def do_something(actually_do_something=True, print_a_bunch_of_numbers=False):
 2    if actually_do_something:
 3        print 'Something has been done'
 4        #
 5        if print_a_bunch_of_numbers:
 6            print range(10)
 7
 8kwargs = {'actually_do_something': True, 'print_a_bunch_of_numbers': True}
 9do_something(**kwargs)
10
11# prints 'Something has been done', then '[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]'

Historical footnote: In older versions of Python (pre-2.3) you called functions with arbitrary arguments using the built-in apply (function, arg_list, keyword_arg_dict)'.

			34.1.5.6 Decorators

Function decorators are fairly simple, but if you've never seen them before you'll have no idea what's going on, as unlike most of Python the syntax isn't very clear. A decorator is a function that wraps another function: the main function is called and its return value is passed to the decorator. The decorator then returns a function that replaces the wrapped function as far as the rest of the program is concerned.

Without further delay, here is the syntax:
# 's

 1def decorator1(func):
 2    return lambda: func() + 1
 3
 4def decorator2(func):
 5    def print_func():
 6        print func()
 7    return print_func
 8
 9@decorator2
10@decorator1
11def function():
12    return 41
13
14function()
15# prints '42'

In this example, 'function' is passed to 'decorator1'. 'decorator1' returns a function that calls 'function' and adds 1. This function is then passed to 'decorator2', which returns a function that calls the function returned by 'decorator1' and prints the result. This last function is the function you are actually calling when you call 'function'. Whew.

This example does the exact same thing, but more verbosely and without decorators:
# 's

 1def decorator1(func):
 2    return lambda: func() + 1
 3
 4def decorator2(func):
 5    def print_func():
 6        print func()
 7    return print_func
 8
 9def function():
10    return 41
11
12function = decorator2(decorator1(function))
13
14function()
15# prints '42'

Typically decorators are used to add abilities to your functions (see Creating Class Methods, below). But even more typically, they're not used at all. But it's good to know what you're looking at.

For more information, check out my article Python Decorators Don't Have to be (that) Scary, Python Decorators on Dr. Dobbs or "Function Definitions" in the Python Docs

			34.1.5.7 'Switch Statements' using Dictionaries of Functions

Ever miss the switch statement? As you probably know, Python doesn't really have a syntactical equivalent, unless you count repeated elif's. What you might not know, though, is that you can replicate the behavior (if not the cleanliness) of the switch statement by creating a dictionary of functions keyed by the value you want to switch on.

For example, say you're handling keystrokes and you need to call a different function for each keystroke. Also say you've already defined these three functions:
# 's

 1def key_1_pressed():
 2    print 'Key 1 Pressed'
 3
 4def key_2_pressed():
 5    print 'Key 2 Pressed'
 6
 7def key_3_pressed():
 8    print 'Key 3 Pressed'
 9
10def unknown_key_pressed():
11    print 'Unknown Key Pressed'

In Python, you would typically use elif's to choose a function:
# 's

 1keycode = 2
 2if keycode == 1:
 3   key_1_pressed()
 4elif keycode == 2:
 5   key_2_pressed()
 6elif number == 3:
 7   key_3_pressed()
 8else:
 9   unknown_key_pressed()
10# prints 'Key 2 Pressed'

But you could also throw all the functions in a dictionary, and key them to the value you're switching on. You could even check see if the key exists and run some code if it doesn't:
# 's

1keycode = 2
2functions = {1: key_1_pressed, 2: key_2_pressed, 3: key_3_pressed}
3functions.get(keycode, unknown_key_pressed)()

You can see that this could be a lot cleaner than the elif example for large numbers of functions.
		34.1.6 Classes

			34.1.6.1 Passing 'self' Manually

Methods are just regular functions that when called from an instance are passed that instance as the first argument (usually called 'self'). If for some reason you're not calling the function from an instance, you can always pass the instance manually as the first argument. For example:
# 's

 1class Class:
 2    def a_method(self):
 3        print 'Hey a method'
 4
 5instance = Class()
 6
 7instance.a_method()
 8# prints 'Hey a method', somewhat unsuprisingly.  You can also do:
 9
10Class.a_method(instance)
11# prints 'Hey a method'

Internally, these statements are exactly the same.

			34.1.6.2 Checking for Property and Method Existence

Need to know if a particular class or instance has a particular property or method? You can use the built-in 'hasattr' function to check; it accepts the object and the attribute (as a string) to check for. You use similarly to the dict 'has_key' method (although it works completely differently):
# 's

1class Class:
2    answer = 42
3
4hasattr(Class, 'answer')
5# returns True
6hasattr(Class, 'question')
7# returns False

You can also check for existence of and access the property in one step using the built-in function 'getattr'. getattr also accepts the object and the attribute, as a string, to check for. It has an optional third argument, giving the default if the attribute is not found. Unlike the dict's 'get' method that you might be more familiar with, if the default is not given and the attribute is not found, an AttributeError is raised:
# 's

1class Class:
2    answer = 42
3
4getattr(Class, 'answer')
5# returns 42
6getattr(Class, 'question', 'What is six times nine?')
7# returns 'What is six times nine?'
8getattr(Class, 'question')
9# raises AttributeError

Don't overuse hasattr and getattr. If you've written your class in manner where you need to keep checking to see if a property exists, you've written it wrong. Just always have the value exist and set it to None (or whatever) if it's not being used. These functions are best used for handling polymorphism, that is, allowing your function/class/whatever to support different kinds of objects.

			34.1.6.3 Modifying Classes After Creation

You can add, modify, or delete a class property or method long after the class has been created, and even after it has been instantiated. Just access the property or method as Class.attribute. No matter when they were created, instances of the class will respect these changes:
# 's

 1class Class:
 2   def method(self):
 3        print 'Hey a method'
 4
 5instance = Class()
 6instance.method()
 7# prints 'Hey a method'
 8
 9def new_method(self):
10    print 'New method wins!'
11
12Class.method = new_method
13instance.method()
14# prints 'New method wins!'

Pretty awesome. But don't get carried away with modifying preexisting methods, it's bad form and can confuse the crap out of any objects using that class. On the other hand, adding methods is a lot less (but still somewhat) dangerous.

			34.1.6.4 Creating Class Methods

Occasionally when writing a class you want to include a function that is called from the class, not the instance. Perhaps this method creates new instances, or perhaps it is independent of any properties of any individual instance. Python actually gives you two ways to do this, depending if your method needs to (or should) know about which class called it. Both involve applying decorators to your methods.

A 'class method' receives the class as the first argument, just as a regular instance method receives the instance as the first argument. So, the method is aware if it is being called from its own class or from a subclass.

A 'static method' receives no information about where it is called; it is essentially a regular function, just in a different scope.

Class and static methods can be called straight from the class, as Class.method(), or from an instance as Class().method(). The instance is ignored except for its class. Here's an example of each, along with a regular instance method:
# 's

 1class Class:
 2    @classmethod
 3    def a_class_method(cls):
 4        print 'I was called from class %s' % cls
 5    #
 6    @staticmethod
 7    def a_static_method():
 8        print 'I have no idea where I was called from'
 9    #
10    def an_instance_method(self):
11        print 'I was called from the instance %s' % self
12
13instance = Class()
14
15Class.a_class_method()
16instance.a_class_method()
17# both print 'I was called from class __main__.Class'
18
19Class.a_static_method()
20instance.a_static_method()
21# both print 'I have no idea where I was called from'
22
23Class.an_instance_method()
24
25# raises TypeError
26instance.an_instance_method()
27# prints something like 'I was called from the instance <__main__.Class instance at 0x2e80d0>'

35. riddles

	35.1 http://www.pythonchallenge.com/ 

		35.1.1  2^34
>>> 2**34
17179869184L


		35.1.2 translation
All letters +2. a->c, b->d etc

see 36.1.1.1    

next: http://www.pythonchallenge.com/pc/def/ocr.html

The picture and the hint imply that the encryption was done by replacing each letter by the the letter two places after it.


First, we load the text and create a translation table

>>> import string
>>> text = """g fmnc wms bgblr rpylqjyrc gr zw fylb. rfyrq ufyr
... amknsrcpq ypc dmp. bmgle gr gl zw fylb gq glcddgagclr ylb rfyr'q
 ... ufw rfgq rcvr gq qm jmle. sqgle qrpgle.kyicrpylq() gq pcamkkclbcb.
... lmu ynnjw ml rfc spj."""
>>> table = string.maketrans(
...   string.ascii_lowercase,
...   string.ascii_lowercase[2:]+string.ascii_lowercase[:2])

Now we apply the translation table on the string:

>>> string.translate(text,table) 
"i hope you didnt translate it by hand. thats what computers are for. 
doing it in by hand is inefficient and that's why this text is so long. 
using string.maketrans() is recommended. now apply on the url."

Alternatively, just use the translate on the "text" variable:

>>> text.translate(table)
"i hope you didnt translate it by hand. thats what computers are for. 
doing it in by hand is inefficient and that's why this text is so long. 
using string.maketrans() is recommended. now apply on the url."

scholi: Solved without translate

s="g fmnc wms bgblr rpylqjyrc gr zw fylb. rfyrq ufyr amknsrcpq ypc dmp. bmgle gr gl zw fylb gq glcddgagclr ylb rfyr'q ufw rfgq rcvr gq qm jmle. sqgle qrpgle.kyicrpylq() gq pcamkkclbcb.  lmu ynnjw ml rfc spj."
o=""
for x in s:
	if ord(x)>=ord('a') and ord(x)<=ord('z'):
		o+=chr((ord(x)+2-ord('a'))%26+ord('a'))
	else:
		o+=x
print o

The hint suggests to change the URL from map.html to ocr.jvon, but since there is no such thing as a .jvon file, try ocr.html.

Similar to above but with some of 2.5's nested ternary operators

for x in s:
    print  chr(ord(x) if ord(x)+2 < ord('a') else  ord(x)+2 if ord(x)+2 < ord('z') else ord(x)-24 ),

Foobat 09:00, 18 Jan 2008 (PST)


Additional:

If you are living in a *nix shell, this will do it in one shot.

$ curl http://www.pythonchallenge.com/pc/def/map.html | tr a-z c-za-b

		35.1.3
	35.2 Find which digit is represented by which letter so that following is true: 
send + more = money


	35.3
36. My examples, recipes 

	36.1  strings

		36.1.1   Translate strings

			36.1.1.1    using ord and chr
All letters +2. a->c, b->d etc

#!/usr/bin/python

from sys import *

if len(argv) != 2:
	print "usage: translate  <string> "
else:
	str = argv[1]
	new_str = ""
	print "Translating string ",str

#my long way :)
	for c in str:
		if ord('a') <= ord(c) and ord(c) <= ord ('z'):
			#print "char is ",c, "ord is ",ord(c), "normalized ord ", ((ord(c))-ord('a')), "trans ord is ",(((ord(c)+2)-ord('a'))%26 +ord('a')),"Tran char ",chr(((ord(c)+2)-ord('a'))%25 +ord('a'))
			new_str += chr(((ord(c)+2)-ord('a'))%26 +ord('a') )
			#new_str += chr((ord(c)+2))
		else:
			new_str += c
	print new_str

			36.1.1.2 Using string.maketrans()
Description:

This method returns a translation table that maps each character in the intab string into the character at the same position in the outtab string. Then this table is passed to the translate() function. Note that both intab and outtab must have the same length.
Syntax:

str.maketrans(intab, outtab]);

Parameters:

Here is the detail of parameters:

    intab: string having actual characters.

    outtab: string having corresponding mapping character.

Return Value:

It returns a translate table to be used translate() function.
Example:

This example every vowel in a string is replaced by its vowel position:

#!/usr/bin/python

from string import maketrans   # Required to call maketrans function.

intab = "aeiou"
outtab = "12345"
trantab = maketrans(intab, outtab)

str = "this is string example....wow!!!";
print str.translate(trantab);

This will produce following result:

th3s 3s str3ng 2x1mpl2....w4w!!!


			36.1.1.3

		36.1.2 Extract filename from full path in Python? 

use split and take last token or much more convenient:
 	

>>> import os.path
>>> print os.path.basename("C:\workdir\project1")
project1

		36.1.3 in python how to remove this \n from string or list

		use strip
data_list = [word.strip() for word in data_list]

		36.1.4 slices
s=[0,5,7,100]

reverse order
s[::-1]

push in middle 
s[2:3] = [1,2,3] #push 1,2,3 instead of 2nd element 

		36.1.5 namedtuple
from collections import namedtuple
>>> Point=namedtuple('point',['x','y'])
>>> Point
<class '__main__.point'>
>>> p1=Point(1,2)
>>> p1
point(x=1, y=2)

		36.1.6 Run code Snipplets from pycharm
Select code, right click, run selected in console

ex:

>>> from  collections import  namedtuple
Version = namedtuple('Version', ['major','minor','patch'])
class Circle(object):
    """ class doc """
    version = Version(1,0,0) #class attr, all instances have it like static in C++
    def __init__(self,radius):
        self.radius = radius #instance attr

>>> c1 = Circle(7)
>>> c1.radius
7
>>> c1.version
Version(major=1, minor=0, patch=0)
>>> c1.__dict__
{'radius': 7}
>>> Circle.__dict__
mappingproxy({'__doc__': ' class doc ', '__dict__': <attribute '__dict__' of 'Circle' objects>, 'version': Version(major=1, minor=0, patch=0), '__weakref__': <attribute '__weakref__' of 'Circle' objects>, '__init__': <function Circle.__init__ at 0x1046e61e0>, '__module__': 'builtins'})

- resolution order, instance, class, inheritance
>>> vars(c1)
{'radius': 7}
>>> 'version' in vars(c1)
False
>>> c1.__class__
<class 'Circle'>
>>> 'version' in vars(c1.__class__)
True
	36.2 directories

		36.2.1 simple directory related tasks
>>> os.mkdir("aaa")
>>> os.listdir(".")
['.python_history',
 'aaa',
 ...
 'xyz']

-will fail if dir already exists
>>> os.mkdir("aaa")
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
OSError: [Errno 17] File exists: 'aaa'
>>> if not os.path.exists("aaa"):
...     os.mkdir("aaa")
... else
  File "<stdin>", line 3
    else
       ^
SyntaxError: invalid syntax
>>> if not os.path.exists("aaa"):
...     os.mkdir("aaa")
... else:
...     print "bbb"
... 
bbb

			36.2.1.1 File and Directory Access 
			http://docs.python.org/library/filesys.html

				36.2.1.1.1 os.path — Common pathname manipulations

This module implements some useful functions on pathnames. To read or write files see open(), and for accessing the filesystem see the os module.

Note

On Windows, many of these functions do not properly support UNC pathnames. splitunc() and ismount() do handle them correctly.

Note

Since different operating systems have different path name conventions, there are several versions of this module in the standard library. The os.path module is always the path module suitable for the operating system Python is running on, and therefore usable for local paths. However, you can also import and use the individual modules if you want to manipulate a path that is always in one of the different formats. They all have the same interface:

    posixpath for UNIX-style paths
    ntpath for Windows paths
    macpath for old-style MacOS paths
    os2emxpath for OS/2 EMX paths

os.path.abspath(path)

    Return a normalized absolutized version of the pathname path. On most platforms, this is equivalent to normpath(join(os.getcwd(), path)).

    New in version 1.5.2.

os.path.basename(path)

    Return the base name of pathname path. This is the second half of the pair returned by split(path). Note that the result of this function is different from the Unix basename program; where basename for '/foo/bar/' returns 'bar', the basename() function returns an empty string ('').

os.path.commonprefix(list)

    Return the longest path prefix (taken character-by-character) that is a prefix of all paths in list. If list is empty, return the empty string (''). Note that this may return invalid paths because it works a character at a time.

os.path.dirname(path)

    Return the directory name of pathname path. This is the first half of the pair returned by split(path).

os.path.exists(path)

    Return True if path refers to an existing path. Returns False for broken symbolic links. On some platforms, this function may return False if permission is not granted to execute os.stat() on the requested file, even if the path physically exists.

os.path.lexists(path)

    Return True if path refers to an existing path. Returns True for broken symbolic links. Equivalent to exists() on platforms lacking os.lstat().

    New in version 2.4.

os.path.expanduser(path)

    On Unix and Windows, return the argument with an initial component of ~ or ~user replaced by that user‘s home directory.

    On Unix, an initial ~ is replaced by the environment variable HOME if it is set; otherwise the current user’s home directory is looked up in the password directory through the built-in module pwd. An initial ~user is looked up directly in the password directory.

    On Windows, HOME and USERPROFILE will be used if set, otherwise a combination of HOMEPATH and HOMEDRIVE will be used. An initial ~user is handled by stripping the last directory component from the created user path derived above.

    If the expansion fails or if the path does not begin with a tilde, the path is returned unchanged.

os.path.expandvars(path)

    Return the argument with environment variables expanded. Substrings of the form $name or ${name} are replaced by the value of environment variable name. Malformed variable names and references to non-existing variables are left unchanged.

    On Windows, %name% expansions are supported in addition to $name and ${name}.

os.path.getatime(path)

    Return the time of last access of path. The return value is a number giving the number of seconds since the epoch (see the time module). Raise os.error if the file does not exist or is inaccessible.

    New in version 1.5.2.

    Changed in version 2.3: If os.stat_float_times() returns True, the result is a floating point number.

os.path.getmtime(path)

    Return the time of last modification of path. The return value is a number giving the number of seconds since the epoch (see the time module). Raise os.error if the file does not exist or is inaccessible.

    New in version 1.5.2.

    Changed in version 2.3: If os.stat_float_times() returns True, the result is a floating point number.

os.path.getctime(path)

    Return the system’s ctime which, on some systems (like Unix) is the time of the last change, and, on others (like Windows), is the creation time for path. The return value is a number giving the number of seconds since the epoch (see the time module). Raise os.error if the file does not exist or is inaccessible.

    New in version 2.3.

os.path.getsize(path)

    Return the size, in bytes, of path. Raise os.error if the file does not exist or is inaccessible.

    New in version 1.5.2.

os.path.isabs(path)

    Return True if path is an absolute pathname. On Unix, that means it begins with a slash, on Windows that it begins with a (back)slash after chopping off a potential drive letter.

os.path.isfile(path)

    Return True if path is an existing regular file. This follows symbolic links, so both islink() and isfile() can be true for the same path.

os.path.isdir(path)

    Return True if path is an existing directory. This follows symbolic links, so both islink() and isdir() can be true for the same path.

os.path.islink(path)

    Return True if path refers to a directory entry that is a symbolic link. Always False if symbolic links are not supported.

os.path.ismount(path)

    Return True if pathname path is a mount point: a point in a file system where a different file system has been mounted. The function checks whether path‘s parent, path/.., is on a different device than path, or whether path/.. and path point to the same i-node on the same device — this should detect mount points for all Unix and POSIX variants.

os.path.join(path1[, path2[, ...]])

    Join one or more path components intelligently. If any component is an absolute path, all previous components (on Windows, including the previous drive letter, if there was one) are thrown away, and joining continues. The return value is the concatenation of path1, and optionally path2, etc., with exactly one directory separator (os.sep) following each non-empty part except the last. (This means that an empty last part will result in a path that ends with a separator.) Note that on Windows, since there is a current directory for each drive, os.path.join("c:", "foo") represents a path relative to the current directory on drive C: (c:foo), not c:\foo.

os.path.normcase(path)

    Normalize the case of a pathname. On Unix and Mac OS X, this returns the path unchanged; on case-insensitive filesystems, it converts the path to lowercase. On Windows, it also converts forward slashes to backward slashes.

os.path.normpath(path)

    Normalize a pathname. This collapses redundant separators and up-level references so that A//B, A/B/, A/./B and A/foo/../B all become A/B.

    It does not normalize the case (use normcase() for that). On Windows, it converts forward slashes to backward slashes. It should be understood that this may change the meaning of the path if it contains symbolic links!

os.path.realpath(path)

    Return the canonical path of the specified filename, eliminating any symbolic links encountered in the path (if they are supported by the operating system).

    New in version 2.2.

os.path.relpath(path[, start])

    Return a relative filepath to path either from the current directory or from an optional start point.

    start defaults to os.curdir.

    Availability: Windows, Unix.

    New in version 2.6.

os.path.samefile(path1, path2)

    Return True if both pathname arguments refer to the same file or directory (as indicated by device number and i-node number). Raise an exception if a os.stat() call on either pathname fails.

    Availability: Unix.

os.path.sameopenfile(fp1, fp2)

    Return True if the file descriptors fp1 and fp2 refer to the same file.

    Availability: Unix.

os.path.samestat(stat1, stat2)

    Return True if the stat tuples stat1 and stat2 refer to the same file. These structures may have been returned by fstat(), lstat(), or stat(). This function implements the underlying comparison used by samefile() and sameopenfile().

    Availability: Unix.

os.path.split(path)

    Split the pathname path into a pair, (head, tail) where tail is the last pathname component and head is everything leading up to that. The tail part will never contain a slash; if path ends in a slash, tail will be empty. If there is no slash in path, head will be empty. If path is empty, both head and tail are empty. Trailing slashes are stripped from head unless it is the root (one or more slashes only). In all cases, join(head, tail) returns a path to the same location as path (but the strings may differ).

os.path.splitdrive(path)

    Split the pathname path into a pair (drive, tail) where drive is either a drive specification or the empty string. On systems which do not use drive specifications, drive will always be the empty string. In all cases, drive + tail will be the same as path.

    New in version 1.3.

os.path.splitext(path)

    Split the pathname path into a pair (root, ext) such that root + ext == path, and ext is empty or begins with a period and contains at most one period. Leading periods on the basename are ignored; splitext('.cshrc') returns ('.cshrc', '').

    Changed in version 2.6: Earlier versions could produce an empty root when the only period was the first character.

os.path.splitunc(path)

    Split the pathname path into a pair (unc, rest) so that unc is the UNC mount point (such as r'\\host\mount'), if present, and rest the rest of the path (such as r'\path\file.ext'). For paths containing drive letters, unc will always be the empty string.

    Availability: Windows.

os.path.walk(path, visit, arg)

    Calls the function visit with arguments (arg, dirname, names) for each directory in the directory tree rooted at path (including path itself, if it is a directory). The argument dirname specifies the visited directory, the argument names lists the files in the directory (gotten from os.listdir(dirname)). The visit function may modify names to influence the set of directories visited below dirname, e.g. to avoid visiting certain parts of the tree. (The object referred to by names must be modified in place, using del or slice assignment.)

    Note

    Symbolic links to directories are not treated as subdirectories, and that walk() therefore will not visit them. To visit linked directories you must identify them with os.path.islink(file) and os.path.isdir(file), and invoke walk() as necessary.

    Note

    This function is deprecated and has been removed in Python 3 in favor of os.walk().

os.path.supports_unicode_filenames

    True if arbitrary Unicode strings can be used as file names (within limitations imposed by the file system).

    New in version 2.3.

				36.2.1.1.2 fileinput — Iterate over lines from multiple input streams

Source code: Lib/fileinput.py

This module implements a helper class and functions to quickly write a loop over standard input or a list of files. If you just want to read or write one file see open().

The typical use is:

import fileinput
for line in fileinput.input():
    process(line)

This iterates over the lines of all files listed in sys.argv[1:], defaulting to sys.stdin if the list is empty. If a filename is '-', it is also replaced by sys.stdin. To specify an alternative list of filenames, pass it as the first argument to input(). A single file name is also allowed.

All files are opened in text mode by default, but you can override this by specifying the mode parameter in the call to input() or FileInput(). If an I/O error occurs during opening or reading a file, IOError is raised.

If sys.stdin is used more than once, the second and further use will return no lines, except perhaps for interactive use, or if it has been explicitly reset (e.g. using sys.stdin.seek(0)).

Empty files are opened and immediately closed; the only time their presence in the list of filenames is noticeable at all is when the last file opened is empty.

Lines are returned with any newlines intact, which means that the last line in a file may not have one.

You can control how files are opened by providing an opening hook via the openhook parameter to fileinput.input() or FileInput(). The hook must be a function that takes two arguments, filename and mode, and returns an accordingly opened file-like object. Two useful hooks are already provided by this module.

The following function is the primary interface of this module:

fileinput.input([files[, inplace[, backup[, mode[, openhook]]]]])

    Create an instance of the FileInput class. The instance will be used as global state for the functions of this module, and is also returned to use during iteration. The parameters to this function will be passed along to the constructor of the FileInput class.

    Changed in version 2.5: Added the mode and openhook parameters.

The following functions use the global state created by fileinput.input(); if there is no active state, RuntimeError is raised.

fileinput.filename()

    Return the name of the file currently being read. Before the first line has been read, returns None.

fileinput.fileno()

    Return the integer “file descriptor” for the current file. When no file is opened (before the first line and between files), returns -1.

    New in version 2.5.

fileinput.lineno()

    Return the cumulative line number of the line that has just been read. Before the first line has been read, returns 0. After the last line of the last file has been read, returns the line number of that line.

fileinput.filelineno()

    Return the line number in the current file. Before the first line has been read, returns 0. After the last line of the last file has been read, returns the line number of that line within the file.

fileinput.isfirstline()

    Returns true if the line just read is the first line of its file, otherwise returns false.

fileinput.isstdin()

    Returns true if the last line was read from sys.stdin, otherwise returns false.

fileinput.nextfile()

    Close the current file so that the next iteration will read the first line from the next file (if any); lines not read from the file will not count towards the cumulative line count. The filename is not changed until after the first line of the next file has been read. Before the first line has been read, this function has no effect; it cannot be used to skip the first file. After the last line of the last file has been read, this function has no effect.

fileinput.close()

    Close the sequence.

The class which implements the sequence behavior provided by the module is available for subclassing as well:

class fileinput.FileInput([files[, inplace[, backup[, mode[, openhook]]]]])

    Class FileInput is the implementation; its methods filename(), fileno(), lineno(), filelineno(), isfirstline(), isstdin(), nextfile() and close() correspond to the functions of the same name in the module. In addition it has a readline() method which returns the next input line, and a __getitem__() method which implements the sequence behavior. The sequence must be accessed in strictly sequential order; random access and readline() cannot be mixed.

    With mode you can specify which file mode will be passed to open(). It must be one of 'r', 'rU', 'U' and 'rb'.

    The openhook, when given, must be a function that takes two arguments, filename and mode, and returns an accordingly opened file-like object. You cannot use inplace and openhook together.

    Changed in version 2.5: Added the mode and openhook parameters.

Optional in-place filtering: if the keyword argument inplace=1 is passed to fileinput.input() or to the FileInput constructor, the file is moved to a backup file and standard output is directed to the input file (if a file of the same name as the backup file already exists, it will be replaced silently). This makes it possible to write a filter that rewrites its input file in place. If the backup parameter is given (typically as backup='.<some extension>'), it specifies the extension for the backup file, and the backup file remains around; by default, the extension is '.bak' and it is deleted when the output file is closed. In-place filtering is disabled when standard input is read.

Note

The current implementation does not work for MS-DOS 8+3 filesystems.

The two following opening hooks are provided by this module:

fileinput.hook_compressed(filename, mode)

    Transparently opens files compressed with gzip and bzip2 (recognized by the extensions '.gz' and '.bz2') using the gzip and bz2 modules. If the filename extension is not '.gz' or '.bz2', the file is opened normally (ie, using open() without any decompression).

    Usage example: fi = fileinput.FileInput(openhook=fileinput.hook_compressed)

    New in version 2.5.

fileinput.hook_encoded(encoding)

    Returns a hook which opens each file with codecs.open(), using the given encoding to read the file.

    Usage example: fi = fileinput.FileInput(openhook=fileinput.hook_encoded("iso-8859-1"))

    Note

    With this hook, FileInput might return Unicode strings depending on the specified encoding.

    New in version 2.5.
| 10.3. stat — Interpreting stat() results
| 10.4. statvfs — Constants used with os.statvfs()
| 10.5. filecmp — File and Directory Comparisons
| 
|     10.5.1. The dircmp class
| 
| 10.6. tempfile — Generate temporary files and directories
| 10.7. glob — Unix style pathname pattern expansion
| 10.8. fnmatch — Unix filename pattern matching
| 10.9. linecache — Random access to text lines
| 10.10. shutil — High-level file operations
| 
|     10.10.1. Directory and files operations
|         10.10.1.1. copytree example
|     10.10.2. Archiving operations
|         10.10.2.1. Archiving example
| 
| 10.11. dircache — Cached directory listings
| 10.12. macpath — Mac OS 9 path manipulation functions

			36.2.1.2 get current working directory
os.getcwd()
  dirname, filename = os.path.split(os.path.abspath(__file__))
			36.2.1.3
		36.2.2


	36.3 Time

	    36.3.1 Simple measure elapsed time
In [1]: import time

In [2]: st = time.time()

In [3]: st
Out[3]: 1619354658.059767

In [4]: et = time.time()

In [5]: et-st
Out[5]: 13.22887897491455

If you just want to measure the elapsed wall-clock time between two points, you could use time.time():

import time

start = time.time()
print("hello")
end = time.time()
print(end - start)
This gives the execution time in seconds.

Another option since 3.3 might be to use perf_counter or process_time, depending on your requirements. Before 3.3 it was recommended to use time.clock (thanks Amber). However, it is currently deprecated:

On Unix, return the current processor time as a floating point number expressed in seconds. The precision, and in fact the very definition of the meaning of “processor time”, depends on that of the C function of the same name.

On Windows, this function returns wall-clock seconds elapsed since the first call to this function, as a floating point number, based on the Win32 function QueryPerformanceCounter(). The resolution is typically better than one microsecond.

Deprecated since version 3.3: The behaviour of this function depends on the platform: use perf_counter() or process_time() instead, depending on your requirements, to have a well defined behaviour.

	    36.3.2

	36.4 dates

	    36.4.1 pretty print date and time
https://stackoverflow.com/questions/311627/how-to-print-a-date-in-a-regular-format
The date, datetime, and time objects all support a strftime(format) method, to create a string representing the time under the control of an explicit format string.

Here is a list of the format codes with their directive and meaning.

    %a  Locale’s abbreviated weekday name.
    %A  Locale’s full weekday name.      
    %b  Locale’s abbreviated month name.     
    %B  Locale’s full month name.
    %c  Locale’s appropriate date and time representation.   
    %d  Day of the month as a decimal number [01,31].    
    %f  Microsecond as a decimal number [0,999999], zero-padded on the left
    %H  Hour (24-hour clock) as a decimal number [00,23].    
    %I  Hour (12-hour clock) as a decimal number [01,12].    
    %j  Day of the year as a decimal number [001,366].   
    %m  Month as a decimal number [01,12].   
    %M  Minute as a decimal number [00,59].      
    %p  Locale’s equivalent of either AM or PM.
    %S  Second as a decimal number [00,61].
    %U  Week number of the year (Sunday as the first day of the week)
    %w  Weekday as a decimal number [0(Sunday),6].   
    %W  Week number of the year (Monday as the first day of the week)
    %x  Locale’s appropriate date representation.    
    %X  Locale’s appropriate time representation.    
    %y  Year without century as a decimal number [00,99].    
    %Y  Year with century as a decimal number.   
    %z  UTC offset in the form +HHMM or -HHMM.
    %Z  Time zone name (empty string if the object is naive).    
    %%  A literal '%' character.
This is what we can do with the datetime and time modules in Python

    import time
    import datetime

    print "Time in seconds since the epoch: %s" %time.time()
    print "Current date and time: ", datetime.datetime.now()
    print "Or like this: ", datetime.datetime.now().strftime("%y-%m-%d-%H-%M")


    print "Current year: ", datetime.date.today().strftime("%Y")
    print "Month of year: ", datetime.date.today().strftime("%B")
    print "Week number of the year: ", datetime.date.today().strftime("%W")
    print "Weekday of the week: ", datetime.date.today().strftime("%w")
    print "Day of year: ", datetime.date.today().strftime("%j")
    print "Day of the month : ", datetime.date.today().strftime("%d")
    print "Day of week: ", datetime.date.today().strftime("%A")
That will print out something like this:

    Time in seconds since the epoch:    1349271346.46
    Current date and time:              2012-10-03 15:35:46.461491
    Or like this:                       12-10-03-15-35
    Current year:                       2012
    Month of year:                      October
    Week number of the year:            40
    Weekday of the week:                3
    Day of year:                        277
    Day of the month :                  03
    Day of week:                        Wednesday
	    36.4.2

	36.5 tips and tricks

	    36.5.1 redirect prints to stdout
on my mac sometime > seems not to work
in this case can do:
import sys
sys.stdout = open('file', 'w')
print('test')
sys.stdout.close()
(from https://stackoverflow.com/questions/4675728/redirect-stdout-to-a-file-in-python)

	36.6
37. Concepts, syntax constructs

	37.1 List comprehension
List comprehensions provide a concise way to create lists. Common applications are to make new lists where each element is the result of some operations applied to each member of another sequence or iterable, or to create a subsequence of those elements that satisfy a certain condition.

For example, assume we want to create a list of squares, like:
>>>

>>> squares = []
>>> for x in range(10):
...     squares.append(x**2)
...
>>> squares
[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]

We can obtain the same result with:

squares = [x**2 for x in range(10)]

This is also equivalent to squares = map(lambda x: x**2, range(10)), but it’s more concise and readable.

A list comprehension consists of brackets containing an expression followed by a for clause, then zero or more for or if clauses. The result will be a new list resulting from evaluating the expression in the context of the for and if clauses which follow it. For example, this listcomp combines the elements of two lists if they are not equal:
>>>

>>> [(x, y) for x in [1,2,3] for y in [3,1,4] if x != y]
[(1, 3), (1, 4), (2, 3), (2, 1), (2, 4), (3, 1), (3, 4)]

and it’s equivalent to:
>>>

>>> combs = []
>>> for x in [1,2,3]:
...     for y in [3,1,4]:
...         if x != y:
...             combs.append((x, y))
...
>>> combs
[(1, 3), (1, 4), (2, 3), (2, 1), (2, 4), (3, 1), (3, 4)]

Note how the order of the for and if statements is the same in both these snippets.

If the expression is a tuple (e.g. the (x, y) in the previous example), it must be parenthesized.
>>>

>>> vec = [-4, -2, 0, 2, 4]
>>> # create a new list with the values doubled
>>> [x*2 for x in vec]
[-8, -4, 0, 4, 8]
>>> # filter the list to exclude negative numbers
>>> [x for x in vec if x >= 0]
[0, 2, 4]
>>> # apply a function to all the elements
>>> [abs(x) for x in vec]
[4, 2, 0, 2, 4]
>>> # call a method on each element
>>> freshfruit = ['  banana', '  loganberry ', 'passion fruit  ']
>>> [weapon.strip() for weapon in freshfruit]
['banana', 'loganberry', 'passion fruit']
>>> # create a list of 2-tuples like (number, square)
>>> [(x, x**2) for x in range(6)]
[(0, 0), (1, 1), (2, 4), (3, 9), (4, 16), (5, 25)]
>>> # the tuple must be parenthesized, otherwise an error is raised
>>> [x, x**2 for x in range(6)]
  File "<stdin>", line 1
    [x, x**2 for x in range(6)]
               ^
SyntaxError: invalid syntax
>>> # flatten a list using a listcomp with two 'for'
>>> vec = [[1,2,3], [4,5,6], [7,8,9]]
>>> [num for elem in vec for num in elem]
[1, 2, 3, 4, 5, 6, 7, 8, 9]

List comprehensions can contain complex expressions and nested functions:
>>>

>>> from math import pi
>>> [str(round(pi, i)) for i in range(1, 6)]
['3.1', '3.14', '3.142', '3.1416', '3.14159']


	37.2 With as statement

		37.2.1 Understanding Python's "with" statement

Fredrik Lundh | October 2006 | Originally posted to online.effbot.org

Judging from comp.lang.python and other forums, Python 2.5’s new with statement seems to be a bit confusing even for experienced Python programmers.

As most other things in Python, the with statement is actually very simple, once you understand the problem it’s trying to solve. Consider this piece of code:

    set things up
    try:
        do something
    finally:
        tear things down

Here, “set things up” could be opening a file, or acquiring some sort of external resource, and “tear things down” would then be closing the file, or releasing or removing the resource. The try-finally construct guarantees that the “tear things down” part is always executed, even if the code that does the work doesn’t finish.

If you do this a lot, it would be quite convenient if you could put the “set things up” and “tear things down” code in a library function, to make it easy to reuse. You can of course do something like

    def controlled_execution(callback):
        set things up
        try:
            callback(thing)
        finally:
            tear things down

    def my_function(thing):
        do something

    controlled_execution(my_function)

But that’s a bit verbose, especially if you need to modify local variables. Another approach is to use a one-shot generator, and use the for-in statement to “wrap” the code:

    def controlled_execution():
        set things up
        try:
            yield thing
        finally:
            tear things down

    for thing in controlled_execution():
        do something with thing

But yield isn’t even allowed inside a try-finally in 2.4 and earlier. And while that could be fixed (and it has been fixed in 2.5), it’s still a bit weird to use a loop construct when you know that you only want to execute something once.

So after contemplating a number of alternatives, GvR and the python-dev team finally came up with a generalization of the latter, using an object instead of a generator to control the behaviour of an external piece of code:

    class controlled_execution:
        def __enter__(self):
            set things up
            return thing
        def __exit__(self, type, value, traceback):
            tear things down

    with controlled_execution() as thing:
         some code

Now, when the “with” statement is executed, Python evaluates the expression, calls the __enter__ method on the resulting value (which is called a “context guard”), and assigns whatever __enter__ returns to the variable given by as. Python will then execute the code body, and no matter what happens in that code, call the guard object’s __exit__ method.

As an extra bonus, the __exit__ method can look at the exception, if any, and suppress it or act on it as necessary. To suppress the exception, just return a true value. For example, the following __exit__ method swallows any TypeError, but lets all other exceptions through:

    def __exit__(self, type, value, traceback):
        return isinstance(value, TypeError)

In Python 2.5, the file object has been equipped with __enter__ and __exit__ methods; the former simply returns the file object itself, and the latter closes the file:

    >>> f = open("x.txt")
    >>> f
    <open file 'x.txt', mode 'r' at 0x00AE82F0>
    >>> f.__enter__()
    <open file 'x.txt', mode 'r' at 0x00AE82F0>
    >>> f.read(1)
    'X'
    >>> f.__exit__(None, None, None)
    >>> f.read(1)
    Traceback (most recent call last):
      File "<stdin>", line 1, in <module>
    ValueError: I/O operation on closed file

so to open a file, process its contents, and make sure to close it, you can simply do:

with open("x.txt") as f:
    data = f.read()
    do something with data

This wasn’t very difficult, was it?

		37.2.2 The with statement

New in version 2.5.

The with statement is used to wrap the execution of a block with methods defined by a context manager (see section With Statement Context Managers). This allows common try...except...finally usage patterns to be encapsulated for convenient reuse.

with_stmt ::=  "with" with_item ("," with_item)* ":" suite
with_item ::=  expression ["as" target]

The execution of the with statement with one “item” proceeds as follows:

    The context expression (the expression given in the with_item) is evaluated to obtain a context manager.

    The context manager’s __exit__() is loaded for later use.

    The context manager’s __enter__() method is invoked.

    If a target was included in the with statement, the return value from __enter__() is assigned to it.

    Note

    The with statement guarantees that if the __enter__() method returns without an error, then __exit__() will always be called. Thus, if an error occurs during the assignment to the target list, it will be treated the same as an error occurring within the suite would be. See step 6
The with statement

New in version 2.5.

The with statement is used to wrap the execution of a block with methods defined by a context manager (see section With Statement Context Managers). This allows common try...except...finally usage patterns to be encapsulated for convenient reuse.

with_stmt ::=  "with" with_item ("," with_item)* ":" suite
with_item ::=  expression ["as" target]

The execution of the with statement with one “item” proceeds as follows:

    The context expression (the expression given in the with_item) is evaluated to obtain a context manager.

    The context manager’s __exit__() is loaded for later use.

    The context manager’s __enter__() method is invoked.

    If a target was included in the with statement, the return value from __enter__() is assigned to it.

    Note

    The with statement guarantees that if the __enter__() method returns without an error, then __exit__() will always be called. Thus, if an error occurs during the assignment to the target list, it will be treated the same as an error occurring within the suite would be. See step 6 below.

    The suite is executed.

    The context manager’s __exit__() method is invoked. If an exception caused the suite to be exited, its type, value, and traceback are passed as arguments to __exit__(). Otherwise, three None arguments are supplied.

    If the suite was exited due to an exception, and the return value from the __exit__() method was false, the exception is reraised. If the return value was true, the exception is suppressed, and execution continues with the statement following the with statement.

    If the suite was exited for any reason other than an exception, the return value from __exit__() is ignored, and execution proceeds at the normal location for the kind of exit that was taken.

With more than one item, the context managers are processed as if multiple with statements were nested:

with A() as a, B() as b:
    suite

is equivalent to

with A() as a:
    with B() as b:
        suite

Note

In Python 2.5, the with statement is only allowed when the with_statement feature has been enabled. It is always enabled in Python 2.6.

Changed in version 2.7: Support for multiple context expressions.


		37.2.3
	37.3
38. Graphs

	38.1  networkx 

		38.1.1  Tutorial
YIZAQ-M-D1BW:AD_map yizaq$ /usr/local/bin/python
Python 2.7.6 (default, Sep 14 2014, 15:59:27) 
[GCC 4.2.1 Compatible Apple LLVM 5.1 (clang-503.0.40)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
>>> import networkx as nx
>>> G=nx.Graph()
>>> G.add_node(1)
>>> G.add_nodes_from([2,3])
>>> G.add_edge(1,2)
>>> e=(2,3)
>>> G.add_edge(*e) # unpack edge tuple*
>>> G.number_of_nodes()
3
>>> G.number_of_edges()
2
>>> G.nodes()
[1, 2, 3]
>>> G.edges()
[(1, 2), (2, 3)]
>>> import matplotlib.pyplot as plt


		38.1.2   
		38.1.3   
		38.1.4   
		38.1.5   
		38.1.6   
		38.1.7   
		38.1.8   
		38.1.9   
		38.1.10  
		38.1.11  
		38.1.12  
		38.1.13  
		38.1.14  
		38.1.15  
		38.1.16  
		38.1.17  
		38.1.18  
		38.1.19  
		38.1.20  
		38.1.21  
		38.1.22  
		38.1.23  
		38.1.24  
		38.1.25  
		38.1.26  
		38.1.27  
		38.1.28  
		38.1.29  
		38.1.30  
		38.1.31  
		38.1.32  
		38.1.33  
		38.1.34  
		38.1.35  
		38.1.36  
		38.1.37  
		38.1.38  
		38.1.39  
		38.1.40  
		38.1.41  


	38.2   graphviz
    use w/o python
[i500695@C02X632CJGH6:2019-07-02 14:13:12:~/work/code/graphviz/sap:]2051$ cat sharedContent.dot 
digraph d {
 Site1 [label="Site1" fillcolor=red shape=circle fontcolor=blue]
 Site2 [label="Site2" fillcolor=red shape=circle fontcolor=blue]
 Site3 [label="Site3" fillcolor=red shape=circle fontcolor=blue]
 SiteRef1 [label="Site-References1" fillcolor=red shape=ellipse fontcolor=blue]
 SiteRef2 [label="Site-References2" fillcolor=red shape=ellipse fontcolor=blue]
 SiteRef3 [label="Site-References3" fillcolor=red shape=ellipse fontcolor=blue]
 Role1 [label="Role1" fillcolor=red shape=ellipse fontcolor=red]
 Role2 [label="Role2" fillcolor=red shape=ellipse fontcolor=red]
 Catalog1 [label="Catalog1" fillcolor=red shape=ellipse fontcolor=green]
 Catalog2 [label="Catalog2" fillcolor=red shape=ellipse fontcolor=green]
 Group1 [label="Group1" fillcolor=red shape=ellipse fontcolor=green]
 Group2 [label="Group2" fillcolor=red shape=ellipse fontcolor=green]
 BussinessApp1 [label="BussinessApp1" fillcolor=red shape=ellipse fontcolor=red]
 BussinessApp2 [label="BussinessApp2" fillcolor=red shape=ellipse fontcolor=red]
 BussinessApp3 [label="BussinessApp3" fillcolor=red shape=ellipse fontcolor=red]
 BussinessApp4 [label="BussinessApp4" fillcolor=red shape=ellipse fontcolor=red]
 Site1 -> {SiteRef1} [style = dotted label="contextId"] 
 Site2 -> {SiteRef2} [style = dotted label="contextId"]
 Site3 -> {SiteRef3} [style = dotted label="contextId"]
 SiteRef1 -> {Role1} [label="relation"]
 SiteRef2 -> {Role1} [label="relation"]
 SiteRef3 -> {Role2} [label="relation"]
 Role1 -> {BussinessApp1} [label="relation"]
 Role1 -> {BussinessApp2} [label="relation"]
 Role2 -> {BussinessApp2} [label="relation"]
 Role2 -> {BussinessApp3} [label="relation"]
 Role2 -> {BussinessApp4} [label="relation"]
 Catalog1 -> {BussinessApp1} [label="relation"]
 Catalog1 -> {BussinessApp2} [label="relation"]
 Group2 -> {BussinessApp3} [label="relation"]
 Group2 -> {BussinessApp4} [label="relation"]
}

$ dot -T png -O sharedContent.dot  && open sharedContent.dot.png


	38.3  
	38.4  
	38.5  
	38.6  
	38.7  
	38.8  
	38.9  
	38.10 
	38.11 
	38.12 
	38.13 
	38.14 
	38.15 
	38.16 
	38.17 
	38.18 
	38.19 
	38.20 
	38.21 
	38.22 
	38.23 
	38.24 
	38.25 
	38.26 
	38.27 
	38.28 
	38.29 
	38.30 
	38.31 


39. One-liners

	39.1 Powerful Python One-Liners
https://wiki.python.org/moin/Powerful%20Python%20One-Liners

		39.1.1 Want to know many bytes a terabyte is? If you know further abbreviations, you can extend the list.
import pprint;pprint.pprint(zip(('Byte', 'KByte', 'MByte', 'GByte', 'TByte'), (1 << 10*i for i in xrange(5))))
[('Byte', 1),
 ('KByte', 1024),
 ('MByte', 1048576),
 ('GByte', 1073741824),
 ('TByte', 1099511627776)]

		39.1.2 Convert decimal to hexadecimal
>>> print "%x"%31
1f
>>> print (hex(31))
0x1f

		39.1.3 Convert decimal to binary
>>> print bin(31)
0b11111
>>> print bin(32)
0b100000

		39.1.4 And what's the largest number that can be represented by 8 Byte?
>>> print '\n'.join("%i Byte = %i Bit = largest number: %i" % (j, j*8, 256**j-1) for j in (1 << i for i in xrange(8)))
1 Byte = 8 Bit = largest number: 255
2 Byte = 16 Bit = largest number: 65535
4 Byte = 32 Bit = largest number: 4294967295
8 Byte = 64 Bit = largest number: 18446744073709551615
16 Byte = 128 Bit = largest number: 340282366920938463463374607431768211455
32 Byte = 256 Bit = largest number: 115792089237316195423570985008687907853269984665640564039457584007913129639935
64 Byte = 512 Bit = largest number: 13407807929942597099574024998205846127479365820592393377723561443721764030073546976801874298166903427690031858186486050853753882811946569946433649006084095
128 Byte = 1024 Bit = largest number: 179769313486231590772930519078902473361797697894230657273430081157732675805500963132708477322407536021120113879871393357658789768814416622492847430639474124377767893424865485276302219601246094119453082952085005768838150682342462881473913110540827237163350510684586298239947245938479716304835356329624224137215

		39.1.5 Find prime numbers
The following example is one way to compute prime numbers in Python (not the most efficient one, though):

>>> nums = range(2, 50) 
>>> for i in range(2, 8): 
...     nums = filter(lambda x: x == i or x % i, nums)
... 
>>> print nums
[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]
How does it work? First, we put all numbers from 2 to 49 into a list called "nums". Then we have a "for" loop that iterates over all possible divisors, i.e. the value of "i" goes from 2 to 7. Naturally, all numbers that are multiples of those divisors cannot be prime numbers, so we use a filter function to remove them from the list. (This algorithm is called "the sieve of Eratosthenes".)

In the above case, the filter function simply says: "Leave the element in the list if it is equal to i, or if it leaves a non-zero remainder when divided by i. Otherwise remove it from the list." After the filtering loop finishes, only prime numbers are left, of course. I am not aware of a language in which you can do the same thing with built-in features as compact and as readable as in Python (except for functional programming languages).

each iteration leaves one divisor and removes all numbers that divide by it. ex. 1st pass sim:
>>> nums = range(2, 50)
>>> print nums
[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]
>>> num2 = filter( lambda x: x==2 or x%2, nums)
>>> print num2
[2, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49]

next pass would leave 3 and remove 9,15 etc 

		39.1.6 find length of each word in sentence 
>>> print map(lambda x: len(x) , "my name is yosi izaq".split() ) 
[2, 4, 2, 4, 4]

long version
>>> sentence = 'It is raining cats and dogs'
>>> words = sentence.split()
>>> print words
['It', 'is', 'raining', 'cats', 'and', 'dogs']
>>> 
>>> lengths = map(lambda word: len(word), words)
>>> print lengths
[2, 2, 7, 4, 3, 4]

		39.1.7 Get list of mounts
Here's an example from the UNIX scripting world: We want to find all mount points in our file system. To do that, we execute the external "mount" command and parse the output.

>>> import commands
>>> 
>>> mount = commands.getoutput('mount -v')
>>> lines = mount.splitlines()
>>> points = map(lambda line: line.split()[2], lines)
>>> 
>>> print points
['/', '/var', '/usr', '/usr/local', '/tmp', '/proc']
The getoutput function from the commands module (which is part of the Python standard library) runs the given command and returns its output as a single string. Therefore, we split it up into separate lines first. Finally we use "map" with a lambda function that splits each line (on whitespace, which is the default) and returns just the third element of the result, which is the mountpoint.

Again, we could write all of that in one single statement, which increases compactness but reduces readability:

print map(lambda x: x.split()[2], commands.getoutput('mount -v').splitlines())
['/', '/var', '/usr', '/usr/local', '/tmp', '/proc']
When writing "real-world" scripts, it is recommended to split up complex statements so that it is easier to see what it does. Also, it is easier to make changes.

However, the task of splitting up the output of a command into a list of lines is very common. You need it all the time when parsing the output of external commands. Therefore, it is common practice to include the split operation on the getoutput line, but do the rest separately. This is a good trade-off between compactness and readability:

>>> lines = commands.getoutput('mount -v').splitlines()
>>> 
>>> points = map(lambda line: line.split()[2], lines)
>>> print points
['/', '/var', '/usr', '/usr/local', '/tmp', '/proc']
An even better idea is probably to write a small function for that task, which encapsulates the job of running the command and splitting the output.

On a related note, you can also use so-called list comprehensions to construct lists from other lists. Sometimes this is preferable because of efficiency or readability. The previous example could very well be rewritten using a list comprehension:

>>> lines = commands.getoutput('mount -v').splitlines()
>>> 
>>> points = [line.split()[2] for line in lines]
>>> print points
['/', '/var', '/usr', '/usr/local', '/tmp', '/proc']
In many cases, you can use list comprehensions instead of map() or filter(). It depends on the situation which one should be preferred.

Note: The commands module is deprecated in newer versions of Python (though it still works in all 2.x versions). Instead, the subprocess module should be used which is available since Python 2.4. The commands.getoutput() function can be replaced by subprocess.check_output(). Please refer to the documentation for details.
------------------------------------------------------------------------------------------------------------------------
My example on MAC:
>>> commands.getoutput('mount -v').splitlines()
['/dev/disk1 on / (hfs, local, journaled)', 'devfs on /dev (devfs, local, nobrowse)', 'map -hosts on /net (autofs, nosuid, automounted, nobrowse)', 'map auto_home on /home (autofs, automounted, nobrowse)', 'localhost:/yEoLxzuyz2wXPaBjb-5qyL on /Volumes/MobileBackups (mtmfs, nosuid, read-only, nobrowse)', '/dev/disk2s2 on /Volumes/My Passport (hfs, local, nodev, nosuid, journaled)']
>>> print map(lambda x: x.split()[2], commands.getoutput('mount -v').splitlines())
['/', '/dev', 'on', 'on', '/Volumes/MobileBackups', '/Volumes/My']

Note it doesnt work perfectly since some mounts are 4th element. 
my fix, filter out not starting with /
>>> print map(lambda x: filter(lambda w: w[0] == '/' , x.split()) , commands.getoutput('mount -v').splitlines())
[['/dev/disk1', '/'], ['/dev'], ['/net'], ['/home'], ['/Volumes/MobileBackups'], ['/dev/disk2s2', '/Volumes/My']]

------------------------------------------------------------------------------------------------------------------------


		39.1.8 Decode a base64 encoded file 
import base64, sys; base64.decode(open(sys.argv[1], "rb"), open(sys.argv[2], "wb"))

		39.1.9 Editing a list of files in place
- print all files
>>> for i in os.walk('.'):
...     print i[2]

-
I came up with this one-liner in response to an article that said it couldn't be done as an one-liner in Python.

What this does is replace the substring "at" by "op" on all lines of all files (in place) under the path specified (here, the current path).

Caution: Don't run this on your home directory or you're going to get all your text files edited.

Toggle line numbers
   1 import sys,os,re,fileinput;a=[i[2] for i in os.walk('.') if i[2]] [0];[sys.stdout.write(re.sub('at','op',j)) for j in fileinput.input(a,inplace=1)]
Clearer is: import os.path; a=[f for f in os.listdir('.') if not os.path.isdir(f)]

		39.1.10 Reimplementing cut 
Print every line from an input file but remove the first two fields.  
python -c "import sys;[sys.stdout.write(' '.join(line.split(' ')[2:])) for line in sys.stdin]" < input.txt

		39.1.11 Set of all subsets

Function that returns the set of all subsets of its argument

f = lambda x: [[y for j, y in enumerate(set(x)) if (i >> j) & 1] for i in range(2**len(set(x)))]

breakdown:
- for i in range(2**len(set(x))) , run 0  - 2^len(set) -1 (number of sets in power set)
-  [y for j, y in enumerate(set(x)) if (i >> j) & 1]  , add element to set if the bit corresponding to its place (j) is 1 in current i
- 

>>>

>>>f([10,9,1,10,9,1,1,1,10,9,7])
[[], [9], [10], [9, 10], [7], [9, 7], [10, 7], [9, 10, 7], [1], [9, 1], [10, 1], [9, 10, 1], [7, 1], [9, 7, 1], [10, 7, 1], [9, 10, 7, 1]]
-RJW

Alternately (shorter, more functional version):


f = lambda l: reduce(lambda z, x: z + [y + [x] for y in z], l, [[]])

		39.1.12 Apply regular expression to lines from stdin
[yizaq@YIZAQ-M-D1BW:Sun Feb 07:~:]$ cat ~/stam    | python -c "import sys,re;[sys.stdout.write(re.sub('PATTERN', 'SUBSTITUTION', line)) for line in sys.stdin]"
Radius,2015-12-07 12:01:14,793,DEBUG,0x7f13f2777700,cntx=0012382022,Received data from 10.172.84.2:58317 : 
0000: 01 5f 00 5f 61 be 76 b7  d5 f8 29 e4 e1 7e 91 f9  ._._a.v...)..~.. 
0010: 98 bd 53 37 01 0a 46 35  5f 50 52 4f 42 45 02 22  ..S7..F5_PROBE." 
0020: f4 68 26 89 09 5b af d8  17 8f 97 29 11 8e 7a cd  .h&..[.....)..z. 
0030: 0c 69 cf 20 a8 37 d5 b9  4f 0f 8b a6 03 dd 1e 9e  .i. .7..O....... 
0040: 20 1f 47 53 46 52 44 43  45 2d 4c 42 30 30 33 2e   .GSFRDCE-LB003. 
0050: 6d 75 78 2e 69 73 69 6e  66 72 61 2e 6e 65 74     mux.isinfra.net ,RADIUSHandler.cpp:371
[yizaq@YIZAQ-M-D1BW:Sun Feb 07:~:]$ cat ~/stam    | python -c "import sys,re;[sys.stdout.write(re.sub('PROBE', 'YOSI', line)) for line in sys.stdin]"
Radius,2015-12-07 12:01:14,793,DEBUG,0x7f13f2777700,cntx=0012382022,Received data from 10.172.84.2:58317 : 
0000: 01 5f 00 5f 61 be 76 b7  d5 f8 29 e4 e1 7e 91 f9  ._._a.v...)..~.. 
0010: 98 bd 53 37 01 0a 46 35  5f 50 52 4f 42 45 02 22  ..S7..F5_YOSI." 
0020: f4 68 26 89 09 5b af d8  17 8f 97 29 11 8e 7a cd  .h&..[.....)..z. 
0030: 0c 69 cf 20 a8 37 d5 b9  4f 0f 8b a6 03 dd 1e 9e  .i. .7..O....... 
0040: 20 1f 47 53 46 52 44 43  45 2d 4c 42 30 30 33 2e   .GSFRDCE-LB003. 
0050: 6d 75 78 2e 69 73 69 6e  66 72 61 2e 6e 65 74     mux.isinfra.net ,RADIUSHandler.cpp:371

		39.1.13 More one-liners
Modify lines from stdin using map


python -c "import sys; tmp = lambda x: sys.stdout.write(x.split()[0]+'\t'+str(int(x.split()[1])+1)+'\n'); map(tmp, sys.stdin);"
Display List of all users on Unix-like systems


print '\n'.join(line.split(":",1)[0] for line in open("/etc/passwd"))
CSV file to json


python -c "import csv,json;print json.dumps(list(csv.reader(open('csv_file.csv'))))"
Compress CSS file


python -c 'import re,sys;print re.sub("\s*([{};,:])\s*", "\\1", re.sub("/\*.*?\*/", "", re.sub("\s+", " ", sys.stdin.read())))'
Decode string written in Hex


python -c "print ''.join(chr(int(''.join(i), 16)) for i in zip(*[iter('474e552773204e6f7420556e6978')]*2))"
Retrieve content text from HTTP data


python -c "import sys; print sys.stdin.read().replace('\r','').split('\n\n',2)[1]";
Prints file extension


print '~/python/one-liners.py'.split('.')[-1]
Escapes content from stdin

This can be used to convert a string into a "url safe" string


python -c "import urllib, sys ; print urllib.quote_plus(sys.stdin.read())";
Reverse lines in stdin


python -c "import sys; print '\n'.join(reversed(sys.stdin.read().split('\n')))"
Print top 10 lines of stdin


python -c "import sys; sys.stdout.write(''.join(sys.stdin.readlines()[:10]))" < /path/to/your/file

		39.1.14
	39.2

40. Python Intermiddiate course

	40.1 Search Engine demo

		40.1.1 DB
YIZAQ-M-W1ZV:docs yizaq$ pwd
/Users/yizaq/Desktop/Work/code/python/python_inter_course/SearchEngine/docs
YIZAQ-M-W1ZV:docs yizaq$ sqlite3 documents.sqlite3 

		40.1.2 REST demo
    python3 -m httpie localhost:8080/df/v1/search?word=barry&word=zip
    python3 -m httpie localhost:8080/df/v1/search?word=barry&word=zip
    python3 -m httpie 'localhost:8080/df/v1/'
    python3 -m httpie POST 'localhost:8080/df/v1/aaa'
     echo 'Mary had a little lamb' | python3 -m httpie POST 'localhost:8080/df/v1/marry'
     python3 -m httpie 'localhost:8080/df/v1/search?word=barry&word=zip'


		40.1.3 Run
Open pycharm
Open SearchEngine project 
Run init 1/ 1 set in main to create db
then run httpd to start web server

		40.1.4

	40.2 Lucene based, whoosh local files search engine

	    40.2.1 whoosh blog
Developing a fast Indexing and Full text Search Engine with Whoosh: A Pure-Python Library
 by Abhijeet Kumar  July 31, 2018  Natural Language Processing

Problem Statement: To simply put, You have 1 million text files in a directory and your application must cater text query search on all files within few seconds (say ~1-2 seconds). How will you develop such system !!.

Motivation: The idea came from my previous post “Performing OCR by running parallel instances of Tesseract 4.0 : Python“. Saying that following could be some use cases where you may have to build such search engine on top of other applications. e.g.

You have built an OCR app and converted millions of images into text files. You may want to build a search engine over converted text files to search contents of images.
You have built a speech to text system where you are converting thousands of recorded audios into text data. You may like to search contents of audio in real time.
Here is a video demonstration of an desktop app developed in QT. It is a whoosh python implementation working in back end.


Introduction: Whoosh
Some of you might have heard about a popular java based library “Lucene” which is a search engine library written entirely in Java. You may find a python wrapper for Lucene. If you are looking for similar pythonic library, “Whoosh” is the one. Whoosh is a fast, featureful full-text indexing and searching library implemented in pure Python. Programmers can use it to easily add search functionality to their applications and websites.

Whoosh pypi package can simply be installed with pip:
pip install Whoosh

For the example demonstrated in this blog-post, You can download  a data-set of 70,000 text files which were taken from simple wiki articles from here.

1. Creating Indexed Data: Whoosh
It is easy to index all your text files with Whoosh. Initially, the schema of the index has to be defined. Schema defines list of fields to be indexed or stored for each text file. It’s similar to how we define it for database. A field is a piece of information for each document in the index, such as its title or text content. Indexing of a field means it can be searched and it is also returned with results if defined as argument (stored=True) in schema. You only need to create the schema once while creating the index.

Finally, all the text documents are added to index writer in loop. Documents are indexed as per schema and has to be added as per schema design. Below is the python implementation for indexing  all the text documents of a directory.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
import os
from whoosh.index import create_in
from whoosh.fields import Schema, TEXT, ID
import sys
 
def createSearchableData(root):   
 
    '''
    Schema definition: title(name of file), path(as ID), content(indexed
    but not stored),textdata (stored text content)
    '''
    schema = Schema(title=TEXT(stored=True),path=ID(stored=True),\
              content=TEXT,textdata=TEXT(stored=True))
    if not os.path.exists("indexdir"):
        os.mkdir("indexdir")
 
    # Creating a index writer to add document as per schema
    ix = create_in("indexdir",schema)
    writer = ix.writer()
 
    filepaths = [os.path.join(root,i) for i in os.listdir(root)]
    for path in filepaths:
        fp = open(path,'r')
        print(path)
        text = fp.read()
        writer.add_document(title=path.split("\\")[1], path=path,\
          content=text,textdata=text)
        fp.close()
    writer.commit()
 
root = "corpus"
createSearchableData(root)
2. Querying Indexed Data : Whoosh
Querying a indexed data has two important parts which you may like to look upon.

Query String : It is passed while searching the indexed data. Query string can be a single word, a single sentence to be matched exactly, multiple words with ‘AND’, multiple words with ‘OR’ etc. For examples –

Query : politics (returns if the word occurs)Query : sports OR games OR play (returns if any one of the strings occur)
Query : alpha beta gamma (return if a document contains all strings)
Query : “alpha beta gamma“ (returns if all strings occur together in a document).

Scoring : Each document is ranked according to a scoring function. There are quite a few types of scoring function supported by whoosh.

Frequency : It simply returns the count of the terms occurred in the document. It does not perform any normalization or weighting.
Tf-Idf scores : It returns tf * idf scores of each document. To know more read here or wiki page here.
BM25F scoring : It is the by default ranking function used by whoosh. BM stands for best matching. It is based on tf-idf along with bunch of factors like length of document in words, average length of documents in the collection. It also has free parameters k = 1.2 and b = 0.75. To read more check here.
Cosine scoring : It is useful for finding document similar to your search query.
There are few more scoring algorithms which has been implemented. Check here to know more.

Below is the python implementation for searching a query in the indexed database.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
from whoosh.qparser import QueryParser
from whoosh import scoring
from whoosh.index import open_dir
 
ix = open_dir("indexdir")
 
# query_str is query string
query_str = sys.argv[1]
# Top 'n' documents as result
topN = int(sys.argv[2])
 
with ix.searcher(weighting=scoring.Frequency) as searcher:
    query = QueryParser("content", ix.schema).parse(query_str)
results = searcher.search(query,limit=topN)
for i in range(topN):
    print(results[i]['title'], str(results[i].score), results[i]['textdata'])
QueryParser class of whoosh implements query language very similar to java Lucene’s.

3. Glossary
Below are the basic terminologies you will always come across in discussions involving searching and indexing documents (taken from whoosh docs).

Corpus
The set of documents you are indexing.

Documents
The individual pieces of content you want to make searchable. The word “documents” might imply files, but the data source could really be anything – articles in a content management system, blog posts in a blogging system, chunks of a very large file, rows returned from an SQL query, individual email messages from a mailbox file, or whatever. When you get search results from Whoosh, the results are a list of documents, whatever “documents” means in your search engine.

Fields
Each document contains a set of fields. Typical fields might be “title”, “content”, “url”, “keywords”, “status”, “date”, etc. Fields can be indexed (so they’re searchable) and/or stored with the document. Storing the field makes it available in search results. For example, you typically want to store the “title” field so your search results can display it.

Forward index
A table listing every document and the words that appear in the document. Whoosh lets you store term vectors that are a kind of forward index.

Indexing
The process of examining documents in the corpus and adding them to the reverse index.

Postings
The reverse index lists every word in the corpus, and for each word, a list of documents in which that word appears, along with some optional information (such as the number of times the word appears in that document). These items in the list, containing a document number and any extra information, are called postings. In Whoosh the information stored in postings is customizable for each field.

Reverse Index
Basically a table listing every word in the corpus, and for each word, the list of documents in which it appears. It can be more complicated (the index can also list how many times the word appears in each document, the positions at which it appears, etc.) but that’s how it basically works.

Schema
Whoosh requires that you specify the fields of the index before you begin indexing. The Schema associates field names with metadata about the field, such as the format of the postings and whether the contents of the field are stored in the index.

Term vector
A forward index for a certain field in a certain document. You can specify in the Schema that a given field should store term vectors.

At the End
Hope it was an easy read and good heads-up to start with. So, what can we do more from here:

Searching for alike documents instead of exact term searches only.
Exploring hierarchical search in file system.
Correcting errors in the queries. Did you mean…?
Search N-grams for getting fast, “search as you type” functionality
Update: Readers can download the backup of QT application built from below link. It was a old work and don’t have a working app now. You may have to figure out codes in order to reproduce it. Good luck with that.
https://drive.google.com/open?id=1skp4JM6LT7ijJ2MF0qrBHTn-GDMkC1c-

If you liked the post, follow this blog to get updates about upcoming articles. Also, share it so that it can reach out to the readers who can actually gain from this. Please feel free to discuss anything regarding the post. I would love to hear feedback from you.

Happy machine learning

	    40.2.2 my implementation

	    40.2.3

	40.3

41. machine learning

    41.1  tutorial point python machine learning
https://www.tutorialspoint.com/machine_learning_with_python/index.htm

        41.1.1   setup
Libraries and Packages
To understand machine learning, you need to have basic knowledge of Python programming. In addition, there are a number of libraries and packages generally used in performing various machine learning tasks as listed below −

numpy − is used for its N-dimensional array objects

pandas − is a data analysis library that includes dataframes

matplotlib − is 2D plotting library for creating graphs and plots

scikit-learn − the algorithms used for data analysis and data mining tasks

seaborn − a data visualization library based on matplotlib

Installation
You can install software for machine learning in any of the two methods as discussed here −

Method 1
Download and install Python separately from python.org on various operating systems as explained below −

To install Python after downloading, double click the .exe (for Windows) or .pkg (for Mac) file and follow the instructions on the screen.

For Linux OS, check if Python is already installed by using the following command at the prompt −

$ python --version. ...
If Python 2.7 or later is not installed, install Python with the distribution's package manager. Note that the command and package name varies.

On Debian derivatives such as Ubuntu, you can use apt −

$ sudo apt-get install python3
Now, open the command prompt and run the following command to verify that Python is installed correctly −

$ python3 --version

Python 3.6.2
Similarly, we can download and install necessary libraries like numpy, matplotlib etc. individually using installers like pip. For this purpose, you can use the commands shown here −

$pip install numpy
$pip install matplotlib
$pip install pandas
$pip install seaborn


[i500695@C02X632CJGH6:2019-01-13 10:41:07:~/Desktop/work/code/python/machine_learning:]586$ pwd
/Users/i500695/Desktop/work/code/python/machine_learning
 pip3 install numpy && pip3 install matplotlib && pip3 install pandas && pip3 install seaborn
[i500695@C02X632CJGH6:2019-01-13 10:41:43:~/Desktop/work/code/nodejs/jwtExample:]504$ python3
Python 3.7.1 (default, Nov  6 2018, 18:46:03) 
[Clang 10.0.0 (clang-1000.11.45.5)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
  File "/Users/i500695/.pythonrc", line 110
    print "Welcome %s, time is %s. Have a good session :-) "%(os.environ['USER'], time.strftime('%X %x %Z') )
                                                           ^
|  SyntaxError: invalid syntax
|  >>> import numpy
|  >>> print(numpy.__version__)
|  1.15.4
|  >>> import matplotlib
|  >>> print(matplotlib.__version__)
|  3.0.2
|  >>> import pandas
|  >>> print(pandas.__version__)
|  0.23.4
|  >>> import seaborn
|  >>> print(seaborn.__version__)
|  0.9.0

Method 2
Alternatively, to install Python and other scientific computing and machine learning packages simultaneously, we should install Anaconda distribution. It is a Python implementation for Linux, Windows and OSX, and comprises various machine learning packages like numpy, scikit-learn, and matplotlib. It also includes Jupyter Notebook, an interactive Python environment. We can install Python 2.7 or any 3.x version as per our requirement.

To download the free Anaconda Python distribution from Continuum Analytics, you can do the following −

Visit the official site of Continuum Analytics and its download page. Note that the installation process may take 15-20 minutes as the installer contains Python, associated packages, a code editor, and some other files. Depending on your operating system, choose the installation process as explained here −

For Windows − Select the Anaconda for Windows section and look in the column with Python 2.7 or 3.x. You can find that there are two versions of the installer, one for 32-bit Windows, and one for 64-bit Windows. Choose the relevant one.

For Mac OS − Scroll to the Anaconda for OS X section. Look in the column with Python 2.7 or 3.x. Note that here there is only one version of the installer: the 64-bit version.

For Linux OS − We select the "Anaconda for Linux" section. Look in the column with Python 2.7 or 3.x.

Note that you have to ensure that Anaconda’s Python distribution installs into a single directory, and does not affect other Python installations, if any, on your system.

To work with graphs and plots, we will need these Python library packages - matplotlib and seaborn.

If you are using Anaconda Python, your system already has numpy, matplotlib, pandas, seaborn, etc. installed. We start the Anaconda Navigator to access either Jupyter Note book or Spyder IDE of python.

After opening either of them, type the following commands −

import numpy

import matplotlib
Now, we need to check if installation is successful. For this, go to the command line and type in the following command −

$ python
Python 3.6.3 |Anaconda custom (32-bit)| (default, Oct 13 2017, 14:21:34)
[GCC 7.2.0] on linux
Next, you can import the required libraries and print their versions as shown −

|  >>>import numpy
|  >>>print numpy.__version__
|  1.14.2
|  
|  >>> import matplotlib
|  >>> print (matplotlib.__version__)
|  2.1.2
|  
|  >> import pandas
|  >>> print (pandas.__version__)
|  0.22.0
|  
|  >>> import seaborn
|  >>> print (seaborn.__version__)
|  0.8.1

        41.1.2 What is machine_learning 
Machine Learning (ML) is an automated learning with little or no human intervention. It involves programming computers so that they learn from the available inputs. The main purpose of machine learning is to explore and construct algorithms that can learn from the previous data and make predictions on new input data.

The input to a learning algorithm is training data, representing experience, and the output is any expertise, which usually takes the form of another algorithm that can perform a task. The input data to a machine learning system can be numerical, textual, audio, visual, or multimedia. The corresponding output data of the system can be a floating-point number, for instance, the velocity of a rocket, an integer representing a category or a class, for example, a pigeon or a sunflower from image recognition.

In this chapter, we will learn about the training data our programs will access and how learning process is automated and how the success and performance of such machine learning algorithms is evaluated.

Concepts of Learning
Learning is the process of converting experience into expertise or knowledge.

Learning can be broadly classified into three categories, as mentioned below, based on the nature of the learning data and interaction between the learner and the environment.

Supervised Learning
Unsupervised Learning
Semi-supervised Learning
Similarly, there are four categories of machine learning algorithms as shown below −

Supervised learning algorithm
Unsupervised learning algorithm
Semi-supervised learning algorithm
Reinforcement learning algorithm
However, the most commonly used ones are supervised and unsupervised learning.

Supervised Learning
Supervised learning is commonly used in real world applications, such as face and speech recognition, products or movie recommendations, and sales forecasting. Supervised learning can be further classified into two types - Regression and Classification.

Regression trains on and predicts a continuous-valued response, for example predicting real estate prices.

Classification attempts to find the appropriate class label, such as analyzing positive/negative sentiment, male and female persons, benign and malignant tumors, secure and unsecure loans etc.

In supervised learning, learning data comes with description, labels, targets or desired outputs and the objective is to find a general rule that maps inputs to outputs. This kind of learning data is called labeled data. The learned rule is then used to label new data with unknown outputs.

Supervised learning involves building a machine learning model that is based on labeled samples. For example, if we build a system to estimate the price of a plot of land or a house based on various features, such as size, location, and so on, we first need to create a database and label it. We need to teach the algorithm what features correspond to what prices. Based on this data, the algorithm will learn how to calculate the price of real estate using the values of the input features.

Supervised learning deals with learning a function from available training data. Here, a learning algorithm analyzes the training data and produces a derived function that can be used for mapping new examples. There are many supervised learning algorithms such as Logistic Regression, Neural networks, Support Vector Machines (SVMs), and Naive Bayes classifiers.

Common examples of supervised learning include classifying e-mails into spam and not-spam categories, labeling webpages based on their content, and voice recognition.

Unsupervised Learning
Unsupervised learning is used to detect anomalies, outliers, such as fraud or defective equipment, or to group customers with similar behaviors for a sales campaign. It is the opposite of supervised learning. There is no labeled data here.

When learning data contains only some indications without any description or labels, it is up to the coder or to the algorithm to find the structure of the underlying data, to discover hidden patterns, or to determine how to describe the data. This kind of learning data is called unlabeled data.

Suppose that we have a number of data points, and we want to classify them into several groups. We may not exactly know what the criteria of classification would be. So, an unsupervised learning algorithm tries to classify the given dataset into a certain number of groups in an optimum way.

Unsupervised learning algorithms are extremely powerful tools for analyzing data and for identifying patterns and trends. They are most commonly used for clustering similar input into logical groups. Unsupervised learning algorithms include Kmeans, Random Forests, Hierarchical clustering and so on.

Semi-supervised Learning
If some learning samples are labeled, but some other are not labeled, then it is semi-supervised learning. It makes use of a large amount of unlabeled data for training and a small amount of labeled data for testing. Semi-supervised learning is applied in cases where it is expensive to acquire a fully labeled dataset while more practical to label a small subset. For example, it often requires skilled experts to label certain remote sensing images, and lots of field experiments to locate oil at a particular location, while acquiring unlabeled data is relatively easy.

Reinforcement Learning
Here learning data gives feedback so that the system adjusts to dynamic conditions in order to achieve a certain objective. The system evaluates its performance based on the feedback responses and reacts accordingly. The best known instances include self-driving cars and chess master algorithm AlphaGo.

Purpose of Machine Learning
Machine learning can be seen as a branch of AI or Artificial Intelligence, since, the ability to change experience into expertise or to detect patterns in complex data is a mark of human or animal intelligence.

As a field of science, machine learning shares common concepts with other disciplines such as statistics, information theory, game theory, and optimization.

As a subfield of information technology, its objective is to program machines so that they will learn.

However, it is to be seen that, the purpose of machine learning is not building an automated duplication of intelligent behavior, but using the power of computers to complement and supplement human intelligence. For example, machine learning programs can scan and process huge databases detecting patterns that are beyond the scope of human perception.

        41.1.3 data preprocessing

            41.1.3.1 mean, std, scale


import numpy as np

from sklearn import preprocessing

#We imported a couple of packages. Let's create some sample data and add the line to this file:

input_data = np.array([[3, -1.5, 3, -6.4], [0, 3, -1.3, 4.1], [1, 2.3, -2.9, -4.3]])
data_standardized = preprocessing.scale(input_data)
print "\nMean = ", data_standardized.mean(axis = 0)
print "Std deviation = ", data_standardized.std(axis = 0)

Mean removal
It involves removing the mean from each feature so that it is centered on zero. Mean removal helps in removing any bias from the features.

You can use the above code for mean removal −

mean removal: It's the sum of n items divided by n. Not difficult. Removal in this context is just mathematical subtraction. To remove a mean from, say, a vector of numbers, sum the numbers, divide the sum by the number of elements in the vector, and subtract that number from each element in the vector.

Scaling
The values of every feature in a data point can vary between random values. So, it is important to scale them so that this matches specified rules.

You can use the following code for scaling −

data_scaler = preprocessing.MinMaxScaler(feature_range = (0, 1))
data_scaled = data_scaler.fit_transform(input_data)
print "\nMin max scaled data = ", data_scaled
Now run the code and you can observe the following output −

Min max scaled data = [ [ 1. 0. 1. 0. ]
                        [ 0. 1. 0.27118644 1. ]
                        [ 0.33333333 0.84444444 0. 0.2 ]
]
Note that all the values have been scaled between the given range.

            41.1.3.2 Normalization
Normalization involves adjusting the values in the feature vector so as to measure them on a common scale. Here, the values of a feature vector are adjusted so that they sum up to 1. We add the following lines to the prefoo.py file −

You can use the following code for normalization −

data_normalized = preprocessing.normalize(input_data, norm  = 'l1')
print "\nL1 normalized data = ", data_normalized
Now run the code and you can observe the following output −

L1 normalized data = [  [ 0.21582734 -0.10791367 0.21582734 -0.46043165]
                        [ 0. 0.35714286 -0.1547619 0.48809524]
                        [ 0.0952381 0.21904762 -0.27619048 -0.40952381]
]
Normalization is used to ensure that data points do not get boosted due to the nature of their features.

            41.1.3.3 One Hot Encoding

                41.1.3.3.1 One Hot Encoding explained
https://medium.com/@michaeldelsole/what-is-one-hot-encoding-and-how-to-do-it-f0ae272f1179

In my words.
Instead of giving categories a number (which can lead to errors since if you give each category a weight according to its number you introduce bias)
represent each category is a k bit number (where k is # categories) with all 0 but nth bit is on (for category n)

                41.1.3.3.2

It may be required to deal with numerical values that are few and scattered, and you may not need to store these values. In such situations you can use One Hot Encoding technique.

If the number of distinct values is k, it will transform the feature into a k-dimensional vector where only one value is 1 and all other values are 0.

You can use the following code for one hot encoding −

encoder = preprocessing.OneHotEncoder()
encoder.fit([  [0, 2, 1, 12], 
               [1, 3, 5, 3], 
               [2, 3, 2, 12], 
               [1, 2, 4, 3]
])
encoded_vector = encoder.transform([[2, 3, 5, 3]]).toarray()
print "\nEncoded vector =", encoded_vector
Now run the code and you can observe the following output −

Encoded vector = [[ 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0.]]
In the example above, let us consider the third feature in each feature vector. The values are 1, 5, 2, and 4.

There are four separate values here, which means the one-hot encoded vector will be of length 4. If we want to encode the value 5, it will be a vector [0, 1, 0, 0]. Only one value can be 1 in this vector. The second element is 1, which indicates that the value is 5.

            41.1.3.4 Label Encoding
In supervised learning, we mostly come across a variety of labels which can be in the form of numbers or words. If they are numbers, then they can be used directly by the algorithm. However, many times, labels need to be in readable form. Hence, the training data is usually labelled with words.

Label encoding refers to changing the word labels into numbers so that the algorithms can understand how to work on them. Let us understand in detail how to perform label encoding −

Create a new Python file, and import the preprocessing package −

from sklearn import preprocessing
label_encoder = preprocessing.LabelEncoder()
input_classes = ['suzuki', 'ford', 'suzuki', 'toyota', 'ford', 'bmw']
label_encoder.fit(input_classes)
print "\nClass mapping:"
for i, item in enumerate(label_encoder.classes_):
print item, '-->', i
Now run the code and you can observe the following output −

Class mapping:
bmw --> 0
ford --> 1
suzuki --> 2
toyota --> 3
As shown in above output, the words have been changed into 0-indexed numbers. Now, when we deal with a set of labels, we can transform them as follows −

labels = ['toyota', 'ford', 'suzuki']
encoded_labels = label_encoder.transform(labels)
print "\nLabels =", labels
print "Encoded labels =", list(encoded_labels)
Now run the code and you can observe the following output −

Labels = ['toyota', 'ford', 'suzuki']
Encoded labels = [3, 1, 2]
This is efficient than manually maintaining mapping between words and numbers. You can check by transforming numbers back to word labels as shown in the code here −

encoded_labels = [3, 2, 0, 2, 1]
decoded_labels = label_encoder.inverse_transform(encoded_labels)
print "\nEncoded labels =", encoded_labels
print "Decoded labels =", list(decoded_labels)
Now run the code and you can observe the following output −

Encoded labels = [3, 2, 0, 2, 1]
Decoded labels = ['toyota', 'suzuki', 'bmw', 'suzuki', 'ford']
From the output, you can observe that the mapping is preserved perfectly.

            41.1.3.5
    41.2 Data analysis

        41.2.1 Loading the Dataset
We can load the data directly from the UCI Machine Learning repository. Note that here we are using pandas to load the data. We will also use pandas next to explore the data both with descriptive statistics and data visualization. Observe the following code and note that we are specifying the names of each column when loading the data.

import pandas
data = ‘pima_indians.csv’
names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', ‘Outcome’]
dataset = pandas.read_csv(data, names = names)
When you run the code, you can observe that the dataset loads and is ready to be analyzed. Here, we have downloaded the pima_indians.csv file and moved it into our working directory and loaded it using the local file name.

Summarizing the Dataset
Summarizing the data can be done in many ways as follows −

Check dimensions of the dataset
List the entire data
View the statistical summary of all attributes
Breakdown of the data by the class variable
Dimensions of Dataset
You can use the following command to check how many instances (rows) and attributes (columns) the data contains with the shape property.

print(dataset.shape)
Then, for the code that we have discussed, we can see 769 instances and 6 attributes −

(769, 6)
List the Entire Data
You can view the entire data and understand its summary −

print(dataset.head(20))
This command prints the first 20 rows of the data as shown −

Sno Pregnancies Glucose BloodPressure SkinThickness Insulin Outcome
1        6        148         72           35          0       1
2        1         85         66           29          0       0
3        8        183         64            0          0       1
4        1         89         66           23         94       0
5        0        137         40           35        168       1
6        5        116         74            0          0       0
7        3         78         50           32         88       1
8       10        115          0            0          0       0
9        2        197         70           45        543       1
10       8        125         96            0          0       1
11       4        110         92            0          0       0
12      10        168         74            0          0       1
13      10        139         80            0          0       0
14       1        189         60           23        846       1
15       5        166         72           19        175       1
16       7        100          0            0          0       1
17       0        118         84           47        230       1
18       7        107         74            0          0       1
19       1        103         30           38         83       0
View the Statistical Summary
You can view the statistical summary of each attribute, which includes the count, unique, top and freq, by using the following command.

print(dataset.describe())
The above command gives you the following output that shows the statistical summary of each attribute −

         Pregnancies Glucose BloodPressur SkinThckns Insulin Outcome
count       769       769       769         769       769     769
unique       18       137        48          52       187       3
top           1       100        70           0         0       0
freq        135        17        57         227       374     500
Breakdown the Data by Class Variable
You can also look at the number of instances (rows) that belong to each outcome as an absolute count, using the command shown here −

print(dataset.groupby('Outcome').size())
Then you can see the number of outcomes of instances as shown −

Outcome
0         500
1         268
Outcome     1
dtype: int64

        41.2.2 Data Visualization
You can visualize data using two types of plots as shown −

Univariate plots to understand each attribute

Multivariate plots to understand the relationships between attributes

Univariate Plots
Univariate plots are plots of each individual variable. Consider a case where the input variables are numeric, and we need to create box and whisker plots of each. You can use the following code for this purpose.

import pandas
import matplotlib.pyplot as plt
data = 'iris_df.csv'
names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']
dataset = pandas.read_csv(data, names=names)
dataset.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)
plt.show()
You can see the output with a clearer idea of the distribution of the input attributes as shown −

Univariate Plots
Box and Whisker Plots
You can create a histogram of each input variable to get an idea of the distribution using the commands shown below −

#histograms
dataset.hist()
plt().show()
Box and Whisker Plots
From the output, you can see that two of the input variables have a Gaussian distribution. Thus these plots help in giving an idea about the algorithms that we can use in our program.

Multivariate Plots
Multivariate plots help us to understand the interactions between the variables.

Scatter Plot Matrix
First, let’s look at scatterplots of all pairs of attributes. This can be helpful to spot structured relationships between input variables.

from pandas.plotting import scatter_matrix
scatter_matrix(dataset)
plt.show()
You can observe the output as shown −

Scatter Plot Matrix
Observe that in the output there is a diagonal grouping of some pairs of attributes. This indicates a high correlation and a predictable relationship.

        41.2.3
    41.3

42. Important packages, modules, tags: Important packages, modules libraries

    42.1 flask, lightweight web framework

        42.1.1 http://flask.pocoo.org/docs/1.0/tutorial/

        42.1.2 https://www.tutorialspoint.com/flask
from flask import Flask
app = Flask(__name__)

@app.route('/')
def hello_world():
   return 'Hello World’

if __name__ == '__main__':
   app.run()

run example:
ipython:
edit
(paste code)
[i500695@C02X632CJGH6:2019-05-29 16:44:14:~/work/SAP/git/maas-client:]1596$ ipython 
Python 3.7.3 (default, Mar 27 2019, 09:23:15) 
Type 'copyright', 'credits' or 'license' for more information
IPython 7.5.0 -- An enhanced Interactive Python. Type '?' for help.
history file:  /Users/i500695/.python_history
Welcome i500695, time is 16:44:24 05/29/19 IDT,  Have a good session :-) 

In [1]: edit                                                                                                                                                                                   
IPython will make a temporary file named: /var/folders/k4/zgbbn6ms2xv0d_4g85qw60vc0000gn/T/ipython_edit_vb8t0uf5/ipython_edit_fjsfx8k_.py
Editing... done. Executing edited code...
 * Serving Flask app "__main__" (lazy loading)
 * Environment: production
   WARNING: This is a development server. Do not use it in a production deployment.
   Use a production WSGI server instead.
 * Debug mode: off
 * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)
127.0.0.1 - - [29/May/2019 16:45:30] "GET / HTTP/1.1" 200 -

From browser:
http://localhost:5000/

        42.1.3 flask tutorial

            42.1.3.1  https://www.tutorialspoint.com/flask/index.htm

                42.1.3.1.1  basics
my code: /Users/I500695/work/code/python/web/flask/flask_tutorial


                    42.1.3.1.1.1  hello flask
- code: [I500695@C02ZR8BSMD6N:2021-12-02 19:04:50:~/work/code/python/web/flask/flask_tutorial:]2021$ cat !$
cat helloWorldFlask.py
from flask import Flask
app = Flask(__name__)

@app.route('/')
def is_alive():
    return "Hello Flask server is alive"

@app.route('/hello/<name>')
def hello_name(name):
    return f"Hello {name}"

def main():
    app.run(port=3333, debug=True)
if __name__=="__main__":
    main()

- run server 

- run client
                    42.1.3.1.1.2

            42.1.3.2

        42.1.4
    42.2 pramiko, ssh, ftp, execute commands remotely

        42.2.1  https://medium.com/@keagileageek/paramiko-how-to-ssh-and-file-transfers-with-python-75766179de73
Paramiko- How to SSH and transfer files with python
Go to the profile of Mokgadi Rasekgala
Mokgadi RasekgalaFollow
Apr 11, 2017
So i’m employed at a social media type of company with a product working a lot like facebook and the past week had to sort out an issue with loosing the linking between video urls on our db with the actual content on the server. Some mismatch with Kaltura. But anyways long story short, I had to write a little script which would download videos from a remote server to upload to Kaltura to and get new urls to replace old ones int the db.
SSH is the method typically used to access a remote machine and run commands, retrieve files or upload files.
You can transfer files from the remote machine to the local or vice versa using SFTP (Secure File Transfer Protocol) and SCP(Secure Copy Protocol).
According to paramiko.org, The python paramiko model gives an abstraction of the SSHv2 protocol with both the client side and server side functionality. As a client, you can authenticate yourself using a password or key and as a server you can decide which users are allowed accesss and the channels you allow
In this blog I focus on the client side.
Let’s get on with it
The primary client of Paramiko as documented in the API, is Paramiko.SSHClient. An instance of the Paramiko.SSHClient can be used to make connections to the remote server and transfer files
MAKING A CONNECTION
import paramiko
ssh_client=paramiko.SSHClient()
ssh_client.connect(hostname=’hostname’,username=’mokgadi’,password=’mypassword’)
#Raises BadHostKeyException,AuthenticationException,SSHException,socket error
when you try this, you get the following error:
missing_host_key raise SSHException(‘Server %r not found in known_hosts’ % hostname) paramiko.ssh_exception.SSHException: Server ‘hostname’ not found in known_hosts
Understanding Known Hosts
You see this error because you have not informed your machine that the remote server you “trust” the server you are trying to access. If you go onto you command line or terminal and try to connect to a server for the first time, You will get a message similar to this:

The authenticity of host ‘hostname’ can’t be established.RSA key fingerprint is ‘key’. Are you sure you want to continue connecting (yes/no)?

When you select yes here, you let your machine know that it can trust the machine and you can now access it without the prompt until the key for that machine changes.
Paramiko similarly requires that you validate your trust with the machine. This validation is handled by calling set_missing_host_key_policy() on the SSHClient an passing the policy you want implemented when accessing a new remote machine. By default, the paramiko.SSHclient sets the policy to the RejectPolicy. The policy rejects connection without validating as we saw above. Paramiko does however give you a way to sort of “Trust all” key policy, the AutoAddPolicy. Parsing an instance of the AutoAddPolicy to set_missing_host_key_policy() changes it to allow any host.
import paramiko
ssh_client =paramiko.SSHClient()
ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
ssh_client.connect(hostname=’hostname’,username=’mokgadi’,password=’mypassword’)
You should now be in the green
RUNNING COMMANDS ON THE REMOTE MACHINE
To run a command exec_command is called on the SSHClient with the command passed. The response is returned as a tuple (stdin,stdout,stderr)
For example to list all the files in a directory:
stdin,stdout,stderr=ssh_client.exec_command(“ls”)
Getting the type for each of the returned,
type(stdin) and type(stdout) is ‘paramiko.channel.ChannelFile’
type(stderr) is class ‘paramiko.channel.ChannelStderrFile’
According to paramiko.org they are all python file like objects.
The stdin is a write-only file which can be used for commands requiring input
The stdout file give the output of the command
The stderr gives the errors returned on executing the command. Will be empty if there is no error
for the command above
>>>print(stdout.readlines()) → [u’anaconda-ks.cfg\n’, u’database_backup\n’, u’Desktop\n’, u’Documents\n’, u’Downloads\n’, …. u’Public\n’, u’Templates\n’, u’Videos\n’]
>>>print(stderr.readlines) → []
COMMANDS REQUIRING INPUT
Sometimes you need to provide a password or extra input to run a command. This is what stdin is used for. Let’s run the same command above with sudo.
stdin, stdout, stderr = ssh.exec_command(“sudo ls”)
stdin.write(‘mypassword\n’)
print stdout.readlines()
Should return list of files and folders as above.
FILE TRANSFERS
File transfers are handled by the paramiko.SFTPClient which you get from calling open_sftp() on an instance of Paramiko.SSHClient.
Downloading a file from remote machine
ftp_client=ssh_client.open_sftp()
ftp_client.get(‘remotefileth’,’localfilepath’)
ftp_client.close()
Uploading file from local to remote machine
ftp_client=ssh.open_sftp()
ftp_client.put(‘localfilepath’,remotefilepath’)
ftp_client.close()

        42.2.2


    42.3 Data science, AI, big data

        42.3.1 overview
These are the five most essential Data Science libraries you have to know:

Numpy
Pandas
Matplotlib
Scikit-Learn
Scipy
Let’s see them one by one!

            42.3.1.1 Numpy

Numpy will help you to manage multi-dimensional arrays very efficiently. Maybe you won’t do that directly, but since the concept is a crucial part of data science, many other libraries (well, almost all of them) are built on Numpy. Simply put: without Numpy you won’t be able to use Pandas, Matplotlib, Scipy or Scikit-Learn. That’s why you need it on the first hand.

python libraries and packages - numpy array
3-dimensional numpy array
But on the other hand, it also has a few well-implemented methods. I quite often use Numpy’s random function, which I found slightly better than the random module of the standard library. And when it comes to simple predictive analytics tasks like linear or polynomial regression, Numpy’s polyfit function is my favorite. (More about that in another article.)

python libraries and packages - prediction with numpy polyfit
prediction with numpy’s polyfit

            42.3.1.2 Pandas, tags: Pandas
documentation: https://pandas.pydata.org/

To analyze data, we like to use two-dimensional tables – like in SQL and in Excel. Originally, Python didn’t have this feature. Weird, isn’t it? But that’s why Pandas is so important! I like to say, Pandas is the “SQL of Python.” (Eh, I can’t wait to see what I will get for this sentence in the comment section… ;-)) Okay, to be more precise: Pandas is the library that will help us to handle two-dimensional data tables in Python. In many senses it’s really similar to SQL, though.

python libraries and packages - pandas dataframe
a pandas dataframe
With pandas, you can load your data into data frames, you can select columns, filter for specific values, group by values, run functions (sum, mean, median, min, max, etc.), merge dataframes and so on. You can also create multi-dimensional data-tables.

That’s a common misunderstanding, so let me clarify: Pandas is not a predictive analytics or machine learning library. It was created for data analysis, data cleaning, data handling and data discovery… By the way, these are the necessary steps before you run machine learning projects, and that’s why you will need pandas for every scientific project, too.

If you start with Python for Data Science and you learned the basics of Python, I recommend that you focus on learning Pandas next. These short article series of mine will help you: Pandas for Data Scientists.

				
                42.3.1.2.1 chatgpt overview of pandas, tags: chatgpt overview of pandas
Pandas is a powerful and widely-used open-source data manipulation and analysis library for Python. It provides easy-to-use data structures and functions for efficiently manipulating large datasets. Here's an overview of key features and functionalities offered by the Pandas library:

### 1. **Data Structures:**
   - **Series:** A one-dimensional labeled array capable of holding any data type.
   - **DataFrame:** A two-dimensional labeled data structure with columns that can be of different data types. It can be thought of as a spreadsheet or SQL table.
   - **Panel (Less Commonly Used):** A three-dimensional data structure that can be used for handling 3D data.

### 2. **Key Features:**
   - **Data Cleaning:** Pandas provides methods for handling missing data, duplications, and outliers.
   - **Data Exploration:** Easily inspect and summarize data, compute descriptive statistics, and explore relationships between variables.
   - **Indexing and Selection:** Efficiently index, slice, and subset data using labels or integer-based indexing.
   - **Grouping and Aggregation:** Group data based on certain criteria and perform aggregations using functions like sum, mean, etc.
   - **Merging and Joining:** Combine datasets using SQL-like merge and join operations.
   - **Time Series Data:** Specialized tools for working with time series data, including date range generation and frequency conversion.
   - **Input/Output:** Read and write data in various formats, including CSV, Excel, SQL databases, and more.

### 3. **Data Visualization:**
   - Pandas integrates well with other Python visualization libraries like Matplotlib and Seaborn for creating insightful plots and graphs.

### 4. **Performance:**
   - Pandas is built on top of NumPy, a highly efficient and fast numerical computing library. It leverages vectorized operations to perform computations on entire arrays without the need for explicit looping.

### 5. **Community and Ecosystem:**
   - Pandas has a large and active community, making it easy to find help, tutorials, and resources.
   - It plays a crucial role in the broader Python data science ecosystem and is often used in conjunction with libraries like NumPy, Matplotlib, SciPy, and scikit-learn.

### Example Usage:
```python
import pandas as pd

# Creating a DataFrame
data = {'Name': ['Alice', 'Bob', 'Charlie'],
        'Age': [25, 30, 35],
        'City': ['New York', 'San Francisco', 'Los Angeles']}

df = pd.DataFrame(data)

# Basic operations
print(df.head())  # Display the first few rows
print(df.describe())  # Display summary statistics
```

Pandas is an essential tool for data scientists, analysts, and researchers working with tabular data in Python. Its versatility and ease of use make it a go-to choice for various data manipulation and analysis tasks.

                42.3.1.2.2 udemy 100 day python course pandas chapter (day 25)

                    42.3.1.2.2.1 pandas101
[i500695@WYLQRXL9LQ:2023-12-19 18:58:44:~/work/code/python/udemy_python_100_days_of_code/day25:]2015$ cat !$
cat ./pandas101.py
#!/opt/homebrew/bin/python3
import csv, pandas

def load_data_naive():
    data = []
    datafname = 'weather_data.csv'
    with open(datafname) as datafile:
        for line in datafile:
            data.append(line.split())

    print(f"loaded CSV data: {data}")
    return data

def load_dataframe():
    datafname = 'weather_data.csv'
    dataframe = pandas.read_csv(datafname)
    return dataframe

def load_csv_data():
    data = []
    datafname = 'weather_data.csv'
    with open(datafname) as datafile:
        csv_reader = csv.reader(datafile)
        for row in csv_reader:
            data.append(row)
    print(f"loaded CSV data: {data}")
    return data

def print_tempratures(data):
    print("printing all the temperatures in the dataset")
    for row in data:
        print(row[1])

def type_checks(data):
    print(f"dataframe type is {type(data)}, temperature column type is {type(data['temp'])}")
    
def columns_work_demo(data):
    print("Panda. work on columns")
    print(f"mean temperature: {data['temp'].mean()}")
    print(f"max temperature: {data.temp.max()}") #cal call column as a property (like in JS) or as dictionary by key as above

def rows_work_demo(data):
    print("Panda. work on rows")
    print(data[data.day == 'Monday'])
    print(f"day in which temp was maximal: {data[data.temp == data.temp.max()]}")
    print("Extract a specific column from a specific row. say Monday row...")
    monday = data[data.day == 'Monday']
    print(monday.condition)
    print(f"convert Monday temperature from celsios to fahrenheit: {monday.temp*(9/5)+32}")

def working_with_data_frames(data):
    print("Create a DataFrame (table) from a dictionary")
    df_dict = {
            "kids": ["May", "Itay", "Kay", "Aimy"],
            "birthdates": ["14.03.2007", "21.06.2011", "08.06.2014", "02.03.2016"]
            }
    kids_df = pandas.DataFrame(df_dict)
    print(f"Created kids bdays DataFrame\n {kids_df}")
    print("Saving DataFrame to .csv file...")
    kids_df.to_csv("KidsBdays.csv")
    print("contents of csv file:")
    with open("KidsBdays.csv") as csv:
        lines = csv.readlines()
    print(lines)

def main():
    data = load_dataframe()
    print(data)
    type_checks(data)
    json_data = data.to_json()
    print(json_data)

    columns_work_demo(data)
    rows_work_demo(data)
    working_with_data_frames(data)

    



    #data = load_csv_data()
    #print_tempratures(data[1:])

    #data = load_data_naive()

if __name__ == "__main__":
    main()
[i500695@WYLQRXL9LQ:2023-12-19 18:37:25:~/work/code/python/udemy_python_100_days_of_code/day25:]2014$ ./pandas101.py 
         day  temp condition
0     Monday    12     Sunny
1    Tuesday    14      Rain
2  Wednesday    15      Rain
3   Thursday    14    Cloudy
4     Friday    21     Sunny
5   Saturday    22     Sunny
6     Sunday    24     Sunny
dataframe type is <class 'pandas.core.frame.DataFrame'>, temperature column type is <class 'pandas.core.series.Series'>
{"day":{"0":"Monday","1":"Tuesday","2":"Wednesday","3":"Thursday","4":"Friday","5":"Saturday","6":"Sunday"},"temp":{"0":12,"1":14,"2":15,"3":14,"4":21,"5":22,"6":24},"condition":{"0":"Sunny","1":"Rain","2":"Rain","3":"Cloudy","4":"Sunny","5":"Sunny","6":"Sunny"}}
Panda. work on columns
mean temperature: 17.428571428571427
max temperature: 24
Panda. work on rows
      day  temp condition
0  Monday    12     Sunny
day in which temp was maximal:       day  temp condition
6  Sunday    24     Sunny
Extract a specific column from a specific row. say Monday row...
0    Sunny
Name: condition, dtype: object
convert Monday temperature from celsios to fahrenheit: 0    53.6
Name: temp, dtype: float64
Create a DataFrame (table) from a dictionary
Created kids bdays DataFrame
    kids  birthdates
0   May  14.03.2007
1  Itay  21.06.2011
2   Kay  08.06.2014
3  Aimy  02.03.2016
Saving DataFrame to .csv file...
contents of csv file:
[',kids,birthdates\n', '0,May,14.03.2007\n', '1,Itay,21.06.2011\n', '2,Kay,08.06.2014\n', '3,Aimy,02.03.2016\n']

                    42.3.1.2.2.2 chatgpt pandas count number of rows with a specific value in a DataFrame
To count the number of rows with a specific value in a DataFrame using pandas in Python, you can use the `sum()` function along with a boolean condition. Here's an example:

```python
import pandas as pd

# Create a sample DataFrame
data = {'Name': ['Alice', 'Bob', 'Charlie', 'David'],
        'Age': [25, 30, 25, 35],
        'City': ['New York', 'San Francisco', 'New York', 'Los Angeles']}

df = pd.DataFrame(data)

# Count the number of rows where the 'City' column has a specific value, e.g., 'New York'
specific_value = 'New York'
count_rows = (df['City'] == specific_value).sum()

print(f"Number of rows with '{specific_value}' in the 'City' column: {count_rows}")
```

In this example, `df['City'] == specific_value` creates a boolean Series where each element is `True` if the corresponding row has the specified value in the 'City' column and `False` otherwise. The `.sum()` function then counts the number of `True` values, which corresponds to the number of rows with the specific value.

Adjust the column name and specific value according to your DataFrame and requirements.

                    42.3.1.2.2.3
                42.3.1.2.3
            42.3.1.3 Matplotlib

I hope I don’t have to detail why data visualization is important. Data visualization helps you to better understand your data, discover things that you wouldn’t discover in raw format and communicate your findings more efficiently to others.
The best and most well-known Python data visualization library is Matplotlib. I wouldn’t say it’s easy to use… But usually if you save for yourself the 4 or 5 most commonly used code blocks for basic line charts and scatter plots, you can create your charts pretty fast.

python libraries and packages - dataviz matplotlib
matplotlib dataviz example
Here’s another article that introduces Matplotlib more in-depth: How to use matplotlib.

                42.3.1.3.1 chatpgpt overview of matplotlib in python
Matplotlib is a popular and widely-used 2D plotting library for Python. It provides a flexible and comprehensive set of tools for creating a wide range of static, animated, and interactive plots. Here's an overview of key features and functionalities offered by the Matplotlib library:

### 1. **Plot Types:**
   - **Line Plots:** Visualize data points with connected lines.
   - **Scatter Plots:** Display individual data points without connecting lines.
   - **Bar Plots:** Represent data with rectangular bars.
   - **Histograms:** Visualize the distribution of a dataset.
   - **Pie Charts:** Display data as sectors of a circle.
   - **Box Plots:** Show the distribution of a dataset's summary statistics.

### 2. **Customization:**
   - **Labels and Titles:** Add titles, axis labels, and annotations to enhance plot clarity.
   - **Colors and Styles:** Customize colors, line styles, markers, and other visual elements.
   - **Legends and Annotations:** Provide additional information about data series in the plot.
   - **Subplots:** Create multiple plots in a single figure for side-by-side comparisons.

### 3. **Support for LaTeX:**
   - Matplotlib supports the use of LaTeX for mathematical expressions, making it suitable for scientific and technical plotting.

### 4. **3D Plotting:**
   - Matplotlib provides tools for creating 3D plots, including surface plots, scatter plots, and more.

### 5. **Integration with Pandas:**
   - Matplotlib seamlessly integrates with Pandas DataFrames, allowing for easy visualization of data stored in Pandas structures.

### 6. **Animation:**
   - Matplotlib can create animated plots, useful for visualizing changes over time or iterations.

### 7. **Extensibility:**
   - Matplotlib is highly extensible, allowing users to create custom plots and functionalities.

### Example Usage:
```python
import matplotlib.pyplot as plt
import numpy as np

# Creating a simple line plot
x = np.linspace(0, 2*np.pi, 100)
y = np.sin(x)

plt.plot(x, y, label='sin(x)')
plt.title('Simple Sinusoidal Plot')
plt.xlabel('x-axis')
plt.ylabel('y-axis')
plt.legend()
plt.grid(True)
plt.show()
```

This example creates a basic sine wave plot using Matplotlib. The library is versatile and can be used for a wide range of visualization tasks, from simple line plots to complex statistical graphics.

Matplotlib is often used in combination with other libraries like NumPy, Pandas, and Seaborn to create comprehensive data visualizations in scientific computing, data analysis, and machine learning.
                42.3.1.3.2

            42.3.1.4 Scikit-Learn

Without any doubt the fanciest things in Python are Machine Learning and Predictive Analytics. And the best library for that is Scikit-Learn, which simply defines itself as “Machine Learning in Python.” Scikit-Learn has several methods, basically covering everything you might need in the first few years of your data career: regression methods, classification methods, and clustering, as well as model validation and model selection. You can also use it for dimensionality reduction and feature extraction.

python libraries and packages - scikit learn random forest
a simple classification with a random forest model in Scikit Learn
Note: You will see that machine learning with Scikit-Learn is nothing but importing the right modules and running the model fitting method of them… That’s not the challenging part – it’s rather the data cleaning, the data formatting, the data preparation, and finding the right input values and the right model. So before you start using Scikit-Learn, I suggest two things. First – as I already said – master your basic Python and pandas skills to become great at data preparation. Secondly, make sure you understand the theory and the mathematical background of the different prediction and classification models, so you know what happens with your data when you apply them.

machine Learning meme
source: pythonprogramming.net

            42.3.1.5 Scipy

This is kind of confusing, but there is a Scipy library and there is a Scipy stack. Most of the libraries and packages I wrote about in this article are part of the Scipy stack (that is for scientific computing in Python). And one of these components is the Scipy library itself, which provides efficient solutions for numerical routines (the math stuff behind machine learning models). These are: integration, interpolation, optimization, etc.

Just like Numpy, you most probably won’t use Scipy itself, but the above-mentioned Scikit-Learn library highly relies on it. Scipy provides the core mathematical methods to do the complex machine learning processes in Scikit-learn. That’s why you have to know it.

        42.3.2


    42.4 20 Python libraries you can’t live without


|      Hi there fellas. Today i am going to list 20 python libraries which have been a part of my toolbelt and should be a part of yours as well. So here they are:
|      
        42.4.1 Requests. The most famous http library written by kenneth reitz. It’s a must have for every python developer.

            42.4.1.1 q. provide explanation about python requests package with some code examples
a.  The Python `requests` package is a popular HTTP library that simplifies making HTTP requests in Python. It provides a convenient and intuitive API for interacting with web services and retrieving data from web servers. Here's an explanation of the `requests` package with some code examples:

Installation:
Before using the `requests` package, you need to install it. You can install it using pip by running the following command:

```
pip install requests
```

Basic GET Request:
To make a GET request to a URL and retrieve the response, you can use the `get` function from the `requests` module. Here's an example:

```python
import requests

response = requests.get("https://api.example.com/data")
print(response.status_code)  # Status code of the response
print(response.text)  # Content of the response
```

Sending Query Parameters:
You can send query parameters along with the URL using the `params` parameter. Here's an example:

```python
import requests

parameters = {"key1": "value1", "key2": "value2"}
response = requests.get("https://api.example.com/data", params=parameters)
print(response.url)  # Full URL with query parameters
```

Sending Headers:
You can send custom headers in the request using the `headers` parameter. Here's an example:

```python
import requests

headers = {"User-Agent": "Mozilla/5.0"}
response = requests.get("https://api.example.com/data", headers=headers)
print(response.text)
```

Sending POST Request with Data:
To send a POST request and include data in the request body, you can use the `post` function and provide the data as a dictionary using the `data` parameter. Here's an example:

```python
import requests

data = {"key1": "value1", "key2": "value2"}
response = requests.post("https://api.example.com/submit", data=data)
print(response.status_code)
```

Handling JSON Response:
When working with JSON responses, you can use the `json` method of the response object to parse the JSON data into a Python dictionary. Here's an example:

```python
import requests

response = requests.get("https://api.example.com/data")
data = response.json()  # Parsed JSON response
print(data["key"])  # Accessing a specific value from the JSON
```

Handling Errors:
The `requests` package raises exceptions for certain errors, such as network issues or invalid URLs. You can handle these exceptions using try-except blocks. Here's an example:

```python
import requests

try:
    response = requests.get("https://api.example.com/data")
    response.raise_for_status()  # Raise exception for non-2xx status codes
except requests.exceptions.RequestException as e:
    print("An error occurred:", e)
```

These are just some basic examples of using the `requests` package. The library provides many more features, such as handling authentication, sending cookies, working with sessions, and more. You can refer to the official `requests` documentation for detailed information on all the available features and options: https://docs.python-requests.org/

            42.4.1.2 perform outh2 authentication using requests
To perform OAuth2 authentication using the `requests` library in Python, you need to include the appropriate authorization headers in your requests. Here's an example of how you can perform OAuth2 authentication using the `requests` library:

```python
import requests

# OAuth2 credentials
client_id = "your_client_id"
client_secret = "your_client_secret"
access_token_url = "https://example.com/oauth2/token"

# Fetch access token
data = {
    "grant_type": "client_credentials",
    "client_id": client_id,
    "client_secret": client_secret
}
response = requests.post(access_token_url, data=data)

if response.status_code == 200:
    access_token = response.json()["access_token"]
    # Make authenticated requests using the access token
    api_url = "https://example.com/api"
    headers = {"Authorization": f"Bearer {access_token}"}
    response = requests.get(api_url, headers=headers)
    if response.status_code == 200:
        print(response.json())
    else:
        print("Request failed with status code:", response.status_code)
else:
    print("Access token request failed with status code:", response.status_code)
```

In this example, you first obtain the access token by making a POST request to the OAuth2 token endpoint (`access_token_url`). You pass the `grant_type`, `client_id`, and `client_secret` as data in the request body. The response contains the access token.

Once you have the access token, you can include it in the `Authorization` header of your authenticated requests. In this example, a GET request is made to an API endpoint (`api_url`) with the access token in the `Authorization` header.

Note: The specific OAuth2 authentication process may vary depending on the OAuth2 provider and their authentication flow. The example above assumes the use of the client credentials grant type for obtaining the access token.
            42.4.1.3 upload a file using requests
To upload a file using the `requests` library in Python, you can utilize the `multipart/form-data` content type and include the file as part of the request payload. Here's an example:

```python
import requests

# File to upload
file_path = "path_to_file/file.txt"
upload_url = "https://example.com/upload"

# Create a file object
with open(file_path, "rb") as file:
    files = {"file": file}

    # Make the upload request
    response = requests.post(upload_url, files=files)

    if response.status_code == 200:
        print("File uploaded successfully.")
    else:
        print("File upload failed with status code:", response.status_code)
```

In this example, you need to specify the file path (`file_path`) and the URL where you want to upload the file (`upload_url`).

The file is opened in binary mode (`"rb"`) and passed as part of the `files` parameter in the `requests.post()` method. The file object is assigned to the key `"file"` in the `files` dictionary. If you have multiple files to upload, you can include them in the `files` dictionary with different keys.

The `requests.post()` method sends the file as a `multipart/form-data` request to the specified URL. The server should handle the file upload and respond with an appropriate status code. In the example, a successful response is indicated by a status code of `200`.

Make sure to replace `"path_to_file/file.txt"` with the actual path to the file you want to upload, and `"https://example.com/upload"` with the URL of the upload endpoint.

            42.4.1.4
        42.4.2. Scrapy. If you are involved in webscraping then this is a must have library for you. After using this library you won’t use any other.

            42.4.2.1 provide explanation about python Scrapy package with some code examples
Scrapy is a powerful Python framework used for web scraping and crawling. It provides a high-level interface to extract data from websites and automate the process of navigating through web pages, submitting forms, and following links. Scrapy is built on top of the Twisted asynchronous networking library, making it efficient and capable of handling thousands of concurrent requests.

To use Scrapy, you need to install it first. You can do this by running the following command in your terminal:

```bash
pip install scrapy
```

Once installed, you can create a Scrapy project using the `scrapy startproject` command. This will generate a basic project structure with the necessary files and directories. Here's an example:

```bash
scrapy startproject myproject
```

This command will create a directory named `myproject` containing the initial project files.

Next, navigate into the project directory:

```bash
cd myproject
```

Within the project, you'll find a file called `spiders.py` inside the `myproject/spiders` directory. This is where you define your spiders, which are responsible for extracting data from websites. Let's create a spider to scrape quotes from http://quotes.toscrape.com.

Open the `spiders.py` file and replace the contents with the following code:

```python
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
    ]

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').get(),
                'author': quote.css('span small::text').get(),
            }

        next_page = response.css('li.next a::attr(href)').get()
        if next_page is not None:
            yield response.follow(next_page, self.parse)
```

In this example, we define a spider class called `QuotesSpider` that inherits from `scrapy.Spider`. We set the `name` attribute to "quotes" and define the `start_urls` list with the initial URLs to scrape.

The `parse` method is the entry point for the spider. It receives the `response` object representing the web page and extracts the desired data using CSS selectors. In this case, we extract the quote text and author. The extracted data is yielded as a dictionary.

We also check if there's a "Next" button on the page and follow the link if it exists using the `response.follow` method. This allows the spider to scrape multiple pages.

To run the spider, use the `scrapy crawl` command followed by the spider name:

```bash
scrapy crawl quotes
```

The scraped data will be displayed in the console. You can also store it in various formats such as CSV or JSON using Scrapy's built-in features.

This is just a basic example of using Scrapy. The framework offers many powerful features and options for handling more complex scraping scenarios, including middleware, pipelines, and settings customization. You can refer to the official Scrapy documentation for more information on these advanced topics.

            42.4.2.2 provide a more complex example using middleware pipeline and settings customization
Certainly! Here's a more complex example that demonstrates the usage of middleware, pipelines, and settings customization in Scrapy.

1. Middleware:
Middleware components in Scrapy are responsible for processing requests and responses. They can be used for various purposes such as handling user agents, logging, proxy rotation, or custom authentication. Let's create a middleware component that adds a custom user agent to each request.

```python
# myproject/middlewares.py
from scrapy import signals

class CustomUserAgentMiddleware:
    def __init__(self, user_agent):
        self.user_agent = user_agent

    @classmethod
    def from_crawler(cls, crawler):
        user_agent = crawler.settings.get('CUSTOM_USER_AGENT')
        return cls(user_agent)

    def process_request(self, request, spider):
        request.headers['User-Agent'] = self.user_agent
```

2. Pipelines:
Pipelines in Scrapy are used for processing scraped items. They can perform tasks such as data validation, cleaning, or storing the items in a database. Let's create a pipeline that stores scraped quotes in a MongoDB database.

```python
# myproject/pipelines.py
from pymongo import MongoClient

class MongoDBPipeline:
    collection_name = 'quotes'

    def __init__(self, mongo_uri, mongo_db):
        self.mongo_uri = mongo_uri
        self.mongo_db = mongo_db

    @classmethod
    def from_crawler(cls, crawler):
        mongo_uri = crawler.settings.get('MONGO_URI')
        mongo_db = crawler.settings.get('MONGO_DATABASE')
        return cls(mongo_uri, mongo_db)

    def open_spider(self, spider):
        self.client = MongoClient(self.mongo_uri)
        self.db = self.client[self.mongo_db]

    def close_spider(self, spider):
        self.client.close()

    def process_item(self, item, spider):
        self.db[self.collection_name].insert_one(dict(item))
        return item
```

3. Settings Customization:
We need to configure the settings for our Scrapy project to include the middleware and pipeline components we created.

```python
# myproject/settings.py
BOT_NAME = 'myproject'

SPIDER_MODULES = ['myproject.spiders']
NEWSPIDER_MODULE = 'myproject.spiders'

# Custom middleware
DOWNLOADER_MIDDLEWARES = {
    'myproject.middlewares.CustomUserAgentMiddleware': 543,
}

# Custom pipeline
ITEM_PIPELINES = {
    'myproject.pipelines.MongoDBPipeline': 300,
}

# MongoDB settings
MONGO_URI = 'mongodb://localhost:27017'
MONGO_DATABASE = 'mydatabase'

# Custom user agent
CUSTOM_USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
```

Now, with the setup complete, let's modify the `QuotesSpider` example from the previous response to use the middleware and pipeline:

```python
# myproject/spiders/quotes_spider.py
import scrapy


class QuotesSpider(scrapy.Spider):
    name = "quotes"
    start_urls = [
        'http://quotes.toscrape.com/page/1/',
    ]

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').get(),
                'author': quote.css('span small::text').get(),
            }

        next_page = response.css('li.next a::attr(href)').get()
        if next_page is not None:
            yield response.follow(next_page, self.parse)
```

To run the spider, execute the `scrapy
            42.4.2.3
        42.4.3. wxPython. A gui toolkit for python. I have primarily used it in place of tkinter. You will really love it.
        42.4.4. Pillow. A friendly fork of PIL (Python Imaging Library). It is more user friendly than PIL and is a must have for anyone who works with images.
        42.4.5. SQLAlchemy. A database library. Many love it and many hate it. The choice is yours.
        42.4.6. BeautifulSoup. I know it’s slow but this xml and html parsing library is very useful for beginners.
        42.4.7. Twisted. The most important tool for any network application developer. It has a very beautiful api and is used by a lot of famous python developers.
        42.4.8. NumPy. How can we leave this very important library ? It provides some advance math functionalities to python.
        42.4.9. SciPy. When we talk about NumPy then we have to talk about scipy. It is a library of algorithms and mathematical tools for python and has caused many scientists to switch from ruby to python.
        42.4.10. matplotlib. A numerical plotting library. It is very useful for any data scientist or any data analyzer.
        42.4.11. Pygame. Which developer does not like to play games and develop them ? This library will help you achieve your goal of 2d game development.
        42.4.12. Pyglet. A 3d animation and game creation engine. This is the engine in which the famous python port of minecraft was made
        42.4.13. pyQT. A GUI toolkit for python. It is my second choice after wxpython for developing GUI’s for my python scripts.
        42.4.14. pyGtk. Another python GUI library. It is the same library in which the famous Bittorrent client is created.
        42.4.15. Scapy. A packet sniffer and analyzer for python made in python.
        42.4.16. pywin32. A python library which provides some useful methods and classes for interacting with windows.
        42.4.17. nltk. Natural Language Toolkit – I realize most people won’t be using this one, but it’s generic enough. It is a very useful library if you want to manipulate strings. But it’s capacity is beyond that. Do check it out.
        42.4.18. nose. A testing framework for python. It is used by millions of python developers. It is a must have if you do test driven development.
        42.4.19. SymPy. SymPy can do algebraic evaluation, differentiation, expansion, complex numbers, etc. It is contained in a pure Python distribution.
        42.4.20. IPython. I just can’t stress enough how useful this tool is. It is a python prompt on steroids. It has completion, history, shell capabilities, and a lot more. Make sure that you take a look at it.

    42.5 IPython
        42.5.1 Introducing IPython
You don’t need to know anything beyond Python to start using IPython – just type commands as you would at the standard Python prompt. But IPython can do much more than the standard prompt. Some key features are described here. For more information, check the tips page, or look at examples in the IPython cookbook.
If you haven’t done that yet see how to install ipython .
If you’ve never used Python before, you might want to look at the official tutorial or an alternative, Dive into Python.
Start IPython by issuing the ipython command from your shell, you should be greeted by the following:
Python 3.6.0 Type 'copyright', 'credits' or 'license' for more information
IPython 6.0.0.dev -- An enhanced Interactive Python. Type '?' for help.
In [1]: Unlike the Python REPL, you will see that the input prompt is In [N]: instead of >>>. The number N in the prompt will be used later in this tutorial but should usually not impact the computation.
You should be able to type single line expressions and press enter to evaluate them. If an expression is incomplete, IPython will automatically detect this and add a new line when you press Enter instead of executing right away.
Feel free to explore multi-line text input. Unlike many other REPLs, with IPython you can use the up and down arrow keys when editing multi-line code blocks.
Here is an example of a longer interaction with the IPython REPL, which we often refer to as an IPython session
In [1]: print('Hello IPython')
Hello IPython
In [2]: 21 * 2 Out[2]: 42
In [3]: def say_hello(name):
   ...:     print('Hello {name}'.format(name=name)) ...:
We won’t get into details right now, but you may notice a few differences to the standard Python REPL. First, your code should be syntax-highlighted as you type. Second, you will see that some results will have an Out[N]: prompt, while some other do not. We’ll come to this later.
Depending on the exact command you are typing you might realize that sometimes Enter will add a new line, and sometimes it will execute the current statement. IPython tries to guess what you are doing, so most of the time you should not have to care. Though if by any chance IPython does not the right thing you can force execution of the current code block by pressing in sequence Esc and Enter. You can also force the insertion of a new line at the position of the cursor by using Ctrl-o.
The four most helpful commands

The four most helpful commands, as well as their brief description, is shown to you in a banner, every time you start IPython:

command

description

?
Introduction and overview of IPython’s features.
%quickref
Quick reference.
help
Python’s own help system.
object?
Details about ‘object’, use ‘object??’ for extra details.
Tab completion

Tab completion, especially for attributes, is a convenient way to explore the structure of any object you’re dealing with. Simply type object_name.<TAB> to view the object’s attributes. Besides Python objects and keywords, tab completion also works on file and directory names.

Starting with IPython 6.0, if jedi is installed, IPython will try to pull completions from Jedi as well. This allows to not only inspect currently existing objects, but also to infer completion statically without executing code. There is nothing particular need to get this to work, simply use tab completion on more complex expressions like the following:

>>> data = ['Number of users', 123456]
... data[0].<tab>
IPython and Jedi will be able to infer that data[0] is actually a string and should show relevant completions like upper(), lower() and other string methods. You can use the Tab key to cycle through completions, and while a completion is highlighted, its type will be shown as well. When the type of the completion is a function, the completer will also show the signature of the function when highlighted.

Exploring your objects

Typing object_name? will print all sorts of details about any object, including docstrings, function definition lines (for call arguments) and constructor details for classes. To get specific information on an object, you can use the magic commands %pdoc, %pdef, %psource and %pfile

Magic functions

IPython has a set of predefined ‘magic functions’ that you can call with a command line style syntax. There are two kinds of magics, line-oriented and cell-oriented. Line magics are prefixed with the % character and work much like OS command-line calls: they get as an argument the rest of the line, where arguments are passed without parentheses or quotes. Lines magics can return results and can be used in the right hand side of an assignment. Cell magics are prefixed with a double %%, and they are functions that get as an argument not only the rest of the line, but also the lines below it in a separate argument.

Magics are useful as convenient functions where Python syntax is not the most natural one, or when one want to embed invalid python syntax in their work flow.

The following examples show how to call the built-in %timeit magic, both in line and cell mode:

In [1]: %timeit range(1000)
100000 loops, best of 3: 7.76 us per loop

In [2]: %%timeit x = range(10000)
...: max(x)
...:
1000 loops, best of 3: 223 us per loop
The built-in magics include:

Functions that work with code: %run, %edit, %save, %macro, %recall, etc.
Functions which affect the shell: %colors, %xmode, %automagic, etc.
Other functions such as %reset, %timeit, %%writefile, %load, or %paste.
You can always call magics using the % prefix, and if you’re calling a line magic on a line by itself, as long as the identifier is not defined in your namespace, you can omit even that:

run thescript.py
You can toggle this behavior by running the %automagic magic. Cell magics must always have the %% prefix.

A more detailed explanation of the magic system can be obtained by calling %magic, and for more details on any magic function, call %somemagic? to read its docstring. To see all the available magic functions, call %lsmagic.

See also

The Magic command system section of the documentation goes more in depth into how the magics works and how to define your own, and Built-in magic commands for a list of built-in magics.

Cell magics example notebook
Running and Editing

The %run magic command allows you to run any python script and load all of its data directly into the interactive namespace. Since the file is re-read from disk each time, changes you make to it are reflected immediately (unlike imported modules, which have to be specifically reloaded). IPython also includes dreload, a recursive reload function.

%run has special flags for timing the execution of your scripts (-t), or for running them under the control of either Python’s pdb debugger (-d) or profiler (-p).

The %edit command gives a reasonable approximation of multi-line editing, by invoking your favorite editor on the spot. IPython will execute the code you type in there as if it were typed interactively. Note that for %edit to work, the call to startup your editor has to be a blocking call. In a GUI environment, your editor likely will have such an option.

Debugging

After an exception occurs, you can call %debug to jump into the Python debugger (pdb) and examine the problem. Alternatively, if you call %pdb, IPython will automatically start the debugger on any uncaught exception. You can print variables, see code, execute statements and even walk up and down the call stack to track down the true source of the problem. This can be an efficient way to develop and debug code, in many cases eliminating the need for print statements or external debugging tools.

You can also step through a program from the beginning by calling %run -d theprogram.py.

History

IPython stores both the commands you enter, and the results it produces. You can easily go through previous commands with the up- and down-arrow keys, or access your history in more sophisticated ways.

Input and output history are kept in variables called In and Out, keyed by the prompt numbers, e.g. In[4]. The last three objects in output history are also kept in variables named _, __ and ___.

You can use the %history magic function to examine past input and output. Input history from previous sessions is saved in a database, and IPython can be configured to save output history.

Several other magic functions can use your input history, including %edit, %rerun, %recall, %macro, %save and %pastebin. You can use a standard format to refer to lines:

%pastebin 3 18-20 ~1/1-5
This will take line 3 and lines 18 to 20 from the current session, and lines 1-5 from the previous session.

System shell commands

To run any command at the system shell, simply prefix it with !, e.g.:

!ping www.bbc.co.uk
You can capture the output into a Python list, e.g.: files = !ls. To pass the values of Python variables or expressions to system commands, prefix them with $: !grep -rF $pattern ipython/* or wrap in {braces}. See our shell section for more details.

Define your own system aliases

It’s convenient to have aliases to the system commands you use most often. This allows you to work seamlessly from inside IPython with the same commands you are used to in your system shell. IPython comes with some pre-defined aliases and a complete system for changing directories, both via a stack (see %pushd, %popd and %dhist) and via direct %cd. The latter keeps a history of visited directories and allows you to go to any previously visited one.

Configuration

Much of IPython can be tweaked through configuration. To get started, use the command ipython profile create to produce the default config files. These will be placed in ~/.ipython/profile_default, and contain comments explaining what the various options do.

Profiles allow you to use IPython for different tasks, keeping separate config files and history for each one. More details in the profiles section.

Startup Files

If you want some code to be run at the beginning of every IPython session, the easiest way is to add Python (.py) or IPython (.ipy) scripts to your profile_default/startup/ directory. Files here will be executed as soon as the IPython shell is constructed, before any other code or scripts you have specified. The files will be run in order of their names, so you can control the ordering with prefixes, like 10-myimports.py.


    42.6 colorgram, tags: colorgram
    https://pypi.org/project/colorgram.py/
 pip3 install colorgram.py
    42.7 pendulum, tags: pendulum
[i500695@WYLQRXL9LQ:2023-10-03 18:24:02:~/work/code/python/modules/pendulum:]2019$ cat pendulum_examples.py 
#!/opt/homebrew/bin/python3

import pendulum
now_israel = pendulum.now("Asia/Jerusalem") 
print(f"now in israel {now_israel}")

same_time_in_other_tz = now_israel.in_timezone('UTC')
print(f"same now of israel in UTC {same_time_in_other_tz}")

tomorrow = pendulum.now().add(days=1)
last_week = pendulum.now().subtract(weeks=1)
print(f"tomorrow={tomorrow}, last_week={last_week}")


past = pendulum.now().subtract(minutes=2)
print(f"past 2 minuts: {past.diff_for_humans()}")

delta = past - last_week

print(f"delta in hours={delta.hours}, delta in words={delta.in_words(locale='en')}")

## Proper handling of datetime normalization
#>>> pendulum.datetime(2013, 3, 31, 2, 30, tz='Europe/Paris')
#'2013-03-31T03:30:00+02:00' # 2:30 does not exist (Skipped time)
#
## Proper handling of dst transitions
#>>> just_before = pendulum.datetime(2013, 3, 31, 1, 59, 59, 999999, tz='Europe/Paris')
#'2013-03-31T01:59:59.999999+01:00'
#>>> just_before.add(microseconds=1)
#'2013-03-31T03:00:00+02:00'


[i500695@WYLQRXL9LQ:2023-10-03 18:24:06:~/work/code/python/modules/pendulum:]2020$ ./pendulum_examples.py 
now in israel 2023-10-03T18:24:07.751636+03:00
same now of israel in UTC 2023-10-03T15:24:07.751636+00:00
tomorrow=2023-10-04T18:24:07.754404+03:00, last_week=2023-09-26T18:24:07.754426+03:00
past 2 minuts: 2 minutes ago
delta in hours=23, delta in words=6 days 23 hours 58 minutes

    42.8 pypdf, tags: pypdf
https://pypi.org/project/pypdf/
[i500695@WYLQRXL9LQ:2023-10-03 18:51:16:~/work/code/python/modules/pypdf:]2036$ cat pypdf_demo.py 
#!/opt/homebrew/bin/python3
from pypdf import PdfReader

reader = PdfReader("Afeka_Challenge_Answers.pdf")
number_of_pages = len(reader.pages)
page = reader.pages[1]
text = page.extract_text()
print(f"The text in page 1 is: {text}")
[i500695@WYLQRXL9LQ:2023-10-03 18:51:28:~/work/code/python/modules/pypdf:]2037$ ./pypdf_demo.py 
The text in page 1 is: count = 0while (n):n &= (n-1)count+= 1return countO(log(n)) / O(1)Arrays, hash tables: https://leetcode.com/problems/two-sum/Naive o(n^2)/0(1)  double loop Better o(n)/o(n) use hash table one loopcode:[i500695@WYLQRXL9LQ:2022-11-28 15:52:45:~/work/code/interviewQs/leetcode:]2017$ cat !$cat twoSum.py#!/usr/local/bin/python3import pdbfrom random import randintfrom timeit import default_timer as timerdef time_func_decorator(func):    def wrapper(*args, **kwargs):        start = timer()        ret = func(*args, **kwargs)        end = timer()        print('function {} took {} seconds'.format(func, end-start))        return ret    return wrapper"""Given an array of integers, return indices of the two numbers such that they add up to a specific target.You may assume that each input would have exactly one solution, and you may not use the same element twice.Example:Given nums = [2, 7, 11, 15], target = 9,Because nums[0] + nums[1] = 2 + 7 = 9,return [0, 1].

    42.9 CSV

        42.9.1 csv description from ChatGPT


In Python, the `csv` module is a standard library module that provides functionality for reading and writing CSV (Comma-Separated Values) files. It makes it easy to work with tabular data in a plain text format. Here's a basic overview of how you can use the `csv` module:

### Reading CSV Files:

```python
import csv

file_path = 'example.csv'

# Reading from a CSV file
with open(file_path, 'r') as file:
    reader = csv.reader(file)
    for row in reader:
        print(row)
```

In this example, the `csv.reader` object is used to read the rows from a CSV file. Each row is represented as a list of values.

### Writing to CSV Files:

```python
import csv

file_path = 'example.csv'

# Writing to a CSV file
data_to_write = [
    ['Name', 'Age', 'City'],
    ['Alice', 28, 'New York'],
    ['Bob', 35, 'San Francisco'],
    ['Charlie', 22, 'Los Angeles']
]

with open(file_path, 'w', newline='') as file:
    writer = csv.writer(file)
    for row in data_to_write:
        writer.writerow(row)
```

In this example, the `csv.writer` object is used to write rows to a CSV file. The `writerow()` method is used to write a single row, and each element in the row is separated by the appropriate delimiter (a comma by default).

### Other Options:

- The `csv` module supports various dialects (e.g., 'excel', 'excel-tab', 'unix') and allows you to customize the delimiter and other parameters.
- You can also use the `DictReader` and `DictWriter` classes for working with dictionaries instead of lists.

Remember to check the Python documentation for the `csv` module for more details and options: [csv — CSV File Reading and Writing](https://docs.python.org/3/library/csv.html).

        42.9.2
    42.10 seaborn

        42.10.1 chatgpt overview of seaborn library in python
Seaborn is a statistical data visualization library in Python that is built on top of Matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics. Seaborn simplifies the process of creating complex visualizations, especially for statistical relationships in datasets. Here's an overview of key features and functionalities offered by the Seaborn library:

### 1. **Statistical Plotting:**
   - Seaborn comes with several high-level functions for creating statistical visualizations, such as:
     - **`sns.scatterplot`:** Scatter plots with options for color and size.
     - **`sns.lineplot`:** Line plots with options for different aggregation functions.
     - **`sns.barplot`:** Bar plots for showing the mean of a quantitative variable across different categories.
     - **`sns.boxplot`:** Box plots for displaying the distribution of quantitative data.
     - **`sns.violinplot`:** Violin plots that combine aspects of box plots and kernel density plots.
     - **`sns.heatmap`:** Heatmaps for displaying matrix-like data.

### 2. **Data Exploration and Relationships:**
   - Seaborn provides functions like `sns.pairplot` and `sns.heatmap` for exploring relationships between variables in a dataset.

### 3. **Categorical Plots:**
   - Seaborn includes functions for visualizing relationships in categorical data, such as:
     - **`sns.catplot`:** Categorical plots that can represent various relationships in long-form data.
     - **`sns.countplot`:** Count plots for showing the count of observations in each category.

### 4. **Color Palettes and Styling:**
   - Seaborn offers a variety of color palettes to enhance the aesthetics of plots.
   - It provides easy-to-use functions for styling plots and controlling plot aesthetics.

### 5. **Distribution Plots:**
   - Seaborn includes functions like `sns.distplot` for visualizing univariate distributions, combining a histogram and a kernel density estimate.

### 6. **Regression Plots:**
   - `sns.regplot` and `sns.lmplot` are used for visualizing the relationships between variables with linear regression models.

### Example Usage:
```python
import seaborn as sns
import matplotlib.pyplot as plt

# Load the example tips dataset
tips = sns.load_dataset("tips")

# Create a scatter plot with a linear fit
sns.lmplot(x="total_bill", y="tip", data=tips, hue="smoker", markers=["o", "x"], palette="Set1")
plt.title('Scatter Plot with Linear Fit')
plt.show()
```

In this example, Seaborn is used to create a scatter plot with a linear fit, visualizing the relationship between the total bill and the tip in a dataset of restaurant tips.

Seaborn is particularly useful for creating visually appealing and informative statistical graphics with minimal code. It complements Matplotlib by providing a higher-level interface tailored for statistical data visualization.
        42.10.2

    42.11 Streamlit

        42.11.1 gemini descrition of Streamlit


| Absolutely! Let's describe the Python Streamlit library and its use cases for development:
| 
| **What is Streamlit?**
| 
| * **Open-Source Framework:**  Streamlit is a powerful and intuitive Python library designed to simplify the process of creating web apps for data science and machine learning projects.
| * **Key Differentiator:** Streamlit's defining feature is its ability to transform your Python scripts into interactive web applications in a remarkably fast and straightforward way.
| * **Minimal Front-End Knowledge:**  Streamlit allows developers, data scientists, and machine learning engineers to build beautiful, interactive data-driven apps without requiring extensive web development experience.  
| 
| **Core Strengths**
| 
| * **Rapid Development:** Build apps in pure Python with a minimal learning curve. 
| * **Interactive Widgets:** Easily add sliders, dropdown menus, text boxes, and more for user input.
| * **Dynamic Updates:** Streamlit apps are reactive; when code or data changes, the app automatically updates to reflect the changes, providing a seamless user experience.
| * **Data & ML Focused:** Works seamlessly with popular Python libraries like NumPy, pandas, matplotlib, scikit-learn, and more.
| 
| **Common Use Cases**
| 
| 1. **Data Exploration & Visualization:**  Create dashboards and tools to analyze, visualize, and interact with datasets in an intuitive way.
| 
| 2. **Machine Learning Model Demos:**  Build interactive applications to showcase your machine learning models, enable users to input data, and observe predictions in real-time.
| 
| 3. **Internal Tools:**  Develop custom internal tools for your team to streamline processes, monitor data, or share analysis results
| 
| 4. **Educational Prototypes:** Quickly prototype machine learning and data science concepts and create visual educational tools.
| 
| **Development Flow**
| 
| 1. **Write Python Code:**  Your Streamlit app is essentially a Python script using Streamlit's provided functions (e.g., `st.write`, `st.dataframe`, `st.slider`).
| 
| 2. **App Structure:** Streamlit offers layout elements and interactive widgets to structure your app
| 
| 3. **Run the Script:** Streamlit runs your script and automatically spins up a local development server, rendering your app in the web browser.
| 
| **Example Code Snippet**
| 
| ```python
| import streamlit as st
| import pandas as pd
| 
| st.title("Interactive Data Explorer")
| 
| data = pd.read_csv("your_data.csv")
| st.dataframe(data)
| 
| selected_column = st.selectbox("Select a column", data.columns)
| st.line_chart(data[selected_column])
| ```
| 
| **Let me know if you'd like to dive deeper into any of these use cases, learn about specific Streamlit components, or see more complex examples!**

        42.11.2
    42.12
43. My sap Course

TOC:
 getting started
        how to run python
        python types
        strings
        lists
        tuples, namedtuple
        dictionaries
        files
        store/read objects pickle
        
    syntax
        indentation syntax,
        : for nesting
        EoL instead of ;
        assignment, tuple assignment 
        conditions
        loops
        range

        read input
        convert string to int, decimal to hexa and vice versa
        print, format

        scopes, LEGB

    list comprehensions

    iterators 

    genrators

    exceptions, with as





    43.1 getting started

        43.1.1 Run python

            43.1.1.1 python REPL
[i500695@C02X632CJGH6:2019-06-10 18:12:01:~:]2003$ alias py3
alias py3='python3'

[i500695@C02X632CJGH6:2019-06-10 18:09:39:~:]2002$ py3
Python 3.7.3 (default, Mar 27 2019, 09:23:15) 
[Clang 10.0.1 (clang-1001.0.46.3)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
history file:  /Users/i500695/.python_history
Welcome i500695, time is 18:11:39 06/10/19 IDT,  Have a good session :-) 
>>> 
            43.1.1.2 Run as script

example:
[i500695@C02X632CJGH6:2019-06-10 18:12:36:~/work/code/python:]2006$ cat numSecsKids.py 
#!/usr/bin/python
# -*- coding: utf-8 -*-
import datetime as dt

import sys
reload(sys)
sys.setdefaultencoding('utf-8')

Bdays={u'מאי': dt.datetime(2007,3,14,11,5,0) , u'איתי': dt.datetime(2011,6,21,9,45,0), u'קאי': dt.datetime(2014,06,8,13,43,0), u'איימי': dt.datetime(2016,3,2,8,5,0), u'דבי': dt.datetime(1977,9,4,17,5,0), u'יוסי': dt.datetime(1976,7,4,6,0,0)}
#mayBday = dt.datetime(2007,03,14,11,05,00)
now = dt.datetime.now()
#secs = (now - Bdays[u'מאי']).total_seconds()
#print("May: seconds {0}, minutes {1}, hours {2}, days {3}".format(secs, secs/60, secs/3600, secs/(3600*24)))
for name,date in Bdays.iteritems():
    secs = (now - date).total_seconds()
    print("{4} is alive for seconds {0}, minutes {1}, hours {2}, days {3}".format(secs, secs/60, secs/3600, secs/(3600*24), name))

[i500695@C02X632CJGH6:2019-06-10 18:13:00:~/work/code/python:]2008$ py numSecsKids.py 
דבי is alive for seconds 1317949686.13, minutes 21965828.1022, hours 366097.135037, days 15254.0472932
מאי is alive for seconds 386320086.134, minutes 6438668.10224, hours 107311.135037, days 4471.29729322
יוסי is alive for seconds 1354882386.13, minutes 22581373.1022, hours 376356.218371, days 15681.5090988
איתי is alive for seconds 251540886.134, minutes 4192348.10224, hours 69872.4683706, days 2911.35284878
איימי is alive for seconds 103284486.134, minutes 1721408.10224, hours 28690.1350373, days 1195.42229322
קאי is alive for seconds 157955406.134, minutes 2632590.10224, hours 43876.5017039, days 1828.187571

[i500695@C02X632CJGH6:2019-06-10 18:13:49:~/work/code/python:]2010$ ll numSecsKids.py 
-rwxr-xr-x  1 i500695  staff   819B Feb 10 11:51 numSecsKids.py*
[i500695@C02X632CJGH6:2019-06-10 18:13:46:~/work/code/python:]2009$ ./numSecsKids.py 
...


            43.1.1.3 IDLE
$ py3 -m idlelib.idle

            43.1.1.4 pycharm, vscode

            43.1.1.5 ipython

            43.1.1.6

        43.1.2


    43.2 SAP intro

        43.2.1  Python strong features
a. "easy" to code. subjective yet backed by popularity and surveys. see: https://adtmag.com/articles/2019/01/08/tiobe-jan-2019.aspx
b. readable. indentation, lack of types lead to more readable code.
c. free and open source with huge amount of libraries and frameworks
d. hardware and platform "agnostic"
e. interpreted
f. Extensible and Embedeble. meaning for example can call native code (C++) and be called from native. Same for java, javascript and NodeJS.
g. OOP. Everything is Object.    

-> example:
class example:
    classVar="like static member"
    def __init__(self, msg):
        self.instanceVar = msg

ex1 = example()
ex1.classVar                                                                                                                                
'like static member'

ex1.instanceVar                                                                                                                             
'hello instance'


-> inheritance:
class mathExample(example): 
    def __init__(self, msg): 
        super().__init__(msg) 
        print('mathExample CTOR called') 
    def aVirtMethod(self): 
        print('custom impl of aVirtMethod') 
                                                                                                                                            

mathEx = mathExample('an example of inheritance')                                                                                           
mathExample CTOR called

mathEx.aVirtMethod()                                                                                                                        
custom impl of aVirtMethod

-> reflection 
    --> type
In [55]: type(mathExample)                                                                                                                           
Out[55]: type

In [56]: type(2)                                                                                                                                     
Out[56]: int

In [57]: type('2')                                                                                                                                   
Out[57]: str

    --> isinstance
In [60]: isinstance(mathEx, mathExample)                                                                                                             
Out[60]: True

    --> callable 
    In [65]: callable(lambda x: print(x) )                                                                                                               
Out[65]: True

In [66]: callable('a')                                                                                                                               
Out[66]: False

    --> dir
In [67]: dir(mathEx)                                                                                                                                 
Out[67]: 
['__class__',
        ...
 'aVirtMethod',
 'classVar',
 'instanceVar']

    --> getattr
    In [68]: getattr(mathEx, 'instanceVar')                                                                                                              
Out[68]: 'an example of inheritance'

h. functional
In [35]: nums = list(range(10))  

    --> map
In [39]: list(map(lambda x: x**3, nums))                                                                                                             
Out[39]: [0, 1, 8, 27, 64, 125, 216, 343, 512, 729]

    --> filter
In [77]: list(filter(lambda x: x%2 == 0, nums))                                                                                                      
Out[77]: [0, 2, 4, 6, 8]

In [78]: list(filter(lambda x: x%2, nums))                                                                                                           
Out[78]: [1, 3, 5, 7, 9]

    --> enumerate
In [79]: for i,e in enumerate(nums): 
    ...:     print('index: {}, elem: {}'.format(i,e)) 
    ...:                                                                                                                                             
index: 0, elem: 0
...
index: 9, elem: 9

    --> sorted
In [80]: sorted([("a", 2), ("c", 1), ("d", 4)])                                                                                                      
Out[80]: [('a', 2), ('c', 1), ('d', 4)]

In [81]: sorted([("a", 2), ("c", 1), ("d", 4)], key=lambda x: x[1])                                                                                  
Out[81]: [('c', 1), ('a', 2), ('d', 4)]

In [83]: sorted([("a", 2), ("c", 1), ("d", 4)], key=lambda x: x[1], reverse=True)                                                                    
Out[83]: [('d', 4), ('a', 2), ('c', 1)]

    --> any, all
In [84]: any(map(lambda x: x==7, nums))                                                                                                              
Out[84]: True

In [85]: all(map(lambda x: x<10, nums))                                                                                                              
Out[85]: True

In [86]: all(map(lambda x: x<5, nums))                                                                                                               
Out[86]: False

    --> zip
In [87]: strs=[ 'zip', 'is', 'great', 'for', 'combining', 'sequences']                                                                               

In [88]: list(zip(strs, map(len, strs)))                                                                                                             
Out[88]: 
[('zip', 3),
 ('is', 2),
 ('great', 5),
 ('for', 3),
 ('combining', 9),
 ('sequences', 9)]

In [89]: dict(zip(strs, map(len, strs)))                                                                                                             
Out[89]: {'zip': 3, 'is': 2, 'great': 5, 'for': 3, 'combining': 9, 'sequences': 9}

In [90]: list(zip(nums,strs))                                                                                                                        
Out[90]: 
[(0, 'zip'),
 (1, 'is'),
 (2, 'great'),
 (3, 'for'),
 (4, 'combining'),
 (5, 'sequences')]

In [91]: for n,s in list(zip(nums,strs)): 
    ...:     print(n,s) 
    ...:                                                                                                                                             
0 zip
1 is
2 great
3 for
4 combining
5 sequences


    --> reduce
In [92]: import functools                                                                                                                            

In [93]: functools.reduce(lambda a,b: a+b, nums)                                                                                                     
Out[93]: 45

In [94]: functools.reduce(lambda a,b: a*b, nums)                                                                                                     
Out[94]: 0

In [95]: functools.reduce(lambda a,b: a*b, nums[1:])                                                                                                 
Out[95]: 362880

    -->

i. Supports closures. Scopeso
    -> closures
A Closure is a function object that remembers values in enclosing scopes even if they are not present in memory.

It is a record that stores a function together with an environment: a mapping associating each free variable of the function (variables that are used locally, but defined in an enclosing scope) with the value or reference to which the name was bound when the closure was created.
A closure—unlike a plain function—allows the function to access those captured variables through the closure’s copies of their values or references, even when the function is invoked outside their scope.

Ex:
In [96]: def outer(msg): 
    ...:     msg=msg 
    ...:     def innerClosure(): 
    ...:         print(msg) 
    ...:     return innerClosure  
    ...:                                                                                                                                             

In [97]: myFunc = outer("Closure example")                                                                                                           

In [98]: myFunc()                                                                                                                                    
Closure example

    -> scopes
Scope resolution via LEGB rule :
In Python, the LEGB rule is used to decide the order in which the namespaces are to be searched for scope resolution.
The scopes are listed below in terms of hierarchy(highest to lowest/narrowest to broadest):

Local(L): Defined inside function/class
Enclosed(E): Defined inside enclosing functions(Nested function concept)
Global(G): Defined at the uppermost level
Built-in(B): Reserved names in Python builtin modules


j. List, dictionary and set comprehensions

    -> syntax
A list comprehension generally consist of these parts :
   Output expression, 
   input sequence, 
   a variable representing member of input sequence and
   an optional predicate part. 

For example :

lst  =  [x ** 2  for x in range (1, 11)   if  x % 2 == 1] 

here, x ** 2 is output expression, 
      range (1, 11)  is input sequence, 
      x is variable and   
      if x % 2 == 1 is predicate part.
    -> example 
In [99]: notprimes = [ j for i in range(2,8) for j in range(i*2, 50, i)]                                                                             

In [100]: notprimes                                                                                                                                  
Out[100]: 
[4,
 6,
 8,
 ...
 35,
 42,
 49]

In [101]: primes= [x for x in range(2,50) if x not in notprimes]                                                                                     

In [102]: primes                                                                                                                                     
Out[102]: [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]
    ->

iterators, Generators expressions and generator functions (lazy iteration)
        -> iterators
        __iter__ method that is called on initialization of an iterator. This should return an object that has a next or __next__ (in Python 3) method.
next ( __next__ in Python 3) The iterator next method should return the next value for the iterable. When an iterator is used with a ‘for in’ loop, the for loop implicitly calls next() on the iterator object. This method should raise a StopIteration to signal the end of the iteration.

class Test: 
  
    # Cosntructor 
    def __init__(self, limit): 
        self.limit = limit 
  
    # Called when iteration is initialized 
    def __iter__(self): 
        self.x = 10
        return self
  
    # To move to next element. In Python 3, 
    # we should replace next with __next__ 
    def next(self): 
  
        # Store current value ofx 
        x = self.x 
  
        # Stop iteration if limit is reached 
        if x > self.limit: 
            raise StopIteration 
  
        # Else increment and return old value 
        self.x = x + 1; 
        return x 
  
# Prints numbers from 10 to 15 
for i in Test(15): 
    print(i) 
  
# Prints nothing 
for i in Test(5): 
    print(i) 

    -> generators
        --> Generator-Function : A generator-function is defined like a normal function, but whenever it needs to generate a value, it does so with the yield keyword rather than return. If the body of a def contains yield, the function automatically becomes a generator function.
In [103]: def myGen(): 
     ...:     yield 4 
     ...:     yield 5 
     ...:     yield 6 
     ...:                                                                                                                                            

In [104]: for n in myGen(): 
     ...:     print(n) 
     ...:                                                                                                                                            
4
5
6


        --> Generator-Object : Generator functions return a generator object. Generator objects are used either by calling the next method on the generator object or using the generator object in a “for in” loop (as shown in the above program).
In [105]: def fib(lim): 
     ...:     a,b = 0,1 
     ...:     while a<lim: 
     ...:         yield a 
     ...:         a,b= b, a+b                                                                                                                        

In [106]: Fibo=fib(20)                                                                                                                               

In [107]: for n in Fibo: 
     ...:     print(n) 
     ...:                                                                                                                                            
g. OOP. Everything is Object.    

-> example:
class example:
    classVar="like static member"
    def __init__(self, msg):
        self.instanceVar = msg

ex1 = example()
ex1.classVar                                                                                                                                
'like static member'

ex1.instanceVar                                                                                                                             
'hello instance'


-> inheritance:
class mathExample(example): 
    def __init__(self, msg): 
        super().__init__(msg) 
        print('mathExample CTOR called') 
    def aVirtMethod(self): 
        print('custom impl of aVirtMethod') 
                                                                                                                                            

mathEx = mathExample('an example of inheritance')                                                                                           
mathExample CTOR called

mathEx.aVirtMethod()                                                                                                                        
custom impl of aVirtMethod

-> reflection 
    --> type
In [55]: type(mathExample)                                                                                                                           
Out[55]: type

In [56]: type(2)                                                                                                                                     
Out[56]: int

In [57]: type('2')                                                                                                                                   
Out[57]: str

    --> isinstance
In [60]: isinstance(mathEx, mathExample)                                                                                                             
Out[60]: True

    --> callable 
    In [65]: callable(lambda x: print(x) )                                                                                                               
Out[65]: True

In [66]: callable('a')                                                                                                                               
Out[66]: False

    --> dir
In [67]: dir(mathEx)                                                                                                                                 
Out[67]: 
['__class__',
        ...
 'aVirtMethod',
 'classVar',
 'instanceVar']

    --> getattr
    In [68]: getattr(mathEx, 'instanceVar')                                                                                                              
Out[68]: 'an example of inheritance'

h. functional
In [35]: nums = list(range(10))  

    --> map
In [39]: list(map(lambda x: x**3, nums))                                                                                                             
Out[39]: [0, 1, 8, 27, 64, 125, 216, 343, 512, 729]

    --> filter
In [77]: list(filter(lambda x: x%2 == 0, nums))                                                                                                      
Out[77]: [0, 2, 4, 6, 8]

In [78]: list(filter(lambda x: x%2, nums))                                                                                                           
Out[78]: [1, 3, 5, 7, 9]

    --> enumerate
In [79]: for i,e in enumerate(nums): 
    ...:     print('index: {}, elem: {}'.format(i,e)) 
    ...:                                                                                                                                             
index: 0, elem: 0
...
index: 9, elem: 9

    --> sorted
In [80]: sorted([("a", 2), ("c", 1), ("d", 4)])                                                                                                      
Out[80]: [('a', 2), ('c', 1), ('d', 4)]

In [81]: sorted([("a", 2), ("c", 1), ("d", 4)], key=lambda x: x[1])                                                                                  
Out[81]: [('c', 1), ('a', 2), ('d', 4)]

In [83]: sorted([("a", 2), ("c", 1), ("d", 4)], key=lambda x: x[1], reverse=True)                                                                    
Out[83]: [('d', 4), ('a', 2), ('c', 1)]

    --> any, all
In [84]: any(map(lambda x: x==7, nums))                                                                                                              
Out[84]: True

In [85]: all(map(lambda x: x<10, nums))                                                                                                              
Out[85]: True

In [86]: all(map(lambda x: x<5, nums))                                                                                                               
Out[86]: False

    --> zip
In [87]: strs=[ 'zip', 'is', 'great', 'for', 'combining', 'sequences']                                                                               

In [88]: list(zip(strs, map(len, strs)))                                                                                                             
Out[88]: 
[('zip', 3),
 ('is', 2),
 ('great', 5),
 ('for', 3),
 ('combining', 9),
 ('sequences', 9)]

In [89]: dict(zip(strs, map(len, strs)))                                                                                                             
Out[89]: {'zip': 3, 'is': 2, 'great': 5, 'for': 3, 'combining': 9, 'sequences': 9}

In [90]: list(zip(nums,strs))                                                                                                                        
Out[90]: 
[(0, 'zip'),
 (1, 'is'),
 (2, 'great'),
 (3, 'for'),
 (4, 'combining'),
 (5, 'sequences')]

In [91]: for n,s in list(zip(nums,strs)): 
    ...:     print(n,s) 
    ...:                                                                                                                                             
0 zip
1 is
2 great
3 for
4 combining
5 sequences


    --> reduce
In [92]: import functools                                                                                                                            

In [93]: functools.reduce(lambda a,b: a+b, nums)                                                                                                     
Out[93]: 45

In [94]: functools.reduce(lambda a,b: a*b, nums)                                                                                                     
Out[94]: 0

In [95]: functools.reduce(lambda a,b: a*b, nums[1:])                                                                                                 
Out[95]: 362880

    -->

i. Supports closures. Scopeso
    -> closures
A Closure is a function object that remembers values in enclosing scopes even if they are not present in memory.

It is a record that stores a function together with an environment: a mapping associating each free variable of the function (variables that are used locally, but defined in an enclosing scope) with the value or reference to which the name was bound when the closure was created.
A closure—unlike a plain function—allows the function to access those captured variables through the closure’s copies of their values or references, even when the function is invoked outside their scope.

Ex:
In [96]: def outer(msg): 
    ...:     msg=msg 
    ...:     def innerClosure(): 
    ...:         print(msg) 
    ...:     return innerClosure  
    ...:                                                                                                                                             

In [97]: myFunc = outer("Closure example")                                                                                                           

In [98]: myFunc()                                                                                                                                    
Closure example

    -> scopes
Scope resolution via LEGB rule :
In Python, the LEGB rule is used to decide the order in which the namespaces are to be searched for scope resolution.
The scopes are listed below in terms of hierarchy(highest to lowest/narrowest to broadest):

Local(L): Defined inside function/class
Enclosed(E): Defined inside enclosing functions(Nested function concept)
Global(G): Defined at the uppermost level
Built-in(B): Reserved names in Python builtin modules


j. List, dictionary and set comprehensions

    -> syntax
A list comprehension generally consist of these parts :
   Output expression, 
   input sequence, 
   a variable representing member of input sequence and
   an optional predicate part. 

For example :

lst  =  [x ** 2  for x in range (1, 11)   if  x % 2 == 1] 

here, x ** 2 is output expression, 
      range (1, 11)  is input sequence, 
      x is variable and   
      if x % 2 == 1 is predicate part.
    -> example 
In [99]: notprimes = [ j for i in range(2,8) for j in range(i*2, 50, i)]                                                                             

In [100]: notprimes                                                                                                                                  
Out[100]: 
[4,
 6,
 8,
 ...
 35,
 42,
 49]

In [101]: primes= [x for x in range(2,50) if x not in notprimes]                                                                                     

In [102]: primes                                                                                                                                     
Out[102]: [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]
    ->

iterators, Generators expressions and generator functions (lazy iteration)
        -> iterators
        __iter__ method that is called on initialization of an iterator. This should return an object that has a next or __next__ (in Python 3) method.
next ( __next__ in Python 3) The iterator next method should return the next value for the iterable. When an iterator is used with a ‘for in’ loop, the for loop implicitly calls next() on the iterator object. This method should raise a StopIteration to signal the end of the iteration.

class Test: 
  
    # Cosntructor 
    def __init__(self, limit): 
        self.limit = limit 
  
    # Called when iteration is initialized 
    def __iter__(self): 
        self.x = 10
        return self
  
    # To move to next element. In Python 3, 
    # we should replace next with __next__ 
    def next(self): 
  
        # Store current value ofx 
        x = self.x 
  
        # Stop iteration if limit is reached 
        if x > self.limit: 
            raise StopIteration 
  
        # Else increment and return old value 
        self.x = x + 1; 
        return x 
  
# Prints numbers from 10 to 15 
for i in Test(15): 
    print(i) 
  
# Prints nothing 
for i in Test(5): 
    print(i) 

    -> generators
        --> Generator-Function : A generator-function is defined like a normal function, but whenever it needs to generate a value, it does so with the yield keyword rather than return. If the body of a def contains yield, the function automatically becomes a generator function.
In [103]: def myGen(): 
     ...:     yield 4 
     ...:     yield 5 
     ...:     yield 6 
     ...:                                                                                                                                            

In [104]: for n in myGen(): 
     ...:     print(n) 
     ...:                                                                                                                                            
4
5
6


        --> Generator-Object : Generator functions return a generator object. Generator objects are used either by calling the next method on the generator object or using the generator object in a “for in” loop (as shown in the above program).
In [105]: def fib(lim): 
     ...:     a,b = 0,1 
     ...:     while a<lim: 
     ...:         yield a 
     ...:         a,b= b, a+b                                                                                                                        

In [106]: Fibo=fib(20)                                                                                                                               

In [107]: for n in Fibo: 
     ...:     print(n) 
     ...:                                                                                                                                            
0
1
1
2
3
5
8
13

        -->
    ->
k. Asynchronous functions and co-routines
l. Standard library support of queues, fixed precisions decimals, rational numbers.
m. Wide ranging standard library including OS access, Internet access, cryptography and much more.
n. Strong dynamic typing
o. Strict nested scoping rules
p. Support for modules and packages

python threadin
https://realpython.com/intro-to-python-threading/

        43.2.2 Python weak features

    43.3

44. Web programming 

    44.1 graphql

        44.1.1 graphql flask tutorial

            44.1.1.1 Creating a GraphQL server with flask
https://medium.com/@marvinkome/creating-a-graphql-server-with-flask-ae767c7e2525
Go to the profile of Marvin Kome
Marvin Kome
May 29, 2018

flask + GraphQL image
In the last two months, I have been working on a project which I used GraphQL to send data to the client-side. So I thought of writing a post on how I did this.

GraphQL is a query language for APIs and a runtime for fulfilling those queries with your existing data. GraphQL provides a complete and understandable description of the data in your API, gives clients the power to ask for exactly what they need and nothing more, makes it easier to evolve APIs over time, and enables powerful developer tools.
You can read more about GraphQL here
So let us get to implementing it in your flask project.

                44.1.1.1.1 Setting up your project
First lets setup your flask projects. Create a directory

$ mkdir flask-graphql-project
$ cd flask-graphql-project
Now create a virtual environment. This helps keep your project dependencies in one place by giving you a custom python installation. If you don’t have it installed already, install it.

$ pip install virtualenv
Now create a virtual environment for your project and activate the virtualenv.

$ virtualenv venv
$ source venv/bin/activate
This creates a virtual environment inside your project folder. Then install your project dependencies.

$ pip install flask flask-graphql flask-migrate flask-sqlalchemy graphene graphene-sqlalchemy
Now create an app.py file in your folder root directory. Add the following code to the file

# Imports
from flask import Flask
# app initialization
app = Flask(__name__)
app.debug = True
# Configs
# TO-DO
# Modules
# TO-DO
# Models
# TO-DO
# Schema Objects
# TO-DO
# Routes
# TO-DO
@app.route('/')
def index():
    return '<p> Hello World</p>'
if __name__ == '__main__':
     app.run()
Note, the to-do comments will be filled as we progress.

Now from the terminal run python app.py runserver this will set up a debug server, then visit 127.0.0.1:5000 If you see hello world written on the page then you’re good to go.

Adding a database
Now, let's add a database to our application. We’re going to be using Sqlalchemy to create our DB models and SQLite for our demo DB.

Add the following code to app.py file.

# Imports
...
from flask_sqlalchemy import SQLAlchemy
import os
basedir = os.path.abspath(os.path.dirname(__file__))
...
# Configs
app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///' +    os.path.join(basedir, 'data.sqlite')
app.config['SQLALCHEMY_COMMIT_ON_TEARDOWN'] = True
app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = True
# Modules
db = SQLAlchemy(app)
...
Now lets create our models. Add the following code to app.py:

...
# Models
class User(db.Model):
    __tablename__ = 'users'
    uuid = db.Column(db.Integer, primary_key=True)
    username = db.Column(db.String(256), index=True, unique=True)
    posts = db.relationship('Post', backref='author')
    
    def __repr__(self):
        return '<User %r>' % self.username
class Post(db.Model):
    __tablename__ = 'posts'
    uuid = db.Column(db.Integer, primary_key=True)
    title = db.Column(db.String(256), index=True)
    body = db.Column(db.Text)
    author_id = db.Column(db.Integer, db.ForeignKey('users.uuid'))
    def __repr__(self):
        return '<Post %r>' % self.title
...
In the code above we created two models Posts and User and created a relationship between the two. Now let's create some demo data.

$ python
>>> from app import db, User, Post
>>> db.create_all()
>>> john = User(username='johndoe')
>>> post = Post()
>>> post.title = "Hello World"
>>> post.body = "This is the first post"
>>> post.author = john
>>> db.session.add(post)
>>> db.session.add(john)
>>> db.session.commit()
>>> User.query.all()
[<User 'johndoe'>]
>>> Post.query.all()
[<Post 'Hello World'>
Adding GraphQL
Now let’s get to business.

Schemas
GraphQL presents your objects to the world as a graph structure rather than a more hierarchical structure to which you may be accustomed. So we need to show which type of object will be shown in the graph. That’s where schemas come in. Schemas are used to describe your data. So let's create a schema for our models.

Add the following content to app.py:

# Imports
...
import graphene
from graphene_sqlalchemy import SQLAlchemyObjectType, SQLAlchemyConnectionField
...
# Schema Objects
class PostObject(SQLAlchemyObjectType):
    class Meta:
        model = Post
        interfaces = (graphene.relay.Node, )
class UserObject(SQLAlchemyObjectType):
   class Meta:
       model = User
       interfaces = (graphene.relay.Node, )
class Query(graphene.ObjectType):
    node = graphene.relay.Node.Field()
    all_posts = SQLAlchemyConnectionField(PostObject)
    all_users = SQLAlchemyConnectionField(UserObject)
schema = graphene.Schema(query=Query)
...
Now add the route to check out our GraphQL interface

# Imports
...
from flask_graphql import GraphQLView
...
# Routes
...
app.add_url_rule(
    '/graphql',
    view_func=GraphQLView.as_view(
        'graphql',
        schema=schema,
        graphiql=True # for having the GraphiQL interface
    )
)
...
Now if you navigate to 127.0.0.1:5000/graphql you’ll see the GraphQLi view. Write your GraphQL query

{
  allPosts{
    edges{
      node{
        title
        body
        author{
          username
        }
      }
    }
  }
}
Mutations
To be able to create posts and users you’ll need mutations. So let's add that to our app.

...
# Schema Objects
...
class CreatePost(graphene.Mutation):
    class Arguments:
        title = graphene.String(required=True)
        body = graphene.String(required=True) 
        username = graphene.String(required=True)
    post = graphene.Field(lambda: PostObject)
    def mutate(self, info, title, body, username):
        user = User.query.filter_by(username=username).first()
        post = Post(title=title, body=body)
        if user is not None:
            post.author = user
        db.session.add(post)
        db.session.commit()
        return CreatePost(post=post)
class Mutation(graphene.ObjectType):
    create_post = CreatePost.Field()
schema = graphene.Schema(query=Query, mutation=Mutation)
Now, let's add a new post with GraphQL mutations:

mutation {
  createPost(username:"johndoe", title:"Hello 2", body:"Hello body 2"){
    post{
      title
      body
      author{
        username
      }
    }
  }
}
You have successfully started a GraphQL server with Flask 👏. The post is open to improvements so if there’s anything you’d like to add please comment about it 😄

If this post was helpful click the clap button as much as possible 😃. Thank You.

        44.1.2


    44.2 tags: local Web server

        44.2.1  one liner http 
python3 -m http.server

    python -m http.server port

    44.3 authentication


        44.3.1 JWT based authentication

            44.3.1.1 https://realpython.com/token-based-authentication-with-flask/

                44.3.1.1.1 Introduction
JSON Web Tokens (or JWTs) provide a means of transmitting information from the client to the server in a stateless, secure way.

On the server, JWTs are generated by signing user information via a secret key, which are then securely stored on the client. This form of auth works well with modern, single page applications. For more on this, along with the pros and cons of using JWTs vs. session and cookie-based auth, please review the following articles:

Cookies vs Tokens: The Definitive Guide. https://auth0.com/blog/cookies-vs-tokens-definitive-guide/
Token Authentication vs. Cookies. http://stackoverflow.com/questions/17000835/token-authentication-vs-cookies
How do sessions work in Flask? https://www.reddit.com/r/flask/comments/5l2gmf/af_eli5_how_sessions_work_in_flask/

                44.3.1.1.2 Project Setup
Start by cloning the project boilerplate and then create a new branch:

$ git clone https://github.com/realpython/flask-jwt-auth.git
$ cd flask-jwt-auth
$ git checkout tags/1.0.0 -b jwt-auth
Create and activate a virtualenv and install the dependencies:

$ python3.6 -m venv env
$ source env/bin/activate
(env)$ pip install -r requirements.txt
This is optional, but it’s a good idea to create a new Github repository and update the remote:

(env)$ git remote set-url origin <newurl>

my setup.
cd /Users/I500695/work/code/python/web/tutorials/jwt_based_auth

(env)$ git remote set-url origin yizaq_flask_jwt_auth_tutorial

                44.3.1.1.3
            44.3.1.2 JWT based authentication step by step

            44.3.1.3
        44.3.2 HTTP basic authentication

            44.3.2.1  HTTP basic authentication step by step 
/Users/I500695/work/code/python/web/tutorials/basic_auth

            44.3.2.2

        44.3.3
    44.4 making http requests, tags: making http requests

        44.4.1 Chatgpt
User
python code to get a json response from https://a.com

ChatGPT
To get a JSON response from a URL using Python, you can use the requests module. Here's an example code that sends a GET request to the specified URL and prints the JSON response:

import requests

url = 'https://a.com'
response = requests.get(url)

if response.status_code == 200:
    json_response = response.json()
    print(json_response)
else:
    print(f'Request failed with status code: {response.status_code}')
In this example, we first import the requests module. Then, we define the URL we want to request (url). We use the requests.get() method to send a GET request to the URL and store the response in the response variable.

Next, we check if the response status code is 200, which indicates a successful request. If the request is successful, we use the response.json() method to parse the JSON response and store it in the json_response variable. Finally, we print the JSON response.

If the request fails for some reason, we print an error message that includes the status code returned by the server.





User
alternative way
ChatGPT
Yes, there are alternative ways to get a JSON response from a URL using Python. Here's an example using the urllib module:

import urllib.request
import json

url = 'https://a.com'
response = urllib.request.urlopen(url)

if response.status == 200:
    json_response = json.loads(response.read())
    print(json_response)
else:
    print(f'Request failed with status code: {response.status}')
In this example, we first import the urllib.request module and the built-in json module. We define the URL we want to request (url) and use the urllib.request.urlopen() method to send a GET request to the URL and store the response in the response variable.

Next, we check if the response status code is 200, which indicates a successful request. If the request is successful, we use the response.read() method to read the response body and the json.loads() method to parse the JSON response and store it in the json_response variable. Finally, we print the JSON response.

If the request fails for some reason, we print an error message that includes the status code returned by the server.





User
more ways
ChatGPT
There are several ways to get a JSON response from a URL using Python. Here are some additional examples:

Using the http.client module
import http.client
import json

conn = http.client.HTTPSConnection("a.com")
conn.request("GET", "/")
response = conn.getresponse()

if response.status == 200:
    json_response = json.loads(response.read())
    print(json_response)
else:
    print(f'Request failed with status code: {response.status}')
conn.close()
In this example, we use the http.client module to send an HTTPS GET request to the specified URL. We create an instance of http.client.HTTPSConnection and use the request() method to send the GET request. We then use the getresponse() method to get the response object.

Next, we check if the response status code is 200, which indicates a successful request. If the request is successful, we use the response.read() method to read the response body and the json.loads() method to parse the JSON response and store it in the json_response variable. Finally, we print the JSON response.

Using the urllib3 module
import urllib3
import json

http = urllib3.PoolManager()
response = http.request('GET', 'https://a.com')

if response.status == 200:
    json_response = json.loads(response.data.decode('utf-8'))
    print(json_response)
else:
    print(f'Request failed with status code: {response.status}')
In this example, we use the urllib3 module to send an HTTPS GET request to the specified URL. We create an instance of urllib3.PoolManager and use its request() method to send the GET request. We then use the status and data attributes of the response object to check if the request was successful and to read the response body.

Next, we check if the response status code is 200, which indicates a successful request. If the request is successful, we use the response.data.decode() method to decode the response body from bytes to a string, and then use the json.loads() method to parse the JSON response and store it in the json_response variable. Finally, we print the JSON response.
        44.4.2


    44.5

45. Virtual Environment, venv

    45.1  tutorial

        45.1.1  https://realpython.com/python-virtual-environments-a-primer/

            45.1.1.1  venv


Global env:
In [1]: import site                                                                                                                                  

In [2]: site.getsitepackages()                                                                                                                       
Out[2]: ['/usr/local/Cellar/python/3.7.3/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages']

install venv:
$ pip install virtualenv

Start by making a new directory to work with:

$ mkdir python-virtual-environments && cd python-virtual-environments
Create a new virtual environment inside the directory:

# Python 2:
$ virtualenv env

# Python 3
$ python3 -m venv env

In order to use this environment’s packages/resources in isolation, you need to “activate” it. To do this, just run the following:

$ source env/bin/activate
(env) $
Notice how your prompt is now prefixed with the name of your environment (env, in our case). This is the indicator that env is currently active, which means the python executable will only use this environment’s packages and settings.

To show the package isolation in action, we can use the bcrypt module as an example. Let’s say we have bcrypt installed system-wide but not in our virtual environment.

Before we test this, we need to go back to the “system” context by executing deactivate:

(env) $ deactivate
$
Now your shell session is back to normal, and the python command refers to the global Python install. Remember to do this whenever you’re done using a specific virtual environment.


            45.1.1.2 virtualenvwrapper
While some of these features may seem small or insignificant, you’ll soon learn that they’re important tools to add to your workflow.

To get started, you can download the wrapper with pip:

$ pip install virtualenvwrapper
Note: For Windows, you should use virtualenvwrapper-win instead.
Once it’s installed, we’ll need to activate its shell functions. We can do this by running source on the installed virtualenvwrapper.sh script. When you first install it with pip, the output of the installation will tell you the exact location of virtualenvwrapper.sh. Or you can simply run the following:

$ which virtualenvwrapper.sh
/usr/local/bin/virtualenvwrapper.sh
Using that path, add the following three lines to your shell’s startup file. If you’re using the Bash shell, you would place these lines in either the ~/.bashrc file or the ~/.profile file. For other shells, like zsh, csh, or fish, you would need to use the startup files specific to that shell. All that matters is that these commands are executed when you log in or open a new shell:

export WORKON_HOME=$HOME/.virtualenvs   # Optional
export PROJECT_HOME=$HOME/projects      # Optional
source /usr/local/bin/virtualenvwrapper.sh
Note: It’s not required to define the WORKON_HOME and PROJECT_HOME environment variables. virtualenvwrapper has default values for those, but you can override them by defining values.
Finally, reload the startup file:

$ source ~/.bashrc
There should now be a directory located at $WORKON_HOME that contains all of the virtualenvwrapper data/files:

$ echo $WORKON_HOME
/Users/michaelherman/.virtualenvs
You’ll also now have the shell commands available to you to help you manage the environments. Here are just a few of the ones available:

workon
deactivate
mkvirtualenv
cdvirtualenv
rmvirtualenv
For more info on commands, installation, and configuring virtualenvwrapper, check out the documentation.

Now, anytime you want to start a new project, you just have to do this:

$ mkvirtualenv my-new-project
(my-new-project) $
This will create and activate a new environment in the directory located at $WORKON_HOME, where all virtualenvwrapper environments are stored.

To stop using that environment, you just need to deactivate it like before:

(my-new-project) $ deactivate
$
If you have many environments to choose from, you can list them all with the workon function:

$ workon
my-new-project
my-django-project
web-scraper
Finally, here’s how to activate:

$ workon web-scraper
(web-scraper) $
If you would like to be able to use a single tool and switch between Python versions, virtualenv will allow you to do just that. virtualenv has a parameter -p that allows you to select which version of Python to use. Combine that with the which command, and we can easily select your preferred version of Python to use in a simple manner. For example, let’s say that we want Python 3 as our preferred version:

$ virtualenv -p $(which python3) blog_virtualenv
This will create a new Python 3 environment.

How does this work? The which command is used for finding a given command in your $PATH variable and returning the full path to that command. So, the full path to python3 was returned, to the -p parameter which takes a PYTHON_EXE. This could also be used for python2 as well. Just substitute python3 for python2 (or python if you system defaults to python2).

Now you don’t have to remember where you installed your environments. You can easily delete or copy them as you wish, and your project directory is less cluttered!

            45.1.1.3 pyenv
Unlike the old virtualenv tool, pyvenv doesn’t support creating environments with arbitrary versions of Python, which means you’re stuck using the default Python 3 installation for all of the environments you create. While you can upgrade an environment to the latest system version of Python (via the --upgrade option), if it changes, you still can’t actually specify a particular version.

There are quite a few ways to install Python, but few of them are easy enough or flexible enough to frequently uninstall and re-install different versions of the binary.

This is where pyenv comes in to play.

Despite the similarity in names (pyvenv vs pyenv), pyenv is different in that its focus is to help you switch between Python versions on a system-level as well as a project-level. While the purpose of pyvenv is to separate out modules, the purpose of pyenv is to separate Python versions.

You can start by installing pyenv with either Homebrew (on OS X) or the pyenv-installer project:

Homebrew

$ brew install pyenv
pyenv-installer

$ curl -L https://raw.githubusercontent.com/yyuu/pyenv-installer/master/bin/pyenv-installer | bash
Note: Unfortunately, pyenv does not support Windows. A few alternatives to try are pywin and anyenv.
Once you have pyenv on your system, here are a few of the basic commands you’re probably interested in:

$ pyenv install 3.5.0   # Install new version
$ pyenv versions        # List installed versions
$ pyenv exec python -V  # Execute 'python -V' using pyenv version
In these few lines, we install the 3.5.0 version of Python, ask pyenv to show us all of the versions available to us, and then execute the python -V command using the pyenv-specified version.

To give you even more control, you can then use any of the available versions for either “global” use or “local” use. Using pyenv with the local command sets the Python version for a specific project or directory by storing the version number in a local .python-version file. We can set the “local” version like this:

$ pyenv local 2.7.11
This creates the .python-version file in our current directory, as you can see here:

$ ls -la
total 16
drwxr-xr-x  4 michaelherman  staff  136 Feb 22 10:57 .
drwxr-xr-x  9 michaelherman  staff  306 Jan 27 20:55 ..
-rw-r--r--  1 michaelherman  staff    7 Feb 22 10:57 .python-version
-rw-r--r--  1 michaelherman  staff   52 Jan 28 17:20 main.py
This file only contains the contents “2.7.11”. Now, when you execute a script using pyenv, it’ll load this file and use the specified version, assuming it’s valid and exists on your system.

Moving on with our example, let’s say we have a simple script called main.py in our project directory that looks like this:

import sys
print('Using version:', sys.version[:5])
All it does is print out the version number of the Python executable being used. Using pyenv and the exec command, we can run the script with any of the different versions of Python we have installed.

$ python main.py
Using version: 2.7.5
$ pyenv global 3.5.0
$ pyenv exec python main.py
Using version: 3.5.0
$ pyenv local 2.7.11
$ pyenv exec python main.py
Using version: 2.7.11
Notice how pyenv exec python main.py uses our “global” Python version by default, but then it uses the “local” version after one is set for the current directory.

This can be very powerful for developers who have lots of projects with varying version requirements. Not only can you easily change the default version for all projects (via global), but you can also override it to specify special cases.


            45.1.1.4

        45.1.2
    45.2

46. Python and vim

    46.1  https://realpython.com/vim-and-python-a-match-made-in-heaven/
It can be hard to maintain consistent use of whitespace characters (space and tab). You may not want spaces before tabs, or trailing whitespace at the end of a line. Sometimes you may want to avoid tabs, or just see where tabs occur.

This tip shows several ways to highlight unwanted whitespace. In addition, the tip explains how to use the listchars option (abbreviated to lcs) to indicate when characters are not displayed on long lines.

Contents[show]
Highlighting with a searchEdit
For occasional use, you can simply search using a suitable pattern to highlight what you want. The following examples assume you use search highlighting (:set hlsearch).

" Show all tabs:
/\t

" Show trailing whitespace:
/\s\+$

" Show trailing whitespace only after some text (ignores blank lines):
/\S\zs\s\+$

" Show spaces before a tab:
/ \+\ze\t
Highlighting with the match commandEdit
The :match command specifies the name of a highlight group and a pattern. Any text matching the pattern will be displayed in the foreground and background colors defined by the highlight group. :help :match :help :2match

Some examples follow. These use a highlight group named ExtraWhitespace which could be defined with one of the following in your vimrc. :help :highlight

:highlight ExtraWhitespace ctermbg=red guibg=red
" The following alternative may be less obtrusive.
:highlight ExtraWhitespace ctermbg=darkgreen guibg=lightgreen
" Try the following if your GUI uses a dark background.
:highlight ExtraWhitespace ctermbg=darkgreen guibg=darkgreen
However, be aware that future colorscheme commands may clear all user-defined highlight groups. Using,

:autocmd ColorScheme * highlight ExtraWhitespace ctermbg=red guibg=red
before the first colorscheme command will ensure that the highlight group gets created and is not cleared by future colorscheme commands. :help :colorscheme

Once this highlight group is created, it can be associated with matching text as in the following examples.

" Show trailing whitespace:
:match ExtraWhitespace /\s\+$/

" Show trailing whitespace and spaces before a tab:
:match ExtraWhitespace /\s\+$\| \+\ze\t/

" Show tabs that are not at the start of a line:
:match ExtraWhitespace /[^\t]\zs\t\+/

" Show spaces used for indenting (so you use only tabs for indenting).
:match ExtraWhitespace /^\t*\zs \+/

" Switch off :match highlighting.
:match
Alternatively, the following pattern will match trailing whitespace, except when typing at the end of a line.

:match ExtraWhitespace /\s\+\%#\@<!$/
If you use this alternate pattern, you may want to consider using the following autocmd to let the highlighting show up as soon as you leave insert mode after entering trailing whitespace:

:autocmd InsertLeave * redraw!
Or alternatively, the following can be used:

:au InsertEnter * match ExtraWhitespace /\s\+\%#\@<!$/
:au InsertLeave * match ExtraWhitespace /\s\+$/
which does not "flash" the screen.

Any :match highlighting applies only to the current window. With the following in your vimrc, the command will be applied to the first window, and to any subsequent windows. The pattern * applies the highlight to all files.

" Show leading whitespace that includes spaces, and trailing whitespace.
:autocmd BufWinEnter * match ExtraWhitespace /^\s* \s*\|\s\+$/
Rather than an autocmd, you may prefer a mapping. With the following, and the default backslash Leader key, you can type \wn to switch highlighting on, and \wf to switch it off.

:nnoremap <Leader>wn :match ExtraWhitespace /^\s* \s*\<Bar>\s\+$/<CR>
:nnoremap <Leader>wf :match<CR>
With Vim 7.1.40 and later, you can use the matchadd() function to define matches (making the :match command available for other purposes). See Highlight long lines for examples.

If your goal is to:

highlight trailing whitespace in red
have this highlighting not appear whilst you are typing in insert mode
have the highlighting of whitespace apply when you open new buffers
then the following 6 commands are what you should put into your .vimrc. They are all listed on this page in separate sections, but this is a consolidated list of precisely what you need.

highlight ExtraWhitespace ctermbg=red guibg=red
match ExtraWhitespace /\s\+$/
autocmd BufWinEnter * match ExtraWhitespace /\s\+$/
autocmd InsertEnter * match ExtraWhitespace /\s\+\%#\@<!$/
autocmd InsertLeave * match ExtraWhitespace /\s\+$/
autocmd BufWinLeave * call clearmatches()
Highlighting with the syntax commandEdit
Using the :match command is easy, but you may want to keep it for another purpose. An alternative is to use syntax highlighting. :help syntax

Here is an example using one of the patterns shown earlier. Put this in your vimrc, after the command :syntax on (which you may already have). Of course you would also need to define ExtraWhitespace with a :highlight command, as shown earlier.

" Show trailing whitepace and spaces before a tab:
:autocmd Syntax * syn match ExtraWhitespace /\s\+$\| \+\ze\t/
If you want the above command to also work if the match is nested in some other syntax group, append containedin=ALL to the end.

Using the list and listchars optionsEdit
In Vim, 'list' is a boolean option that defaults to off. If 'list' is on, whitespace characters are made visible. The 'listchars' option can be used to customize the way whitespace characters are shown. The default displays "^I" for each tab, and "$" at each EOL (end of line, so trailing whitespace can be seen). :help 'list'

The command :set list displays whitespace, while :set nolist displays normally. It is convenient to use :set list! to toggle the option on, so that you can later press : followed by the up arrow to repeat the previous command, to toggle 'list' off.

The following example toggles list, then sets listchars to not display an end-of-line character, and to display > for the first character occupied by a tab, and - for any subsequent characters that the tab may occupy.

:set list!
:set listchars=tab:>-
Here are some alternatives that you can try (each sets list and listchars in one command):

:set list listchars=tab:>-,trail:.,extends:>
" Enter the middle-dot by pressing Ctrl-k then .M
:set list listchars=tab:\|_,trail:·
" Enter the right-angle-quote by pressing Ctrl-k then >>
:set list listchars=tab:»·,trail:·
" Enter the Pilcrow mark by pressing Ctrl-k then PI
:set list listchars=tab:>-,eol:¶
" The command :dig displays other digraphs you can use.
The listchars option uses the "NonText" highlighting group for "eol", "extends" and "precedes", and the "SpecialKey" highlighting group for "nbsp", "tab" and "trail". :help 'listchars'

Using syntax space errorsEdit
Vim has several syntax files that support the display of "space errors". For example, for the C programming language (:set filetype=c), you could put the following in your vimrc:

let c_space_errors = 1
Supported languages are: ada, c, chill, csc, forth, groovy, icon, java, lpc, mel, nqc, nroff, ora, pascal, plm, plsql, python and ruby. The c settings also apply to cpp.

To highlight space errors in java files, you would use:

let java_space_errors = 1
For C, if you don't want to see trailing space errors at end-of-line set:

let c_no_trail_space_error = 1
If you only use spaces to indent, and don't want to see space errors in front of tabs:

let c_no_tab_space_error = 1
If you are interested in learning more, open the syntax file for C (in Vim, put the cursor on the path $VIMRUNTIME/syntax/c.vim and type gf). Then look for the definition of the highlight group cSpaceError (near the end of the file). That group specifies the color used to display space errors in C files. The command :hi cSpaceError will show "xxx" in that color.

Showing long linesEdit
By default, lines longer than the screen width are not wrapped, and you can horizontally scroll the text, for example by moving the cursor to the end of a long line. Therefore there may be characters preceding the visible text (non-displayed characters on the left), and there may be characters that extend after the visible text (non-displayed characters on the right). The listchars option can be used to show a highlighted symbol when characters are not displayed, for example:

" Show < or > when characters are not displayed on the left or right.
:set list listchars=precedes:<,extends:>
" Same, but also show tabs and trailing spaces.
:set list listchars=tab:>-,trail:.,precedes:<,extends:>
Resolving performance problemsEdit
It seems that vim does not handle sucessive calls of the match command gracefully. Since BufWinEnter commands are executed every time a buffer is displayed (i.e., switching to another file), the match command is executed many times during a vim session. This seems to lead to a memory leak which slowly impacts performance (for example scrolling and writing become unbearably slow). Include the following line to fix the issue:

autocmd BufWinLeave * call clearmatches()
NOTE: Versions < 7.2

While I would suggest you keep your VIM up to date, there are reasons why maybe you cannot. To avoid errors you could modify part of the settings above to make sure clearmatches() is an available function (apparently available in 7.2+).

if version >= 702
  autocmd BufWinLeave * call clearmatches()
endif

    46.2

47. oneliners

    47.1 https://blog.finxter.com/10-python-one-liners/ 
# 10. Palindrome Python One-Liner
phrase.find(phrase[::-1])
# 9. Swap Two Variables Python One-Liner
a, b = b, a
# 8. Sum Over Every Other Value Python One-Liner
sum(stock_prices[::2])
# 7. Read File Python One-Liner
[line.strip() for line in open(filename)]
# 6. Factorial Python One-Liner
reduce(lambda x, y: x * y, range(1, n+1))
# 5. Performance Profiling Python One-Liner
python -m cProfile foo.py
# 4. Superset Python One-Liner
lambda l: reduce(lambda z, x: z + [y + [x] for y in z], l, [[]])
# 3. Fibonacci Python One-Liner
lambda x: x if x<=1 else fib(x-1) + fib(x-2) 
# 2. Quicksort Python One-liner
lambda L: [] if L==[] else qsort([x for x in L[1:] if x< L[0]]) + L[0:1] + qsort([x for x in L[1:] if x>=L[0]])
# 1. Sieve of Eratosthenes Python One-liner
reduce( (lambda r,x: r-set(range(x**2,n,x)) if (x in r) else r), range(2,int(n**0.5)), set(range(2,n)))


One-liner #10: check if a string is a palindrome
What is a palindrome? A palindrome is a sequence of characters or numbers “which reads the same backward as forward, such as madam or racecar or the number 10201” (Wikipedia).

Write a Python one-liner that returns the integer value 0, if the sequence is a palindrome. Otherwise, your script should return the integer value -1.

# THE DATA
phrase = "anna"
 
# THE ONE LINER
is_palindrome = phrase.find(phrase[::-1])
 
# THE RESULT
print(is_palindrome)
# 0 (if it wasn't a palindrome, the result would be -1)
The Python one-liner uses two tools to achieve the goal: the find() function and slicing.

The find() function returns an index of a searched subsequence within a sequence. For example, you call y.find(x) on any string y in Python. If the string y contains the string x, the find function returns the start index of the string x within y. Otherwise, it returns the index -1 (the string y does not contain a substring x).

Slicing is a Python-specific concept for carving out a range of values from sequence types such as lists or strings. Slicing is based on the concise notation [start:stop:step] to carve out a sequence starting in index “start” (inclusive) and ending in index “end” (exclusive). The third parameter “step” allows you to define the step size, i.e., how many characters from the original sequence your slice will skip before taking the next character (e.g. step=2 means that your slice will consist of only every other character). If you want to become a true master in the powerful slicing concept, download my ebook “Coffee Break Python Slicing” for free.

A negative step size indicates that the slice is carved out “backward”, i.e., from the right to the left. In this way, the one-liner reverses the string by using the slicing operation (phrase[::-1]).

Why reverse the string at all? Easy: If the original string contains the reversed string, it is a palindrome. Think about it for a moment and enjoy this wonderful concise Python one-liner.

One-liner #9: swap two variables
Suppose, you have two variables a and b. You want to assign the value of a to the variable b and the value of b to the variable a. How to do this in Python?

# THE DATA
a = "hello"
b = "bye"
 
# THE ONE-LINER
a, b = b, a
 
# THE RESULT
print(a)
# bye
print(b)
# hello
This is a nice little trick of the Python programming language. Although the one-liner is very simple, it is definitely worth this label because it nicely demonstrates the expressiveness of the Python language. The standard way of achieving this simple task in other programming languages is to create a third “container” variable that does nothing but keeping the value for a short moment while both variables a and b have the same value:

# THE UGLY THREE-LINER
c = a
a = b
b = c
Two lines saved while improving readability – not bad for a one-liner!

One-liner #8: sum over every second list value
Given a list of values which can be either integers or floats. The goal of this task is to sum over all values in the list that have an even index value (e.g. list values with index 0, 2, 4, 6, 8, …). How to solve it in a single line of code?

# THE DATA
stock_prices = [23, 24, 26, 29, 41, 29, 35]
 
# THE ONE-LINER
res = sum(stock_prices[::2])
 
# THE RESULT
print(res)
# 125
Again, the one-liner uses the advanced slicing notation. Recap that slicing carves out a range of values from the list using the notation [start:stop:step]. The first index of the slice is “start” (inclusive) and the last index of the slice is “end” (exclusive). As you may have guessed, the third parameter “step” defines the step size, i.e., how many characters from the original sequence will be skipped before taking the next character (e.g. step=2 means that your slice will consist of only every other character).

Get the free slicing book.

Finally, the sum operation takes any iterable and sums over all values in the iterable. In the one-liner, it simply sums over all remaining values after excluding every other value from the sequence.

One-liner #7: read lines from a file and store them in a list
Your program must communicate with the outside world to have any impact. One way of doing this is to use the file system for input and output.

How can you read all the lines from a file in one line? As a bonus, how can you get rid of trailing whitespaces while reading the lines (e.g. the newline character ‘\n’)?

# THE DATA
filename = 'oneliners.py'
 
# THE ONE-LINER
lines = [line.strip() for line in open(filename)]
 
# THE RESULT
print(lines)
# ['# THE DATA', "filename = 'oneliners.py'", '', '# THE ONE-LINER', 'lines = [line.strip() for line in open(filename)]', '', '# THE RESULT', 'print(lines)', '']
Complex Python one-liners built upon simpler one-liners. You have to master the simple ones first in order to be able to understand the more complex ones.

This one-liner is simple but nevertheless very important. It uses list comprehension to create a new list ‘lines’ that stores all lines from the file as separate string values.

What’s list comprehension anyways? List comprehension is a compact way of creating lists. If you really want to master list comprehension once and for all, read this in-depth tutorial about list comprehension. The simple formula of list comprehension is [ expression + context ].

Expression: What to do with each list element?
Context: Which list elements to select? It consists of an arbitrary number of for and if statements.
For example, the one-liner uses the expression line.strip(). The function strip() simply removes the leading and trailing whitespaces (e.g. the newline character ‘\n’) of each line returned by the context.

The context iterates over the file object that is returned by the open(filename) statement. It specifies the universe of objects on which the expression should be performed. In this case, the universe is all the lines in the file (string objects).

Thus, the result is a list of lines that are contained in the file ‘oneliners.py’ (let’s assume that all the code of this one-liner is stored in this file and nothing else).

One-liner #6: calculate the mathematical factorial function (n!)
This is a fun challenge that is often asked in Python programming interviews. As always, there are many ways of solving this problem, but why not impress the interviewer with a beautiful Python one-liner solution?

# THE DATA
from functools import reduce
n = 100
 
# THE ONE-LINER
factorial = reduce(lambda x, y: x * y, range(1, n+1))
 
# THE RESULT
print(factorial)
# 93326215443944152681699238856266700490715968264381621468592963895217599993229915608941463976156518286253697920827223758251185210916864000000000000000000000000
The one-liner uses the reduce() function. The reduce function takes two arguments: a function and a sequence. It takes two values from the sequence and combines them according to the function argument. Then it repeats this procedure until only one value is left. This value is returned as a result of the reduce() function. It’s that simple.

In Python 3, the reduce function is not a built-in function anymore (like in Python 2), so you have to import it from the functools library.

The one-liner makes use of the lambda expression which defines an anonymous function. The lambda function takes two arguments x and y and combines them via multiplication, i.e., x * y. In this way, the reduce function multiplies all values in the sequence.

One-liner #5: profile a Python script
Performance optimization is of critical importance for all applications that go beyond small scripts to solve a small problem. Performance matters. A few examples are web development, algorithmic trading, security profiling, automatic driving, and robotics.

This one-liner can be executed directly in the terminal without opening the Python application. Say, you want to profile the Python script ‘foo.py’.

# THE ONE-LINER
python -m cProfile foo.py
The one-liner uses the cProfile application to profile the performance bottlenecks of the file ‘foo.py’. You don’t have to execute the Python script to profile it – the cProfile application does it for you. Then, it tracks statistics (e.g. which functions took the most amount of time) and outputs these statistics to the console.

Here is an example output of such a profiling call (source)

      197 function calls (192 primitive calls) in 0.002 seconds
 
Ordered by: standard name
 
ncalls  tottime  percall  cumtime  percall filename:lineno(function)
     1    0.000    0.000    0.001    0.001 <string>:1(<module>)
     1    0.000    0.000    0.001    0.001 re.py:212(compile)
     1    0.000    0.000    0.001    0.001 re.py:268(_compile)
     1    0.000    0.000    0.000    0.000 sre_compile.py:172(_compile_charset)
     1    0.000    0.000    0.000    0.000 sre_compile.py:201(_optimize_charset)
     4    0.000    0.000    0.000    0.000 sre_compile.py:25(_identityfunction)
   3/1    0.000    0.000    0.000    0.000 sre_compile.py:33(_compile)
As you can see, the profiler keeps track of the numbers of calls of each function and how long each function runs (its latency). These statistics will help you greatly when optimizing your code. It certainly helped me a lot

One-liner #4: write a function that returns the superset of a set
What is a superset? The superset is the set of all subsets of this set. Note that the function argument is a list rather than a set. It’s not possible to define a set of sets in Python because the set data type is not hashable. But sets can only consist of data elements that are hashable – because the hash value of elements is used to determine, in a very fast manner, whether the element is already in the set.

# THE DATA
from functools import reduce
dataset = {1,2,3}
 
# THE ONE-LINER
f = lambda l: reduce(lambda z, x: z + [y + [x] for y in z], l, [[]])
 
# THE RESULT
print(f(dataset))
# [[], [1], [2], [1, 2], [3], [1, 3], [2, 3], [1, 2, 3]]
The source code is modified from this article.

The one-liner shows an elegant way to solve the problem of calculating the superset. Still, I have to admit, it’s hard to understand. The idea is to start the superset as an empty list and repeatedly adding subsets to it until no more subsets can be found. Initially, only the empty set is in the superset. Now, in each step, we take one element x out of the dataset and create a bunch of new subsets that arise when adding x to all of the subsets that are already in the superset. Hence, the superset grows one dataset element at a time.

The one-liner uses the reduce function to accomplish this idea. In the variable z, it stores the current superset (which contains initially only the empty set []). Using list comprehension, it creates a number of new subsets for all existing subsets and adds them to the superset z. In particular, it adds the value x from the dataset to each subset and thus doubles the size of the superset (containing the subsets WITH and WITHOUT the dataset element x). In this way, the reduce function repeatedly “merges” two elements: the superset z and an element x from the dataset.

This was a very hard one-liner but you can see how important it is to properly understand the basics (lambda function, list comprehension, list operations).

One-liner #3: find the first n Fibonacci numbers
What are Fibonacci numbers? They are the values of the Fibonacci series. The Fibonacci series is created by the Italian mathematician “Leonardo of Pisa”. It is very popular because it has many applications in math, art, and biology (read more fascinating applications).

The series starts with the numbers 0 and 1. Each following series element is the sum of the two previous series elements.

# THE DATA
n = 10
# THE ONE-LINER
fib = lambda x: x if x<=1 else fib(x-1) + fib(x-2) 
# THE RESULT
for i in range(n):
    print(fib(i))
"""
0
1
1
2
3
5
8
13
21
34
"""
The one-liner computes the Fibonacci series recursively. It defines an anonymous function with one parameter x to calculate the x-th Fibonacci element. The recursion base cases are x=0 and x=1 for which the Fibonacci numbers are 0 and 1, respectively. On top of that, the function calls itself to calculate the (x-1)-th and (x-2)-th Fibonacci numbers – and sums over both to calculate the x-th Fibonacci number.

While this is an intuitive way of defining the Fibonacci series, it is very inefficient because of highly redundant function calls. For example, the third Fibonacci number is calculated from scratch to find the fourth, the fifth, and the sixth Fibonacci number. A much better way would be to store the third (and every other) Fibonacci number in a list rather than recompute it again and again.

Therefore, the function becomes very slow to calculate only a few dozen Fibonacci values. Still, this one-liner serves educative purposes and is both readable and concise.

One-liner #2: write the Quicksort algorithm
If you don’t know the quicksort algorithm, have a look at this video:


Now, create a one-liner that runs this algorithm!

# THE DATA
unsorted = [33,2,3,45,6,54]
 
# THE ONE-LINER
qsort = lambda L: [] if L==[] else qsort([x for x in L[1:] if x< L[0]]) + L[0:1] + qsort([x for x in L[1:] if x>=L[0]])
 
# THE RESULT
print(qsort(unsorted))
# [2, 3, 6, 33, 45, 54]
The Quicksort algorithm (source) selects a pivot element from the list. In the code, it selects the first element of the list, i.e., L[0]. Then, the algorithm moves all elements that are smaller than the pivot to the left side. Similarly, it moves elements that are larger or equal than the pivot to the right side.

This is repeated in a recursive manner for the left and the right lists. Suppose, you create a new list as follows. You put all elements that are smaller than the pivot to the left, the pivot to the center, and all elements that are larger or equal the pivot to the right. You would consider the resulting list to be a bit more sorted, right? If the two sublists were already sorted, the list would be perfectly sorted. This is where the recursive call of qsort comes into play. It takes over the problem of sorting each sublist by applying the same scheme of pivoting and recursion to the sublist.

One-liner #1: write the Sieve of Eratosthenes
The Sieve of Eratosthenes is an ancient algorithm to find the prime numbers up to a specified number n. If you don’t know the algorithm, here is a very nice video from Khan Academy:


The idea is based on the fact that all non-prime numbers are composites of prime numbers. Thus, you exclude all numbers that are not prime using the following method. First, you mark all multiples of 2 as “not prime”. Second, you mark all multiples of the next unmarked number in your array (which is 3) as “not prime”. In general, for the unmarked number x, you mark the multiples from x starting in the number x**2. So if x**2 is larger than n, you are done. The rest of the numbers are prime.

# THE DATA
n=100
 
# THE ONE-LINER
primes = reduce( (lambda r,x: r-set(range(x**2,n,x)) if (x in r) else r), range(2,int(n**0.5)), set(range(2,n)))
 
# THE RESULT
print(primes)
# {2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97}
This one-liner is based on a StackOverflow answer.

It’s very elegant but you need to invest some time to understand it. The one-liner uses the reduce function to remove, one step at a time, all “marked” numbers from the initial set of all numbers between 2 and n (in the one-liner: set(range(2,n))). It takes this set as the initial value for r. Now it goes over all numbers x between 2 and the square root of n (in the one-liner: range(2,int(n**0.5))) and removes the multiples of x from the set r (starting at x**2) – but only if the number x is a prime number (i.e., it is not removed from the set r at this point in time). Spend 5 minutes to watch the video above and revisit this one-liner, then you will understand the algorithm.


    47.2


48. JSON support

    48.1 Validate JSON one liner
$ cat  examples/designtime/3.0/site-default-entities/role.everyone.json | python -c "import sys,json;json.loads(sys.stdin.read());print 'OK'"

redirect to python -m json.tool

    48.2 python jwt decoder
https://pyjwt.readthedocs.io/en/latest/



 pip3 install pyjwt          
Collecting pyjwt
  Downloading https://files.pythonhosted.org/packages/87/8b/6a9f14b5f781697e51259d81657e6048fd31a113229cf346880bb7545565/PyJWT-1.7.1-py2.py3-none-any.whl
Installing collected packages: pyjwt
Successfully installed pyjwt-1.7.1

SAP/CF_LocAL/portal-cf-site-semantic-service  printClientToken ✗                                                                             67d ⚑ ◒  
▶ pyjwt decode --no-verify 

$ pip install pyjwt
See Installation for more information.
Example Usage¶
>>> import jwt

>>> encoded_jwt = jwt.encode({'some': 'payload'}, 'secret', algorithm='HS256')
>>> encoded_jwt
'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzb21lIjoicGF5bG9hZCJ9.4twFt5NiznN84AWoo1d7KO1T_yoc0Z6XOpOVswacPZg'

>>> jwt.decode(encoded_jwt, 'secret', algorithms=['HS256'])
{'some': 'payload'}
See Usage Examples for more examples.
Command line¶
Usage:
pyjwt [options] INPUT
Decoding examples:
pyjwt --key=secret decode TOKEN
pyjwt decode --no-verify TOKEN

        48.2.1  my usage examples
 pyjwt decode --no-verify <token> | python -m json.tool
 pyjwt decode --no-verify eyJhbGciOiJSUzI1NiIsImprdSI6Imh0dHBzOi8vYmFja2JvbmUtc2Fhcy5hdXRoZW50aWNhdGlvbi5zYXAuaGFuYS5vbmRlbWFuZC5jb20vdG9rZW5fa2V5cyIsImtpZCI6ImtleS1pZC0xIiwidHlwIjoiSldUIn0.eyJqdGkiOiJkNmIxYmNjZGNkMDk0ZmUxYWUzOWNhZmMzMDQxYzlkMCIsImV4dF9hdHRyIjp7ImVuaGFuY2VyIjoiWFNVQUEiLCJ6ZG4iOiJiYWNrYm9uZS1zYWFzIiwic2VydmljZWluc3RhbmNlaWQiOiJiYmY1NTE3Zi1mMjhjLTQxNjQtOThlZi03ODUzZjViYWY4ZjcifSwieHMudXNlci5hdHRyaWJ1dGVzIjp7fSwiZ3JhbnRlZF9zY29wZXMiOlsieHNfYXV0aG9yaXphdGlvbi5yZWFkIiwib3BlbmlkIiwicG9ydGFsLWNmLXNlcnZpY2UteW9zaWIzLWk1MDA2OTUhYjQ5NTUuU3VwZXJBZG1pbiIsInhzX2F1dGhvcml6YXRpb24ud3JpdGUiLCJ1YWEudXNlciJdLCJ4cy5zeXN0ZW0uYXR0cmlidXRlcyI6eyJ4cy5yb2xlY29sbGVjdGlvbnMiOlsic3VwZXJfYWRtaW4iLCJNeVJvbGUiLCJ5b3NpYjMtc2Fhcy1zdXBlcmFkbWluIiwieW9zaWIzLnN1cGVyYWRtaW4zIl19LCJnaXZlbl9uYW1lIjoiWW9zaSIsImZhbWlseV9uYW1lIjoiSXphcSIsInN1YiI6IjI1MzhkOWM0LTk3YWQtNDI4NS04N2NiLWEwMDY3YjA5OWU5OCIsInNjb3BlIjpbInhzX2F1dGhvcml6YXRpb24ucmVhZCIsIm9wZW5pZCIsInBvcnRhbC1jZi1zZXJ2aWNlLXlvc2liMy1pNTAwNjk1IWI0OTU1LlN1cGVyQWRtaW4iLCJ4c19hdXRob3JpemF0aW9uLndyaXRlIiwidWFhLnVzZXIiXSwiY2xpZW50X2lkIjoic2ItYmJmNTUxN2YtZjI4Yy00MTY0LTk4ZWYtNzg1M2Y1YmFmOGY3IWI0OTU1fHBvcnRhbC1jZi1zZXJ2aWNlLXlvc2liMy1pNTAwNjk1IWI0OTU1IiwiY2lkIjoic2ItYmJmNTUxN2YtZjI4Yy00MTY0LTk4ZWYtNzg1M2Y1YmFmOGY3IWI0OTU1fHBvcnRhbC1jZi1zZXJ2aWNlLXlvc2liMy1pNTAwNjk1IWI0OTU1IiwiYXpwIjoic2ItYmJmNTUxN2YtZjI4Yy00MTY0LTk4ZWYtNzg1M2Y1YmFmOGY3IWI0OTU1fHBvcnRhbC1jZi1zZXJ2aWNlLXlvc2liMy1pNTAwNjk1IWI0OTU1IiwicmV2b2NhYmxlIjp0cnVlLCJncmFudF90eXBlIjoidXNlcl90b2tlbiIsInVzZXJfaWQiOiIyNTM4ZDljNC05N2FkLTQyODUtODdjYi1hMDA2N2IwOTllOTgiLCJvcmlnaW4iOiJsZGFwIiwidXNlcl9uYW1lIjoieW9zaS5pemFxQHNhcC5jb20iLCJlbWFpbCI6Inlvc2kuaXphcUBzYXAuY29tIiwiYXV0aF90aW1lIjoxNTg0MDA1MzU2LCJyZXZfc2lnIjoiZDU4OTRlMWMiLCJpYXQiOjE1ODQwMDUzNTYsImV4cCI6MTU4NDA0ODU1NiwiaXNzIjoiaHR0cDovL2JhY2tib25lLXNhYXMubG9jYWxob3N0OjgwODAvdWFhL29hdXRoL3Rva2VuIiwiemlkIjoiMTllNWFiYmQtMjkyMC00MDgwLTk0NDktMmM2NTQ4OTgyZjVlIiwiYXVkIjpbXX0.qBKdsHwxLqCThgVKbFUjoKFUrvKlH8_g98SKXn7vSTIoUUjvgKh4I9X_12R_h7URSm09y5UbNavOEbJgfIsgscbw9olZ4gwzGUuwIO5OEfUNoN1ly0fR5j1l9HsupHi48cGv7Dlv3Ey8mXR8rZUSabv3Y9AZbLKD1GgvIN_mJSTJRuIAgHJTZlNnVowHmhLXlEZLFKOp3iSRTKHvsEfFIPP4AonLg5NPvw30AnTZJF1zdbaekC1nCaJH2mRxUGpa-cKEgw1ycTxOxCYXHEJTc0QNmdRurz6GTFFce0N3ZfFnxD3mDywlY_MBpXvZ4RCUZv25ecmXvoWwbBLyWRUsNg | python -m json.tool

Or use printJwt alias
__printJWT__(){
     pyjwt decode --no-verify $1 | python -m json.tool
}
alias printJWT='__printJWT__'


        48.2.2

    48.3 tutorials

        48.3.1 https://realpython.com/python-json/#a-very-brief-history-of-json

            48.3.1.1  introduction


Since its inception, JSON has quickly become the de facto standard for information exchange. Chances are you’re here because you need to transport some data from here to there. Perhaps you’re gathering information through an API or storing your data in a document database. One way or another, you’re up to your neck in JSON, and you’ve got to Python your way out.

Luckily, this is a pretty common task, and—as with most common tasks—Python makes it almost disgustingly easy. Have no fear, fellow Pythoneers and Pythonistas. This one’s gonna be a breeze!

So, we use JSON to store and exchange data? Yup, you got it! It’s nothing more than a standardized format the community uses to pass data around. Keep in mind, JSON isn’t the only format available for this kind of work, but XML and YAML are probably the only other ones worth mentioning in the same breath.


A (Very) Brief History of JSON
Not so surprisingly, JavaScript Object Notation was inspired by a subset of the JavaScript programming language dealing with object literal syntax. They’ve got a nifty website that explains the whole thing. Don’t worry though: JSON has long since become language agnostic and exists as its own standard, so we can thankfully avoid JavaScript for the sake of this discussion.

Ultimately, the community at large adopted JSON because it’s easy for both humans and machines to create and understand.


Look, it’s JSON!
Get ready. I’m about to show you some real life JSON—just like you’d see out there in the wild. It’s okay: JSON is supposed to be readable by anyone who’s used a C-style language, and Python is a C-style language…so that’s you!

{
    "firstName": "Jane",
    "lastName": "Doe",
    "hobbies": ["running", "sky diving", "singing"],
    "age": 35,
    "children": [
        {
            "firstName": "Alice",
            "age": 6
        },
        {
            "firstName": "Bob",
            "age": 8
        }
    ]
}
As you can see, JSON supports primitive types, like strings and numbers, as well as nested lists and objects.

Wait, that looks like a Python dictionary! I know, right? It’s pretty much universal object notation at this point, but I don’t think UON rolls off the tongue quite as nicely. Feel free to discuss alternatives in the comments.

Whew! You survived your first encounter with some wild JSON. Now you just need to learn how to tame it.

Python Supports JSON Natively!
Python comes with a built-in package called json for encoding and decoding JSON data.

Just throw this little guy up at the top of your file:

import json
A Little Vocabulary
The process of encoding JSON is usually called serialization. This term refers to the transformation of data into a series of bytes (hence serial) to be stored or transmitted across a network. You may also hear the term marshaling, but that’s a whole other discussion. Naturally, deserialization is the reciprocal process of decoding data that has been stored or delivered in the JSON standard.

Yikes! That sounds pretty technical. Definitely. But in reality, all we’re talking about here is reading and writing. Think of it like this: encoding is for writing data to disk, while decoding is for reading data into memory.

            48.3.1.2 Serializing JSON
What happens after a computer processes lots of information? It needs to take a data dump. Accordingly, the json library exposes the dump() method for writing data to files. There is also a dumps() method (pronounced as “dump-s”) for writing to a Python string.

Simple Python objects are translated to JSON according to a fairly intuitive conversion.

Python	JSON
dict	object
list, tuple	array
str	string
int, long, float	number
True	true
False	false
None	null

ex:
[I500695@C02ZR8BSMD6N:2022-01-02 18:53:57:~/work/code/python/json:]2026$ cat json101_serialize1.py 
import json 

def main():
    json_data_str = '{"id": "foo", "age": 45, "city": "hadera"}'
    json_data = json.loads(json_data_str)
    with open("out.json", "w") as out:
        json.dump(json_data, out)
    with open("out_indent.json", "w") as out_indent:
        json.dump(json_data,out_indent, indent=4 )

if __name__ == "__main__":
    main()
[I500695@C02ZR8BSMD6N:2022-01-02 18:18:54:~/work/code/python/json:]2019$ py3 json101_serialize1.py 
[I500695@C02ZR8BSMD6N:2022-01-02 18:53:51:~/work/code/python/json:]2024$ cat out.json 
{"id": "foo", "age": 45, "city": "hadera"}[I500695@C02ZR8BSMD6N:2022-01-02 18:53:54:~/work/code/python/json:]2025$ cat out_indent.json 
{
    "id": "foo",
    "age": 45,
    "city": "hadera"
}

        48.3.2 deserialization
[I500695@C02ZR8BSMD6N:2022-01-03 18:23:32:~/work/code/python/json:]2031$ cat !$
cat json101_deserialize1.py
import json 

def main():
    json_data_str = '{"id": "foo", "age": 45, "city": "hadera"}'
    with open("out.json", "r") as infile:
        json_data=json.load(infile)
    print(f"read json from file: {json_data}")
    json_data_assert = json.loads(json_data_str)
    assert(json_data_assert == json_data)

if __name__ == "__main__":
    main()
[I500695@C02ZR8BSMD6N:2022-01-03 18:23:37:~/work/code/python/json:]2032$ py3 json101_deserialize1.py 
read json from file: {'id': 'foo', 'age': 45, 'city': 'hadera'}

        48.3.3
    48.4
49. signals

    49.1 capture SIGINT, ctrl-c
https://stackoverflow.com/questions/1112343/how-do-i-capture-sigint-in-python

a. 
#!/usr/bin/env python
import signal
import sys
def signal_handler(sig, frame):
        print('You pressed Ctrl+C!')
        sys.exit(0)
signal.signal(signal.SIGINT, signal_handler)
print('Press Ctrl+C')
signal.pause()

b.
You can treat it like an exception (KeyboardInterrupt), like any other. Make a new file and run it from your shell with the following contents to see what I mean:

import time, sys

x = 1
while True:
    try:
        print x
        time.sleep(.3)
        x += 1
    except KeyboardInterrupt:
        print "Bye"
        sys.exit()

c. 
And as a context manager:

import signal

class GracefulInterruptHandler(object):

    def __init__(self, sig=signal.SIGINT):
        self.sig = sig

    def __enter__(self):

        self.interrupted = False
        self.released = False

        self.original_handler = signal.getsignal(self.sig)

        def handler(signum, frame):
            self.release()
            self.interrupted = True

        signal.signal(self.sig, handler)

        return self

    def __exit__(self, type, value, tb):
        self.release()

    def release(self):

        if self.released:
            return False

        signal.signal(self.sig, self.original_handler)

        self.released = True

        return True
To use:

with GracefulInterruptHandler() as h:
    for i in xrange(1000):
        print "..."
        time.sleep(1)
        if h.interrupted:
            print "interrupted!"
            time.sleep(2)
            break
Nested handlers:

with GracefulInterruptHandler() as h1:
    while True:
        print "(1)..."
        time.sleep(1)
        with GracefulInterruptHandler() as h2:
            while True:
                print "\t(2)..."
                time.sleep(1)
                if h2.interrupted:
                    print "\t(2) interrupted!"
                    time.sleep(2)
                    break
        if h1.interrupted:
            print "(1) interrupted!"
            time.sleep(2)
            break

    49.2
50. Multi threading, parallelism, concurrency

    50.1  intro to python multi threading

        50.1.1  https://realpython.com/intro-to-python-threading/

            50.1.1.1  single thread


import logging
import threading
import time

def thread_function(name):
    logging.info("Thread %s: starting", name)
    time.sleep(2)
    logging.info("Thread %s: finishing", name)

if __name__ == "__main__":
    format = "%(asctime)s: %(message)s"
    logging.basicConfig(format=format, level=logging.INFO,
                        datefmt="%H:%M:%S")

    logging.info("Main    : before creating thread")
    x = threading.Thread(target=thread_function, args=(1,))
    logging.info("Main    : before running thread")
    x.start()
    logging.info("Main    : wait for the thread to finish")
    # x.join()
    logging.info("Main    : all done")

$ ./single_thread.py
Main    : before creating thread
Main    : before running thread
Thread 1: starting
Main    : wait for the thread to finish
Main    : all done
Thread 1: finishing

            50.1.1.2 multiple threads
import logging
import threading
import time

def thread_function(name):
    logging.info("Thread %s: starting", name)
    time.sleep(2)
    logging.info("Thread %s: finishing", name)

if __name__ == "__main__":
    format = "%(asctime)s: %(message)s"
    logging.basicConfig(format=format, level=logging.INFO,
                        datefmt="%H:%M:%S")

    threads = list()
    for index in range(3):
        logging.info("Main    : create and start thread %d.", index)
        x = threading.Thread(target=thread_function, args=(index,))
        threads.append(x)
        x.start()

    for index, thread in enumerate(threads):
        logging.info("Main    : before joining thread %d.", index)
        thread.join()
        logging.info("Main    : thread %d done", index)

$ ./multiple_threads.py
Main    : create and start thread 0.
Thread 0: starting
Main    : create and start thread 1.
Thread 1: starting
Main    : create and start thread 2.
Thread 2: starting
Main    : before joining thread 0.
Thread 2: finishing
Thread 1: finishing
Thread 0: finishing
Main    : thread 0 done
Main    : before joining thread 1.
Main    : thread 1 done
Main    : before joining thread 2.
Main    : thread 2 done

            50.1.1.3 ThreadPoolExecutor
import concurrent.futures

# [rest of code]

if __name__ == "__main__":
    format = "%(asctime)s: %(message)s"
    logging.basicConfig(format=format, level=logging.INFO,
                        datefmt="%H:%M:%S")

    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
        executor.map(thread_function, range(3))

The code creates a ThreadPoolExecutor as a context manager, telling it how many worker threads it wants in the pool. It then uses .map() to step through an iterable of things, in your case range(3), passing each one to a thread in the pool.

The end of the with block causes the ThreadPoolExecutor to do a .join() on each of the threads in the pool. It is strongly recommended that you use ThreadPoolExecutor as a context manager when you can so that you never forget to .join() the threads.

$ ./executor.py
Thread 0: starting
Thread 1: starting
Thread 2: starting
Thread 1: finishing
Thread 0: finishing
Thread 2: finishing

            50.1.1.4 More
Basic Synchronization Using Lock
There are a number of ways to avoid or solve race conditions. You won’t look at all of them here, but there are a couple that are used frequently. Let’s start with Lock.

To solve your race condition above, you need to find a way to allow only one thread at a time into the read-modify-write section of your code. The most common way to do this is called Lock in Python. In some other languages this same idea is called a mutex. Mutex comes from MUTual EXclusion, which is exactly what a Lock does.

A Lock is an object that acts like a hall pass. Only one thread at a time can have the Lock. Any other thread that wants the Lock must wait until the owner of the Lock gives it up.

The basic functions to do this are .acquire() and .release(). A thread will call my_lock.acquire() to get the lock. If the lock is already held, the calling thread will wait until it is released. There’s an important point here. If one thread gets the lock but never gives it back, your program will be stuck. You’ll read more about this later.

Fortunately, Python’s Lock will also operate as a context manager, so you can use it in a with statement, and it gets released automatically when the with block exits for any reason.

Let’s look at the FakeDatabase with a Lock added to it. The calling function stays the same:

class FakeDatabase:
    def __init__(self):
        self.value = 0
        self._lock = threading.Lock()

    def locked_update(self, name):
        logging.info("Thread %s: starting update", name)
        logging.debug("Thread %s about to lock", name)
        with self._lock:
            logging.debug("Thread %s has lock", name)
            local_copy = self.value
            local_copy += 1
            time.sleep(0.1)
            self.value = local_copy
            logging.debug("Thread %s about to release lock", name)
        logging.debug("Thread %s after release", name)
        logging.info("Thread %s: finishing update", name)
Other than adding a bunch of debug logging so you can see the locking more clearly, the big change here is to add a member called ._lock, which is a threading.Lock() object. This ._lock is initialized in the unlocked state and locked and released by the with statement.

It’s worth noting here that the thread running this function will hold on to that Lock until it is completely finished updating the database. In this case, that means it will hold the Lock while it copies, updates, sleeps, and then writes the value back to the database.

If you run this version with logging set to warning level, you’ll see this:

$ ./fixrace.py
Testing locked update. Starting value is 0.
Thread 0: starting update
Thread 1: starting update
Thread 0: finishing update
Thread 1: finishing update
Testing locked update. Ending value is 2.
Look at that. Your program finally works!

You can turn on full logging by setting the level to DEBUG by adding this statement after you configure the logging output in __main__:

logging.getLogger().setLevel(logging.DEBUG)
Running this program with DEBUG logging turned on looks like this:

$ ./fixrace.py
Testing locked update. Starting value is 0.
Thread 0: starting update
Thread 0 about to lock
Thread 0 has lock
Thread 1: starting update
Thread 1 about to lock
Thread 0 about to release lock
Thread 0 after release
Thread 0: finishing update
Thread 1 has lock
Thread 1 about to release lock
Thread 1 after release
Thread 1: finishing update
Testing locked update. Ending value is 2.
In this output you can see Thread 0 acquires the lock and is still holding it when it goes to sleep. Thread 1 then starts and attempts to acquire the same lock. Because Thread 0 is still holding it, Thread 1 has to wait. This is the mutual exclusion that a Lock provides.

Many of the examples in the rest of this article will have WARNING and DEBUG level logging. We’ll generally only show the WARNING level output, as the DEBUG logs can be quite lengthy. Try out the programs with the logging turned up and see what they do.


 Remove ads
Deadlock
Before you move on, you should look at a common problem when using Locks. As you saw, if the Lock has already been acquired, a second call to .acquire() will wait until the thread that is holding the Lock calls .release(). What do you think happens when you run this code:

import threading

l = threading.Lock()
print("before first acquire")
l.acquire()
print("before second acquire")
l.acquire()
print("acquired lock twice")
When the program calls l.acquire() the second time, it hangs waiting for the Lock to be released. In this example, you can fix the deadlock by removing the second call, but deadlocks usually happen from one of two subtle things:

An implementation bug where a Lock is not released properly
A design issue where a utility function needs to be called by functions that might or might not already have the Lock
The first situation happens sometimes, but using a Lock as a context manager greatly reduces how often. It is recommended to write code whenever possible to make use of context managers, as they help to avoid situations where an exception skips you over the .release() call.

The design issue can be a bit trickier in some languages. Thankfully, Python threading has a second object, called RLock, that is designed for just this situation. It allows a thread to .acquire() an RLock multiple times before it calls .release(). That thread is still required to call .release() the same number of times it called .acquire(), but it should be doing that anyway.

Lock and RLock are two of the basic tools used in threaded programming to prevent race conditions. There are a few other that work in different ways. Before you look at them, let’s shift to a slightly different problem domain.

Producer-Consumer Threading
The Producer-Consumer Problem is a standard computer science problem used to look at threading or process synchronization issues. You’re going to look at a variant of it to get some ideas of what primitives the Python threading module provides.

For this example, you’re going to imagine a program that needs to read messages from a network and write them to disk. The program does not request a message when it wants. It must be listening and accept messages as they come in. The messages will not come in at a regular pace, but will be coming in bursts. This part of the program is called the producer.

On the other side, once you have a message, you need to write it to a database. The database access is slow, but fast enough to keep up to the average pace of messages. It is not fast enough to keep up when a burst of messages comes in. This part is the consumer.

In between the producer and the consumer, you will create a Pipeline that will be the part that changes as you learn about different synchronization objects.

That’s the basic layout. Let’s look at a solution using Lock. It doesn’t work perfectly, but it uses tools you already know, so it’s a good place to start.

Producer-Consumer Using Lock
Since this is an article about Python threading, and since you just read about the Lock primitive, let’s try to solve this problem with two threads using a Lock or two.

The general design is that there is a producer thread that reads from the fake network and puts the message into a Pipeline:

import random 

SENTINEL = object()

def producer(pipeline):
    """Pretend we're getting a message from the network."""
    for index in range(10):
        message = random.randint(1, 101)
        logging.info("Producer got message: %s", message)
        pipeline.set_message(message, "Producer")

    # Send a sentinel message to tell consumer we're done
    pipeline.set_message(SENTINEL, "Producer")
To generate a fake message, the producer gets a random number between one and one hundred. It calls .set_message() on the pipeline to send it to the consumer.

The producer also uses a SENTINEL value to signal the consumer to stop after it has sent ten values. This is a little awkward, but don’t worry, you’ll see ways to get rid of this SENTINEL value after you work through this example.

On the other side of the pipeline is the consumer:

def consumer(pipeline):
    """Pretend we're saving a number in the database."""
    message = 0
    while message is not SENTINEL:
        message = pipeline.get_message("Consumer")
        if message is not SENTINEL:
            logging.info("Consumer storing message: %s", message)
The consumer reads a message from the pipeline and writes it to a fake database, which in this case is just printing it to the display. If it gets the SENTINEL value, it returns from the function, which will terminate the thread.

Before you look at the really interesting part, the Pipeline, here’s the __main__ section, which spawns these threads:

if __name__ == "__main__":
    format = "%(asctime)s: %(message)s"
    logging.basicConfig(format=format, level=logging.INFO,
                        datefmt="%H:%M:%S")
    # logging.getLogger().setLevel(logging.DEBUG)

    pipeline = Pipeline()
    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        executor.submit(producer, pipeline)
        executor.submit(consumer, pipeline)
This should look fairly familiar as it’s close to the __main__ code in the previous examples.

Remember that you can turn on DEBUG logging to see all of the logging messages by uncommenting this line:

# logging.getLogger().setLevel(logging.DEBUG)
It can be worthwhile to walk through the DEBUG logging messages to see exactly where each thread acquires and releases the locks.

Now let’s take a look at the Pipeline that passes messages from the producer to the consumer:

class Pipeline:
    """
    Class to allow a single element pipeline between producer and consumer.
    """
    def __init__(self):
        self.message = 0
        self.producer_lock = threading.Lock()
        self.consumer_lock = threading.Lock()
        self.consumer_lock.acquire()

    def get_message(self, name):
        logging.debug("%s:about to acquire getlock", name)
        self.consumer_lock.acquire()
        logging.debug("%s:have getlock", name)
        message = self.message
        logging.debug("%s:about to release setlock", name)
        self.producer_lock.release()
        logging.debug("%s:setlock released", name)
        return message

    def set_message(self, message, name):
        logging.debug("%s:about to acquire setlock", name)
        self.producer_lock.acquire()
        logging.debug("%s:have setlock", name)
        self.message = message
        logging.debug("%s:about to release getlock", name)
        self.consumer_lock.release()
        logging.debug("%s:getlock released", name)
Woah! That’s a lot of code. A pretty high percentage of that is just logging statements to make it easier to see what’s happening when you run it. Here’s the same code with all of the logging statements removed:

class Pipeline:
    """
    Class to allow a single element pipeline between producer and consumer.
    """
    def __init__(self):
        self.message = 0
        self.producer_lock = threading.Lock()
        self.consumer_lock = threading.Lock()
        self.consumer_lock.acquire()

    def get_message(self, name):
        self.consumer_lock.acquire()
        message = self.message
        self.producer_lock.release()
        return message

    def set_message(self, message, name):
        self.producer_lock.acquire()
        self.message = message
        self.consumer_lock.release()
That seems a bit more manageable. The Pipeline in this version of your code has three members:

.message stores the message to pass.
.producer_lock is a threading.Lock object that restricts access to the message by the producer thread.
.consumer_lock is also a threading.Lock that restricts access to the message by the consumer thread.
__init__() initializes these three members and then calls .acquire() on the .consumer_lock. This is the state you want to start in. The producer is allowed to add a new message, but the consumer needs to wait until a message is present.

.get_message() and .set_messages() are nearly opposites. .get_message() calls .acquire() on the consumer_lock. This is the call that will make the consumer wait until a message is ready.

Once the consumer has acquired the .consumer_lock, it copies out the value in .message and then calls .release() on the .producer_lock. Releasing this lock is what allows the producer to insert the next message into the pipeline.

Before you go on to .set_message(), there’s something subtle going on in .get_message() that’s pretty easy to miss. It might seem tempting to get rid of message and just have the function end with return self.message. See if you can figure out why you don’t want to do that before moving on.

Here’s the answer. As soon as the consumer calls .producer_lock.release(), it can be swapped out, and the producer can start running. That could happen before .release() returns! This means that there is a slight possibility that when the function returns self.message, that could actually be the next message generated, so you would lose the first message. This is another example of a race condition.

Moving on to .set_message(), you can see the opposite side of the transaction. The producer will call this with a message. It will acquire the .producer_lock, set the .message, and the call .release() on then consumer_lock, which will allow the consumer to read that value.

Let’s run the code that has logging set to WARNING and see what it looks like:

$ ./prodcom_lock.py
Producer got data 43
Producer got data 45
Consumer storing data: 43
Producer got data 86
Consumer storing data: 45
Producer got data 40
Consumer storing data: 86
Producer got data 62
Consumer storing data: 40
Producer got data 15
Consumer storing data: 62
Producer got data 16
Consumer storing data: 15
Producer got data 61
Consumer storing data: 16
Producer got data 73
Consumer storing data: 61
Producer got data 22
Consumer storing data: 73
Consumer storing data: 22
At first, you might find it odd that the producer gets two messages before the consumer even runs. If you look back at the producer and .set_message(), you will notice that the only place it will wait for a Lock is when it attempts to put the message into the pipeline. This is done after the producer gets the message and logs that it has it.

When the producer attempts to send this second message, it will call .set_message() the second time and it will block.

The operating system can swap threads at any time, but it generally lets each thread have a reasonable amount of time to run before swapping it out. That’s why the producer usually runs until it blocks in the second call to .set_message().

Once a thread is blocked, however, the operating system will always swap it out and find a different thread to run. In this case, the only other thread with anything to do is the consumer.

The consumer calls .get_message(), which reads the message and calls .release() on the .producer_lock, thus allowing the producer to run again the next time threads are swapped.

Notice that the first message was 43, and that is exactly what the consumer read, even though the producer had already generated the 45 message.

While it works for this limited test, it is not a great solution to the producer-consumer problem in general because it only allows a single value in the pipeline at a time. When the producer gets a burst of messages, it will have nowhere to put them.

Let’s move on to a better way to solve this problem, using a Queue.


 Remove ads
Producer-Consumer Using Queue
If you want to be able to handle more than one value in the pipeline at a time, you’ll need a data structure for the pipeline that allows the number to grow and shrink as data backs up from the producer.

Python’s standard library has a queue module which, in turn, has a Queue class. Let’s change the Pipeline to use a Queue instead of just a variable protected by a Lock. You’ll also use a different way to stop the worker threads by using a different primitive from Python threading, an Event.

Let’s start with the Event. The threading.Event object allows one thread to signal an event while many other threads can be waiting for that event to happen. The key usage in this code is that the threads that are waiting for the event do not necessarily need to stop what they are doing, they can just check the status of the Event every once in a while.

The triggering of the event can be many things. In this example, the main thread will simply sleep for a while and then .set() it:

if __name__ == "__main__":
    format = "%(asctime)s: %(message)s"
    logging.basicConfig(format=format, level=logging.INFO,
                        datefmt="%H:%M:%S")
    # logging.getLogger().setLevel(logging.DEBUG)

    pipeline = Pipeline()
    event = threading.Event()
    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        executor.submit(producer, pipeline, event)
        executor.submit(consumer, pipeline, event)

        time.sleep(0.1)
        logging.info("Main: about to set event")
        event.set()
The only changes here are the creation of the event object on line 6, passing the event as a parameter on lines 8 and 9, and the final section on lines 11 to 13, which sleep for a second, log a message, and then call .set() on the event.

The producer also did not have to change too much:

def producer(pipeline, event):
    """Pretend we're getting a number from the network."""
    while not event.is_set():
        message = random.randint(1, 101)
        logging.info("Producer got message: %s", message)
        pipeline.set_message(message, "Producer")

    logging.info("Producer received EXIT event. Exiting")
It now will loop until it sees that the event was set on line 3. It also no longer puts the SENTINEL value into the pipeline.

consumer had to change a little more:

def consumer(pipeline, event):
    """Pretend we're saving a number in the database."""
    while not event.is_set() or not pipeline.empty():
        message = pipeline.get_message("Consumer")
        logging.info(
            "Consumer storing message: %s  (queue size=%s)",
            message,
            pipeline.qsize(),
        )

    logging.info("Consumer received EXIT event. Exiting")
While you got to take out the code related to the SENTINEL value, you did have to do a slightly more complicated while condition. Not only does it loop until the event is set, but it also needs to keep looping until the pipeline has been emptied.

Making sure the queue is empty before the consumer finishes prevents another fun issue. If the consumer does exit while the pipeline has messages in it, there are two bad things that can happen. The first is that you lose those final messages, but the more serious one is that the producer can get caught attempting to add a message to a full queue and never return.

This happens if the event gets triggered after the producer has checked the .is_set() condition but before it calls pipeline.set_message().

If that happens, it’s possible for the producer to wake up and exit with the queue still completely full. The producer will then call .set_message() which will wait until there is space on the queue for the new message. The consumer has already exited, so this will not happen and the producer will not exit.

The rest of the consumer should look familiar.

The Pipeline has changed dramatically, however:

class Pipeline(queue.Queue):
    def __init__(self):
        super().__init__(maxsize=10)

    def get_message(self, name):
        logging.debug("%s:about to get from queue", name)
        value = self.get()
        logging.debug("%s:got %d from queue", name, value)
        return value

    def set_message(self, value, name):
        logging.debug("%s:about to add %d to queue", name, value)
        self.put(value)
        logging.debug("%s:added %d to queue", name, value)
You can see that Pipeline is a subclass of queue.Queue. Queue has an optional parameter when initializing to specify a maximum size of the queue.

If you give a positive number for maxsize, it will limit the queue to that number of elements, causing .put() to block until there are fewer than maxsize elements. If you don’t specify maxsize, then the queue will grow to the limits of your computer’s memory.

.get_message() and .set_message() got much smaller. They basically wrap .get() and .put() on the Queue. You might be wondering where all of the locking code that prevents the threads from causing race conditions went.

The core devs who wrote the standard library knew that a Queue is frequently used in multi-threading environments and incorporated all of that locking code inside the Queue itself. Queue is thread-safe.

Running this program looks like the following:

$ ./prodcom_queue.py
Producer got message: 32
Producer got message: 51
Producer got message: 25
Producer got message: 94
Producer got message: 29
Consumer storing message: 32 (queue size=3)
Producer got message: 96
Consumer storing message: 51 (queue size=3)
Producer got message: 6
Consumer storing message: 25 (queue size=3)
Producer got message: 31

[many lines deleted]

Producer got message: 80
Consumer storing message: 94 (queue size=6)
Producer got message: 33
Consumer storing message: 20 (queue size=6)
Producer got message: 48
Consumer storing message: 31 (queue size=6)
Producer got message: 52
Consumer storing message: 98 (queue size=6)
Main: about to set event
Producer got message: 13
Consumer storing message: 59 (queue size=6)
Producer received EXIT event. Exiting
Consumer storing message: 75 (queue size=6)
Consumer storing message: 97 (queue size=5)
Consumer storing message: 80 (queue size=4)
Consumer storing message: 33 (queue size=3)
Consumer storing message: 48 (queue size=2)
Consumer storing message: 52 (queue size=1)
Consumer storing message: 13 (queue size=0)
Consumer received EXIT event. Exiting
If you read through the output in my example, you can see some interesting things happening. Right at the top, you can see the producer got to create five messages and place four of them on the queue. It got swapped out by the operating system before it could place the fifth one.

The consumer then ran and pulled off the first message. It printed out that message as well as how deep the queue was at that point:

Consumer storing message: 32 (queue size=3)
This is how you know that the fifth message hasn’t made it into the pipeline yet. The queue is down to size three after a single message was removed. You also know that the queue can hold ten messages, so the producer thread didn’t get blocked by the queue. It was swapped out by the OS.

Note: Your output will be different. Your output will change from run to run. That’s the fun part of working with threads!

As the program starts to wrap up, can you see the main thread generating the event which causes the producer to exit immediately. The consumer still has a bunch of work do to, so it keeps running until it has cleaned out the pipeline.

Try playing with different queue sizes and calls to time.sleep() in the producer or the consumer to simulate longer network or disk access times respectively. Even slight changes to these elements of the program will make large differences in your results.

This is a much better solution to the producer-consumer problem, but you can simplify it even more. The Pipeline really isn’t needed for this problem. Once you take away the logging, it just becomes a queue.Queue.

Here’s what the final code looks like using queue.Queue directly:

import concurrent.futures
import logging
import queue
import random
import threading
import time

def producer(queue, event):
    """Pretend we're getting a number from the network."""
    while not event.is_set():
        message = random.randint(1, 101)
        logging.info("Producer got message: %s", message)
        queue.put(message)

    logging.info("Producer received event. Exiting")

def consumer(queue, event):
    """Pretend we're saving a number in the database."""
    while not event.is_set() or not queue.empty():
        message = queue.get()
        logging.info(
            "Consumer storing message: %s (size=%d)", message, queue.qsize()
        )

    logging.info("Consumer received event. Exiting")

if __name__ == "__main__":
    format = "%(asctime)s: %(message)s"
    logging.basicConfig(format=format, level=logging.INFO,
                        datefmt="%H:%M:%S")

    pipeline = queue.Queue(maxsize=10)
    event = threading.Event()
    with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
        executor.submit(producer, pipeline, event)
        executor.submit(consumer, pipeline, event)

        time.sleep(0.1)
        logging.info("Main: about to set event")
        event.set()
That’s easier to read and shows how using Python’s built-in primitives can simplify a complex problem.

Lock and Queue are handy classes to solve concurrency issues, but there are others provided by the standard library. Before you wrap up this tutorial, let’s do a quick survey of some of them.


 Remove ads
Threading Objects
There are a few more primitives offered by the Python threading module. While you didn’t need these for the examples above, they can come in handy in different use cases, so it’s good to be familiar with them.

Semaphore
The first Python threading object to look at is threading.Semaphore. A Semaphore is a counter with a few special properties. The first one is that the counting is atomic. This means that there is a guarantee that the operating system will not swap out the thread in the middle of incrementing or decrementing the counter.

The internal counter is incremented when you call .release() and decremented when you call .acquire().

The next special property is that if a thread calls .acquire() when the counter is zero, that thread will block until a different thread calls .release() and increments the counter to one.

Semaphores are frequently used to protect a resource that has a limited capacity. An example would be if you have a pool of connections and want to limit the size of that pool to a specific number.

Timer
A threading.Timer is a way to schedule a function to be called after a certain amount of time has passed. You create a Timer by passing in a number of seconds to wait and a function to call:

t = threading.Timer(30.0, my_function)
You start the Timer by calling .start(). The function will be called on a new thread at some point after the specified time, but be aware that there is no promise that it will be called exactly at the time you want.

If you want to stop a Timer that you’ve already started, you can cancel it by calling .cancel(). Calling .cancel() after the Timer has triggered does nothing and does not produce an exception.

A Timer can be used to prompt a user for action after a specific amount of time. If the user does the action before the Timer expires, .cancel() can be called.

Barrier
A threading.Barrier can be used to keep a fixed number of threads in sync. When creating a Barrier, the caller must specify how many threads will be synchronizing on it. Each thread calls .wait() on the Barrier. They all will remain blocked until the specified number of threads are waiting, and then the are all released at the same time.

Remember that threads are scheduled by the operating system so, even though all of the threads are released simultaneously, they will be scheduled to run one at a time.

One use for a Barrier is to allow a pool of threads to initialize themselves. Having the threads wait on a Barrier after they are initialized will ensure that none of the threads start running before all of the threads are finished with their initialization.

Conclusion: Threading in Python
You’ve now seen much of what Python threading has to offer and some examples of how to build threaded programs and the problems they solve. You’ve also seen a few instances of the problems that arise when writing and debugging threaded programs.

If you’d like to explore other options for concurrency in Python, check out Speed Up Your Python Program With Concurrency.

If you’re interested in doing a deep dive on the asyncio module, go read Async IO in Python: A Complete Walkthrough.

            50.1.1.5
        50.1.2
    50.2

51. requests

    51.1 https://realpython.com/python-requests/

        51.1.1 install
Let’s begin by installing the requests library. To do so, run the following command:

$ pip install requests
If you prefer to use Pipenv for managing Python packages, you can run the following:

$ pipenv install requests
Once requests is installed, you can use it in your application. Importing requests looks like this:

import requests
Now that you’re all set up, it’s time to begin your journey through requests. Your first goal will be learning how to make a GET request.

        51.1.2 basic
 http get, requests.get('https://api.github.com')

raise exception for invalid status code
import requests
from requests.exceptions import HTTPError

for url in ['https://api.github.com', 'https://api.github.com/invalid']:
    try:
        response = requests.get(url)

        # If the response was successful, no Exception will be raised
        response.raise_for_status()
    except HTTPError as http_err:
        print(f'HTTP error occurred: {http_err}')  # Python 3.6
    except Exception as err:
        print(f'Other error occurred: {err}')  # Python 3.6
    else:
        print('Success!')

        51.1.3 response object
raw response content: response.content
content converted to utf-8: response.text

can give python a hint re. encoding
response.encoding = 'utf-8' # Optional: requests infers this internally

if the response contains json it can be parsed using json.loads or more simply:
    response.json()

headers:
 response.headers
 .headers returns a dictionary-like object, allowing you to access header values by key. For example, to see the content type of the response payload, you can access Content-Type:

>>> response.headers['Content-Type']
'application/json; charset=utf-8'
There is something special about this dictionary-like headers object, though. The HTTP spec defines headers to be case-insensitive, which means we are able to access these headers without worrying about their capitalization:

>>> response.headers['content-type']
'application/json; charset=utf-8'

        51.1.4 Query params
One common way to customize a GET request is to pass values through query string parameters in the URL. To do this using get(), you pass data to params. For example, you can use GitHub’s Search API to look for the requests library:

import requests

# Search GitHub's repositories for requests
response = requests.get(
    'https://api.github.com/search/repositories',
    params={'q': 'requests+language:python'},
)

# Inspect some attributes of the `requests` repository
json_response = response.json()
repository = json_response['items'][0]
print(f'Repository name: {repository["name"]}')  # Python 3.6+
print(f'Repository description: {repository["description"]}')  # Python 3.6+

By passing the dictionary {'q': 'requests+language:python'} to the params parameter of .get(), you are able to modify the results that come back from the Search API.

You can pass params to get() in the form of a dictionary, as you have just done, or as a list of tuples:

>>> requests.get(
...     'https://api.github.com/search/repositories',
...     params=[('q', 'requests+language:python')],
... )
<Response [200]>

ou can even pass the values as bytes:

>>> requests.get(
...     'https://api.github.com/search/repositories',
...     params=b'q=requests+language:python',
... )
<Response [200]>
Query strings are useful for parameterizing GET requests. You can also customize your requests by adding or modifying the headers you send

        51.1.5 Request Headers
To customize headers, you pass a dictionary of HTTP headers to get() using the headers parameter. For example, you can change your previous search request to highlight matching search terms in the results by specifying the text-match media type in the Accept header:

import requests

response = requests.get(
    'https://api.github.com/search/repositories',
    params={'q': 'requests+language:python'},
    headers={'Accept': 'application/vnd.github.v3.text-match+json'},
)

# View the new `text-matches` array which provides information
# about your search term within the results
json_response = response.json()
repository = json_response['items'][0]
print(f'Text matches: {repository["text_matches"]}')
The Accept header tells the server what content types your application can handle. In this case, since you’re expecting the matching search terms to be highlighted, you’re using the header value application/vnd.github.v3.text-match+json, which is a proprietary GitHub Accept header where the content is a special JSON format.

Before you learn more ways to customize requests, let’s broaden the horizon by exploring other HTTP methods.

        51.1.6 Other HTTP Methods
Aside from GET, other popular HTTP methods include POST, PUT, DELETE, HEAD, PATCH, and OPTIONS. requests provides a method, with a similar signature to get(), for each of these HTTP methods:

>>> requests.post('https://httpbin.org/post', data={'key':'value'})
>>> requests.put('https://httpbin.org/put', data={'key':'value'})
>>> requests.delete('https://httpbin.org/delete')
>>> requests.head('https://httpbin.org/get')
>>> requests.patch('https://httpbin.org/patch', data={'key':'value'})
>>> requests.options('https://httpbin.org/get')

        51.1.7 The Message Body
According to the HTTP specification, POST, PUT, and the less common PATCH requests pass their data through the message body rather than through parameters in the query string. Using requests, you’ll pass the payload to the corresponding function’s data parameter.

data takes a dictionary, a list of tuples, bytes, or a file-like object. You’ll want to adapt the data you send in the body of your request to the specific needs of the service you’re interacting with.

For example, if your request’s content type is application/x-www-form-urlencoded, you can send the form data as a dictionary:

>>> requests.post('https://httpbin.org/post', data={'key':'value'})
<Response [200]>
You can also send that same data as a list of tuples:

>>> requests.post('https://httpbin.org/post', data=[('key', 'value')])
<Response [200]>

If, however, you need to send JSON data, you can use the json parameter. When you pass JSON data via json, requests will serialize your data and add the correct Content-Type header for you.

httpbin.org is a great resource created by the author of requests, Kenneth Reitz. It’s a service that accepts test requests and responds with data about the requests. For instance, you can use it to inspect a basic POST request:

>>> response = requests.post('https://httpbin.org/post', json={'key':'value'})
>>> json_response = response.json()
>>> json_response['data']
'{"key": "value"}'
>>> json_response['headers']['Content-Type']
'application/json'
You can see from the response that the server received your request data and headers as you sent them. requests also provides this information to you in the form of a PreparedRequest.



        51.1.8 Inspecting Your Request
When you make a request, the requests library prepares the request before actually sending it to the destination server. Request preparation includes things like validating headers and serializing JSON content.

You can view the PreparedRequest by accessing .request:

>>> response = requests.post('https://httpbin.org/post', json={'key':'value'})
>>> response.request.headers['Content-Type']
'application/json'
>>> response.request.url
'https://httpbin.org/post'
>>> response.request.body
b'{"key": "value"}'
Inspecting the PreparedRequest gives you access to all kinds of information about the request being made such as payload, URL, headers, authentication, and more.

So far, you’ve made a lot of different kinds of requests, but they’ve all had one thing in common: they’re unauthenticated requests to public APIs. Many services you may come across will want you to authenticate in some way.


        51.1.9 Authentication
Authentication helps a service understand who you are. Typically, you provide your credentials to a server by passing data through the Authorization header or a custom header defined by the service. All the request functions you’ve seen to this point provide a parameter called auth, which allows you to pass your credentials.

One example of an API that requires authentication is GitHub’s Authenticated User API. This endpoint provides information about the authenticated user’s profile. To make a request to the Authenticated User API, you can pass your GitHub username and password in a tuple to get():

>>> from getpass import getpass
>>> requests.get('https://api.github.com/user', auth=('username', getpass()))
<Response [200]>
The request succeeded if the credentials you passed in the tuple to auth are valid. If you try to make this request with no credentials, you’ll see that the status code is 401 Unauthorized:

>>> requests.get('https://api.github.com/user')
<Response [401]>
When you pass your username and password in a tuple to the auth parameter, requests is applying the credentials using HTTP’s Basic access authentication scheme under the hood.

Therefore, you could make the same request by passing explicit Basic authentication credentials using HTTPBasicAuth:

>>> from requests.auth import HTTPBasicAuth
>>> from getpass import getpass
>>> requests.get(
...     'https://api.github.com/user',
...     auth=HTTPBasicAuth('username', getpass())
... )
<Response [200]>
Though you don’t need to be explicit for Basic authentication, you may want to authenticate using another method. requests provides other methods of authentication out of the box such as HTTPDigestAuth and HTTPProxyAuth.

You can even supply your own authentication mechanism. To do so, you must first create a subclass of AuthBase. Then, you implement __call__():

import requests
from requests.auth import AuthBase

class TokenAuth(AuthBase):
    """Implements a custom authentication scheme."""

    def __init__(self, token):
        self.token = token

    def __call__(self, r):
        """Attach an API token to a custom auth header."""
        r.headers['X-TokenAuth'] = f'{self.token}'  # Python 3.6+
        return r


requests.get('https://httpbin.org/get', auth=TokenAuth('12345abcde-token'))
Here, your custom TokenAuth mechanism receives a token, then includes that token in the X-TokenAuth header of your request.

Bad authentication mechanisms can lead to security vulnerabilities, so unless a service requires a custom authentication mechanism for some reason, you’ll always want to use a tried-and-true auth scheme like Basic or OAuth.

While you’re thinking about security, let’s consider dealing with SSL Certificates using requests.


        51.1.10 SSL Certificate Verification
Any time the data you are trying to send or receive is sensitive, security is important. The way that you communicate with secure sites over HTTP is by establishing an encrypted connection using SSL, which means that verifying the target server’s SSL Certificate is critical.

The good news is that requests does this for you by default. However, there are some cases where you might want to change this behavior.

If you want to disable SSL Certificate verification, you pass False to the verify parameter of the request function:

>>> requests.get('https://api.github.com', verify=False)
InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings
  InsecureRequestWarning)
<Response [200]>
requests even warns you when you’re making an insecure request to help you keep your data safe!

Note: requests uses a package called certifi to provide Certificate Authorities. This lets requests know which authorities it can trust. Therefore, you should update certifi frequently to keep your connections as secure as possible.

        51.1.11 Performance
When using requests, especially in a production application environment, it’s important to consider performance implications. Features like timeout control, sessions, and retry limits can help you keep your application running smoothly.

Timeouts
When you make an inline request to an external service, your system will need to wait upon the response before moving on. If your application waits too long for that response, requests to your service could back up, your user experience could suffer, or your background jobs could hang.

By default, requests will wait indefinitely on the response, so you should almost always specify a timeout duration to prevent these things from happening. To set the request’s timeout, use the timeout parameter. timeout can be an integer or float representing the number of seconds to wait on a response before timing out:

>>> requests.get('https://api.github.com', timeout=1)
<Response [200]>
>>> requests.get('https://api.github.com', timeout=3.05)
<Response [200]>
In the first request, the request will timeout after 1 second. In the second request, the request will timeout after 3.05 seconds.

You can also pass a tuple to timeout with the first element being a connect timeout (the time it allows for the client to establish a connection to the server), and the second being a read timeout (the time it will wait on a response once your client has established a connection):

>>> requests.get('https://api.github.com', timeout=(2, 5))
<Response [200]>
If the request establishes a connection within 2 seconds and receives data within 5 seconds of the connection being established, then the response will be returned as it was before. If the request times out, then the function will raise a Timeout exception:

import requests
from requests.exceptions import Timeout

try:
    response = requests.get('https://api.github.com', timeout=1)
except Timeout:
    print('The request timed out')
else:
    print('The request did not time out')
Your program can catch the Timeout exception and respond accordingly.

        51.1.12 The Session Object
Until now, you’ve been dealing with high level requests APIs such as get() and post(). These functions are abstractions of what’s going on when you make your requests. They hide implementation details such as how connections are managed so that you don’t have to worry about them.

Underneath those abstractions is a class called Session. If you need to fine-tune your control over how requests are being made or improve the performance of your requests, you may need to use a Session instance directly.

Sessions are used to persist parameters across requests. For example, if you want to use the same authentication across multiple requests, you could use a session:

import requests
from getpass import getpass

# By using a context manager, you can ensure the resources used by
# the session will be released after use
with requests.Session() as session:
    session.auth = ('username', getpass())

    # Instead of requests.get(), you'll use session.get()
    response = session.get('https://api.github.com/user')

# You can inspect the response just like you did before
print(response.headers)
print(response.json())
Each time you make a request with session, once it has been initialized with authentication credentials, the credentials will be persisted.

The primary performance optimization of sessions comes in the form of persistent connections. When your app makes a connection to a server using a Session, it keeps that connection around in a connection pool. When your app wants to connect to the same server again, it will reuse a connection from the pool rather than establishing a new one.

        51.1.13 Max Retries
When a request fails, you may want your application to retry the same request. However, requests will not do this for you by default. To apply this functionality, you need to implement a custom Transport Adapter.

Transport Adapters let you define a set of configurations per service you’re interacting with. For example, let’s say you want all requests to https://api.github.com to retry three times before finally raising a ConnectionError. You would build a Transport Adapter, set its max_retries parameter, and mount it to an existing Session:

import requests
from requests.adapters import HTTPAdapter
from requests.exceptions import ConnectionError

github_adapter = HTTPAdapter(max_retries=3)

session = requests.Session()

# Use `github_adapter` for all requests to endpoints that start with this URL
session.mount('https://api.github.com', github_adapter)

try:
    session.get('https://api.github.com')
except ConnectionError as ce:
    print(ce)
When you mount the HTTPAdapter, github_adapter, to session, session will adhere to its configuration for each request to https://api.github.com.

Timeouts, Transport Adapters, and sessions are for keeping your code efficient and your application resilient.

        51.1.14
    51.2
52. regexp

    52.1  https://www.geeksforgeeks.org/regular-expression-python-examples-set-1/

    52.2 Regular Expression Basics
.	Any character except newline
a	The character a
ab	The string ab
a|b	a or b
a*	0 or more a's
\	Escapes a special character
Regular Expression Quantifiers
*	0 or more
+	1 or more
?	0 or 1
{2}	Exactly 2
{2, 5}	Between 2 and 5
{2,}	2 or more
(,5}	Up to 5
Default is greedy. Append ? for reluctant.
Regular Expression Groups
(...)	Capturing group
(?P<Y>...)	Capturing group named Y
(?:...)	Non-capturing group
\Y	Match the Y'th captured group
(?P=Y)	Match the named group Y
(?#...)	Comment
Regular Expression Character Classes
[ab-d]	One character of: a, b, c, d
[^ab-d]	One character except: a, b, c, d
[\b]	Backspace character
\d	One digit
\D	One non-digit
\s	One whitespace
\S	One non-whitespace
\w	One word character
\W	One non-word character
Regular Expression Assertions
^	Start of string
\A	Start of string, ignores m flag
$	End of string
\Z	End of string, ignores m flag
\b	Word boundary
\B	Non-word boundary
(?=...)	Positive lookahead
(?!...)	Negative lookahead
(?<=...)	Positive lookbehind
(?<!...)	Negative lookbehind
(?()|)	Conditional
Regular Expression Flags
i	Ignore case
m	^ and $ match start and end of line
s	. matches newline as well
x	Allow spaces and comments
L	Locale character classes
u	Unicode character classes
(?iLmsux)	Set flags within regex
Regular Expression Special Characters
\n	Newline
\r	Carriage return
\t	Tab
\YYY	Octal character YYY
\xYY	Hexadecimal character YY


    52.3 https://www.tutorialspoint.com/python/python_reg_expressions.htm
A regular expression is a special sequence of characters that helps you match or find other strings or sets of strings, using a specialized syntax held in a pattern. Regular expressions are widely used in UNIX world.

The Python module re provides full support for Perl-like regular expressions in Python. The re module raises the exception re.error if an error occurs while compiling or using a regular expression.

We would cover two important functions, which would be used to handle regular expressions. But a small thing first: There are various characters, which would have special meaning when they are used in regular expression. To avoid any confusion while dealing with regular expressions, we would use Raw Strings as r'expression'.

The match Function
This function attempts to match RE pattern to string with optional flags.

Here is the syntax for this function −

re.match(pattern, string, flags=0)
Here is the description of the parameters −

Sr.No.	Parameter & Description
| 1	pattern This is the regular expression to be matched.
| 
| 2	string This is the string, which would be searched to match the pattern at the beginning of string.
| 
| 3	flags You can specify different flags using bitwise OR (|). These are modifiers, which are listed in the table below.

The re.match function returns a match object on success, None on failure. We usegroup(num) or groups() function of match object to get matched expression.

Sr.No.	Match Object Method & Description
| 1	group(num=0) This method returns entire match (or specific subgroup num)
| 
| 2	groups() This method returns all matching subgroups in a tuple (empty if there weren't any)

Example

Live Demo
#!/usr/bin/python
import re

line = "Cats are smarter than dogs"

matchObj = re.match( r'(.*) are (.*?) .*', line, re.M|re.I)

if matchObj:
   print "matchObj.group() : ", matchObj.group()
   print "matchObj.group(1) : ", matchObj.group(1)
   print "matchObj.group(2) : ", matchObj.group(2)
else:
   print "No match!!"
When the above code is executed, it produces following result −

matchObj.group() :  Cats are smarter than dogs
matchObj.group(1) :  Cats
matchObj.group(2) :  smarter
The search Function
This function searches for first occurrence of RE pattern within string with optional flags.

Here is the syntax for this function −

re.search(pattern, string, flags=0)
Here is the description of the parameters −

Sr.No.	Parameter & Description
| 1	pattern This is the regular expression to be matched.
| 
| 2	string This is the string, which would be searched to match the pattern anywhere in the string.
| 
| 3	flags You can specify different flags using bitwise OR (|). These are modifiers, which are listed in the table below.
| 
| The re.search function returns a match object on success, none on failure. We use group(num) or groups() function of match object to get matched expression.
| 
| Sr.No.	Match Object Methods & Description
| 1	group(num=0) This method returns entire match (or specific subgroup num)
| 
| 2	groups() This method returns all matching subgroups in a tuple (empty if there weren't any)

Example

Live Demo
#!/usr/bin/python
import re

line = "Cats are smarter than dogs";

searchObj = re.search( r'(.*) are (.*?) .*', line, re.M|re.I)

if searchObj:
   print "searchObj.group() : ", searchObj.group()
   print "searchObj.group(1) : ", searchObj.group(1)
   print "searchObj.group(2) : ", searchObj.group(2)
else:
   print "Nothing found!!"
When the above code is executed, it produces following result −

searchObj.group() :  Cats are smarter than dogs
searchObj.group(1) :  Cats
searchObj.group(2) :  smarter
Matching Versus Searching
Python offers two different primitive operations based on regular expressions: match checks for a match only at the beginning of the string, while search checks for a match anywhere in the string (this is what Perl does by default).

Example

Live Demo
#!/usr/bin/python
import re

line = "Cats are smarter than dogs";

matchObj = re.match( r'dogs', line, re.M|re.I)
if matchObj:
   print "match --> matchObj.group() : ", matchObj.group()
else:
   print "No match!!"

searchObj = re.search( r'dogs', line, re.M|re.I)
if searchObj:
   print "search --> searchObj.group() : ", searchObj.group()
else:
   print "Nothing found!!"
When the above code is executed, it produces the following result −

No match!!
search --> searchObj.group() :  dogs
Search and Replace
One of the most important re methods that use regular expressions is sub.

Syntax

re.sub(pattern, repl, string, max=0)
This method replaces all occurrences of the RE pattern in string with repl, substituting all occurrences unless max provided. This method returns modified string.

Example

Live Demo
#!/usr/bin/python
import re

phone = "2004-959-559 # This is Phone Number"

# Delete Python-style comments
num = re.sub(r'#.*$', "", phone)
print "Phone Num : ", num

# Remove anything other than digits
num = re.sub(r'\D', "", phone)    
print "Phone Num : ", num
When the above code is executed, it produces the following result −

Phone Num :  2004-959-559
Phone Num :  2004959559
Regular Expression Modifiers: Option Flags
Regular expression literals may include an optional modifier to control various aspects of matching. The modifiers are specified as an optional flag. You can provide multiple modifiers using exclusive OR (|), as shown previously and may be represented by one of these −

Sr.No.	Modifier & Description
| 1	re.I Performs case-insensitive matching.
| 
| 2	re.L Interprets words according to the current locale. This interpretation affects the alphabetic group (\w and \W), as well as word boundary behavior(\b and \B).
| 
| 3	re.M Makes $ match the end of a line (not just the end of the string) and makes ^ match the start of any line (not just the start of the string).
| 
| 4	re.S Makes a period (dot) match any character, including a newline.
| 
| 5	re.U Interprets letters according to the Unicode character set. This flag affects the behavior of \w, \W, \b, \B.
| 
| 6	re.X Permits "cuter" regular expression syntax. It ignores whitespace (except inside a set [] or when escaped by a backslash) and treats unescaped # as a comment marker.
| 
| Regular Expression Patterns
| Except for control characters, (+ ? . * ^ $ ( ) [ ] { } | \), all characters match themselves. You can escape a control character by preceding it with a backslash.
| 
| Following table lists the regular expression syntax that is available in Python −
| 
| Sr.No.	Pattern & Description
| 1	^ Matches beginning of line.
| 
| 2	$ Matches end of line.
| 
| 3	. Matches any single character except newline. Using m option allows it to match newline as well.
| 
| 4	[...] Matches any single character in brackets.
| 
| 5	[^...] Matches any single character not in brackets
| 
| 6	re* Matches 0 or more occurrences of preceding expression.
| 
| 7	re+ Matches 1 or more occurrence of preceding expression.
| 
| 8	re? Matches 0 or 1 occurrence of preceding expression.
| 
| 9	re{ n} Matches exactly n number of occurrences of preceding expression.
| 
| 10	re{ n,} Matches n or more occurrences of preceding expression.
| 
| 11	re{ n, m} Matches at least n and at most m occurrences of preceding expression.
| 
| 12	a| b Matches either a or b.
| 
| 13	(re) Groups regular expressions and remembers matched text.
| 
| 14	(?imx) Temporarily toggles on i, m, or x options within a regular expression. If in parentheses, only that area is affected.
| 
| 15	(?-imx) Temporarily toggles off i, m, or x options within a regular expression. If in parentheses, only that area is affected.
| 
| 16	(?: re) Groups regular expressions without remembering matched text.
| 
| 17	(?imx: re) Temporarily toggles on i, m, or x options within parentheses.
| 
| 18	(?-imx: re) Temporarily toggles off i, m, or x options within parentheses.
| 
| 19	(?#...) Comment.
| 
| 20	(?= re) Specifies position using a pattern. Doesn't have a range.
| 
| 21	(?! re) Specifies position using pattern negation. Doesn't have a range.
| 
| 22	(?> re) Matches independent pattern without backtracking.
| 
| 23	\w Matches word characters.
| 
| 24	\W Matches nonword characters.
| 
| 25	\s Matches whitespace. Equivalent to [\t\n\r\f].
| 
| 26	\S Matches nonwhitespace.
| 
| 27	\d Matches digits. Equivalent to [0-9].
| 
| 28	\D Matches nondigits.
| 
| 29	\A Matches beginning of string.
| 
| 30	\Z Matches end of string. If a newline exists, it matches just before newline.
| 
| 31	\z Matches end of string.
| 
| 32	\G Matches point where last match finished.
| 
| 33	\b Matches word boundaries when outside brackets. Matches backspace (0x08) when inside brackets.
| 
| 34	\B Matches nonword boundaries.
| 
| 35	\n, \t, etc. Matches newlines, carriage returns, tabs, etc.
| 
| 36	\1...\9 Matches nth grouped subexpression.
| 
| 37	\10 Matches nth grouped subexpression if it matched already. Otherwise refers to the octal representation of a character code.
| 
| Regular Expression Examples
| Literal characters
| 
| Sr.No.	Example & Description
| 1	python Match "python".
| 
| Character classes
| Sr.No.	Example & Description
| 1	[Pp]ython Match "Python" or "python"
| 
| 2	rub[ye] Match "ruby" or "rube"
| 
| 3	[aeiou] Match any one lowercase vowel
| 
| 4	[0-9] Match any digit; same as [0123456789]
| 
| 5	[a-z] Match any lowercase ASCII letter
| 
| 6	[A-Z] Match any uppercase ASCII letter
| 
| 7	[a-zA-Z0-9] Match any of the above
| 
| 8	[^aeiou] Match anything other than a lowercase vowel
| 
| 9	[^0-9] Match anything other than a digit
| 
| Special Character Classes
| Sr.No.	Example & Description
| 1	. Match any character except newline
| 
| 2	\d Match a digit: [0-9]
| 
| 3	\D Match a nondigit: [^0-9]
| 
| 4	\s Match a whitespace character: [ \t\r\n\f]
| 
| 5	\S Match nonwhitespace: [^ \t\r\n\f]
| 
| 6	\w Match a single word character: [A-Za-z0-9_]
| 
| 7	\W Match a nonword character: [^A-Za-z0-9_]
| 
| Repetition Cases
| Sr.No.	Example & Description
| 1	ruby? Match "rub" or "ruby": the y is optional
| 
| 2	ruby* Match "rub" plus 0 or more ys
| 
| 3	ruby+ Match "rub" plus 1 or more ys
| 
| 4	\d{3} Match exactly 3 digits
| 
| 5	\d{3,} Match 3 or more digits
| 
| 6	\d{3,5} Match 3, 4, or 5 digits
| 
| Nongreedy repetition
| This matches the smallest number of repetitions −
| 
| Sr.No.	Example & Description 1	<.*>
| Greedy repetition: matches "<python>perl>"
| 2	<.*?>
| Nongreedy: matches "<python>" in "<python>perl>"
| 
| Grouping with Parentheses
| Sr.No.	Example & Description
| 1	\D\d+ No group: + repeats \d
| 
| 2	(\D\d)+ Grouped: + repeats \D\d pair
| 
| 3	([Pp]ython(, )?)+ Match "Python", "Python, python, python", etc.
| 
| Backreferences
| This matches a previously matched group again −
| 
| Sr.No.	Example & Description
| 1	([Pp])ython&\1ails Match python&pails or Python&Pails
| 
| 2	(['"])[^\1]*\1 Single or double-quoted string. \1 matches whatever the 1st group matched. \2 matches whatever the 2nd group matched, etc.
| 
| Alternatives
| Sr.No.	Example & Description
| 1	python|perl Match "python" or "perl"
| 
| 2	rub(y|le)) Match "ruby" or "ruble"
| 
| 3	Python(!+|\?) "Python" followed by one or more ! or one ?
| 
| Anchors
| This needs to specify match position.
| 
| Sr.No.	Example & Description
| 1	^Python Match "Python" at the start of a string or internal line
| 
| 2	Python$ Match "Python" at the end of a string or line
| 
| 3	\APython Match "Python" at the start of a string
| 
| 4	Python\Z Match "Python" at the end of a string
| 
| 5	\bPython\b Match "Python" at a word boundary
| 
| 6	\brub\B \B is nonword boundary: match "rub" in "rube" and "ruby" but not alone
| 
| 7	Python(?=!) Match "Python", if followed by an exclamation point.
| 
| 8	Python(?!!) Match "Python", if not followed by an exclamation point.
| 
| Special Syntax with Parentheses
| Sr.No.	Example & Description
| 1	R(?#comment) Matches "R". All the rest is a comment
| 
| 2	R(?i)uby Case-insensitive while matching "uby"
| 
| 3	R(?i:uby) Same as above
| 
| 4	rub(?:y|le)) Group only without creating \1 backreference

    52.4
53. my scripts, tags: scripts, tools

    53.1 tocify.py 
[I500695@C02ZR8BSMD6N:2021-11-28 17:06:55:~/work/scripts/python/util:]2005$ cat tocify.py | pbcopy 
<url:~/work/scripts/python/util/tocify.py>
#!/usr/bin/python3


# Description, create TOC for my kb files.
# Their format is (example):
# start new section                      ****************
# section number                         1. section1
# subsections                                   1.1 subsection1_1
#                                                             1.2 subsection1_2
#                                                             1.2.1 subsubsection 1_2_1
# end section                                   ****************

# Author Yosi Izaq

#Note that since most of the KB files are in the following format:
# 1. subject
# 1.1 example1
# 1.2 example2.
# ...
# 
# It makes more sense to build a TOC only based on the subjects. So for now that's what the script does.

# Future ideas.
# 1. update flag, remove old TOC and create a new one.
# 2. support multiple nesting levels.  with correct indentation.

import optparse
import re
import os

def tocify(filename, embed_util_links=True):
    toc_pref = ".........................................Table Of Contents...............................................................";
    toc_pref_re = '^.*\.\.Table Of Contents\.\..*$'
    toc_suff  = ".................................................END TOC..............................................";
    toc_suff_re = '^.*\.\.END TOC\.\..*$'

    newlines = [toc_pref+os.linesep]
    old_toc_detected = False
    old_toc_start, old_toc_end = -1, -1
    newtoc = []
    with open(filename, "r+") as infile:
        filelines = infile.readlines()
        for i, line in enumerate(filelines):
            #print(f"scanning line {line}")
            #remove non utf8 characters
            #line = bytes(line, 'utf-8').decode('utf-8', 'ignore')
            if old_toc_end == -1 and re.match(toc_pref_re, line):
                #print('Found old ToC start at line', i)
                old_toc_detected = True
                old_toc_start = i
            elif re.match(toc_suff_re, line):
                #print('Found old ToC end at line', i)
                if old_toc_detected:
                    old_toc_detected = False
                    old_toc_end = i
            else:
                if not old_toc_detected:
                    #print(f"Found content line {i}")
                    matches = re.match('^(\s)*(\d+\.)+', line)
                    if matches:
                        #remove non utf8 characters
                        normazlized_line = bytes(line, 'utf-8').decode('utf-8', 'ignore')
                        normazlized_line = normazlized_line.rstrip()
                        #print(f"Found header line {i}:{normazlized_line}")
                        if embed_util_links:
                            newtoc.append(f"{normazlized_line} <URL:#tn={normazlized_line}>{os.linesep}")
                        else:
                            newtoc.append(normazlized_line+os.linesep)

        filelines = filelines[:old_toc_start]+filelines[old_toc_end+1:]
        newtoc.append(toc_suff+os.linesep)
        #insert new toc at 0
        newlines = newlines+newtoc  
        outputlines = newlines+filelines
        infile.seek(0)
        infile.writelines(outputlines)
        infile.truncate()

    #    #write back to file
    #with open(filename, 'w') outfile:
    #    outfile.writelines(outputlines)
    


def main():
    parser = optparse.OptionParser()
    parser.add_option('-f', '--filename', action="store", dest="filename", help="file for which to create ToC")
    options, args = parser.parse_args()
    filename = options.filename
    if not filename:
        print('Please provide a file name')
        exit(0)
    print('Creating ToC for file', filename)
    tocify(filename)

if __name__ == "__main__":
    main()


    53.2 pdf to text
    based on: https://apple.stackexchange.com/questions/2487/how-to-convert-a-pdf-file-into-a-text-file
   [I500695@C02ZR8BSMD6N:2022-01-20 15:49:25:~/work/code/python/tools:]2109$ cat  pdf2txt.py 
#!/usr/local/bin/python3
# coding: utf-8

import os, sys

#To install Quartz, $pip3 install pyobjc-framework-Quartz
from Quartz import PDFDocument
from CoreFoundation import (NSURL, NSString)
NSUTF8StringEncoding = 4

def pdf2txt(printToScreen=True):
    for filename in sys.argv[1:]:   
        inputfile =filename
        shortName = os.path.splitext(filename)[0]
        outputfile = shortName+".txt"
        print(inputfile, shortName)
        pdfURL = NSURL.fileURLWithPath_(inputfile)
        pdfDoc = PDFDocument.alloc().initWithURL_(pdfURL)
        if pdfDoc :
            pdfString = NSString.stringWithString_(pdfDoc.string())
            pdfString.writeToFile_atomically_encoding_error_(outputfile, True, NSUTF8StringEncoding, None)
        if printToScreen:
            print(outputfile)
            with open(outputfile, 'r') as output:
                lines = output.readlines()
                print(lines)

if __name__ == "__main__":
    pdf2txt(True) 
    53.3
54. itertools

    54.1 itertools basics

    54.2 itertools accumulate
accumulate()
This iterator takes two arguments, iterable target and the function which would be followed at each iteration of value in target. If no function is passed, addition takes place by default. If the input iterable is empty, the output iterable will also be empty.

Syntax
itertools.accumulate(iterable[, func]) –> accumulate object

In [1]: import itertools
In [2]: import operator
In [5]: asc = [i for i in range(6)]

In [6]: asc
Out[6]: [0, 1, 2, 3, 4, 5]
In [13]: sumasc = itertools.accumulate(asc)

In [14]: for s in sumasc:
    ...:     print(s)
    ...: 
0
1
3
6
10
15

In [15]: #as you can see addition is default

In [16]: #lets do mult using operator

In [17]: multasc = itertools.accumulate(asc, operator.mul)

In [18]: for s in multasc:
    ...:     print(s)
    ...: 
    ...: 
0
0
0
0
0
0

In [19]: asc = [i for i in range(1,7)]

In [20]: asc
Out[20]: [1, 2, 3, 4, 5, 6]

In [21]: multasc = itertools.accumulate(asc, operator.mul)

In [22]: for s in multasc:
    ...:     print(s)
    ...: 
    ...: 
1
2
6
24
120
720

In [23]: #now calc max and min

In [24]: 

In [24]: res = itertools.accumulate(asc, max)

In [25]: res
Out[25]: <itertools.accumulate at 0x10938b980>

In [26]: for s in res:
    ...:     print(s)
    ...: 
    ...: 
1
2
3
4
5
6

In [27]: res = itertools.accumulate(asc, min)

In [28]: for s in res:
    ...:     print(s)
    ...: 
    ...: 
1
1
1
1
1
1
    54.3
55. tags: python regex

    55.1  tags: python regex tutorial

        55.1.1  simple examples

            55.1.1.1  Extract number from string
import sys
import re

def main():
    #print(sys.argv)
    arg1 = sys.argv[1]
    if arg1:
        print('arg1 '+arg1)
        match = re.match("\D*(\d+)",arg1)
        if match: 
            takt=int(match.group(1))
            print(f"extracted takt {takt}")
        else:
            print('could not detect takt number. ex: Takt02')
    else:
        print('Please provide takt name')

if __name__ == '__main__':
    main()

[i500695@WYLQRXL9LQ:2022-02-08 11:32:41:~/work/code/python/practice:]2019$ py3  roster.py takt01
arg1 takt01
extracted takt 1
            55.1.1.2

        55.1.2

    55.2

56. Chatgpt

    56.1 Query:  python3     def put(self, elem: T): self.q.append(elem) generates error NameError: name 'T' is not defined
answer:
The error message NameError: name 'T' is not defined is indicating that the type hint T is not recognized by Python.

Type hints in Python are a way to provide additional information about the types of variables, arguments, and return values in your code. However, in order to use a type hint like T, you must first import it from the typing module.

To fix the error, you can add an import statement at the beginning of your code to import the T type hint:

from typing import TypeVar

T = TypeVar('T')

class Queue:
    def __init__(self):
        self.q = []

    def put(self, elem: T):
        self.q.append(elem)
In this example, we import the TypeVar class from the typing module and use it to define the T type hint. We can then use T as a generic type hint for the put method argument.

Note that the TypeVar class allows you to create custom type variables that can be used as generic type hints. By convention, the name of the type variable is a single uppercase letter.

    56.2 q. list different ways to iterate a list in reverse in python

a. In Python, there are several ways to iterate through a list in reverse. Here are a few of them:

Using reverse() method: The reverse() method can be used to reverse the original list, after which you can iterate through it from end to start.

my_list = [1, 2, 3, 4, 5]
my_list.reverse()
for item in my_list:
    print(item)
Using the reversed() built-in function: This function returns a reverse iterator that you can use to iterate through the list in reverse.

my_list = [1, 2, 3, 4, 5]
for item in reversed(my_list):
    print(item)
Using the range() function: You can use the range() function to create a range of indices in reverse order, which you can then use to access the elements of the list in reverse.

my_list = [1, 2, 3, 4, 5]
for i in range(len(my_list)-1, -1, -1):
    print(my_list[i])
Using a while loop: You can use a while loop to iterate through the list in reverse by starting with the index of the last element and decrementing it in each iteration until you reach the first element.

my_list = [1, 2, 3, 4, 5]
i = len(my_list) - 1
while i >= 0:
    print(my_list[i])
    i -= 1

q. which is most performant
a. The performance of each method depends on the size of the list and the specific use case. However, in general, the built-in reversed() function tends to be the most performant way to iterate through a list in reverse.

This is because the reversed() function returns a reverse iterator that doesn't create a new list, which is more memory-efficient than creating a new reversed list or using a while loop that accesses the elements by index. Additionally, the reversed() function can be used with any iterable object, not just lists.

Here's an example that compares the performance of the four methods:

import timeit

my_list = [i for i in range(100000)]

def reverse_using_reverse():
    my_list.reverse()
    for item in my_list:
        pass

def reverse_using_reversed():
    for item in reversed(my_list):
        pass

def reverse_using_range():
    for i in range(len(my_list)-1, -1, -1):
        item = my_list[i]

def reverse_using_while():
    i = len(my_list) - 1
    while i >= 0:
        item = my_list[i]
        i -= 1

print('reverse_using_reverse:', timeit.timeit(reverse_using_reverse, number=1000))
print('reverse_using_reversed:', timeit.timeit(reverse_using_reversed, number=1000))
print('reverse_using_range:', timeit.timeit(reverse_using_range, number=1000))
print('reverse_using_while:', timeit.timeit(reverse_using_while, number=1000))
This code creates a list of 100000 integers and then times how long it takes to iterate through the list using each of the four methods 1000 times. When I run this code on my machine, the reverse_using_reversed() method is the fastest, followed by reverse_using_range(), reverse_using_while(), and reverse_using_reverse() in last place.

However, this performance difference may not be noticeable for smaller lists or in cases where the iteration is not the bottleneck in the program. Therefore, it's always a good idea to test the performance of different methods in your specific use case.

on my laptop results are different
[i500695@WYLQRXL9LQ:2023-04-03 18:17:03:~/work/code/python/chatgpt:]2008$ py3 !$
py3 reverse_iterate_list_methods.py
reverse_using_reverse: 0.5418624160083709
reverse_using_reversed: 0.5200721669971244
reverse_using_range: 1.8538421249977546
reverse_using_while: 2.3299186249932973
[i500695@WYLQRXL9LQ:2023-04-03 18:17:11:~/work/code/python/chatgpt:]2009$ py3 reverse_iterate_list_methods.py
reverse_using_reverse: 0.5395271250017686
reverse_using_reversed: 0.5109755829907954
reverse_using_range: 1.831657417002134
reverse_using_while: 2.3286920829996234
    56.3
57. algorithms, data-structures tags: algorithms

    57.1  BFS, Dijkstra, astar, tags: BFS, Dijkstra, astar


        57.1.1  https://www.redblobgames.com/pathfinding/a-star/implementation.html

        57.1.2 astar Chatgpt
User
explain astar algorithm

ChatGPT
A* (pronounced "A-star") algorithm is a pathfinding algorithm that is widely used in video games and robotics to find the shortest path between two points. The algorithm is based on the Dijkstra's algorithm, which is a more basic algorithm for finding the shortest path between nodes in a graph. However, A* algorithm uses heuristics to guide the search towards the goal, which makes it more efficient and faster than Dijkstra's algorithm.

The A* algorithm works by exploring the graph from the start node to the goal node. At each step, the algorithm considers all the possible paths that can be taken from the current node to the neighboring nodes. It assigns a cost to each path, which is the sum of the cost to reach the current node and the estimated cost to reach the goal from the neighboring node. The estimated cost is calculated using a heuristic function, which is an estimate of the remaining distance to the goal.

The A* algorithm maintains two lists: an open list and a closed list. The open list contains the nodes that are yet to be explored, while the closed list contains the nodes that have already been explored. Initially, the open list contains only the start node. At each step, the algorithm selects the node from the open list that has the lowest cost, and moves it to the closed list. Then, it considers all the neighboring nodes of the current node and calculates the cost of the paths to those nodes. If a neighboring node is not in the open or closed list, it is added to the open list, and its cost is updated. If a neighboring node is already in the open list, its cost is updated only if the new cost is lower than the previous cost. If a neighboring node is already in the closed list, it is ignored.

The algorithm continues to explore the graph in this manner until it reaches the goal node or the open list becomes empty. If the goal node is reached, the algorithm reconstructs the path by following the nodes from the goal node to the start node, using the cost and parent information stored for each node. If the open list becomes empty before the goal node is reached, it means that there is no path from the start node to the goal node.

One of the advantages of A* algorithm is that it uses heuristics to guide the search towards the goal, which makes it more efficient and faster than Dijkstra's algorithm. However, the quality of the heuristic function can affect the performance of the algorithm. A good heuristic function should be admissible, which means that it should never overestimate the cost to reach the goal. If the heuristic function overestimates the cost, the algorithm may not find the shortest path.





User
example of astar heuristic
ChatGPT
The A* algorithm uses a heuristic function to estimate the remaining distance to the goal from a given node. The heuristic function is used to guide the search towards the goal, which makes the algorithm more efficient and faster than Dijkstra's algorithm.

The choice of heuristic function depends on the problem being solved. The heuristic function should be admissible, which means that it should never overestimate the actual cost to reach the goal. In other words, the estimated cost should be less than or equal to the actual cost.

Here's an example of a heuristic function for the problem of finding the shortest path between two points on a 2D plane:

arduino
Copy code
def euclidean_distance(node, goal):
    dx = abs(node.x - goal.x)
    dy = abs(node.y - goal.y)
    return math.sqrt(dx*dx + dy*dy)
In this example, node is the current node being considered, and goal is the goal node. The function calculates the Euclidean distance between the current node and the goal node using the formula sqrt(dx*dx + dy*dy), where dx is the absolute difference in x-coordinates between the two nodes, and dy is the absolute difference in y-coordinates between the two nodes. The function returns the distance as the estimated cost.

This heuristic function is admissible because the actual distance between two points in a 2D plane is always greater than or equal to the Euclidean distance. Therefore, the A* algorithm using this heuristic function will always find the shortest path between two points.





User
example astar implementation
ChatGPT
Sure, here is a simple implementation of the A* algorithm in Python:

python
Copy code
import heapq
import math

def astar(start, goal, graph):
    """
    Find the shortest path from the start node to the goal node in the given graph.
    The graph is represented as a dictionary where the keys are nodes, and the values are dictionaries of neighboring nodes and their edge costs.
    """
    # Initialize the open and closed lists
    open_list = [(0, start)]
    closed_list = set()
    
    # Initialize the cost and parent dictionaries
    cost = {start: 0}
    parent = {start: None}
    
    # Loop until the goal is found or the open list is empty
    while open_list:
        # Get the node with the lowest cost from the open list
        current_cost, current_node = heapq.heappop(open_list)
        
        # If the current node is the goal, reconstruct the path and return it
        if current_node == goal:
            path = [current_node]
            while parent[current_node]:
                path.append(parent[current_node])
                current_node = parent[current_node]
            return path[::-1]
        
        # Add the current node to the closed list
        closed_list.add(current_node)
        
        # Loop through the neighbors of the current node
        for neighbor, edge_cost in graph[current_node].items():
            # If the neighbor is already in the closed list, skip it
            if neighbor in closed_list:
                continue
            
            # Calculate the tentative cost to reach the neighbor through the current node
            tentative_cost = cost[current_node] + edge_cost
            
            # If the neighbor is not in the open list, add it and update its cost and parent
            if neighbor not in [n[1] for n in open_list]:
                heur = euclidean_distance(neighbor, goal)  # Calculate the heuristic cost
                heapq.heappush(open_list, (tentative_cost + heur, neighbor))
                cost[neighbor] = tentative_cost
                parent[neighbor] = current_node
            
            # If the neighbor is already in the open list, update its cost and parent if the new cost is lower
            else:
                for i, node in enumerate(open_list):
                    if node[1] == neighbor:
                        if tentative_cost < cost[neighbor]:
                            heur = euclidean_distance(neighbor, goal)  # Calculate the heuristic cost
                            open_list[i] = (tentative_cost + heur, neighbor)
                            cost[neighbor] = tentative_cost
                            parent[neighbor] = current_node
                        break
    
    # If the goal was not found, return None
    return None


def euclidean_distance(node1, node2):
    """
    Calculate the Euclidean distance between two nodes in a 2D plane.
    """
    dx = abs(node1[0] - node2[0])
    dy = abs(node1[1] - node2[1])
    return math.sqrt(dx*dx + dy*dy)
This implementation assumes that the graph is represented as a dictionary, where the keys are nodes, and the values are dictionaries of neighboring nodes and their edge costs. The astar() function takes the start node, goal node, and graph as input, and returns the shortest path from the start node to the goal node, as a list of nodes.

The function uses a priority queue (implemented as a heap) to keep track of the nodes in the open list, sorted by their estimated cost (the sum of the cost to reach the node and the heuristic cost to the goal). The function also maintains a set of nodes in the closed list, to avoid revisiting them.

The function uses the





User
add some test cases to this example
ChatGPT
Sure, here are some test cases you can use to test the A* implementation:

css
Copy code
# Test case 1
graph = {
    'A': {'B': 5, 'C': 1},
    'B': {'D': 3},
    'C': {'D': 2},
    'D': {'E': 4},
    'E': {}
}
start = 'A'
goal = 'E'
expected_path = ['A', 'C', 'D', 'E']

assert astar(start, goal, graph) == expected_path

# Test case 2
graph = {
    'A': {'B': 3, 'C': 2},
    'B': {'D': 4},
    'C': {'D': 1},
    'D': {'E': 6},
    'E': {}
}
start = 'A'
goal = 'E'
expected_path = ['A', 'C', 'D', 'E']

assert astar(start, goal, graph) == expected_path

# Test case 3
graph = {
    'A': {'B': 2, 'C': 1},
    'B': {'D': 2},
    'C': {'D': 3},
    'D': {'E': 1},
    'E': {}
}
start = 'A'
goal = 'E'
expected_path = ['A', 'B', 'D', 'E']

assert astar(start, goal, graph) == expected_path

# Test case 4
graph = {
    'A': {'B': 3, 'C': 2},
    'B': {'D': 4},
    'C': {'D': 1},
    'D': {'E': 6},
    'E': {}
}
start = 'A'
goal = 'F'

assert astar(start, goal, graph) is None
These test cases cover different scenarios, such as graphs with different topologies, and cases where the goal is not reachable from the start node.

        57.1.3

    57.2 Fibonacci heaps 

https://youtu.be/6JxvKfSV9Ns

Fibonacci heaps are a type of priority queue data structure used in computer science to efficiently implement algorithms like Dijkstra's shortest path algorithm and Prim's minimum spanning tree algorithm. They were first introduced by Michael L. Fredman and Robert E. Tarjan in 1984.
Fibonacci heaps are based on the Fibonacci sequence, which is a sequence of numbers in which each number is the sum of the two preceding ones, starting from 0 and 1. In a Fibonacci heap, nodes are organized into trees, where each tree represents a heap of nodes with the same number of children. The roots of the trees are linked together in a circular, doubly linked list.
The key feature of Fibonacci heaps is that they allow efficient merging of two heaps. When two Fibonacci heaps are merged, the root lists of the two heaps are concatenated, and the minimum of the two heaps is updated. Because of this, the time complexity of merging two Fibonacci heaps is O(1).
Fibonacci heaps also have a very fast amortized time complexity for operations like insertion, deletion, and finding the minimum element, which is O(1) in the amortized case. However, in the worst case, these operations can take O(log n) time.
Overall, Fibonacci heaps are a powerful data structure for implementing algorithms that require a priority queue, particularly when merging two heaps is a common operation.



class FibonacciHeapNode:
    def __init__(self, value, key):
        self.value = value
        self.key = key
        self.degree = 0
        self.parent = None
        self.child = None
        self.left = self
        self.right = self

class FibonacciHeap:
    def __init__(self):
        self.min = None
        self.num_nodes = 0

    def is_empty(self):
        return self.min is None

    def insert(self, value, key):
        node = FibonacciHeapNode(value, key)
        if self.min is None:
            self.min = node
        else:
            self._insert_node(node)
            if node.key < self.min.key:
                self.min = node
        self.num_nodes += 1

    def find_min(self):
        if self.min is None:
            return None
        return self.min.value

    def delete_min(self):
        if self.min is None:
            return None
        min_node = self.min
        if min_node.child is not None:
            children = self._remove_children(min_node)
            for child in children:
                self._insert_node(child)
        self._remove_node(min_node)
        if min_node == min_node.right:
            self.min = None
        else:
            self.min = min_node.right
            self._consolidate()
        self.num_nodes -= 1
        return min_node.value

    def _insert_node(self, node):
        node.left = self.min.left
        node.right = self.min
        self.min.left.right = node
        self.min.left = node

    def _remove_node(self, node):
        node.left.right = node.right
        node.right.left = node.left
        node.left = node
        node.right = node

    def _remove_children(self, node):
        children = []
        child = node.child
        while True:
            children.append(child)
            if child.right == node.child:
                break
            child = child.right
        for child in children:
            self._remove_node(child)
        node.child = None
        node.degree = 0
        return children

    def _consolidate(self):
        A = [None] * (self.num_nodes.bit_length() + 1)
        nodes = []
        node = self.min
        while True:
            nodes.append(node)
            if node.right == self.min:
                break
            node = node.right
        for node in nodes:
            degree = node.degree
            while A[degree] is not None:
                other = A[degree]
                if node.key > other.key:
                    node, other = other, node
                self._link_nodes(other, node)
                A[degree] = None
                degree += 1
            A[degree] = node
            if node.key <= self.min.key:
                self.min = node

    def _link_nodes(self, node1, node2):
        self._remove_node(node1)
        node1.parent = node2
        node2.child = self._insert_node(node1)
        node2.degree += 1

# Test cases
heap = FibonacciHeap()
assert heap.is_empty() == True
assert heap.find_min() == None

heap.insert("A", 5)
assert heap.is_empty() == False
assert heap.find_min() == "A"

heap.insert("B", 3)
assert heap.find_min() == "B"

heap.insert("C", 4)
assert heap.find_min() == "B"

heap.insert("D




This code is an example implementation of a Fibonacci heap data structure in Python. A Fibonacci heap is a type of heap data structure that supports several operations such as inserting an element, finding the minimum element, and deleting the minimum element, all in O(1) time on average.

The `FibonacciHeapNode` class represents a single node in the Fibonacci heap. Each node has a `value`, which is the actual value of the node, a `key`, which is the priority of the node, a `degree`, which is the number of children of the node, and pointers to its parent, child, left sibling, and right sibling nodes.

The `FibonacciHeap` class is the actual Fibonacci heap data structure. It has a `min` field that points to the node with the minimum key value, and a `num_nodes` field that keeps track of the total number of nodes in the heap.

The `is_empty()` method returns `True` if the heap is empty and `False` otherwise. The `insert(value, key)` method inserts a new node with the given `value` and `key` into the heap. The new node is added to the root list and its `key` value is compared with the current minimum key value to update the `min` field if necessary.

The `find_min()` method returns the `value` of the node with the minimum key value, or `None` if the heap is empty. The `delete_min()` method removes the node with the minimum key value from the heap, and returns its `value`. If the minimum node has children, they are added to the root list, and the `consolidate()` method is called to rearrange the heap.

The `_insert_node(node)` method inserts the given node into the root list. The `_remove_node(node)` method removes the given node from the root list. The `_remove_children(node)` method removes all children of the given node from the heap and returns them in a list.

The `_consolidate()` method performs the consolidate step of the Fibonacci heap merge operation. It creates an array `A` of size `log(n)` (where `n` is the number of nodes in the heap) to store nodes of the same degree. It iterates through all the nodes in the root list, and for each node, it merges it with another node of the same degree until no two nodes have the same degree. The resulting nodes are added to the root list, and the minimum node is updated if necessary.

The `_link_nodes(node1, node2)` method links the two given nodes, making `node2` the parent of `node1`.

Finally, the code includes some test cases that create a new Fibonacci heap, insert some nodes, and verify that the `is_empty()` and `find_min()` methods work as expected.

    57.3
58. turtle, tags: turtle

    58.1  documentation

        58.1.1  https://docs.python.org/3/library/turtle.html

        58.1.2 Chatgpt

            58.1.2.1 python turtle most used API
Python's Turtle library provides a variety of APIs for creating graphics and animations using a turtle graphics metaphor. Some of the most commonly used APIs in the Turtle library include:

Turtle(): Creates a new turtle object to draw on the screen.

forward(distance): Moves the turtle forward by the specified distance.

backward(distance): Moves the turtle backward by the specified distance.

right(angle): Turns the turtle to the right by the specified angle.

left(angle): Turns the turtle to the left by the specified angle.

penup(): Lifts the turtle's pen off the screen so that it no longer draws.

pendown(): Puts the turtle's pen back on the screen so that it can draw.

pencolor(color): Sets the color of the turtle's pen.

fillcolor(color): Sets the color of the turtle's fill.

begin_fill(): Begins a new shape to fill.

end_fill(): Fills the shape that was begun with begin_fill().

speed(speed): Sets the speed of the turtle's animation.

circle(radius): Draws a circle with the specified radius.

dot(size): Draws a dot with the specified size.

setposition(x, y): Moves the turtle to the specified position.

These are just a few of the most commonly used APIs in the Turtle library. There are many others that can be used to create more complex graphics and animations.

            58.1.2.2 begin_fill example
            Sure, here's an example of using the begin_fill() and end_fill() methods in the Turtle library to fill a shape with a specified color:

import turtle

# Create a turtle object
t = turtle.Turtle()

# Set the fill color to blue
t.fillcolor("blue")

# Begin filling a shape
t.begin_fill()

# Draw a square
for i in range(4):
    t.forward(50)
    t.left(90)

# End the fill
t.end_fill()

# Exit the turtle window
turtle.done()
In this example, the begin_fill() method is called before drawing the square, which signals that any subsequent drawing should be filled with the specified color (in this case, blue). The end_fill() method is called after drawing the square to indicate that the filling should stop.

When you run this code, you'll see a blue square with filled color on the screen.

        58.1.3 circle star example
[i500695@WYLQRXL9LQ:2023-04-18 18:37:12:~/work/code/python/udemy_python_100_days_of_code/day16:]2026$ cat !$ |pbcopy
cat turtle_star_circle.py |pbcopy
from turtle import *
color('blue', 'green')
begin_fill()
while True:
    forward(600)
    left(170)
    if abs(pos()) < 1:
        break
end_fill()
done()

This code uses the Turtle library to draw a complex shape filled with a gradient color.

Here's a breakdown of the code:

from turtle import *
This line imports everything from the Turtle library, making all its functions and classes available in the current program.

color('blue', 'green')
This line sets the color of the turtle's pen and fill. The first argument, 'blue', sets the pen color to blue, and the second argument, 'green', sets the fill color to green.

begin_fill()
This line signals that the shape being drawn should be filled with the specified color. Any subsequent drawing commands will fill the shape until end_fill() is called.

while True:
    forward(600)
    left(170)
    if abs(pos()) < 1:
        break
This is the core of the code, where the shape is drawn. The forward(600) command moves the turtle forward by 600 units, and left(170) rotates the turtle to the left by 170 degrees. This combination of commands creates a complex, looping shape. The if abs(pos()) < 1: line checks if the turtle has returned to its starting position (within a very small tolerance), and if so, the loop is exited with break.

end_fill()
This line signals that the filling of the shape should stop. The shape created by the loop will be filled with the color specified earlier.

done()
This line tells the Turtle library that the drawing is complete and it should exit and close the turtle window.

When you run this code, you'll see a looping shape filled with a gradient from blue to green.

        58.1.4 spiral
import turtle

# Create a turtle object
t = turtle.Turtle()

from turtle import color, begin_fill, end_fill
color('blue', 'green')
begin_fill()

# Draw a spiral
for i in range(50):
    t.forward(i * 10)
    t.right(144)

# Exit the turtle window
end_fill()
turtle.done()

        58.1.5 rainbound circle
import turtle

# Create a turtle object
t = turtle.Turtle()

# Draw a circle with rainbow colors
colors = ["red", "orange", "yellow", "green", "blue", "purple", "pink", "brown", "black", "gray", ]
n = len(colors)
for i in range(n):
    t.pencolor(colors[i % n])
    t.fillcolor(colors[i % n])
    t.begin_fill()
    t.circle(200)
    t.end_fill()
    t.right(360//n)

# Exit the turtle window
turtle.done()

        58.1.6
    58.2 turtle examples

    58.3 turtle shapes
import turtle

# Get a list of all supported shapes
shapes = turtle.getshapes()

# Print the list of shapes
print(shapes)

['arrow', 'turtle', 'circle', 'square', 'triangle', 'classic']

turtle.shape("circle")


    58.4 turtle colors
turtle.color(*args)
Return or set pencolor and fillcolor.

Several input formats are allowed. They use 0 to 3 arguments as follows:

color()
Return the current pencolor and the current fillcolor as a pair of color specification strings or tuples as returned by pencolor() and fillcolor().

color(colorstring), color((r,g,b)), color(r,g,b)
Inputs as in pencolor(), set both, fillcolor and pencolor, to the given value.

color(colorstring1, colorstring2), color((r1,g1,b1), (r2,g2,b2))
Equivalent to pencolor(colorstring1) and fillcolor(colorstring2) and analogously if the other input format is used.

If turtleshape is a polygon, outline and interior of that polygon is drawn with the newly set colors.

 >>> turtle.color("red", "green")
 >>> turtle.color()
 ('red', 'green')
 >>> color("#285078", "#a0c8f0")
 >>> color()
 ((40.0, 80.0, 120.0), (160.0, 200.0, 240.0))

https://cs111.wellesley.edu/labs/lab02/colors

- online resources for rgb colors
RGB Color Picker: This website allows you to choose colors using a color picker tool. You can adjust the RGB values of the selected color, and the website will display the resulting color in real-time. https://www.rgbcolorpicker.com/

ColorHexa: This website provides detailed information about RGB colors, including their hexadecimal codes, HSL values, and CMYK values. You can browse colors by name, or search for specific colors by entering their RGB values. https://www.colorhexa.com/

Adobe Color: This website allows you to create and save custom color palettes, and provides a variety of tools for working with RGB colors. You can browse existing color palettes, or create your own by adjusting the RGB values of the colors. https://color.adobe.com/create/color-wheel/

W3Schools Color Picker: This website provides a simple color picker tool that allows you to choose colors by adjusting their RGB values. You can also preview how the selected color will look on different types of backgrounds. https://www.w3schools.com/colors/colors_rgb.asp
-
    58.5

59. python packages repositories

    59.1 pypi

        59.1.1 prettytable
pip install prettytable
PrettyTable lets you print tables in an attractive ASCII form:

+-----------+------+------------+-----------------+
| City name | Area | Population | Annual Rainfall |
+-----------+------+------------+-----------------+
| Adelaide  | 1295 |  1158259   |      600.5      |
| Brisbane  | 5905 |  1857594   |      1146.4     |
| Darwin    | 112  |   120900   |      1714.7     |
| Hobart    | 1357 |   205556   |      619.5      |
| Melbourne | 1566 |  3806092   |      646.9      |
| Perth     | 5386 |  1554769   |      869.4      |
| Sydney    | 2058 |  4336374   |      1214.8     |
+-----------+------+------------+-----------------+

        59.1.2
    59.2
60. strings, tags: strings

    60.1 q. python check char is a-z

ChatGPT
In Python, you can check if a character is a lowercase alphabet (a-z) using the islower() method of the string class. Here's an example:

char = 'c'
if char.islower():
    print("The character is a lowercase alphabet.")
else:
    print("The character is not a lowercase alphabet.")
In this example, the islower() method checks if the character 'c' is a lowercase alphabet, and since it is, the output of the program will be "The character is a lowercase alphabet."

Note that the islower() method will return True for any lowercase alphabet and False for any other character, including uppercase alphabets, digits, punctuation marks, and whitespace characters. If you want to check for uppercase alphabets (A-Z), you can use the isupper() method instead.





User
alternative way
ChatGPT
Another way to check if a character is a lowercase alphabet in Python is to compare its ASCII code with the ASCII codes of lowercase alphabets. In ASCII, the lowercase alphabets 'a' to 'z' have the ASCII codes 97 to 122. Therefore, you can check if a character is a lowercase alphabet by checking if its ASCII code is between 97 and 122.

Here's an example:

char = 'c'
if ord(char) >= 97 and ord(char) <= 122:
    print("The character is a lowercase alphabet.")
else:
    print("The character is not a lowercase alphabet.")
In this example, the ord() function is used to get the ASCII code of the character 'c', which is 99. Then, the code checks if this value is between 97 and 122, which it is, so the output of the program will be "The character is a lowercase alphabet."

Note that this method can be used to check for uppercase alphabets by comparing the ASCII code with the ASCII codes of uppercase alphabets, which are 65 to 90.

    60.2
61. python data structures and collections, tags: python data structures and collections 

    61.1  named tuples
Named tuples are a useful data structure in Python that provide a convenient way to define lightweight classes for storing data. They are similar to tuples, but with named fields, allowing you to access values by name rather than position.

To use named tuples in Python, you need to import the `namedtuple` function from the `collections` module. Here's an example:

```python
from collections import namedtuple

# Define a named tuple called 'Person' with fields 'name', 'age', and 'city'
Person = namedtuple('Person', ['name', 'age', 'city'])

# Create an instance of the named tuple
person1 = Person('Alice', 25, 'New York')

# Access the fields using dot notation
print(person1.name)  # Output: Alice
print(person1.age)   # Output: 25
print(person1.city)  # Output: New York

# Named tuples are immutable, just like regular tuples
# You cannot modify their fields directly, but you can create a new named tuple with updated values
person2 = person1._replace(age=30)

print(person2.name)  # Output: Alice
print(person2.age)   # Output: 30
print(person2.city)  # Output: New York
```

In the example above, we define a named tuple called 'Person' using the `namedtuple` function. The first argument is the name of the named tuple, and the second argument is a list of field names. We then create an instance of the named tuple, specifying values for each field.

You can access the fields of a named tuple using dot notation, just like accessing attributes of an object. Named tuples are immutable, so you cannot modify their fields directly. However, you can create a new named tuple with updated values using the `_replace` method, as shown in the example.

Named tuples are particularly useful when you want to store data records or define lightweight classes where you only need to store data without any additional methods. They provide a more readable and self-documenting alternative to using plain tuples or dictionaries.
    61.2

62. Python versions, tags: versions

    62.1  python version 2, tags: python version 2

    62.2  python version 3, tags: python version 3

        62.2.1   placeholder
        62.2.2   placeholder
        62.2.3   placeholder
        62.2.4   placeholder
        62.2.5   placeholder
        62.2.6   placeholder
        62.2.7   placeholder
        62.2.8   placeholder
        62.2.9   placeholder
        62.2.10  placeholder
        62.2.11  placeholder
        62.2.12  python version 3.12, tags: python version 3.12

            62.2.12.1  better error messages

            62.2.12.2 comprehension performance improvements 
comprehensions are now more performant b/c the implementation has changed from functions to inlining 

            62.2.12.3 optimizations
- immortal objects. required by the system. no ref count or cleanup required.
- per interpreter GIL (which potentially allows better usage of multiple cors), currently in cpython, available in 3.13

            62.2.12.4 f-strings inner quotes

            62.2.12.5 better typing for kwargs (add unpack)

            62.2.12.6 @override
if child class overrides incorrectly -> compilation error

            62.2.12.7 generics syntax simplified
simplified syntax example:

class myClass[T: str]:
    def foo() -> T

for functions:
foo[T](a: T, b: T) -> T 

type aliases
type ListOrSet[T] = List[T] | Set[T]

            62.2.12.8
        62.2.13  placeholder
        62.2.14  placeholder
        62.2.15  placeholder
        62.2.16  placeholder
        62.2.17  placeholder
        62.2.18  placeholder
        62.2.19  placeholder
        62.2.20  placeholder
        62.2.21  placeholder
        62.2.22  placeholder
        62.2.23  placeholder
        62.2.24  placeholder
        62.2.25  placeholder
        62.2.26  placeholder
        62.2.27  placeholder
        62.2.28  placeholder
        62.2.29  placeholder
        62.2.30  placeholder
        62.2.31  placeholder


    62.3

63.
