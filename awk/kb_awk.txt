.........................................Table Of Contents...............................................................
1.  <URL:#tn=1. >
    1.1 print specifires <URL:#tn=    1.1 print specifires>
    1.2 pretty print results of awk <URL:#tn=    1.2 pretty print results of awk>
	1.1 example: <URL:#tn=	1.1 example:>
	1.3 Awk One-Liners Explained, Part III: Selective Printing and Deleting of Certain Lines <URL:#tn=	1.3 Awk One-Liners Explained, Part III: Selective Printing and Deleting of Certain Lines>
	    1.3.1 Selective Printing of Certain Lines <URL:#tn=	    1.3.1 Selective Printing of Certain Lines>
	        1.3.1.1 Print the first 10 lines of a file (emulates "head -10"). <URL:#tn=	        1.3.1.1 Print the first 10 lines of a file (emulates "head -10").>
	        1.3.1.2 Print the first line of a file (emulates "head -1"). <URL:#tn=	        1.3.1.2 Print the first line of a file (emulates "head -1").>
	        1.3.1.3 Print the last 2 lines of a file (emulates "tail -2"). <URL:#tn=	        1.3.1.3 Print the last 2 lines of a file (emulates "tail -2").>
	        1.3.1.4 Print the last line of a file (emulates "tail -1"). <URL:#tn=	        1.3.1.4 Print the last line of a file (emulates "tail -1").>
	        1.3.1.5 Print only the lines that match a regular expression "/regex/" (emulates "grep"). <URL:#tn=	        1.3.1.5 Print only the lines that match a regular expression "/regex/" (emulates "grep").>
	        1.3.1.6 Print only the lines that do not match a regular expression "/regex/" (emulates "grep -v"). <URL:#tn=	        1.3.1.6 Print only the lines that do not match a regular expression "/regex/" (emulates "grep -v").>
	        1.3.1.7 Print the line immediately before a line that matches "/regex/" (but not the line that matches itself). <URL:#tn=	        1.3.1.7 Print the line immediately before a line that matches "/regex/" (but not the line that matches itself).>
	        1.3.1.8 Print the line immediately after a line that matches "/regex/" (but not the line that matches itself). <URL:#tn=	        1.3.1.8 Print the line immediately after a line that matches "/regex/" (but not the line that matches itself).>
	        1.3.1.9 Print lines that match any of "AAA" or "BBB", or "CCC". <URL:#tn=	        1.3.1.9 Print lines that match any of "AAA" or "BBB", or "CCC".>
	        1.3.1.10 Print lines that contain "AAA" and "BBB", and "CCC" in this order. <URL:#tn=	        1.3.1.10 Print lines that contain "AAA" and "BBB", and "CCC" in this order.>
	        1.3.1.11 Print only the lines that are 65 characters in length or longer. <URL:#tn=	        1.3.1.11 Print only the lines that are 65 characters in length or longer.>
	        1.3.1.12 Print only the lines that are less than 64 characters in length. <URL:#tn=	        1.3.1.12 Print only the lines that are less than 64 characters in length.>
	        1.3.1.13 Print a section of file from regular expression to end of file. <URL:#tn=	        1.3.1.13 Print a section of file from regular expression to end of file.>
	        1.3.1.14 Print lines 8 to 12 (inclusive). <URL:#tn=	        1.3.1.14 Print lines 8 to 12 (inclusive).>
59. Print line number 52. <URL:#tn=59. Print line number 52.>
60. Print section of a file between two regular expressions (inclusive). <URL:#tn=60. Print section of a file between two regular expressions (inclusive).>
	    1.3.2 Selective Deletion of Certain Lines <URL:#tn=	    1.3.2 Selective Deletion of Certain Lines>
	        1.3.2.1 Delete all blank lines from a file. <URL:#tn=	        1.3.2.1 Delete all blank lines from a file.>
	        1.3.2.2 <URL:#tn=	        1.3.2.2>
	    1.3.3 <URL:#tn=	    1.3.3>
	1.4 <URL:#tn=	1.4>
2. Count number of lines on multiple files <URL:#tn=2. Count number of lines on multiple files>
3. Calculate average for a column of numbers <URL:#tn=3. Calculate average for a column of numbers>
4. Add line after line specific line number <URL:#tn=4. Add line after line specific line number>
	4.1 example add after first line the line "bbb" <URL:#tn=	4.1 example add after first line the line "bbb">
	4.2 Using sed for similar tast, add comment in line 137-139. <URL:#tn=	4.2 Using sed for similar tast, add comment in line 137-139.>
5. Awk Tutorials, tags: awk Tutorials <URL:#tn=5. Awk Tutorials, tags: awk Tutorials>
	5.1 Using AWK <URL:#tn=	5.1 Using AWK>
		5.1.1 invoking awk <URL:#tn=		5.1.1 invoking awk>
		5.1.2 how does it work? <URL:#tn=		5.1.2 how does it work?>
		5.1.3 Basic examples <URL:#tn=		5.1.3 Basic examples>
	5.2 Pattern Matching <URL:#tn=	5.2 Pattern Matching>
		5.2.1 print all empty lines: <URL:#tn=		5.2.1 print all empty lines:>
		5.2.2 Classify acording to content <URL:#tn=		5.2.2 Classify acording to content>
	5.3 Records and Fields <URL:#tn=	5.3 Records and Fields>
		5.3.1 Referencing and Separating Fields <URL:#tn=		5.3.1 Referencing and Separating Fields>
		5.3.2 Field Splitting <URL:#tn=		5.3.2 Field Splitting>
	5.4 Expressions <URL:#tn=	5.4 Expressions>
		5.4.1 Escape sequences <URL:#tn=		5.4.1 Escape sequences>
		5.4.2 Variables <URL:#tn=		5.4.2 Variables>
		5.4.3 Arithmetic Operators <URL:#tn=		5.4.3 Arithmetic Operators>
		5.4.4 Variable assignment <URL:#tn=		5.4.4 Variable assignment>
		5.4.5 Assignment Operators <URL:#tn=		5.4.5 Assignment Operators>
		5.4.5 Calculate average <URL:#tn=		5.4.5 Calculate average>
	5.5 System Variables <URL:#tn=	5.5 System Variables>
		5.5.1 Examples <URL:#tn=		5.5.1 Examples>
	5.6 Working with Multiline Records <URL:#tn=	5.6 Working with Multiline Records>
	5.7 Simple arithmetics <URL:#tn=	5.7 Simple arithmetics>
	5.8 Relational and Boolean Operators <URL:#tn=	5.8 Relational and Boolean Operators>
		5.8.1 Relational Operators <URL:#tn=		5.8.1 Relational Operators>
		5.8.2 Boolean Operators <URL:#tn=		5.8.2 Boolean Operators>
		5.8.3 Getting Information About Files <URL:#tn=		5.8.3 Getting Information About Files>
	5.9 Formatted Printing <URL:#tn=	5.9 Formatted Printing>
	5.10 Passing Parameters Into a Script <URL:#tn=	5.10 Passing Parameters Into a Script>
	5.11 Example <URL:#tn=	5.11 Example>
	5.12  Information Retrieval <URL:#tn=	5.12  Information Retrieval>
		5.12.1 Basic Example <URL:#tn=		5.12.1 Basic Example>
		5.12.2 Advance example, print a block of text that contains a certain pattern <URL:#tn=		5.12.2 Advance example, print a block of text that contains a certain pattern>
	5.13 Conditionals, Loops, and Arrays <URL:#tn=	5.13 Conditionals, Loops, and Arrays>
		5.13.1 Conditions <URL:#tn=		5.13.1 Conditions>
		5.13.2 Loops <URL:#tn=		5.13.2 Loops>
				5.13.2.1 Examples <URL:#tn=				5.13.2.1 Examples>
		5.13.3 Other Statements That Affect Flow Control <URL:#tn=		5.13.3 Other Statements That Affect Flow Control>
			5.13.3.1 break, continue <URL:#tn=			5.13.3.1 break, continue>
			5.13.3.2 main input loop. next, exit. <URL:#tn=			5.13.3.2 main input loop. next, exit.>
		5.13.4 Arrays <URL:#tn=		5.13.4 Arrays>
		5.13.5 Associative Arrays <URL:#tn=		5.13.5 Associative Arrays>
		5.13.6 Testing for Membership in an Array <URL:#tn=		5.13.6 Testing for Membership in an Array>
		5.13.7 A Glossary Lookup Script <URL:#tn=		5.13.7 A Glossary Lookup Script>
		5.13.8 Using split() to Create Arrays <URL:#tn=		5.13.8 Using split() to Create Arrays>
		5.13.9 Making Conversions <URL:#tn=		5.13.9 Making Conversions>
		5.13.10 Deleting Elements of an Array <URL:#tn=		5.13.10 Deleting Elements of an Array>
	5.14 An Acronym Processor <URL:#tn=	5.14 An Acronym Processor>
	5.15 Multidimensional Arrays <URL:#tn=	5.15 Multidimensional Arrays>
	5.15 System Variables That Are Arrays <URL:#tn=	5.15 System Variables That Are Arrays>
		5.15.1 An Array of Command-Line Parameters <URL:#tn=		5.15.1 An Array of Command-Line Parameters>
		5.15.2 An Array of Environment Variables <URL:#tn=		5.15.2 An Array of Environment Variables>
	5.16	Functions <URL:#tn=	5.16	Functions>
		5.16.1 Arithmetic Functions <URL:#tn=		5.16.1 Arithmetic Functions>
			5.16.1.1 Trigonometric Functions <URL:#tn=			5.16.1.1 Trigonometric Functions>
			5.16.1.2  Integer Function <URL:#tn=			5.16.1.2  Integer Function>
			5.16.1.3  Random Number Generation <URL:#tn=			5.16.1.3  Random Number Generation>
			5.16.1.4  Pick 'em <URL:#tn=			5.16.1.4  Pick 'em>
		5.16.2 String Functions <URL:#tn=		5.16.2 String Functions>
		5.16.3 Substrings <URL:#tn=		5.16.3 Substrings>
		5.16.4 String Length <URL:#tn=		5.16.4 String Length>
		5.16.6 Substitution Functions <URL:#tn=		5.16.6 Substitution Functions>
		5.16.7 Converting Case <URL:#tn=		5.16.7 Converting Case>
		5.16.8 The match() Function <URL:#tn=		5.16.8 The match() Function>
	5.17 Writing Your Own Functions <URL:#tn=	5.17 Writing Your Own Functions>
		5.17.1 Writing a Sort Function <URL:#tn=		5.17.1 Writing a Sort Function>
		5.17.2 Maintaining a Function Library <URL:#tn=		5.17.2 Maintaining a Function Library>
		5.17.3 Another Sorted Example <URL:#tn=		5.17.3 Another Sorted Example>
	5.18 The Bottom Drawer <URL:#tn=	5.18 The Bottom Drawer>
		5.18.1 The getline function  <URL:#tn=		5.18.1 The getline function >
		5.18.1.1 Assigning the Input to a Variable <URL:#tn=		5.18.1.1 Assigning the Input to a Variable>
		5.18.1.2 Reading Input from a Pipe <URL:#tn=		5.18.1.2 Reading Input from a Pipe>
		5.18.2 The close() Function <URL:#tn=		5.18.2 The close() Function>
		5.18.3 The system() Function <URL:#tn=		5.18.3 The system() Function>
		5.18.4 A Menu-Based Command Generator <URL:#tn=		5.18.4 A Menu-Based Command Generator>
		5.18.5 Directing Output to Files and Pipes <URL:#tn=		5.18.5 Directing Output to Files and Pipes>
		5.18.5.1 Directing Output to a Pipe <URL:#tn=		5.18.5.1 Directing Output to a Pipe>
			5.18.5.2 Working with Multiple Files <URL:#tn=			5.18.5.2 Working with Multiple Files>
		5.18.6 Generating Columnar Reports <URL:#tn=		5.18.6 Generating Columnar Reports>
		5.18.7 Debugging <URL:#tn=		5.18.7 Debugging>
			5.18.7.1 Make a Copy <URL:#tn=			5.18.7.1 Make a Copy>
			5.18.7.2 Before and After Photos <URL:#tn=			5.18.7.2 Before and After Photos>
			5.18.7.3 Finding Out Where the Problem Is <URL:#tn=			5.18.7.3 Finding Out Where the Problem Is>
			5.18.7.4 Commenting Out Loud <URL:#tn=			5.18.7.4 Commenting Out Loud>
			5.18.7.5 Slash and Burn <URL:#tn=			5.18.7.5 Slash and Burn>
			5.18.7.5 Getting Defensive About Your Script <URL:#tn=			5.18.7.5 Getting Defensive About Your Script>
		5.18.8 Limitations <URL:#tn=		5.18.8 Limitations>
		5.18.9 Invoking awk Using the #! <URL:#tn=		5.18.9 Invoking awk Using the #!>
		5.18.10 Full-Featured Applications <URL:#tn=		5.18.10 Full-Featured Applications>
			5.18.10.1 An Interactive Spelling Checker <URL:#tn=			5.18.10.1 An Interactive Spelling Checker>
			5.18.10.1.1 BEGIN Procedure <URL:#tn=			5.18.10.1.1 BEGIN Procedure>
				5.18.10.1.2 Main Procedure <URL:#tn=				5.18.10.1.2 Main Procedure>
				5.18.10.1.3 END Procedure <URL:#tn=				5.18.10.1.3 END Procedure>
12.1.4. Supporting Functions <URL:#tn=12.1.4. Supporting Functions>
				5.18.10.1.4 he spellcheck Shell Script <URL:#tn=				5.18.10.1.4 he spellcheck Shell Script>
			5.18.10.2 A Miscellany of Scripts <URL:#tn=			5.18.10.2 A Miscellany of Scripts>
				5.18.10.2.1  uutot.awk--Report UUCP Statistics <URL:#tn=				5.18.10.2.1  uutot.awk--Report UUCP Statistics>
				5.18.10.2.2  phonebill--Track Phone Usage <URL:#tn=				5.18.10.2.2  phonebill--Track Phone Usage>
				5.18.10.2.3 combine--Extract Multipart uuencoded Binaries <URL:#tn=				5.18.10.2.3 combine--Extract Multipart uuencoded Binaries>
				5.18.10.2.4  mailavg--Check Size of Mailboxes <URL:#tn=				5.18.10.2.4  mailavg--Check Size of Mailboxes>
				5.18.10.2.5 adj--Adjust Lines for Text Files <URL:#tn=				5.18.10.2.5 adj--Adjust Lines for Text Files>
				5.18.10.2.6 lpr--lpr Preprocessor <URL:#tn=				5.18.10.2.6 lpr--lpr Preprocessor>
				5.18.10.2.7 transpose--Perform a Matrix Transposition <URL:#tn=				5.18.10.2.7 transpose--Perform a Matrix Transposition>
				5.18.10.2.8 m1--Simple Macro Processor <URL:#tn=				5.18.10.2.8 m1--Simple Macro Processor>
	5.19 Quick Reference for awk <URL:#tn=	5.19 Quick Reference for awk>
		5.19.1 Command-Line Syntax <URL:#tn=		5.19.1 Command-Line Syntax>
			5.19.1.1 Shell Wrapper for Invoking awk <URL:#tn=			5.19.1.1 Shell Wrapper for Invoking awk>
		5.19.2 Language Summary for awk <URL:#tn=		5.19.2 Language Summary for awk>
			5.19.2.1. Records and Fields <URL:#tn=			5.19.2.1. Records and Fields>
			5.19.2.2. Format of a Script <URL:#tn=			5.19.2.2. Format of a Script>
			5.19.2.2.1. Line termination <URL:#tn=			5.19.2.2.1. Line termination>
			5.19.2.2.2. Comments <URL:#tn=			5.19.2.2.2. Comments>
			5.19.2.3. Patterns <URL:#tn=			5.19.2.3. Patterns>
			5.19.2.4. Regular Expressions <URL:#tn=			5.19.2.4. Regular Expressions>
			5.19.2.5. Expressions <URL:#tn=			5.19.2.5. Expressions>
			5.19.2.5.1. Constants <URL:#tn=			5.19.2.5.1. Constants>
			5.19.2.5.2. Escape sequences <URL:#tn=			5.19.2.5.2. Escape sequences>
			5.19.2.5.3. Variables <URL:#tn=			5.19.2.5.3. Variables>
			5.19.2.5.4. Arrays <URL:#tn=			5.19.2.5.4. Arrays>
			5.19.2.5.5. System variables <URL:#tn=			5.19.2.5.5. System variables>
			5.19.2.5.6. Operators <URL:#tn=			5.19.2.5.6. Operators>
			5.19.2.6. Statements and Functions <URL:#tn=			5.19.2.6. Statements and Functions>
		5.19.3  Command Summary for awk <URL:#tn=		5.19.3  Command Summary for awk>
			5.19.3.1. Format Expressions Used in printf and sprintf <URL:#tn=			5.19.3.1. Format Expressions Used in printf and sprintf>
    5.20 awk tutorialspoint , tags: awk tutorialspoint  <URL:#tn=    5.20 awk tutorialspoint , tags: awk tutorialspoint >
        5.20.1  practice <URL:#tn=        5.20.1  practice>
            5.20.1.1  basics <URL:#tn=            5.20.1.1  basics>
                5.20.1.1.1  add header <URL:#tn=                5.20.1.1.1  add header>
                5.20.1.1.2 provide command file <URL:#tn=                5.20.1.1.2 provide command file>
                5.20.1.1.3 awk options <URL:#tn=                5.20.1.1.3 awk options>
                    5.20.1.1.3.1 The -v option <URL:#tn=                    5.20.1.1.3.1 The -v option>
                    5.20.1.1.3.2 The --dump-variables[=file] option <URL:#tn=                    5.20.1.1.3.2 The --dump-variables[=file] option>
                5.20.1.1.4 print columns <URL:#tn=                5.20.1.1.4 print columns>
                5.20.1.1.5 count matches <URL:#tn=                5.20.1.1.5 count matches>
                5.20.1.1.6 print lines longer than X characters <URL:#tn=                5.20.1.1.6 print lines longer than X characters>
                5.20.1.1.7 ARGV and ARGC <URL:#tn=                5.20.1.1.7 ARGV and ARGC>
                5.20.1.1.8 built in variables CONVFMT, ENVIRON, FILENAME, FS, NF, NR etc <URL:#tn=                5.20.1.1.8 built in variables CONVFMT, ENVIRON, FILENAME, FS, NF, NR etc>
                5.20.1.1.9 <URL:#tn=                5.20.1.1.9>
        5.20.2 operators <URL:#tn=        5.20.2 operators>
            5.20.2.1  arithmetic <URL:#tn=            5.20.2.1  arithmetic>
            5.20.2.2 increment decrement <URL:#tn=            5.20.2.2 increment decrement>
            5.20.2.3 assignment  <URL:#tn=            5.20.2.3 assignment >
            5.20.2.4 relational <URL:#tn=            5.20.2.4 relational>
            5.20.2.5 logical <URL:#tn=            5.20.2.5 logical>
            5.20.2.6 ternary <URL:#tn=            5.20.2.6 ternary>
            5.20.2.7 unary <URL:#tn=            5.20.2.7 unary>
            5.20.2.8 exponential <URL:#tn=            5.20.2.8 exponential>
            5.20.2.9 string concat <URL:#tn=            5.20.2.9 string concat>
            5.20.2.10 array membership and iteration over array <URL:#tn=            5.20.2.10 array membership and iteration over array>
            5.20.2.11 regular expression <URL:#tn=            5.20.2.11 regular expression>
            5.20.2.12 <URL:#tn=            5.20.2.12>
        5.20.3 regular expressions  <URL:#tn=        5.20.3 regular expressions >
        5.20.4 arrays <URL:#tn=        5.20.4 arrays>
        5.20.5 control flows <URL:#tn=        5.20.5 control flows>
        5.20.6 loops <URL:#tn=        5.20.6 loops>
        5.20.7 builtin functions <URL:#tn=        5.20.7 builtin functions>
            5.20.7.1 arithmetic <URL:#tn=            5.20.7.1 arithmetic>
            5.20.7.2 string <URL:#tn=            5.20.7.2 string>
            5.20.7.3 time functions <URL:#tn=            5.20.7.3 time functions>
            5.20.7.4 bit manipulation <URL:#tn=            5.20.7.4 bit manipulation>
            5.20.7.5 miscellaneous <URL:#tn=            5.20.7.5 miscellaneous>
        5.20.8 functions syntax <URL:#tn=        5.20.8 functions syntax>
        5.20.9 redirection operators <URL:#tn=        5.20.9 redirection operators>
        5.20.10 pretty printing  <URL:#tn=        5.20.10 pretty printing >
        5.20.11 <URL:#tn=        5.20.11>
    5.21 <URL:#tn=    5.21>
6. Tips <URL:#tn=6. Tips>
	6.1 Count number of files in directory <URL:#tn=	6.1 Count number of files in directory>
7. AWK one liners <URL:#tn=7. AWK one liners>
8. My examples <URL:#tn=8. My examples>
	8.1 Counters <URL:#tn=	8.1 Counters>
	    8.1.1 sed and awk to count many things <URL:#tn=	    8.1.1 sed and awk to count many things>
	    8.1.2 count a number in lines matching patter, ex: how many tests failed. <URL:#tn=	    8.1.2 count a number in lines matching patter, ex: how many tests failed.>
	    8.1.3 <URL:#tn=	    8.1.3>
	8.2 Calculate linux total disk size <URL:#tn=	8.2 Calculate linux total disk size>
	8.3 average csv file column <URL:#tn=	8.3 average csv file column>
		8.3.1 on say 4th column <URL:#tn=		8.3.1 on say 4th column>
		8.3.2 <URL:#tn=		8.3.2>
	8.4 <URL:#tn=	8.4>
9. My Cookbook  <URL:#tn=9. My Cookbook >
	9.1 Using awk to print all columns from the nth to the last  <URL:#tn=	9.1 Using awk to print all columns from the nth to the last >
	9.2 <URL:#tn=	9.2>
10.  FAQS <URL:#tn=10.  FAQS>
	10.1  Round up/down, truncate  <URL:#tn=	10.1  Round up/down, truncate >
		10.1.1  Via Integer Function <URL:#tn=		10.1.1  Via Integer Function>
		10.1.2 Discussion in forum <URL:#tn=		10.1.2 Discussion in forum>
		10.1.3 <URL:#tn=		10.1.3>
	10.2 <URL:#tn=	10.2>
11. Summary <URL:#tn=11. Summary>
	11.1 Intro This week-of-unix-tools  <URL:#tn=	11.1 Intro This week-of-unix-tools >
	11.2 <URL:#tn=	11.2>
12. <URL:#tn=12.>
.................................................END TOC..............................................












Description: 	AWK Knowledge. AWK is an extremelly important unix tool for manipulating text files.
Author:		Yosi Izaq.

1. 

    1.1 print specifires
$0 - full line
$n - nth field
$NF - last field , # of fields

    1.2 pretty print results of awk
	1.1 example:
		$ grep  ismg change_set_12_06  | awk -F"@" '{print $1}' | awk -F"\\" '{print "Lib: " $4 ", File " $5}' |sort | uniq
		ignore the grep and sort and the first awk, note the print command.

	1.3 Awk One-Liners Explained, Part III: Selective Printing and Deleting of Certain Lines
http://www.catonmat.net/blog/awk-one-liners-explained-part-three/

This is the third and final part of a three-part article on the Awk one-liners. This part will explain Awk one-liners for selective printing and deletion of certain lines. See part one for introduction of the series.

If you just came to my website, then you might wonder, "What are these Awk one-liners and why are they famous?" The answer is very simple - they are small and beautiful Awk programs that do one and only text manipulation task very well. They have been circulating around the Internet as awk1line.txt text file and they have been written by Eric Pement.

If you are intrigued by this article series, I suggest that you subscribe to my posts, as I will have a lot more interesting and educational articles this year.

Eric Pement's Awk one-liner collection consists of five sections:

| 1. File spacing (explained in part one).
| 2. Numbering and calculations (explained in part one).
| 3. Text conversion and substitution (explained in part two).
| 4. Selective printing of certain lines (explained in this part).
| 5. Selective deletion of certain lines (explained in this part).
| 6. String creation, array creation and update on selective printing of certain lines (explained in bonus article).
| 7. Release of Awk One-Liners Explained e-book.
Awesome news: I have written an e-book based on this article series. Check it out:


Awk book

Grab my Awk cheat sheet and the local copy of Awk one-liners file awk1line.txt and let's roll.

	    1.3.1 Selective Printing of Certain Lines

	        1.3.1.1 Print the first 10 lines of a file (emulates "head -10").

awk 'NR < 11'
Awk has a special variable called "NR" that stands for "Number of Lines seen so far in the current file". After reading each line, Awk increments this variable by one. So for the first line it's 1, for the second line 2, ..., etc. As I explained in the very first one-liner, every Awk program consists of a sequence of pattern-action statements "pattern { action statements }". The "action statements" part get executed only on those lines that match "pattern" (pattern evaluates to true). In this one-liner the pattern is "NR < 11" and there are no "action statements". The default action in case of missing "action statements" is to print the line as-is (it's equivalent to "{ print $0 }"). The pattern in this one-liner is an expression that tests if the current line number is less than 11. If the line number is less than 11, Awk prints the line. As soon as the line number is 11 or more, the pattern evaluates to false and Awk skips the line.

A much better way to do the same is to quit after seeing the first 10 lines (otherwise we are looping over lines > 10 and doing nothing):

awk '1; NR == 10 { exit }'
The "NR == 10 { exit }" part guarantees that as soon as the line number 10 is reached, Awk quits. For lines smaller than 10, Awk evaluates "1" that is always a true-statement. And as we just learned, true statements without the "action statements" part are equal to "{ print $0 }" that just prints the first ten lines!

	        1.3.1.2 Print the first line of a file (emulates "head -1").

awk 'NR > 1 { exit }; 1'
This one-liner is very similar to previous one. The "NR > 1" is true only for lines greater than one, so it does not get executed on the first line. On the first line only the "1", the true statement, gets executed. It makes Awk print the line and read the next line. Now the "NR" variable is 2, and "NR > 1" is true. At this moment "{ exit }" gets executed and Awk quits. That's it. Awk printed just the first line of the file.

	        1.3.1.3 Print the last 2 lines of a file (emulates "tail -2").

awk '{ y=x "\n" $0; x=$0 }; END { print y }'
Okay, so what does this one do? First of all, notice that "{y=x "\n" $0; x=$0}" action statement group is missing the pattern. When the pattern is missing, Awk executes the statement group for all lines. For the first line, it sets variable "y" to "\nline1" (because x is not yet defined). For the second line it sets variable "y" to "line1\nline2". For the third line it sets variable "y" to "line2\nline3". As you can see, for line N it sets the variable "y" to "lineN-1\nlineN". Finally, when it reaches EOF, variable "y" contains the last two lines and they get printed via "print y" statement.

Thinking about this one-liner for a second one concludes that it is very ineffective - it reads the whole file line by line just to print out the last two lines! Unfortunately there is no seek() statement in Awk, so you can't seek to the end-2 lines in the file (that's what tail does). It's recommended to use "tail -2" to print the last 2 lines of a file.

	        1.3.1.4 Print the last line of a file (emulates "tail -1").

awk 'END { print }'
This one-liner may or may not work. It relies on an assumption that the "$0" variable that contains the entire line does not get reset after the input has been exhausted. The special "END" pattern gets executed after the input has been exhausted (or "exit" called). In this one-liner the "print" statement is supposed to print "$0" at EOF, which may or may not have been reset.

It depends on your awk program's version and implementation, if it will work. Works with GNU Awk for example, but doesn't seem to work with nawk or xpg4/bin/awk.

The most compatible way to print the last line is:

awk '{ rec=$0 } END{ print rec }'
Just like the previous one-liner, it's computationally expensive to print the last line of the file this way, and "tail -1" should be the preferred way.

	        1.3.1.5 Print only the lines that match a regular expression "/regex/" (emulates "grep").

awk '/regex/'
This one-liner uses a regular expression "/regex/" as a pattern. If the current line matches the regex, it evaluates to true, and Awk prints the line (remember that missing action statement is equal to "{ print }" that prints the whole line).

	        1.3.1.6 Print only the lines that do not match a regular expression "/regex/" (emulates "grep -v").

awk '!/regex/'
Pattern matching expressions can be negated by appending "!" in front of them. If they were to evaluate to true, appending "!" in front makes them evaluate to false, and the other way around. This one-liner inverts the regex match of the previous (#49) one-liner and prints all the lines that do not match the regular expression "/regex/".

	        1.3.1.7 Print the line immediately before a line that matches "/regex/" (but not the line that matches itself).

awk '/regex/ { print x }; { x=$0 }'
This one-liner always saves the current line in the variable "x". When it reads in the next line, the previous line is still available in the "x" variable. If that line matches "/regex/", it prints out the variable x, and as a result, the previous line gets printed.

It does not work, if the first line of the file matches "/regex/", in that case, we might want to print "match on line 1", for example:

awk '/regex/ { print (x=="" ? "match on line 1" : x) }; { x=$0 }'
This one-liner tests if variable "x" contains something. The only time that x is empty is at very first line. In that case "match on line 1" gets printed. Otherwise variable "x" gets printed (that as we found out contains the previous line). Notice that this one-liner uses a ternary operator "foo?bar:baz" that is short for "if foo, then bar, else baz".

	        1.3.1.8 Print the line immediately after a line that matches "/regex/" (but not the line that matches itself).

awk '/regex/ { getline; print }'
This one-liner calls the "getline" function on all the lines that match "/regex/". This function sets $0 to the next line (and also updates NF, NR, FNR variables). The "print" statement then prints this next line. As a result, only the line after a line matching "/regex/" gets printed.

If it is the last line that matches "/regex/", then "getline" actually returns error and does not set $0. In this case the last line gets printed itself.

	        1.3.1.9 Print lines that match any of "AAA" or "BBB", or "CCC".

awk '/AAA|BBB|CCC/'
This one-liner uses a feature of extended regular expressions that support the | or alternation meta-character. This meta-character separates "AAA" from "BBB", and from "CCC", and tries to match them separately on each line. Only the lines that contain one (or more) of them get matched and printed.

	        1.3.1.10 Print lines that contain "AAA" and "BBB", and "CCC" in this order.

awk '/AAA.*BBB.*CCC/'
This one-liner uses a regular expression "AAA.*BBB.*CCC" to print lines. This regular expression says, "match lines containing AAA followed by any text, followed by BBB, followed by any text, followed by CCC in this order!" If a line matches, it gets printed.

	        1.3.1.11 Print only the lines that are 65 characters in length or longer.

awk 'length > 64'
This one-liner uses the "length" function. This function is defined as "length([str])" - it returns the length of the string "str". If none is given, it returns the length of the string in variable $0. For historical reasons, parenthesis () at the end of "length" can be omitted. This one-liner tests if the current line is longer than 64 chars, if it is, the "length > 64" evaluates to true and line gets printed.

	        1.3.1.12 Print only the lines that are less than 64 characters in length.

awk 'length < 64'
This one-liner is almost byte-by-byte equivalent to the previous one. Here it tests if the length if line less than 64 characters. If it is, Awk prints it out. Otherwise nothing gets printed.

	        1.3.1.13 Print a section of file from regular expression to end of file.

awk '/regex/,0'
This one-liner uses a pattern match in form 'pattern1, pattern2' that is called "range pattern". The 3rd Awk Tip from article "10 Awk Tips, Tricks and Pitfalls" explains this match very carefully. It matches all the lines starting with a line that matches "pattern1" and continuing until a line matches "pattern2" (inclusive). In this one-liner "pattern1" is a regular expression "/regex/" and "pattern2" is just 0 (false). So this one-liner prints all lines starting from a line that matches "/regex/" continuing to end-of-file (because 0 is always false, and "pattern2" never matches).

	        1.3.1.14 Print lines 8 to 12 (inclusive).

awk 'NR==8,NR==12'
This one-liner also uses a range pattern in format "pattern1, pattern2". The "pattern1" here is "NR==8" and "pattern2" is "NR==12". The first pattern means "the current line is 8th" and the second pattern means "the current line is 12th". This one-liner prints lines between these two patterns.

59. Print line number 52.

awk 'NR==52'
This one-liner tests to see if current line is number 52. If it is, "NR==52" evaluates to true and the line gets implicitly printed out (patterns without statements print the line unmodified).

The correct way, though, is to quit after line 52:

awk 'NR==52 { print; exit }'
This one-liner forces Awk to quit after line number 52 is printed. It is the correct way to print line 52 because there is nothing else to be done, so why loop over the whole doing nothing.

60. Print section of a file between two regular expressions (inclusive).

awk '/Iowa/,/Montana/'
I explained what a range pattern such as "pattern1,pattern2" does in general in one-liner #57. In this one-liner "pattern1" is "/Iowa/" and "pattern2" is "/Montana/". Both of these patterns are regular expressions. This one-liner prints all the lines starting with a line that matches "Iowa" and ending with a line that matches "Montana" (inclusive).

	    1.3.2 Selective Deletion of Certain Lines
There is just one one-liner in this section.

	        1.3.2.1 Delete all blank lines from a file.

awk NF
This one-liner uses the special NF variable that contains number of fields on the line. For empty lines, NF is 0, that evaluates to false, and false statements do not get the line printed.

Another way to do the same is:

awk '/./'
This one-liner uses a regular-expression match "." that matches any character. Empty lines do not have any characters, so it does not match.

	        1.3.2.2
	    1.3.3

	1.4
2. Count number of lines on multiple files
	example:
	find . -name "*.csv" -exec wc -l '{}' \; | awk '{sum += $1} END {print sum}

3. Calculate average for a column of numbers
	example, calulate MSC average:
	cat Yosi_Izaq_MSc_Grades.strip  | awk '{sum += $5; courses+=1} END {print sum " " courses-1 " " sum/(courses-1)}'

4. Add line after line specific line number
	4.1 example add after first line the line "bbb"
	awk ' {print} NR==1 {print "bbb"} ' stam 

	4.2 Using sed for similar tast, add comment in line 137-139.
	sed -i~ -e '137/*' -e '139i*/' test.cpp

5. Awk Tutorials, tags: awk Tutorials
	
	5.1 Using AWK

		5.1.1 invoking awk
		    awk 'instructions' files
		    awk -f script files

		5.1.2 how does it work?
		Awk, in the usual case, interprets each input line as a record and each word on that line, delimited by spaces or tabs, as a field. (These defaults can be changed.) One or more consecutive spaces or tabs count as a single delimiter. Awk allows you to reference these fields, in either patterns or procedures. $0 represents the entire input line. $1, $2, ... refer to the individual fields on the input line. Awk splits the input record before the script is applied.
		5.1.3 Basic examples

		a single instruction that prints the first field of each line in the input file.
		awk '{ print $1 }' list

		a pattern "/MA/" is specified but there is no procedure. The default action is to print each line that matches the pattern.
		awk '/MA/' list

		print first field of only lines that match "MA".
		awk '/MA/ { print $1 }' list

		Default awk will seperate words (delimiter is white spaces), this example changes delimiter to ,
		awk -F, '/MA/ { print $1 }' list

		 print each field on its own line. Multiple commands are separated by semicolons.
		$ awk -F, '{ print $1; print $2; print $3 }' list

	5.2 Pattern Matching
		
	When awk reads an input line, it attempts to match each pattern-matching rule in a script. Only the lines matching the particular pattern are the object of an action. If no action is specified, the line that matches the pattern is printed (executing the print statement is the default action). 

		5.2.1 print all empty lines:
		/^$/ { print "This is a blank line." }

		5.2.2 Classify acording to content
		# test for integer, string or empty line.
		/[0-9]+/    { print "That is an integer" }
		/[A-Za-z]+/ { print "This is a string" }
		/^$/        { print "This is a blank line." }

		See running with std input example:
		[yizaq@yytzhak-wxp:Sun Jul 22:/cygdrive/c/work/KB/awk/examples:]$ awk -f classifyContent.awk  
		4 yosi
		That is an integer
		This is a string
		4
		That is an integer
		
		This is a blank line.

	5.3 Records and Fields
	 Awk takes each input line as a record and each word, separated by spaces or tabs, as a field. (The characters separating the fields are often referred to as delimiters.)

		5.3.1 Referencing and Separating Fields
		Awk allows you to refer to fields in actions using the field operator $. This operator is followed by a number or a variable that identifies the position of a field by number. "$1" refers to the first field, "$2" to the second field, and so on. "$0" refers to the entire input record. The following example displays the last name first and the first name second, followed by the phone number.

		example, print second first and then third field of each line in file
		awk '{ print $2, $1, $3 }' file

		$1 refers to the first name, $2 to the last name, and $3 to the phone number. The commas that separate each argument in the print statement cause a space to be output between the values. (Later on, we'll discuss the output field separator (OFS), whose value the comma outputs and which is by default a space.) In this example, a single input line forms one record containing three fields: there is a space between the first and last names and a tab between the last name and the phone number. If you wanted to grab the first and last name as a single field, you could set the field separator explicitly so that only tabs are recognized. Then, awk would recognize only two fields in this record.
		example:
		[yizaq@yytzhak-wxp:Sun Jul 22:/cygdrive/c/work/KB/awk/examples:]$ echo a b c d | awk 'BEGIN { one = 1; two = 2}
> { print $(one + two) }'
c

		You can change the field separator with the -F option on the command line. It is followed by the delimiter character (either immediately, or separated by whitespace). In the following example, the field separator is changed to a tab.

		$ awk -F"\t" '{ print $2 }' names

		Commas delimit fields in the following two address records.

    John Robinson,Koren Inc.,978 4th Ave.,Boston,MA 01760,696-0987 
    Phyllis Chapman,GVE Corp.,34 Sea Drive,Amesbury,MA 01881,879-0900

An awk program can print the name and address in block format.

    # blocklist.awk -- print name and address in block form.
    # input file -- name, company, street, city, state and zip, phone
    { 	print ""	# output blank line
    	print $1	# name
    	print $2	# company
    	print $3	# street 
    	print $4, $5	# city, state zip 
    }

	It is usually a better practice, and more convenient, to specify the field separator in the script itself. The system variable FS can be defined to change the field separator. Because this must be done before the first input line is read, we must assign this variable in an action controlled by the BEGIN rule.

    BEGIN { FS = "," }

    script to print out the names and phone numbers.

    # phonelist.awk -- print name and phone number. 
    # input file -- name, company, street, city, state and zip, phone

    BEGIN { FS = "," }  # comma-delimited fields

    { print $1 ", " $6 }


    To print only for lines that contain pattern MA,
    /MA/ { print $1 ", " $6 }

    To print only for lines that contain pattern MA in specific field, #5:
        $5 ~ /MA/   { print $1 ", " $6 }
    You can reverse the meaning of the rule by using bang-tilde (!~).
	$5 !~ /MA/   { print $1 ", " $6 }

	. The following regular expression looks for an area code.
	$6 ~ /1?(-| )?\(?[0-9]+\)?( |-)?[0-9]+-[0-9]+/
	The regular expression can be deciphered by breaking down its parts. "1?" means zero or one occurrences of "1". "(-|Figure )?" looks for either a hyphen or a space in the next position, or nothing at all. "\(?" looks for zero or one left parenthesis; the backslash prevents the interpretation of "(" as the grouping metacharacter. "[0-9]+" looks for one or more digits; note that we took the lazy way out and specified one or more digits rather than exactly three. In the next position, we are looking for an optional right parenthesis, and again, either a space or a hyphen, or nothing at all. Then we look for one or more digits "[0-9]+" followed by a hyphen followed by one or more digits "[0-9]+".
	This will match the following:
	707-724-0000
	(707) 724-0000
	(707)724-0000
	1-707-724-0000   
	1 707-724-0000   
	1(707)724-0000
	
		5.3.2 Field Splitting
	There are three distinct ways you can have awk separate fields. The first method is to have fields separated by whitespace. To do this, set FS equal to a single space. In this case, leading and trailing whitespace (spaces and/or tabs) are stripped from the record, and fields are separated by runs of spaces and/or tabs. Since the default value of FS is a single space, this is the way awk normally splits each record into fields.

The second method is to have some other single character separate fields. For example, awk programs for processing the UNIX /etc/passwd file usually use a ":" as the field separator. When FS is any single character, each occurrence of that character separates another field. If there are two successive occurrences, the field between them simply has the empty string as its value.

Finally, if you specify more than a single character as the field separator, it will be interpreted as a regular expression. That is, the field separator will be the "leftmost longest non-null and nonoverlapping" substring that matches the regular expression. You can see the difference between specifying:


    FS = "\t"

which causes each tab to be interpreted as a field separator, and:

    FS = "\t+"

which specifies that one or more consecutive tabs separate a field. Using the first specification, the following line would have three fields:

    abc\t\tdef

whereas the second specification would only recognize two fields. Using a regular expression allows you to specify several characters to be used as delimiters:

    FS = "[':\t]"

Any of the three characters in brackets will be interpreted as the field separator.

or, FS = "[':\t]+"  which will cause any consecutive run of those characters to be considered as field separator.

	5.4 Expressions
		
		5.4.1 Escape sequences

	Sequence 	Description
	\a		Alert character, usually ASCII BEL character
	\b 		Backspace
	\f 		Formfeed
	\n 		Newline
	\r 		Carriage return
	\t 		Horizontal tab
	\v 		Vertical tab
	\ddd 		Character represented as 1 to 3 digit octal value
	\xhex 		Character represented as hexadecimal value
	\c 		Any literal character c (e.g., \" for ")

		5.4.2 Variables
			Numeric example, x = 1
			String example, x = "aa"
			String concatenation operator is space, example: z = "Hello" "World"
			The dollar sign ($) operator is used to reference fields. The following expression assigns the value of the first field of the current input record to the variable w: w = $1

		5.4.3 Arithmetic Operators

	Operator 	Description
	+		Addition
	- 		Subtraction
	* 		Multiplication
	/ 		Division
	% 		Modulo
	^ 		Exponentiation
	** 		Exponentiation

		5.4.4 Variable assignment
		Once a variable has been assigned a value, that value can be referenced using the name of the variable. The following expression adds 1 to the value of x and assigns it to the variable y: y = x + 1
So, evaluate x, add 1 to it, and put the result into the variable y. The statement: print y
prints the value of y.

		5.4.5 Assignment Operators

	Operator 	Description
	++		Add 1 to variable.
	-- 		Subtract 1 from variable.
	+= 		Assign result of addition.
	-= 		Assign result of subtraction.
	*= 		Assign result of multiplication.
	/= 		Assign result of division.
	%= 		Assign result of modulo.
	^= 		Assign result of exponentiation.
	**= 		Assign result of exponentiation.

	The following example counts blank lines:
	# blankLines2.awk, count blank lines
/^$/ {
        x += 1
}
 END {print "number of blank lines is: " x }

		5.4.5 Calculate average
		 input file looks like:
    john 85 92 78 94 88
    andrea 89 90 75 90 86
    jasper 84 88 80 92 84

	Awk script:
	# average five grades 
{ total = $2 + $3 + $4 + $5 + $6
  avg = total / 5
  print $1, avg }

  This script adds together fields 2 through 6 to get the sum total of the five grades. The value of total is divided by 5 and assigned to the variable avg. ("/" is the operator for division.) The print statement outputs the student's name and average. Note that we could have skipped the assignment of avg and instead calculated the average as part of the print statement, as follows:

    print $1, total / 5


	5.5 System Variables
	There are a number of system or built-in variables defined by awk. Awk has two types of system variables. The first type defines values whose default can be changed, such as the default field and record separators. The second type defines values that can be used in reports or processing, such as the number of fields found in the current record, the count of the current record, and others. These are automatically updated by awk; for example, the current record number and input file name.
There are a set of default values that affect the recognition of records and fields on input and their display on output. The system variable FS defines the field separator. By default, its value is a single space, which tells awk that any number of spaces and/or tabs separate fields. FS can also be set to any single character, or to a regular expression.
The output equivalent of FS is OFS, which is a space by default.
Awk defines the variable NF to be the number of fields for the current input record. Changing the value of NF actually has side effects and is not well defined across different awk implementations (awk, nawk, gawk, mawk etc)

	Awk also defines RS, the record separator, as a newline. RS is a bit unusual; it's the only variable where awk only pays attention to the first character of the value.

	The output equivalent to RS is ORS, which is also a newline by default. In the next section, "Working with Multiline Records," we'll show how to change the default record separator. Awk sets the variable NR to the number of the current input record. It can be used to number records in a list. The variable FILENAME contains the name of the current input file. The variable FNR is useful when multiple input files are used as it provides the number of the current record relative to the current input file.

	Typically, the field and record separators are defined in the BEGIN procedure because you want these values set before the first input line is read. However, you can redefine these values anywhere in the script. In POSIX awk, assigning a new value to FS has no effect on the current input line; it only affects the next input line.

	Finally, POSIX added a new variable, CONVFMT, which is used to control number-to-string conversions. For example,

    str = (5.5 + 3.2) " is a nice value"

Here, the result of the numeric expression 5.5 + 3.2 (which is 8.7) must be converted to a string before it can be used in the string concatenation. CONVFMT controls this conversion. Its default value is "%.6g", which is a printf-style format specification for floating-point numbers. Changing CONVFMT to "%d", for instance, would cause all numbers to be converted to strings as integers. Prior to the POSIX standard, awk used OFMT for this purpose. OFMT does the same job, but controlling the conversion of numeric values when using the print statement. The POSIX committee wanted to separate the tasks of output conversion from simple string conversion. Note that numbers that are integers are always converted to strings as integers, no matter what the values of CONVFMT and OFMT may be.

		5.5.1 Examples
NR variable. Here's a revised print statement for the script that calculates student averages:

    print NR ".", $1, avg

Running the revised script produces the following output:

    a. john 87.4
    b. andrea 86
    c. jasper 85.6

After the last line of input is read, NR contains the number of input records that were read. It can be used in the END action to provide a report summary. Here's a revised version of the phonelist.awk script.

After the last line of input is read, NR contains the number of input records that were read. It can be used in the END action to provide a report summary. Here's a revised version of the phonelist.awk script.

    # phonelist.awk -- print name and phone number. 
    # input file -- name, company, street, city, state and zip, phone
    BEGIN { FS = ", *" }  # comma-delimited fields
    { print $1 ", " $6 } 
    END { 	print ""
    	print NR, "records processed." }

	This program changes the default field separator and uses NR to print the total number of records printed. Note that this program uses a regular expression for the value of FS. This program produces the following output:

    John Robinson, 696-0987
    Phyllis Chapman, 879-0900

    2 records processed.
The output field separator (OFS) is generated when a comma is used to separate the arguments in a print statement. You may have wondered what effect the comma has in the following expression:

    print NR ".", $1, avg

By default, the comma causes a space (the default value of OFS) to be output. For instance, you could redefine OFS to be a tab in a BEGIN action. Then the preceding print statement would produce the following output:

    1      john    87.4
    2      andrea  86
    3      jasper  85.6

    Another commonly used system variable is NF, which is set to the number of fields for the current record. As we'll see in the next section, you can use NF to check that a record has the same number of fields that you expect. You can also use NF to reference the last field of each record. Using the "$" field operator and NF produces that reference. If there are six fields, then "$NF" is the same as "$6."


	5.6 Working with Multiline Records
	read a record where each field consists of a single line
	 data is stored on file in block format. Instead of having all the information on one line, the person's name is on one line, followed by the company's name on the next line and so on. Here's a sample record:

    John Robinson
    Koren Inc.
    978 Commonwealth Ave.
    Boston
    MA 01760
    696-0987

    To process this data, we can specify a multiline record by defining the field separator to be a newline, represented as "\n", and set the record separator to the empty string, which stands for a blank line.

    BEGIN { FS = "\n"; RS = "" }

We can print the first and last fields using the following script:

    # block.awk - print first and last fields 
    # $1 = name; $NF = phone number

    BEGIN { FS = "\n"; RS = "" }

    { print $1, $NF }

Here's a sample run:

    $ awk -f block.awk phones.block
    John Robinson 696-0987
    Phyllis Chapman 879-0900
    Jeffrey Willis 914-636-0000
    Alice Gold (707) 724-0000
    Bill Gold 1-707-724-0000

    The two fields are printed on the same line because the default output separator (OFS) remains a single space. If you want the fields to be output on separate lines, change OFS to a newline. While you're at it, you probably want to preserve the blank line between records, so you must specify the output record separator ORS to be two newlines.

    OFS = "\n"; ORS = "\n\n"

	5.7 Simple arithmetics

	An example that presumes you have entered in a file the following information:

    1000
    125	Market         -125.45
    126	Hardware Store  -34.95
    127	Video Store      -7.45
    128	Book Store      -14.32
    129	Gasoline        -16.10
	
The first line contains the beginning balance. Each of the other lines represent information from a single check: the check number, a description of where it was spent, and the amount of the check. The three fields are separated by tabs. Using negative amounts for checks allows positive amounts to represent deposits.

The core task of the script is that it must get the beginning balance and then deduct the amount of each check from that balance. We can provide detail lines for each check to compare against the check register. Finally, we can print the ending balance. Here it is:

# checkbook.awk
BEGIN { FS = "\t" }

#1 Expect the first record to have the starting balance.
NR == 1 { print "Beginning Balance: \t" $1
	balance = $1
	next		# get next record and start over, this makes sure first line isn't processed by #2.
}

#2 Apply to each check record, adding amount from balance.
{	print $1, $2, $3
	print balance += $3   # checks have negative amounts

	5.8 Relational and Boolean Operators

	A relational expression can be used in place of a pattern to control a particular action. For instance, if we wanted to limit the records selected for processing to those that have five fields, we could use the following expression:

    NF == 5

This relational expression compares the value of NF (the number of fields for each input record) to five. If it is true, the action will be executed; otherwise, it will not.

We can use a relational expression to validate the phonelist database before attempting to print out the record.

    NF == 6 { print $1, $6 }

Then only lines with six fields will be printed.

The opposite of "==" is "!=" ("is not equal to"). Similarly, you can compare one expression to another to see if it is greater than (>) or less than (<) or greater than or equal to (>=) or less than or equal to (<=). The expression

    NR > 1

tests whether the number of the current record is greater than 1. As we'll see in the next chapter, relational expressions are typically used in conditional (if) statements and are evaluated to determine whether or not a particular statement should be executed.

Regular expressions are usually written enclosed in slashes. These can be thought of as regular expression constants, much as "hello" is a string constant. We've seen many examples so far:

    /^$/ { print "This is a blank line." }

However, you are not limited to regular expression constants. When used with the relational operators ~ ("match") and !~ ("no match"), the right-hand side of the expression can be any awk expression; awk treats it as a string that specifies a regular expression.[48] We've already seen an example of the ~ operator used in a pattern-matching rule for the phone database:

    You may also use strings instead of regular expression constants when calling the match(), split(), sub(), and gsub() functions.

    $5 ~ /MA/   { print $1 ", " $6 }

Since any expression can be used with ~ and !~, regular expressions can be supplied through variables. For instance, in the phonelist script, we could replace "/MA/" with state and have a procedure that defines the value of state.

    $5 ~ state  { print $1 ", " $6 }

This makes the script much more general to use because a pattern can change dynamically during execution of the script. For instance, it allows us to get the value of state from a command-line parameter.

		5.8.1 Relational Operators

		Operator 	Description
		<		Less than
		> 		Greater than
		<= 		Less than or equal to
		>= 		Greater than or equal to
		== 		Equal to
		!= 		Not equal to
		~ 		Matches
		!~ 		Does not match

		5.8.2 Boolean Operators

		Operator 	Description
		||		Logical OR
		&&		Logical AND
		!		Logical NOT

		Given two or more expressions, || specifies that one of them must evaluate to true (non-zero or non-empty) for the whole expression to be true. && specifies that both of the expressions must be true to return true.

The following expression:

    NF == 6 && NR > 1

states that the number of fields must be equal to 6 and that the number of the record must be greater than 1.

&& has higher precedence than ||. Can you tell how the following expression will be evaluated?

    NR > 1 && NF >= 2 || $1 ~ /\t/

The parentheses in the next example show which expression would be evaluated first based on the rules of precedence.

    (NR > 1 && NF >= 2) || $1 ~ /\t/

In other words, both of the expressions in parentheses must be true or the right hand side must be true. You can use parentheses to override the rules of precedence, as in the following example which specifies that two conditions must be true.

    NR > 1 && (NF >= 2 || $1 ~ /\t/)

The first condition must be true and either of two other conditions must be true.

Given an expression that is either true or false, the ! operator inverts the sense of the expression.

    ! (NR > 1 && NF > 3)

This expression is true if the parenthesized expression is false. This operator is most useful with awk's in operator to see if an index is not in an array (as we shall see later), although it has other uses as well.

		5.8.3 Getting Information About Files
		First example, pipe the output of ls to an awk script that prints selected fields.

    ls -l $* | awk 'script'

The $* variable is used by the shell and expands to all arguments passed from the command line. (We could use $1 here, which would pass the first argument, but passing all the arguments provides greater flexibility.) These arguments can be the names of files or directories or additional options to the ls command. If no arguments are specified, the "$*" will be empty and the current directory will be listed. Thus, the output of the ls command will be directed to awk, which will automatically read standard input, since no filenames have been given.

We'd like our awk script to print the size and name of the file. That is, print field 5 ($5) and field 9 ($9).

    ls -l $* | awk '{ 
    	print $5, "\t", $9
    }'

	usage:
	$fls
	$fls -rt
	$fls com*

	So what our program does is take the long listing and reduce it to two fields. Now, let's add new functionality to our report by producing some information that the ls -l listing does not provide. We add each file's size to a running total, to produce the total number of bytes used by all files in the listing. We can also keep track of the number of files and produce that total. There are two parts to adding this functionality. The first is to accumulate the totals for each input line. We create the variable sum to accumulate the size of files and the variable filenum to accumulate the number of files in the listing.

	    {
    	sum += $5
    	++filenum
    	print $5, "\t", $9 
    }

The first expression uses the assignment operator +=. It adds the value of field 5 to the present value of the variable sum. The second expression increments the present value of the variable filenum. This variable is used as a counter, and each time the expression is evaluated, 1 is added to the count.

The action we've written will be applied to all input lines. The totals that are accumulated in this action must be printed after awk has read all the input lines. Therefore, we write an action that is controlled by the END rule.

    END { print "Total: ", sum, "bytes (" filenum " files)" }

We can also use the BEGIN rule to add column headings to the report.

    BEGIN { print "BYTES", "\t", "FILE" }

Now we can put this script in an executable file named filesum and execute it as a single-word command.

    $ filesum c*

    While the basic mechanism works, there are a few problems to be taken care of. The first problem occurs when you list the entire directory using the ls -l command. The listing contains a line that specifies the total number of blocks in the directory. The partial listing (all files beginning with "c") in the previous example does not have this line. But the following line would be included in the output if the full directory was listed:

    total 555

The block total does not interest us because the program displays the total file size in bytes. Currently, filesum does not print this line; however, it does read this line and cause the filenum counter to be incremented.

There is also a problem with this script in how it handles subdirectories. Look at the following line from an ls -l:

    drwxrwxrwx   3 dale     project         960 Feb  1 15:47 sed

A "d" as the first character in column 1 (file permissions) indicates that the file is a subdirectory. The size of this file (960 bytes) does not indicate the size of files in that subdirectory and therefore, it is slightly misleading to add it to the file size totals. Also, it might be helpful to indicate that it is a directory.

If you want to list the files in subdirectories, supply the -R (recursive) option on the command line. It will be passed to the ls command. However, the listing is slightly different as it identifies each directory. For instance, to identify the subdirectory old, the ls -lR listing produces a blank line followed by:

    ./old:

Our script ignores that line and a blank line preceding it but nonetheless they increment the file counter. Fortunately, we can devise rules to handle these cases. Let's look at the revised, commented script:

    ls -l $* | awk '
    # filesum: list files and total size in bytes
    # input: long listing produced by "ls -l"

    #1 output column headers
    BEGIN { print "BYTES", "\t", "FILE" }

    #2 test for 9 fields; files begin with "-"
    NF == 9 && /^-/ {
            sum += $5       # accumulate size of file
            ++filenum       # count number of files
            print $5, "\t", $9       # print size and filename
    }

    #3 test for 9 fields; directory begins with "d"
    NF == 9 && /^d/ {
            print "<dir>", "\t", $9  # print <dir> and name
    }

    #4 test for ls -lR line ./dir:
    $1 ~ /^\..*:$/ {
            print "\t" $0 # print that line preceded by tab
    }

    #5 once all is done,
    END {
    	# print total file size and number of files
    	print "Total: ", sum, "bytes (" filenum " files)"
    }'

    The rules and their associated actions have been numbered to make it easier to discuss them. The listing produced by ls -l contains nine fields for a file. Awk supplies the number of fields for a record in the system variable NF. Therefore, rules 2 and 3 test that NF is equal to 9. This helps us avoid matching odd blank lines or the line stating the block total. Because we want to handle directories and files differently, we use another pattern to match the first character of the line. In rule 2 we test for "-" in the first position on the line, which indicates a file. The associated action increments the file counter and adds the file size to the previous total. In rule 3, we test for a directory, indicated by "d" as the first character. The associated action prints "<dir>" in place of the file size. Rules 2 and 3 are compound expressions, specifying two patterns that are combined using the && operator. Both patterns must be matched for the expression to be true.

Rule 4 tests for the special case produced by the ls -lR listing ("./old:"). There are a number of patterns that we can write to match that line, using regular expressions or relational expressions:

    NF == 1			If the number of fields equals 1 ...
    /^\..*:$/		If the line begins with a period followed by any number of
                                                       characters and ends in a colon...
    $1 ~ /^\..*:$/		If field 1 matches the regular expression...

We used the latter expression because it seems to be the most specific. It employs the match operator (~) to test the first field against a regular expression. The associated action consists of only a print statement.

Rule 5 is the END pattern and its action is only executed once, printing the sum of file sizes as well as the number of files.

The filesum program demonstrates many of the basic constructs used in awk. What's more, it gives you a pretty good idea of the process of developing a program (although syntax errors produced by typos and hasty thinking have been gracefully omitted). If you wish to tinker with this program, you might add a counter for a directories, or a rule that handles symbolic links.

	5.9 Formatted Printing
	Awk offers an alternative to the print statement, printf, which is borrowed from the C programming language. The printf statement can output a simple string just like the print statement.

    awk 'BEGIN { printf ("Hello, world\n") }'

The main difference that you will notice at the outset is that, unlike print, printf does not automatically supply a newline. You must specify it explicitly as "\n".

The full syntax of the printf statement has two parts:

    printf ( format-expression [, arguments] )

The parentheses are optional. The first part is an expression that describes the format specifications; usually this is supplied as a string constant in quotes. The second part is an argument list, such as a list of variable names, that correspond to the format specifications. A format specification is preceded by a percent sign (%) and the specifier is one of the characters shown in the next table. The two main format specifiers are s for strings and d for decimal integers.

	character 	Description
	c		ASCII character
	d 		Decimal integer
	i 		Decimal integer. (Added in POSIX)
	e 		Floating-point format ([-]d.precisione[+-]dd)
	E 		Floating-point format ([-]d.precisionE[+-]dd)
	f 		Floating-point format ([-]ddd.precision)
	g 		e or f conversion, whichever is shortest, with trailing zeros removed
	G 		E or f conversion, whichever is shortest, with trailing zeros removed
	o 		Unsigned octal value
	s 		String
	x 		Unsigned hexadecimal number. Uses a-f for 10 to 15
	X 		Unsigned hexadecimal number. Uses A-F for 10 to 15
	% 		Literal %

	output a string and a decimal value found in two different fields:
	printf("%d\t%s\n", $5, $9)
	The value of $5 is to be output, followed by a tab (\t) and $9 and then a newline (\n)

	his printf statement can be used to specify the width and alignment of output fields. A format expression can take three optional modifiers following "%" and preceding the format specifier:

    %-width.precision format-specifier

The width of the output field is a numeric value. When you specify a field width, the contents of the field will be right-justified by default. You must specify "-" to get left-justification. Thus, "%-20s" outputs a string left-justified in a field 20 characters wide. If the string is less than 20 characters, the field will be padded with whitespace to fill. In the following examples, a "|" is output to indicate the actual width of the field. The first example right-justifies the text:

    printf("|%10s|\n", "hello")

It produces:

    |     hello|

The next example left-justifies the text:

    printf("|%-10s|\n", "hello")

It produces:

    |hello     |

The precision modifier, used for decimal or floating-point values, controls the number of digits that appear to the right of the decimal point. For string values, it controls the maximum number of characters from the string that will be printed. Note that the default precision for the output of numeric values is "%.6g".

You can specify both the width and precision dynamically, via values in the printf or sprintf argument list. You do this by specifying asterisks, instead of literal values.

    printf("%*.*g\n", 5, 3, myvar);

In this example, the width is 5, the precision is 3, and the value to print will come from myvar.

The default precision used by the print statement when outputting numbers can be changed by setting the system variable OFMT. For instance, if you are using awk to write reports that contain dollar values, you might prefer to change OFMT to "%.2f".

Using the full syntax of the format expression can solve the problem with filesum of getting fields and headings properly aligned. One reason we output the file size before the filename was that the fields had a greater chance of aligning themselves if they were output in that order. The solution that printf offers us is the ability to fix the width of output fields; therefore, each field begins in the same column.

	5.10 Passing Parameters Into a Script

	5.11 Example
	
	One of the more confusing subtleties of programming in awk is passing parameters into a script. A parameter assigns a value to a variable that can be accessed within the awk script. The variable can be set on the command line, after the script and before the filename.

    awk 'script' var=value inputfile

Each parameter must be interpreted as a single argument. Therefore, spaces are not permitted on either side of the equal sign. Multiple parameters can be passed this way. For instance, if you wanted to define the variables high and low from the command line, you could invoke awk as follows:

    $ awk -f scriptfile high=100 low=60 datafile

Inside the script, these two variables are available and can be accessed as any awk variable. If you were to put this script in a shell script wrapper, then you could pass the shell's command-line arguments as values. (The shell makes available command-line arguments in the positional variables--$1 for the first parameter, $2 for the second, and so on.)[52] For instance, look at the shell script version of the previous command:

    [52]Careful! Don't confuse the shell's parameters with awk's field variables.

    awk -f scriptfile "high=$1" "low=$2" datafile

If this shell script were named awket, it could be invoked as:

    $ awket 100 60

"100" would be $1 and passed as the value assigned to the variable high.

In addition, environment variables or the output of a command can be passed as the value of a variable. Here are two examples:

    awk '{ ... }' directory=$cwd file1 ...
    awk '{ ... }' directory=`pwd` file1 ...

"$cwd" returns the value of the variable cwd, the current working directory (csh only). The second example uses backquotes to execute the pwd command and assign its result to the variable directory (this is more portable).

You can also use command-line parameters to define system variables, as in the following example:

#    $ awk '{ print NR, $0 }' OFS='. ' names
#    1. Tom 656-5789
#    2. Dale 653-2133
#    3. Mary 543-1122
#    4. Joe 543-2211

The output field separator is redefined to be a period followed by a space.

An important restriction on command-line parameters is that they are not available in the BEGIN procedure. That is, they are not available until after the first line of input is read. Why? Well, here's the confusing part. A parameter passed from the command line is treated as though it were a filename. The assignment does not occur until the parameter, if it were a filename, is actually evaluated.

Look at the following script that sets a variable n as a command-line parameter.

    awk  'BEGIN { print n }
    {
    if (n == 1) print "Reading the first file"
    if (n == 2) print "Reading the second file"
    }' n=1 test n=2 test2

There are four command-line parameters: "n=1," "test," "n=2," and "test2". Now, if you remember that a BEGIN procedure is "what we do before processing input," you'll understand why the reference to n in the BEGIN procedure returns nothing. So the print statement will print a blank line. If the first parameter were a file and not a variable assignment, the file would not be opened until the BEGIN procedure had been executed.

The variable n is given an initial value of 1 from the first parameter. The second parameter supplies the name of the file. Thus, for each line in test, the conditional "n == 1" will be true. After the input is exhausted from test, the third parameter is evaluated, and it sets n to 2. Finally, the fourth parameter supplies the name of a second file. Now the conditional "n == 2" in the main procedure will be true.

One consequence of the way parameters are evaluated is that you cannot use the BEGIN procedure to test or verify parameters that are supplied on the command line. They are available only after a line of input has been read. You can get around this limitation by composing the rule "NR == 1" and using its procedure to verify the assignment. Another way is to test the command-line parameters in the shell script before invoking awk.

POSIX awk provides a solution to the problem of defining parameters before any input is read. The -v option[53] specifies variable assignments that you want to take place before executing the BEGIN procedure (i.e., before the first line of input is read.) The -v option must be specified before a command-line script. For instance, the following command uses the -v option to set the record separator for multiline records.

    [53]The -v option was not part of the original (1987) version of nawk (still used on SunOS 4.1.x systems and some System V Release 3.x systems). It was added in 1989 after Brian Kernighan of Bell Labs, the GNU awk authors, and the authors of MKS awk agreed on a way to set variables on the command line that would be available inside the BEGIN block. It is now part of the POSIX specification for awk.

    $ awk -F"\n" -v RS="" '{ print }' phones.block

A separate -v option is required for each variable assignment that is passed to the program.

Awk also provides the system variables ARGC and ARGV

	5.12  Information Retrieval
		5.12.1 Basic Example
	An awk program can be used to retrieve information from a database, the database basically being any kind of text file. The more structured the text file, the easier it is to work with, although the structure might be no more than a line consisting of individual words.

The list of acronyms below is a simple database.

    $ cat acronyms
    BASIC	Beginner's All-Purpose Symbolic Instruction Code
    CICS	Customer Information Control System
    COBOL	Common Business Oriented Language
    DBMS	Data Base Management System
    GIGO	Garbage In, Garbage Out
    GIRL 	Generalized Information Retrieval Language

A tab is used as the field separator. We're going to look at a program that takes an acronym as input and displays the appropriate line from the database as output. (In the next chapter, we're going to look at two other programs that use the acronym database. One program reads the list of acronyms and then finds occurrences of these acronyms in another file. The other program locates the first occurrence of these acronyms in a text file and inserts the description of the acronym.)

The shell script that we develop is named acro. It takes the first argument from the command line (the name of the acronym) and passes it to the awk script. The acro script follows:

    $ cat acro
    #! /bin/sh
    # assign shell's $1 to awk search variable
    awk '$1 == search' search=$1 acronyms

The first argument specified on the shell command line ($1) is assigned to the variable named search; this variable is passed as a parameter into the awk program. Parameters passed to an awk program are specified after the script section. (This gets somewhat confusing, because $1 inside the awk program represents the first field of each input line, while $1 in the shell represents the first argument supplied on the command line.)

The example below demonstrates how this program can be used to find a particular acronym on our list.

    $ acro CICS
    CICS Customer Information Control System

Notice that we tested the parameter as a string ($1 == search). We could also have written this as a regular expression match ($1 ~ search).

		5.12.2 Advance example, print a block of text that contains a certain pattern

	5.13 Conditionals, Loops, and Arrays

		5.13.1 Conditions

		if ( expression )
		   action1
		[else
		   action2]

		   if ( expression ) { 
			statement1
			statement2
		  }

		    if ( expression ) action1
		    [else action2]

		    if ( expression ) action1; [else action2]

		    if ( x == y ) print x

		    if ( x ~ /[yY](es)?/ ) print x

		Multiple conditional statements can be used to test whether one of several possible conditions is true. For example, perhaps the students are given a letter grade instead of a pass-fail mark. Here's a conditional that assigns a letter grade based on a student's average:

		    if (avg >= 90)  grade = "A"
		    else if (avg >= 80) grade = "B"
		    else if (avg >= 70) grade = "C"
		    else if (avg >= 60) grade = "D"
		    else grade = "F"

		The important thing to recognize is that successive conditionals like this are evaluated until one of them returns true; once that occurs, the rest of the conditionals are skipped. If none of the conditional expressions evaluates to true, the last else is accepted, constituting the default action; in this case, it assigns "F" to grade.

			Awk provides a conditional operator that is found in the C programming language. Its form is:

    expr ? action1 : action2

The previous simple if/else condition can be written using a conditional operator:

    grade = (avg >= 65) ? "Pass" : "Fail"

This form has the advantage of brevity and is appropriate for simple conditionals such as the one shown here.

		5.13.2 Loops

		    while (condition) 
			action

			do
				action
			while (condition)

		    for ( set_counter ; test_counter ; increment_counter ) 
			action

				5.13.2.1 Examples

				Derive factorial
				awk '# factorial: return factorial of user-supplied number
				BEGIN {
					# prompt user; use printf, not print, to avoid the newline
					printf("Enter number: ")
				}

# check that user enters a number
				$1 ~ /^[0-9]+$/ { 
					# assign value of $1 to number & fact
					number = $1
					if (number == 0)
						fact = 1
					else
						fact = number
					# loop to multiply fact*x until x = 1
					for (x = number - 1; x > 1; x--)
						fact *= x
					printf("The factorial of %d is %g\n", number, fact)
					# exit -- saves user from typing CRTL-D.
					exit
				}

# if not a number, prompt again.
				{ printf("\nInvalid entry. Enter a number: ") 
				}' -
		
				This is an interesting example of a main input loop that prompts for input and reads the reply from standard input. The BEGIN rule is used to prompt the user to enter a number. Because we have specified that input is to come not from a file but from standard input, the program will halt after putting out the prompt and then wait for the user to type a number. The first rule checks that a number has been entered. If not, the second rule will be applied, prompting the user again to re-enter a number. We set up an input loop that will continue to read from standard input until a valid entry is found. See the lookup program in the next section for another example of constructing an input loop.

		5.13.3 Other Statements That Affect Flow Control

			5.13.3.1 break, continue
			The if, while, for, and do statements allow you to change the normal flow through a procedure. In this section, we look at several other statements that also affect a change in flow control.

There are two statements that affect the flow control of a loop, break and continue. The break statement, as you'd expect, breaks out of the loop, such that no more iterations of the loop are performed. The continue statement stops the current iteration before reaching the bottom of the loop and starts a new iteration at the top

			5.13.3.2 main input loop. next, exit.

			There are two statements that affect the main input loop, next and exit. The next statement causes the next line of input to be read and then resumes execution at the top of the script. This allows you to avoid applying other procedures on the current input line. A typical use of the next statement is to continue reading input from a file, ignoring the other actions in the script until that file is exhausted. The system variable FILENAME provides the name of the current input file. Thus, a pattern can be written:

			    FILENAME == "acronyms" {
					action
					next
				    }
				    { print }

This causes the action to be performed for each line in the file acronyms. After the action is performed, the next line of input is read. Control does not pass to the print statement until the input is taken from a different source.

The exit statement exits the main input loop and passes control to the END rule, if there is one. If the END rule is not defined, or the exit statement is used in the END rule, then the script terminates. We used the exit statement earlier in the factorial program to exit after reading one line of input.

An exit statement can take an expression as an argument. The value of this expression will be returned as the exit status of awk. If the expression is not supplied, the exit status is 0. If you supply a value to an initial exit statement, and then call exit again from the END rule without a value, the first value is used. For example:

    awk '{
    	...
    	exit 5
    }
    END { exit }'

Here, the exit status from awk will be 5.

		5.13.4 Arrays
	An array is a variable that can be used to store a set of values. Usually the values are related in some way. Individual elements are accessed by their index in the array. Each index is enclosed in square brackets. The following statement assigns a value to an element of an array:

    array[subscript] = value

In awk, you don't have to declare the size of the array; you only have to use the identifier as an array. This is best done by assigning a value to an array element. For instance, the following example assigns the string "cherry" to an element of the array named flavor.

    flavor[1] = "cherry"

The index or subscript of this element of the array is "1". The following statement prints the string "cherry":

    print flavor[1]

Loops can be used to load and extract elements from arrays. For instance, if the array flavor has five elements, you can write a loop to print each element:

    flavor_count = 5
    for (x = 1; x <= flavor_count; ++x)
    	print flavor[x]

One way that arrays are used in awk is to store a value from each record, using the record number as the index to the array. Let's suppose we wanted to keep track of the averages calculated for each student and come up with a class average. Each time a record is read we make the following assignment.

    student_avg[NR] = avg

The system variable NR is used as the subscript for the array because it is incremented for each record. When the first record is read, the value of avg is placed in student_avg[1]; for the second record, the value is placed in student_avg[2], and so on. After we have read all of the records, we have a list of averages in the array student_avg. In an END rule, we can average all of these grades by writing a loop to get the total of the grades and then dividing it by the value of NR. Then we can compare each student average to the class average to collect totals for the number of students at or above average and the number below.

    END {
    	for ( x = 1; x <= NR; x++ ) 
    		class_avg_total += student_avg[x]
    	
    	class_average = class_avg_total / NR
    	
    	for ( x = 1; x <= NR; x++ )
    		if (student_avg[x] >= class_average)
    			++above_average
    		else
    			++below_average
    	
    	print "Class Average: ", class_average
    	print "At or Above Average: ", above_average
    	print "Below Average: ", below_average
    }

There are two for loops for accessing the elements of the array. The first one totals the averages so that it can be divided by the number of student records. The next loop retrieves each student average so that it can be compared to the class average. If it is at or above average, we increment the variable above_average; otherwise, we increment below_average.


		5.13.5 Associative Arrays

In awk, all arrays are associative arrays. What makes an associative array unique is that its index can be a string or a number.

In most programming languages, the indices of arrays are exclusively numeric. In these implementations, an array is a sequence of locations where values are stored. The indices of the array are derived from the order in which the values are stored. There is no need to keep track of indices. For instance, the index of the first element of an array is "1" or the first location in the array.

An associative array makes an "association" between the indices and the elements of an array. For each element of the array, a pair of values is maintained: the index of the element and the value of the element. The elements are not stored in any particular order as in a conventional array. Thus, even though you can use numeric subscripts in awk, the numbers do not have the same meaning that they do in other programming languages--they do not necessarily refer to sequential locations. However, with numeric indices, you can still access all the elements of an array in sequence, as we did in previous examples. You can create a loop to increment a counter that references the elements of the array in order.

Sometimes, the distinction between numeric and string indices is important. For instance, if you use "04" as the index to an element of the array, you cannot reference that element using "4" as its subscript. You'll see how to handle this problem in a sample program date-month, shown later in this chapter.

Associative arrays are a distinctive feature of awk, and a very powerful one that allows you to use a string as an index to another value. For instance, you could use a word as the index to its definition. If you know the word, you can retrieve the definition.

For example, you could use the first field of the input line as the index to the second field with the following assignment:

    array[$1] = $2

Using this technique, we could take our list of acronyms and load it into an array named acro.

    acro[$1] = $2

Each element of the array would be the description of an acronym and the subscript used to retrieve the element would be the acronym itself. The following expression:

    acro["BASIC"]

produces:

    Beginner's All-Purpose Symbolic Instruction Code

There is a special looping syntax for accessing all the elements of an associative array. It is a version of the for loop.

    for ( variable in array )
         do something with array[variable]

The array is the name of an array, as it was defined. The variable is any variable, which you can think of as a temporary variable similar to a counter that is incremented in a conventional for loop. This variable is set to a particular subscript each time through the loop. (Because variable is an arbitrary name, you often see item used, regardless of what variable name was used for the subscript when the array was loaded.) For example, the following for loop prints the name of the acronym item and the definition referenced by that name, acro[item].

    for ( item in acro )
    	print item, acro[item]

In this example, the print statement prints the current subscript ("BASIC," for instance) followed by the element of the acro array referenced by the subscript ("Beginner's All-Purpose Symbolic Instruction Code").

This syntax can be applied to arrays with numeric subscripts. However, the order in which the items are retrieved is somewhat random.[56] The order is very likely to vary among awk implementations; be careful to write your programs so that they don't depend on any one version of awk.

    [56]The technical term used in The AWK Programming Language is "implementation dependent."

It is important to remember that all array indices in awk are strings. Even when you use a number as an index, awk automatically converts it to a string first. You don't have to worry about this when you use integer indices, since they get converted to strings as integers, no matter what the value may be of OFMT (original awk and earlier versions of new awk) or CONVFMT (POSIX awk). But if you use a real number as an index, the number to string conversion might affect you. For instance:

    $ gawk 'BEGIN { data[1.23] = "3.21"; CONVFMT = "%d"
    > printf "<%s>\n", data[1.23] }'
    <>

Here, nothing was printed between the angle brackets, since the second time, 1.23 was converted to just 1, and data["1"] has the empty string as its value.

    NOTE: Not all implementations of awk get the number to string conversion right when CONVFMT has changed between one use of a number and the next. Test the above example with your awk to be sure it works correctly.

Now let's return to our student grade program for an example. Let's say that we wanted to report how many students got an "A," how many got a "B," and so on. Once we determine the grade, we could increment a counter for that grade. We could set up individual variables for each letter grade and then test which one to increment.

    if ( grade == "A" )
    	++gradeA
    else if (grade == "B" )
    	++gradeB
    .
    .
    .

However, an array makes this task much easier. We can define an array called class_grade, and simply use the letter grade (A through F) as the index to the array.

    ++class_grade[grade]

Thus, if the grade is an "A" then the value of class_grade["A"] is incremented by one. At the end of the program, we can print out these values in the END rule using the special for loop:

    for (letter_grade in class_grade)
         print letter_grade ":", class_grade[letter_grade] | "sort"

The variable letter_grade references a single subscript of the array class_grade each time through the loop. The output is piped to sort, to make sure the grades come out in the proper order. (Piping output to programs is discussed in Chapter 10, "The Bottom Drawer".) Since this is the last addition we make to the grades.awk script, we can look at the full listing.

    # grades.awk -- average student grades and determine 
    # letter grade as well as class averages.
    # $1 = student name; $2 - $NF = test scores.

    # set output field separator to tab.
    BEGIN { OFS = "\t" }

    # action applied to all input lines
    { 
      # add up grades
    	total = 0
    	for (i = 2; i <= NF; ++i)
    		total += $i 
      # calculate average
    	avg = total / (NF - 1)
      # assign student's average to element of array
    	student_avg[NR] = avg
      # determine letter grade
    	if (avg >= 90)  grade = "A"
    	else if (avg >= 80) grade = "B"
    	else if (avg >= 70) grade = "C"
    	else if (avg >= 60) grade = "D"
    	else grade = "F"	
      # increment counter for letter grade array
    	++class_grade[grade]
      # print student name, average and letter grade
    	print $1, avg, grade 
    }
    # print out class statistics
    END {
      # calculate class average
    	for (x = 1; x <= NR; x++)
    		class_avg_total += student_avg[x]
    	class_average = class_avg_total / NR
      # determine how many above/below average
    	for (x = 1; x <= NR; x++)
    		if (student_avg[x] >= class_average)
    			++above_average
    		else
    			++below_average
      # print results
    	print ""
    	print "Class Average: ", class_average
    	print "At or Above Average: ", above_average
    	print "Below Average: ", below_average     
      # print number of students per letter grade
    	for (letter_grade in class_grade)
    		print letter_grade ":", class_grade[letter_grade] | "sort"
    }

Here's a sample run:

    $ cat grades.test
    mona 70 77 85 83 70 89
    john 85 92 78 94 88 91
    andrea 89 90 85 94 90 95
    jasper 84 88 80 92 84 82
    dunce 64 80 60 60 61 62
    ellis 90 98 89 96 96 92
    $ awk -f grades.awk grades.test
    mona    79      C
    john    88      B
    andrea  90.5    A
    jasper  85      B
    dunce   64.5    D
    ellis   93.5    A

    Class Average:  83.4167
    At or Above Average:    4
    Below Average:  2
    A:      2
    B:      2
    C:      1
    D:      1

		5.13.6 Testing for Membership in an Array

The keyword in is also an operator that can be used in a conditional expression to test that a subscript is a member of an array. The expression:

    item in array

returns 1 if array[item] exists and 0 if it does not. For example, the following conditional statement is true if the string "BASIC" is a subscript of the array acro.

    if ( "BASIC" in acro )
    	print "Found BASIC"

This is true if "BASIC" is a subscript used to access an element of acro. This syntax cannot tell you whether "BASIC" is the value of an element of acro. This expression is the same as writing a loop to check that such a subscript exists, although the above expression is much easier to write, and much more efficient to execute.

		5.13.7 A Glossary Lookup Script
This program reads a series of glossary entries from a file named glossary and puts them into an array. The user is prompted to enter a glossary term and if it is found, the definition of the term is printed.

Here's the lookup program:

    awk '# lookup -- reads local glossary file and prompts user for query

    #0
    BEGIN { FS = "\t"; OFS = "\t"
    	# prompt user
    	printf("Enter a glossary term: ")
    } 

    #1 read local file named glossary
    FILENAME == "glossary" {
    	# load each glossary entry into an array
    	entry[$1] = $2
    	next
    } 

    #2 scan for command to exit program
    $0 ~ /^(quit|[qQ]|exit|[Xx])$/ { exit }

    #3 process any non-empty line 
    $0 != "" {
    	if ( $0 in entry ) {
    		# it is there, print definition
    		print entry[$0]
    	} else
    		print $0 " not found"
    }

    #4 prompt user again for another term
    {
    	printf("Enter another glossary term (q to quit): ")
    }' glossary -

The pattern-matching rules are numbered to make this discussion easier. As we look at the individual rules, we'll discuss them in the order in which they are encountered in the flow of the script. Rule #0 is the BEGIN rule, which is performed only once before any input is read. It sets FS and OFS to a tab and then prompts the user to enter a glossary item. The response will come from standard input, but that is read after the glossary file.

Rule #1 tests to see if the current filename (the value of FILENAME) is "glossary" and is therefore only applied while reading input from this file. This rule loads the glossary entries into an array:

    entry[term] = definition

where $1 is the term and $2 is the definition. The next statement at the end of rule #1 is used to skip other rules in the script and causes a new line of input to be read. So, until all the entries in the glossary file are read, no other rule is evaluated.

Once input from glossary is exhausted, awk reads from standard input because "-" is specified on the command line. Standard input is where the user's response comes from. Rule #3 tests that the input line ($0) is not empty. This rule should match whatever the user types. The action uses in to see if the input line is an index in the array. If it is, it simply prints out the corresponding value. Otherwise, we tell the user that no valid entry was found.

After rule #3, rule #4 will be evaluated. This rule simply prompts the user for another entry. Note that regardless of whether a valid entry was processed in rule #3, rule #4 is executed. The prompt also tells the user how to quit the program. After this rule, awk looks for the next line of input.

If the user chooses to quit by entering "q" as the next line of input, rule #2 will be matched. The pattern looks for a complete line consisting of alternative words or single letters that the user might enter to quit. The "^" and "$" are important, signifying that the input line contains no other characters but these; otherwise a "q" appearing in a glossary entry would be matched. Note that the placement of this rule in the sequence of rules is significant. It must appear before rules #3 and #4 because these rules will match anything, including the words "quit" and "exit."

Let's look at how the program works. For this example, we will make a copy of the acronyms file and use it as the glossary file.

    $ cp acronyms glossary
    $ lookup
    Enter a glossary term: GIGO
    Garbage in, garbage out
    Enter another glossary term (q to quit): BASIC
    Beginner's All-Purpose Symbolic Instruction Code
    Enter another glossary term (q to quit): q 

As you can see, the program is set up to prompt the user for additional items until the user enters "q".

Note that this program can be easily revised to read a glossary anywhere on the file system, including the user's home directory. The shell script that invokes awk could handle command-line options that allow the user to specify the glossary filename. You could also read a shared glossary file and then read a local one by writing separate rules to process the entries.

		5.13.8 Using split() to Create Arrays
	The built-in function split() can parse any string into elements of an array. This function can be useful to extract "subfields" from a field. The syntax of the split() function is:

    n = split(string, array, separator)

string is the input string to be parsed into elements of the named array. The array's indices start at 1 and go to n, the number of elements in the array. The elements will be split based on the specified separator character. If a separator is not specified, then the field separator (FS) is used. The separator can be a full regular expression, not just a single character. Array splitting behaves identically to field splitting;

For example, if you had a record in which the first field consisted of the person's full name, you could use the split() function to extract the person's first and last names. The following statement breaks up the first field into elements of the array fullname:

    z = split($1, fullname, " ")

A space is specified as the delimiter. The person's first name can be referenced as:

    fullname[1]

and the person's last name can be referenced as:

    fullname[z]

because z contains the number of elements in the array. This works, regardless of whether the person's full name contains a middle name. If z is the value returned by split(), you can write a loop to read all the elements of this array.

    z = split($1, array, " ")
    for (i = 1; i <= z; ++i)
    	print i, array[i]
		
		5.13.9 Making Conversions

		This shell script takes the first argument from the command line and echoes it as input to the awk program.

    echo $1 | 
    awk '# romanum -- convert number 1-10 to roman numeral

    # define numerals as list of roman numerals 1-10
    BEGIN { 
    	# create array named numerals from list of roman numerals
    	split("I,II,III,IV,V,VI,VII,VIII,IX,X", numerals, ",")
    }

    # look for number between 1 and 10
    $1 > 0 && $1 <= 10 {
    	# print specified element
    	print numerals[$1]
    	exit
    }

    { 	print "invalid number"
      	exit
    }'

This script defines a list of 10 roman numerals, then uses split() to load them into an array named numerals. This is done in the BEGIN action because it only needs to be done once.

The second rule checks that the first field of the input line contains a number between 1 and 10. If it does, this number is used as the index to the numerals array, retrieving the corresponding element. The exit statement terminates the program. The last rule is executed only if there is no valid entry.

Here's an example of how it works:

    $ romanum 4
    IV

ollowing along on the same idea, here's a script that converts dates in the form "mm-dd-yy" or "mm/dd/yy" to "month day, year."

    awk '
    # date-month -- convert mm/dd/yy or mm-dd-yy to month day, year

    # build list of months and put in array. 
    BEGIN { 
    	# the 3-step assignment is done for printing in book
    	listmonths = "January,February,March,April,May,June,"
    	listmonths = listmonths "July,August,September,"
    	listmonths = listmonths "October,November,December" 
    	split(listmonths, month, ",")
    }

    # check that there is input
    $1 != "" {

    # split on "/" the first input field into elements of array
    	sizeOfArray = split($1, date, "/")

    # check that only one field is returned
    	if (sizeOfArray == 1)
    		# try to split on "-"
    		sizeOfArray = split($1, date, "-")

    # must be invalid
    	if (sizeOfArray == 1)
    		exit

    # add 0 to number of month to coerce numeric type 
    	date[1] += 0

    # print month day, year
    	print month[date[1]], (date[2] ", 19" date[3])
    }'

This script reads from standard input. The BEGIN action creates an array named month whose elements are the names of the months of the year. The second rule verifies that we have a non-empty input line. The first statement in the associated action splits the first field of input looking for "/" as the delimiter. sizeOfArray contains the number of elements in the array. If awk was unable to parse the string, it creates the array with only one element. Thus, we can test the value of sizeOfArray to determine if we have several elements. If we do not, we assume that perhaps "-" was used as the delimiter. If that fails to produce an array with multiple elements, we assume the input is invalid, and exit. If we have successfully parsed the input, date[1] contains the number of the month. This value can be used as the index to the array month, nesting one array inside another. However, before using date[1], we coerce the type of date[1] by adding 0 to it. While awk will correctly interpret "11" as a number, leading zeros may cause a number to be treated as a string. Thus, "06" might not be recognized properly without type coercion. The element referenced by date[1] is used as the subscript for month.

Here's a sample run:

    $ echo "5/11/55" | date-month
    May 11, 1955


		5.13.10 Deleting Elements of an Array

Awk provides a statement for deleting an element of an array. The syntax is:

    delete array[subscript]

The brackets are required. This statement removes the element indexed by subscript from array. In particular, the in test for subscript will now return false. This is different than just assigning the empty string to that element; in that case in would still be true. See the lotto script in the next chapter for an example of using the delete statement.

	5.14 An Acronym Processor
	a program that scans a file for acronyms. Each acronym is replaced with a full text description, and the acronym in parentheses. If a line refers to "BASIC," we'd like to replace it with the description "Beginner's All-Purpose Symbolic Instruction Code" and put the acronym in parentheses afterwards. (This is probably not a useful program in and of itself, but the techniques used in the program are general and have many such uses.)

	a program that scans a file for acronyms. Each acronym is replaced with a full text description, and the acronym in parentheses. If a line refers to "BASIC," we'd like to replace it with the description "Beginner's All-Purpose Symbolic Instruction Code" and put the acronym in parentheses afterwards. (This is probably not a useful program in and of itself, but the techniques used in the program are general and have many such uses.)

	We can design this program for use as a filter that prints all lines, regardless of whether a change has been made. We'll call it awkro.

    awk '# awkro - expand acronyms 
    # load acronyms file into array "acro"
    FILENAME == "acronyms" {
    	split($0, entry, "\t")
    	acro[entry[1]] = entry[2]
    	next
    } 

    # process any input line containing caps 
    /[A-Z][A-Z]+/ {

    	# see if any field is an acronym
    	for (i = 1; i <= NF; i++)
    		if ( $i in acro ) {
    			# if it matches, add description 
    			$i = acro[$i] " (" $i ")"
    		}
    }

    {
    	# print all lines
    	print $0
    }' acronyms  $*

Let's first see it in action. Here's a sample input file.

    $ cat sample
    The USGCRP is a comprehensive 
    research effort that includes applied 
    as well as basic research.
    The NASA program Mission to Planet Earth 
    represents the principal space-based component
    of the USGCRP and includes new initiatives
    such as EOS and Earthprobes.

And here is the file acronyms:

    $ cat acronyms
    USGCRP	U.S. Global Change Research Program
    NASA	National Aeronautic and Space Administration
    EOS	Earth Observing System

Now we run the program on the sample file.

    $ awkro sample
    The U.S. Global Change Research Program (USGCRP) is a comprehensive
    research effort that includes applied
    as well as basic research.
    The National Aeronautic and Space Administration (NASA) program
    Mission to Planet Earth
    represents the principal space-based component
    of the U.S. Global Change Research Program (USGCRP) and includes new
    initiatives
    such as Earth Observing System (EOS) and Earthprobes.

We'll look at this program in two parts. The first part reads records from the acronyms file.

    # load acronyms file into array "acro"
    FILENAME == "acronyms" {
    	split($0, entry, "\t")
    	acro[entry[1]] = entry[2]
    	next
    }

The two fields from these records are loaded into an array using the first field as the subscript and assigning the second field to an element of the array. In other words, the acronym itself is the index to its description.

Note that we did not change the field separator, but instead used the split() function to create the array entry. This array is then used in creating an array named acro.

Here is the second half of the program:

    # process any input line containing caps 
    /[A-Z][A-Z]+/ {
    	# see if any field is an acronym
    	for (i = 1; i <= NF; i++)
    		if ( $i in acro ) {
    			acronym =$i 
    			# if it matches, add description 
    			$i = acro[$i] " (" $i ")"
    		}
    }

    {
    	# print all lines
    	print $0
    }

Only lines that contain more than one consecutive capital letter are processed by the first of the two actions shown here. This action loops through each field of the record. At the heart of this section is the conditional statement that tests if the current field ($i) is a subscript of the array (acro). If the field is a subscript, we replace the original value of the field with the array element and the original value in parentheses. (Fields can be assigned new values, just like regular variables.) Note that the insertion of the description of the acronym results in lines that may be too long. See the next chapter for a discussion of the length() function, which can be used to determine the length of a string so you can divide it up if it is too long.

Now we're going to change the program so it makes a replacement only the first time an acronym appears. After we've found it, we don't want to search for that acronym any more. This is easy to do; we simply delete that acronym from the array.

    if ( $i in acro ) {
    	# if it matches, add description 
    	$i = acro[$i] " (" $i ")"
    	# only expand the acronym once
    	delete acro[acronym]
    }

There are other changes that would be good to make. In running the awkro program, we soon discovered that it failed to match the acronym if it was followed by a punctuation mark. Our initial solution was not to handle it in awk at all. Instead, we used two sed scripts, one before processing:

    sed 's/\([^.,;:!][^.,;:!]*\)\([.,;:!]\)/\1 @@@\2/g'

and one after:

    sed 's/ @@@\([.,;:!]\)/\1/g'

A sed script, run prior to invoking awk, could simply insert a space before any punctuation mark, causing it to be interpreted as a separate field. A string of garbage characters (@@@) was also added so we'd be able to easily identify and restore the punctuation mark. (The complicated expression used in the first sed command makes sure that we catch the case of more than one punctuation mark on a line.)

This kind of solution, using another tool in the UNIX toolbox, demonstrates that not everything needs to be done as an awk procedure. Awk is all the more valuable because it is situated in the UNIX environment.

However, with POSIX awk, we can implement a different solution, one that uses a regular expression to match the acronym. Such a solution can be implemented with the match() and sub() functions described in the next chapter.

	5.15 Multidimensional Arrays
	Awk supports linear arrays in which the index to each element of the array is a single subscript. If you imagine a linear array as a row of numbers, a two-dimensional array represents rows and columns of numbers. You might refer to the element in the second column of the third row as "array[3, 2]." Two- and three-dimensional arrays are examples of multidimensional arrays. Awk does not support multidimensional arrays but instead offers a syntax for subscripts that simulate a reference to a multidimensional array. For instance, you could write the following expression:

    file_array[NR, i] = $i

where each field of an input record is indexed by its record number and field number. Thus, the following reference:

    file_array[2, 4]

would produce the value of the fourth field of the second record.

This syntax does not create a multidimensional array. It is converted into a string that uniquely identifies the element in a linear array. The components of a multidimensional subscript are interpreted as individual strings ("2" and "4," for instance) and concatenated together separated by the value of the system variable SUBSEP. The subscript-component separator is defined as "\034" by default, an unprintable character rarely found in ASCII text. Thus, awk maintains a one-dimensional array and the subscript for our previous example would actually be "2\0344" (the concatenation of "2," the value of SUBSEP, and "4"). The main consequence of this simulation of multidimensional arrays is that the larger the array, the slower it is to access individual elements. However, you should time this, using your own application, with different awk implementations (see Chapter 11, "A Flock of awks").

Here is a sample awk script named bitmap.awk that shows how to load and output the elements of a multidimensional array. This array represents a two-dimensional bitmap that is 12 characters in width and height.

    BEGIN { FS = ","   # comma-separated fields
    	# assign width and height of bitmap
    	WIDTH = 12
    	HEIGHT = 12
    	# loop to load entire array with "O"
    	for (i = 1; i <= WIDTH; ++i)
    		for (j = 1; j <= HEIGHT; ++j)
    			bitmap[i, j] = "O"
    }
    # read input of the form x,y. 
    {
    	# assign "X" to that element of array 
    	bitmap[$1, $2] = "X"
    }
    # at end output multidimensional array
    END {
    	for (i = 1; i <= WIDTH; ++i){
    		for (j = 1; j <= HEIGHT; ++j)
    			printf("%s", bitmap[i, j] )
    		# after each row, print newline
    		printf("\n")	
    	}
    }

Before any input is read, the bitmap array is loaded with O's. This array has 144 elements. The input to this program is a series of coordinates, one per line.

    $ cat bitmap.test
    1,1
    2,2
    3,3
    4,4
    5,5
    6,6
    7,7
    8,8
    9,9
    10,10
    11,11
    12,12
    1,12
    2,11
    3,10
    4,9
    5,8
    6,7
    7,6
    8,5
    9,4
    10,3
    11,2
    12,1

For each coordinate, the program will put an "X" in place of an "O" as that element of the array. At the end of the script, the same kind of loop that loaded the array, now outputs it. The following example reads the input from the file bitmap.test.

    $ awk -f bitmap.awk bitmap.test
    XOOOOOOOOOOX
    OXOOOOOOOOXO
    OOXOOOOOOXOO
    OOOXOOOOXOOO
    OOOOXOOXOOOO
    OOOOOXXOOOOO
    OOOOOXXOOOOO
    OOOOXOOXOOOO
    OOOXOOOOXOOO
    OOXOOOOOOXOO
    OXOOOOOOOOXO
    XOOOOOOOOOOX

The multidimensional array syntax is also supported in testing for array membership. The subscripts must be placed inside parentheses.

    if ((i, j) in array)

This tests whether the subscript i,j (actually, i SUBSEP j) exists in the specified array.

Looping over a multidimensional array is the same as with one-dimensional arrays.

    for (item in array)

You must use the split() function to access individual subscript components. Thus:

    split(item, subscr, SUBSEP)

creates the array subscr from the subscript item.

Note that we needed to use the loop-within-a-loop to output the two-dimensional bitmap array in the previous example because we needed to maintain rows and columns.

	5.15 System Variables That Are Arrays
	Awk provides two system variables that are arrays:

ARGV
    An array of command-line arguments, excluding the script itself and any options specified with the invocation of awk. The number of elements in this array is available in ARGC. The index of the first element of the array is 0 (unlike all other arrays in awk but consistent with C) and the last is ARGC - 1.

ENVIRON
    An array of environment variables. Each element of the array is the value in the current environment and the index is the name of the environment variable.

		5.15.1 An Array of Command-Line Parameters
		You can write a loop to reference all the elements of the ARGV array.

    # argv.awk - print command-line parameters
    BEGIN { for (x = 0; x < ARGC; ++x)
    	    print ARGV[x]
    	print ARGC
    }

This example also prints out the value of ARGC, the number of command-line arguments. Here's an example of how it works on a sample command line:

    $ awk -f argv.awk 1234 "John Wayne" Westerns n=44 -
    awk
    1234
    John Wayne
    Westerns
    n=44
    - 
    6

    As you can see, there are six elements in the array. The first element is the name of the command that invoked the script. The last argument, in this case, is the filename, "-", for standard input. Note the "-f argv.awk" does not appear in the parameter list.

Generally, the value of ARGC will be at least 2. If you don't want to refer to the program name or the filename, you can initialize the counter to 1 and then test against ARGC - 1 to avoid referencing the last parameter (assuming that there is only one filename).

Remember that if you invoke awk from a shell script, the command-line parameters are passed to the shell script and not to awk. You have to pass the shell script's command-line parameters to the awk program inside the shell script. For instance, you can pass all command-line parameters from the shell script to awk, using "$*". Look at the following shell script:

    awk '
    # argv.sh - print command-line parameters
    BEGIN {
    	for (x = 0; x < ARGC; ++x)
    		print ARGV[x]
    	print ARGC
    }' $*

This shell script works the same as the first example of invoking awk.

	This shell script works the same as the first example of invoking awk.

One practical use is to test the command-line parameters in the BEGIN rule using a regular expression. The following example tests that all the parameters, except the first, are integers.

    # number.awk - test command-line parameters
    BEGIN {
    	for (x = 1; x < ARGC; ++x)
    		if ( ARGV[x] !~ /^[0-9]+$/ ) {
    			print ARGV[x], "is not an integer."
    			exit 1
    		}
    }

If the parameters contain any character that is not a digit, the program will print the message and quit.
	If the parameters contain any character that is not a digit, the program will print the message and quit.

After testing the value, you can, of course, assign it to a variable. For instance, we could write a BEGIN procedure of a script that checks the command-line parameters before prompting the user. Let's look at the following shell script that uses the phone and address database from the previous chapter:

    awk '# phone - find phone number for person 
    # supply name of person on command line or at prompt.
    BEGIN { FS = "," 
    	# look for parameter
    	if ( ARGC > 2 ){ 
    		name = ARGV[1]
    		delete ARGV[1]
    	} else { 

    		# loop until we get a name
    		while (! name) { 
    			printf("Enter a name? ")
    			getline name < "-"
    		}
    	}
    }
    $1 ~ name {
    	print $1, $NF 
    }' $* phones.data

We test the ARGC variable to see if there are more than two parameters. By specifying "$*", we can pass all the parameters from the shell command line inside to the awk command line. If this parameter has been supplied, we assume the second parameter, ARGV[1], is the one we want and it is assigned to the variable name. Then that parameter is deleted from the array. This is very important if the parameter that is supplied on the command line is not of the form "var=value"; otherwise, it will later be interpreted as a filename. If additional parameters are supplied, they will be interpreted as filenames of alternative phone databases. If there are not more than two parameters, then we prompt for the name. The getline function is discussed in Chapter 10, "The Bottom Drawer"; using this syntax, it reads the next line from standard input.

Here is an example of this script in action:
$ phone  Yosi
Yosi 0508464424

Because you can add to and delete from the ARGV array, there is the potential for doing a lot of interesting manipulation. You can place a filename at the end of the ARGV array, for instance, and it will be opened as though it were specified on the command line. Similarly, you can delete a filename from the array and it will never be opened. Note that if you add new elements to ARGV, you should also increment ARGC; awk uses the value of ARGC to know how many elements in ARGV it should process. Thus, simply decrementing ARGC will keep awk from examining the final element in ARGV.

As a special case, if the value of an ARGV element is the empty string (""), awk will skip over it and continue on to the next element.

		5.15.2 An Array of Environment Variables
		The ENVIRON array was added independently to both gawk and MKS awk. It was then added to the System V Release 4 nawk, and is now included in the POSIX standard for awk. It allows you to access variables in the environment. The following script loops through the elements of the ENVIRON array and prints them.

    # environ.awk - print environment variable
    BEGIN {
    	for (env in ENVIRON)
    		print env "=" ENVIRON[env]
    }

The index of the array is the variable name. The script generates the same output produced by the env command (printenv on some systems).

    $ awk -f environ.awk
    DISPLAY=scribe:0.0
    FRAME=Shell 3
    LOGNAME=dale
    MAIL=/usr/mail/dale
    PATH=:/bin:/usr/bin:/usr/ucb:/work/bin:/mac/bin:.
    TERM=mac2cs
    HOME=/work/dale
    SHELL=/bin/csh
    TZ=PST8PDT
    EDITOR=/usr/bin/vi

    You can reference any element, using the variable name as the index of the array:

    ENVIRON["LOGNAME"]

You can also change any element of the ENVIRON array.

    ENVIRON["LOGNAME"] = "Tom"

However, this change does not affect the user's actual environment (i.e., when awk is done, the value of LOGNAME will not be changed) nor does it affect the environment inherited by programs that are invoked from awk via the getline or system() functions

	5.16	Functions

		5.16.1 Arithmetic Functions

		awk's Built-In Arithmetic Functions
		Awk Function 	Description
		cos(x)		Returns cosine of x (x is in radians).
		exp(x) 		Returns e to the power x.
		int(x) 		Returns truncated value of x.
		log(x) 		Returns natural logarithm (base-e) of x.
		sin(x) 		Returns sine of x (x is in radians).
		sqrt(x) 	Returns square root of x.
		atan2(y,x) 	Returns arctangent of y/x in the range -Figure to Figure .
		rand()		Returns pseudo-random number r, where 0 <= r < 1.
		srand(x) 	Establishes new seed for rand(). If no seed is specified, uses time of day. Returns the old seed

			5.16.1.1 Trigonometric Functions

The trigonometric functions cos() and sin() work the same way, taking a single argument that is the size of an angle in radians and returning the cosine or sine for that angle. (To convert from degrees to radians, multiply the number by Figure /180.) The trigonometric function atan2() takes two arguments and returns the arctangent of their quotient. The expression

    atan2(0, -1)

produces Figure .

The function exp() uses the natural exponential, which is also known as base-e exponentiation. The expression

    exp(1)

returns the natural number 2.71828, the base of the natural logarithms, referred to as e. Thus, exp(x) is e to the x-th power.

The log() function gives the inverse of the exp() function, the natural logarithm of x. The sqrt() function takes a single argument and returns the (positive) square root of that argument.

			5.16.1.2  Integer Function

The int() function truncates a numeric value by removing digits to the right of the decimal point. Look at the following two statements:

    print 100/3
    print int(100/3)

The output from these statements is shown below:

$    33.3333
    33

The int() function simply truncates; it does not round up or down. (Use the printf format "%.0f" to perform rounding.)

			5.16.1.3  Random Number Generation

The rand() function generates a pseudo-random floating-point number between 0 and 1. The srand() function sets the seed or starting point for random number generation. If srand() is called without an argument, it uses the time of day to generate the seed. With an argument x, srand() uses x as the seed.

If you don't call srand() at all, awk acts as if srand() had been called with a constant argument before your program started, causing you to get the same starting point every time you run your program. This is useful if you want reproducible behavior for testing, but inappropriate if you really do want your program to behave differently every time. Look at the following script:

    # rand.awk -- test random number generation
    BEGIN {
    	print rand()
    	print rand()
    	srand()
    	print rand()
    	print rand()
    }

We print the result of the rand() function twice, and then call the srand() function before printing the result of the rand() function two more times. Let's run the script.

    $ awk -f rand.awk
$    0.513871
$    0.175726
$    0.760277
$    0.263863

Four random numbers are generated. Now look what happens when we run the program again:

    $ awk -f rand.awk
$    0.513871
$    0.175726
$    0.787988
$    0.305033

The first two "random" numbers are identical to the numbers generated in the previous run of the program while the last two numbers are different. The last two numbers are different because we provided the rand() function with a new seed.

The return value of the srand() function is the seed it was using. This can be used to keep track of sequences of random numbers, and re-run them if needed.

			5.16.1.4  Pick 'em

To show how to use rand(), we'll look at a script that implements a "quick-pick" for a lottery game. This script, named lotto, picks x numbers from a series of numbers 1 to y. Two arguments can be supplied on the command line: how many numbers to pick (the default is 6) and the highest number in the series (the default is 30). Using the default values for x and y, the script generates six unique random numbers between 1 and 30. The numbers are sorted for readability from lowest to highest and output. Before looking at the script itself, let's run the program:

    $ lotto
    Pick 6 of 30
    9 13 25 28 29 30
    $ lotto 7 35
    Pick 7 of 35
    1 6 9 16 20 22 27

The first example uses the default values to print six random numbers from 1 to 30. The second example prints seven random numbers out of 35.

The full lotto script is fairly complicated, so before looking at the entire script, let's look at a smaller script that generates a single random number in a series:

    awk -v TOPNUM=$1 '
    # pick1 - pick one random number out of y 
    # main routine
    BEGIN {
    # seed random number using time of day 
    	srand() 
    # get a random number
    	select = 1 + int(rand() * TOPNUM)
    # print pick
    	print select
    }'

The shell script expects a single argument from the command line and this is passed into the program as "TOPNUM=$1," using the -v option. All the action happens in the BEGIN procedure. Since there are no other statements in the program, awk exits when the BEGIN procedure is done.

The main routine first calls the srand() function to seed the random number generator. Then we get a random number by calling the rand() function:

    select = 1 + int(rand() * TOPNUM)

It might be helpful to see this expression broken up so each part of it is obvious.
Statement			Result
print r = rand() 		0.467315
print r * TOPNUM 		14.0195
print int(r * TOPNUM) 		14
print 1 + int(r * TOPNUM) 	15

Because the rand() function returns a number between 0 and 1, we multiply it by TOPNUM to get a number between 0 and TOPNUM. We then truncate the number to remove the fractional values and then add 1 to the number. The latter is necessary because rand() could return 0. In this example, the random number that is generated is 15. You could use this program to print any single number, such as picking a number between 1 and 100.

    $ pick1 100
    83

The lotto script must "pick one" multiple times. Basically, we need to set up a for loop to execute the rand() function as many times as needed. One of the reasons this is difficult is that we have to worry about duplicates. In other words, it is possible for a number to be picked again; therefore we have to keep track of the numbers already picked.

Here's the lotto script:

    awk -v NUM=$1 -v TOPNUM=$2 '
    # lotto - pick x random numbers out of y 
    # main routine
    BEGIN {
    # test command line args; NUM = $1, how many numbers to pick 
    # 	              TOPNUM = $2, last number in series
    	if (NUM <= 0) 
    		NUM = 6
    	if (TOPNUM <= 0) 
    		TOPNUM = 30
    # print "Pick x of y"
    	printf("Pick %d of %d\n", NUM, TOPNUM) 
    # seed random number using time and date; do this once
    	srand() 
    # loop until we have NUM selections
    	for (j = 1; j <= NUM; ++j) {
    		# loop to find a not-yet-seen selection
    		do {
    			select = 1 + int(rand() * TOPNUM)
    		} while (select in pick)
    		pick[select] = select
    	}
    # loop through array and print picks.
    	for (j in pick) 
    		printf("%s ", pick[j])
    	printf("\n")
    }'

Unlike the previous program, this one looks for two command-line arguments, indicating x numbers out of y. The main routine looks to see if these numbers were supplied and if not, assigns default values.

There is only one array, pick, for holding the random numbers that are selected. Each number is guaranteed to be in the desired range, because the result of rand() (a value between 0 and 1) is multiplied by TOPNUM and then truncated. The heart of the script is a loop that occurs NUM times to assign NUM elements to the pick array.

To get a new non-duplicate random number, we use an inner loop that generates selections and tests to see if they are in the pick array. (Using the in operator is much faster than looping through the array comparing subscripts.) While (select in pick), the corresponding element has been found already, so the selection is a duplicate and we reject the selection. If it is not true that select in pick, then we assign select to an element of the pick array. This will make future in tests return true, causing the do loop to continue.

Finally, the program loops through the pick array and prints the elements. This version of the lotto script leaves one thing out. See if you can tell what it is if we run it again:

    $ lotto 7 35
    Pick 7 of 35
    5 21 9 30 29 20 2

That's right, the numbers are not sorted. We'll defer showing the code for the sort routine until we discuss user-defined functions. While it's not necessary to have written the sorting code as a function, it makes a lot of sense. One reason is that you can tackle a more generalized problem and retain the solution for use in other programs. Later on, we will write a function that sorts the elements of an array.

Note that the pick array isn't ready for sorting, since its indices are the same as its values, not numbers in order. We would have to set up a separate array for sorting by our sort function:

    # create a numerically indexed array for sorting
    i = 1
    for (j in pick)
    	sortedpick[i++] = pick[j]

The lotto program is set up to do everything in the BEGIN block. No input is processed. You could, however, revise this script to read a list of names from a file and for each name generate a "quick-pick."

		5.16.2 String Functions
		Awk Function 	Description
gsub(r,s,t) 	Globally substitutes s for each match of the regular expression r in the string t. Returns the number of substitutions. If t is not supplied, defaults to $0.
index(s,t) 	Returns position of substring t in string s or zero if not present.
length(s) 	Returns length of string s or length of $0 if no string is supplied.
match(s,r) 	Returns either the position in s where the regular expression r begins, or 0 if no occurrences are found. Sets the values of RSTART and RLENGTH.
split(s,a,sep) 	Parses string s into elements of array a using field separator sep; returns number of elements. If sep is not supplied, FS is used. Array splitting works the same way as field splitting.
sprintf("fmt",expr) 	Uses printf format specification for expr.
sub(r,s,t) 	Substitutes s for first match of the regular expression r in the string t. Returns 1 if successful; 0 otherwise. If t is not supplied, defaults to $0.
substr(s,p,n) 	Returns substring of string s at beginning position p up to a maximum length of n. If n is not supplied, the rest of the string from p is used.
tolower(s) 	Translates all uppercase characters in string s to lowercase and returns the new string.
toupper(s) 	Translates all lowercase characters in string s to uppercase and returns the new string.

The sprintf() function uses the same format specifications as printf(), which is discussed in Chapter 7, "Writing Scripts for awk". It allows you to apply the format specifications on a string. Instead of printing the result, sprintf() returns a string that can be assigned to a variable. It can do specialized processing of input records or fields, such as performing character conversions. For instance, the following example uses the sprintf() function to convert a number into an ASCII character.

    for (i = 97; i <= 122; ++i) {
    	nextletter = sprintf("%c", i)
    	...
    }
A loop supplies numbers from 97 to 122, which produce ASCII characters from a to z.
That leaves us with three basic built-in string functions to discuss: index(), substr(), and length().

		5.16.3 Substrings
		The index() and substr() functions both deal with substrings. Given a string s, index(s,t) returns the leftmost position where string t is found in s. The beginning of the string is position 1 (which is different from the C language, where the first character in a string is at position 0). Look at the following example:
    pos = index("Mississippi", "is")
The value of pos is 2. If the substring is not found, the index() function returns 0.
Given a string s, substr(s,p) returns the characters beginning at position p. The following example creates a phone number without an area code.
    phone = substr("707-555-1111", 5)
You can also supply a third argument which is the number of characters to return. The next example returns just the area code:
    area_code = substr("707-555-1111", 1, 3)
The two functions can be and often are used together, as in the next example. This example capitalizes the first letter of the first word for each input record.
(Note, Much easier with python. See:
 echo "astring"  | python -c "print raw_input().capitalize()"
Astring
)
Note, to capitalize words or strings use toupper() function
    awk '# caps - capitalize 1st letter of 1st word
    # initialize strings
    BEGIN { upper = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
            lower = "abcdefghijklmnopqrstuvwxyz" 
    }
    # for each input line
    {
    # get first character of first word
    	FIRSTCHAR = substr($1, 1, 1)
    # get position of FIRSTCHAR in lowercase array; if 0, ignore
    	if (CHAR = index(lower, FIRSTCHAR)) 
    		# change $1, using position to retrieve
    		# uppercase character 
    		$1 = substr(upper, CHAR, 1) substr($1, 2)
    # print record
    	print $0
    }'
This script creates two variables, upper and lower, consisting of uppercase and lowercase letters. Any character that we find in lower can be found at the same position in upper. The first statement of the main procedure extracts a single character, the first one, from the first field. The conditional statement tests to see if that character can be found in lower using the index() function. If CHAR is not 0, then CHAR can be used to extract the uppercase character from upper. There are two substr() function calls: the first one retrieves the capitalized letter and the second call gets the rest of the first field, extracting all characters, beginning with the second character. The values returned by both substr() functions are concatenated and assigned to $1. Making an assignment to a field as we do here is a new twist, but it has the added benefit that the record can be output normally. (If the assignment was made to a variable, you'd have to output the variable and then output the record's remaining fields.) The print statement prints the changed record. Let's see it in action:
    $ caps
    root user
    Root user
    dale
    Dale
    Tom
    Tom
In a little bit, we'll see how to revise this program to change all characters in a string from lower- to uppercase or vice versa.
	
		5.16.4 String Length
		When presenting the awkro program in the previous chapter, we noted that the program was likely to produce lines that exceed 80 characters. After all, the descriptions are quite long. We can find out how many characters are in a string using the built-in function length(). For instance, to evaluate the length of the current input record, we specify length($0). (As it happens, if length() is called without an argument, it returns the length of $0.)
The length() function is often used to find the length of the current input record, in order to determine if we need to break the line.
One way to handle the line break, perhaps more efficiently, is to use the length() function to get the length of each field. By accumulating those lengths, we could specify a line break when a new field causes the total to exceed a certain number.

		
		5.16.6 Substitution Functions
Awk provides two substitution functions: sub() and gsub(). The difference between them is that gsub() performs its substitution globally on the input string whereas sub() makes only the first possible substitution. This makes gsub() equivalent to the sed substitution command with the g (global) flag.
Both functions take at least two arguments. The first is a regular expression (surrounded by slashes) that matches a pattern and the second argument is a string that replaces what the pattern matches. The regular expression can be supplied by a variable, in which case the slashes are omitted. An optional third argument specifies the string that is the target of the substitution. If there is no third argument, the substitution is made for the current input record ($0).
The substitution functions change the specified string directly. You might expect, given the way functions work, that the function returns the new string created when the substitution is made. The substitution functions actually return the number of substitutions made. sub() will always return 1 if successful; both return 0 if not successful. Thus, you can test the result to see if a substitution was made.
For example, the following example uses gsub() to replace all occurrences of "UNIX" with "POSIX".
    if (gsub(/UNIX/, "POSIX"))
    	print
The conditional statement tests the return value of gsub() such that the current input line is printed only if a change is made.
As with sed, if an "&" appears in the substitution string, it will be replaced by the string matched by the regular expression. Use "\&" to output an ampersand. (Remember that to get a literal "\" into a string, you have to type two of them.) Also, note that awk does not "remember" the previous regular expression, as does sed, so you cannot use the syntax "//" to refer to the last regular expression.
The following example surrounds any occurrence of "UNIX" with the troff font-change escape sequences.
    gsub(/UNIX/, "\\fB&\\fR")
If the input is "the UNIX operating system", the output is "the \fBUNIX\fR operating system".
In Chapter 4, "Writing sed Scripts", we presented the following sed script named do.outline:
    sed -n '
    s/"//g
    s/^\.Se /Chapter /p
    s/^\.Ah /A. /p
    s/^\.Bh /B.  /p' $*
Now here's that script rewritten using the substitution functions:
    awk '
    {
    gsub(/"/, "")
    if (sub(/^\.Se /, "Chapter ")) print
    if (sub(/^\.Ah /, "\tA. ")) print
    if (sub(/^\.Bh /, "\t\tB.  ")) print
    }' $*
The two scripts are exactly equivalent, printing out only those lines that are changed. For the first edition of this book, Dale compared the run-time of both scripts and, as he expected, the awk script was slower. For the second edition, new timings showed that performance varies by implementation, and in fact, all tested versions of new awk were faster than sed! This is nice, since we have the capabilities in awk to make the script do more things. For instance, instead of using letters of the alphabet, we could number the headings. Here's the revised awk script:
    awk '# do.outline -- number headings in chapter.
    {
    gsub(/"/, "")
    }
    /^\.Se/ {
    	sub(/^\.Se /, "Chapter ") 
    	ch = $2
    	ah = 0
    	bh = 0
    	print
    	next
    }
    /^\.Ah/ {
    	sub(/^\.Ah /, "\t " ch "." ++ah " ") 
    	bh = 0
    	print
    	next
    }
    /^\.Bh/ {
    	sub(/^\.Bh /, "\t\t " ch "."  ah "." ++bh " ")
    	print
    }' $*
In this version, we break out each heading into its own pattern-matching rule. This is not necessary but seems more efficient since we know that once a rule is applied, we don't need to look at the others. Note the use of the next statement to bypass further examination of a line that has already been identified.
The chapter number is read as the first argument to the ".Se" macro and is thus the second field on that line. The numbering scheme is done by incrementing a variable each time the substitution is made. The action associated with the chapter-level heading initializes the section-heading counters to zero. The action associated with the top-level heading ".Ah" zeroes the second-level heading counter. Obviously, you can create as many levels of heading as you need. Note how we can specify a concatenation of strings and variables as a single argument to the sub() function.
    $ do.outline ch02
If you wanted the option of choosing either numbers or letters, you could maintain both programs and construct a shell wrapper that uses some flag to determine which program should be invoked.

		5.16.7 Converting Case
		POSIX awk provides two functions for converting the case of characters within a string. The functions are tolower() and toupper(). Each takes a single string argument, and returns a copy of that string, with all the characters of one case converted to the other (upper to lower and lower to upper, respectively). Their use is straightforward:
    $ cat test
    Hello, World!
    Good-bye CRUEL world!
    1, 2, 3, and away we GO!
    $ awk '{ printf("<%s>, <%s>\n", tolower($0), toupper($0)) }' test
    <hello, world!>, <HELLO, WORLD!>
    <good-bye cruel world!>, <GOOD-BYE CRUEL WORLD!>
    <1, 2, 3, and away we go!>, <1, 2, 3, AND AWAY WE GO!>
Note that nonalphabetic characters are left unchanged.

		5.16.8 The match() Function
		The match() function allows you to determine if a regular expression matches a specified string. It takes two arguments, the string and the regular expression. (This function is confusing because the regular expression is in the second position, whereas it is in the first position for the substitution functions.)

The match() function returns the starting position of the substring that was matched by the regular expression. You might consider it a close relation to the index() function. In the following example, the regular expression matches any sequence of capital letters in the string "the UNIX operating system".

    match("the UNIX operating system", /[A-Z]+/)

The value returned by this function is 5, the character position of "U," the first capital letter in the string.

The match() function also sets two system variables: RSTART and RLENGTH. RSTART contains the same value returned by the function, the starting position of the substring. RLENGTH contains the length of the string in characters (not the ending position of the substring). When the pattern does not match, RSTART is set to 0 and RLENGTH is set to -1. In the previous example, RSTART is equal to 5 and RLENGTH is equal to 4. (Adding them together gives you the position of the first character after the match.)

Let's look at a rather simple example that prints out a string matched by a specified regular expression, demonstrating the "extent of the match," as discussed in Chapter 3, "Understanding Regular Expression Syntax". The following shell script takes two command-line arguments: the regular expression, which should be specified in quotes, and the name of the file to search.

    awk '# match -- print string that matches line
    # for lines match pattern 
    match($0, pattern) {
    	# extract string matching pattern using
    	# starting position and length of string in $0 
    	# print string
    	print substr($0, RSTART, RLENGTH)
    }' pattern="$1" $2

The first command-line parameter is passed as the value of pattern. Note that $1 is surrounded by quotes, necessary to protect any spaces that might appear in the regular expression. The match() function appears in a conditional expression that controls execution of the only procedure in this awk script. The match() function returns 0 if the pattern is not found, and a non-zero value (RSTART) if it is found, allowing the return value to be used as a condition. If the current record matches the pattern, then the string is extracted from $0, using the values of RSTART and RLENGTH in the substr() function to specify the starting position of the substring to be extracted and its length. The substring is printed. This procedure only matches the first occurrence in $0.

Here's a trial run, given a regular expression that matches "emp" and any number of characters up to a blank space:

    $ match "emp[^ ]*" personnel.txt
    employees
    employee
    employee.
    employment,
    employer
    employment
    employee's
    employee

The match script could be a useful tool in improving your understanding of regular expressions.

The next script uses the match() function to locate any sequence of uppercase letters so that they can be converted to lowercase. Compare it to the caps program shown earlier in the chapter.

    awk '# lower - change upper case to lower case 
    # initialize strings
    BEGIN { upper = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
            lower = "abcdefghijklmnopqrstuvwxyz" 
    }

    # for each input line
    {
    # see if there is a match for all caps 
    	 while (match($0, /[A-Z]+/))  
    		# get each cap letter
    		for (x = RSTART; x < RSTART+RLENGTH; ++x) { 
    			CAP = substr($0, x, 1)
    			CHAR = index(upper, CAP)
    			# substitute lowercase for upper 
    			gsub(CAP, substr(lower, CHAR, 1))
    		}
    		
    # print record
           print $0
    }' $*

In this script, the match() function appears in a conditional expression that determines whether a while loop will be executed. By placing this function in a loop, we apply the body of the loop as many times as the pattern occurs in the current input record.

The regular expression matches any sequence of uppercase letters in $0. If a match is made, a for loop does the lookup of each character in the substring that was matched, similar to what we did in the caps sample program, shown earlier in this chapter. What's different here is how we use the system variables RSTART and RLENGTH. RSTART initializes the counter variable x. It is used in the substr() function to extract one character at a time from $0, beginning with the first character that matched the pattern. By adding RLENGTH to RSTART, we get the position of the first character after the ones that matched the pattern. That is why the loop uses "<" instead of "<=". At the end, we use gsub() to replace the uppercase letter with the corresponding lowercase letter.[58] Notice that we use gsub() instead of sub() because it offers us the advantage of making several substitutions if there are multiple instances of the same letter on the line.

    [58]You may be wondering, "why not just use tolower()?" Good question. Some early versions of nawk, including the one on SunOS 4.1.x systems, don't have tolower() and toupper(); thus it's useful to know how to do it yourself.

    $ cat test
    Every NOW and then, a WORD I type appears in CAPS.
    $ lower test
    every now and then, a word i type appears in caps.

Note that you could change the regular expression to avoid matching individual capital letters by matching a sequence of two or more uppercase characters, by using: "/[A-Z][A-Z]+/." This would also require revising the way the lowercase conversion was made using gsub(), since it matches a single character on the line.

In our discussion of the sed substitution command, you saw how to save and recall a portion of a string matched by a pattern, using \( and \) to surround the pattern to be saved and \n to recall the saved string in the replacement pattern. Unfortunately, awk's standard substitution functions offer no equivalent syntax. The match() function can solve many such problems, though.

For instance, if you match a string using the match() function, you can single out characters or a substring at the head or tail of the string. Given the values of RSTART and RLENGTH, you can use the substr() function to extract the characters. In the following example, we replace the second of two colons with a semicolon. We can't use gsub() to make the replacement because "/:/" matches the first colon and "/:[^:]*:/" matches the whole string of characters. We can use match() to match the string of characters and to extract the last character of the string.

    # replace 2nd colon with semicolon using match, substr
    if (match($1, /:[^:]*:/)) {
    	before = substr($1, 1, (RSTART + RLENGTH - 2))
    	after = substr($1, (RSTART + RLENGTH))
    	$1 = before ";" after
    }

The match() function is placed within a conditional statement that tests that a match was found. If there is a match, we use the substr() function to extract the substring before the second colon as well as the substring after it. Then we concatenate before, the literal ";", and after, assigning it to $1.

	
	5.17 Writing Your Own Functions

	A function definition can be placed anywhere in a script that a pattern-action rule can appear. Typically, we put the function definitions at the top of the script before the pattern-action rules. A function is defined using the following syntax:

    function name (parameter-list) {
    	statements
    }

The newlines after the left brace and before the right brace are optional. You can also have a newline after the close-parenthesis of the parameter list and before the left brace.

The parameter-list is a comma-separated list of variables that are passed as arguments into the function when it is called. The body of the function consists of one or more statements. The function typically contains a return statement that returns control to that point in the script where the function was called; it often has an expression that returns a value as well.

    return expression

The following example shows the definition for an insert() function:

    function insert(STRING, POS, INS) {
            before_tmp = substr(STRING, 1, POS)
            after_tmp = substr(STRING, POS + 1)
            return before_tmp INS after_tmp
    }

This function takes three arguments, inserting one string INS in another string STRING after the character at position POS.[60] The body of this function uses the substr() function to divide the value of STRING into two parts. The return statement returns a string that is the result of concatenating the first part of STRING, the INS string, and the last part of STRING. A function call can appear anywhere that an expression can. Thus, the following statement:

	    print insert($1, 4, "XX")

If the value of $1 is "Hello," then this functions returns "HellXXo." Note that when calling a user-defined function, there can be no spaces between the function name and the left parenthesis. This is not true of built-in functions.

It is important to understand the notion of local and global variables. A local variable is a variable that is local to a function and cannot be accessed outside of it. A global variable, on the other hand, can be accessed or changed anywhere in the script. There can be potentially damaging side effects of global variables if a function changes a variable that is used elsewhere in the script. Therefore, it is usually a good idea to eliminate global variables in a function.

When we call the insert() function, and specify $1 as the first argument, then a copy of that variable is passed to the function, where it is manipulated as a local variable named STRING. All the variables in the function definition's parameter list are local variables and their values are not accessible outside the function. Similarly, the arguments in the function call are not changed by the function itself. When the insert() function returns, the value of $1 is not changed.

However, the variables defined in the body of the function are global variables, by default. Given the above definition of the insert() function, the temporary variables before_tmp and after_tmp are visible outside the function. Awk provides what its developers call an "inelegant" means of declaring variables local to a function, and that is by specifying those variables in the parameter list.

The local temporary variables are put at the end of the parameter list. This is essential; parameters in the parameter list receive their values, in order, from the values passed in the function call. Any extra parameters, like normal awk variables, are initialized to the empty string. By convention, the local variables are separated from the "real" parameters by several spaces. For instance, the following example shows how to define the insert() function with two local variables.

    function insert(STRING, POS, INS,   before_tmp, after_tmp) {
    		body
    }

    function insert(STRING, POS, INS,   before_tmp) {
    	before_tmp = substr(STRING, 1, POS)
    	after_tmp = substr(STRING, POS + 1)
    	return before_tmp INS after_tmp
    }

    # main routine
    {
    print "Function returns", insert($1, 4, "XX")
    print "The value of $1 after is:", $1
    print "The value of STRING is:", STRING
    print "The value of before_tmp:", before_tmp
    print "The value of after_tmp:", after_tmp
    }

Notice that we specify before_tmp in the parameter list. In the main routine, we call the insert() function and print its result. Then we print different variables to see what their value is, if any. Now let's run the above script and look at the output:

    $ echo "Hello" | awk -f insert.awk -
    Function returns HellXXo
    The value of $1 after is: Hello
    The value of STRING is:
    The value of before_tmp:
    The value of after_tmp: o

The insert() function returns "HellXXo," as expected. The value of $1 is the same after the function was called as it was before. The variable STRING is local to the function and it does not have a value when called from the main routine. The same is true for before_tmp because its name was placed in the parameter list for the function definition. The variable after_tmp which was not specified in the parameter list does have a value, the letter "o."

As this example shows, $1 is passed "by value" into the function. This means that a copy is made of the value when the function is called and the function manipulates the copy, not the original. Arrays, however, are passed "by reference." That is, the function does not work with a copy of the array but is passed the array itself. Thus, any changes that the function makes to the array are visible outside of the function. (This distinction between "scalar" variables and arrays also holds true for functions written in the C language.) The next section presents an example of a function that operates on an array.

		5.17.1 Writing a Sort Function

Earlier in this chapter we presented the lotto script for picking x random numbers out of a series of y numbers. That script did not sort the list of numbers that were selected. In this section, we develop a sort function for elements of an array.

We define a function that takes two arguments, the name of the array and the number of elements in the array. This function can be called this way:

    sort(sortedpick, NUM)

The function definition lists the two arguments and three local variables used in the function.

    # sort numbers in ascending order
    function sort(ARRAY, ELEMENTS,   temp, i, j) {
            for (i = 2; i <= ELEMENTS; ++i) {
                    for (j = i; (j-1) in ARRAY && ARRAY[j-1] > ARRAY[j]; --j) {
                            temp = ARRAY[j]
                            ARRAY[j] = ARRAY[j-1]
                            ARRAY[j-1] = temp
                    }
            }
            return
    }

The body of the function implements an insertion sort. This sorting algorithm is very simple. We loop through each element of the array and compare it to the value preceding it. If the first element is greater than the second, the first and second elements are swapped.[62] To actually swap the values, we use a temporary variable to hold a copy of the value while we overwrite the original. The loop continues swapping adjacent elements until all are in order. At the end of the function, we use the return statement to simply return control.[63] The function does not need to pass the array back to the main routine because the array itself is changed and it can be accessed directly.

	Here's proof positive:

    $ lotto 7 35
    Pick 7 of 35
    6 7 17 19 24 29 35

In fact, many of the scripts that we developed in this chapter could be turned into functions. For instance, if we only had the original, 1987, version of nawk, we might want to write our own tolower() and toupper() functions.

The value of writing the sort() function in a general fashion is that you can easily reuse it. To demonstrate this, we'll take the above sort function and use it to sort student grades. In the following script, we read all of the student grades into an array and then call sort() to put the grades in ascending order.

    # grade.sort.awk -- script for sorting student grades
    # input: student name followed by a series of grades

    # sort function -- sort numbers in ascending order
    function sort(ARRAY, ELEMENTS, 	temp, i, j) {
    	for (i = 2; i <= ELEMENTS; ++i) 
    		for (j = i; ARRAY[j-1] > ARRAY[j]; --j) { 
    			temp = ARRAY[j]
    			ARRAY[j] = ARRAY[j-1]
    			ARRAY[j-1] = temp
    	}
    	return 
    }

    # main routine
    { 
    # loop through fields 2 through NF and assign values to
    # array named grades
    for (i = 2; i <= NF; ++i)
    	grades[i-1] = $i 

    # call sort function to sort elements

    sort(grades, NF-1)

    # print student name
    printf("%s: ", $1)

    # output loop
    for (j = 1; j <= NF-1; ++j)
    	printf("%d ", grades[j])
    printf("\n")
    }

Note that the sort routine is identical to the previous version. In this example, once we've sorted the grades we simply output them:

    $ awk -f grade.sort.awk grades.test
    mona: 70 70 77 83 85 89
    john: 78 85 88 91 92 94
    andrea: 85 89 90 90 94 95
    jasper: 80 82 84 84 88 92
    dunce: 60 60 61 62 64 80
    ellis: 89 90 92 96 96 98

However, you could, for instance, delete the first element of the sort array if you wanted to average the student grades after dropping the lowest grade.

As another exercise, you could write a version of the sort function that takes a third argument indicating an ascending or descending sort.

		5.17.2 Maintaining a Function Library
		
		You might want to put a useful function in its own file and store it in a central directory. Awk permits multiple uses of the -f option to specify more than one program file.[64] For instance, we could have written the previous example such that the sort function was placed in a separate file from the main program grade.awk. The following command specifies both program files:

		    $ awk -f grade.awk -f /usr/local/share/awk/sort.awk grades.test

This command assumes that grade.awk is in the working directory and that the sort function is defined in sort.awk in the directory /usr/local/share/awk.

		5.17.3 Another Sorted Example

Lenny, our production editor, is back with another request.

    Dale:

    The last section of each Xlib manpage is called "Related Commands"
    (that is the argument of a .SH) and it's followed by a list of commands
    (often 10 or 20) that are now in random order.  It'd be more
    useful and professional if they were alphabetized.  Currently, commands
    are separated by a comma after each one except the last, which has a
    period.

    The question is: could awk alphabetize these lists?  We're talking
    about a couple of hundred manpages.  Again, don't bother if this is a
    bigger job than it seems to someone who doesn't know what's involved.

    Best to you and yours, 

    Lenny

To see what he is talking about, a simplified version of an Xlib manpage is shown below:

    .SH "Name"
    XSubImage -- create a subimage from part of an image.
    .
    .
    .
    .SH "Related Commands"
    XDestroyImage, XPutImage, XGetImage, 
    XCreateImage, XGetSubImage, XAddPixel, 
    XPutPixel, XGetPixel, ImageByteOrder.

You can see that the names of related commands appear on several lines following the heading. You can also see that they are in no particular order.

To sort the list of related commands is actually fairly simple, given that we've already covered sorting. The structure of the program is somewhat interesting, as we must read several lines after matching the "Related Commands" heading.

Looking at the input, it is obvious that the list of related commands is the last section in the file. All other lines except these we want to print as is. The key is to match all lines from the heading "Related Commands" to the end of the file. Our script can consist of four rules, that match:

   a.

      The "Related Commands" heading
   b.

      The lines following that heading
   c.

      All other lines
   d.

      After all lines have been read (END)

Most of the "action" takes place in the END procedure. That's where we sort and output the list of commands. Here's the script:

    # sorter.awk -- sort list of related commands
    # requires sort.awk as function in separate file
    BEGIN { relcmds = 0 } 

    #1 Match related commands; enable flag x 
    /\.SH "Related Commands"/ {
    	print
    	relcmds = 1
    	next
    }

    #2 Apply to lines following "Related Commands" 
    (relcmds == 1) {
    	commandList = commandList $0
    }


    #3 Print all other lines, as is.
    (relcmds == 0) { print }

    #4 now sort and output list of commands 
    END {
    # remove leading spaces and final period.
    	gsub(/, */, ",", commandList)
    	gsub(/\. *$/, "", commandList)
    # split list into array
    	sizeOfArray = split(commandList, comArray, ",")
    # sort
    	sort(comArray, sizeOfArray)
    # output elements
    	for (i = 1; i < sizeOfArray; i++)
    		printf("%s,\n", comArray[i])  
    	printf("%s.\n", comArray[i])
    }

Once the "Related Commands" heading is matched, we print that line and then set a flag, the variable relcmds, which indicates that subsequent input lines are to be collected.[65] The second procedure actually collects each line into the variable commandList. The third procedure is executed for all other lines, simply printing them.

    [65]The getline function introduced in the next chapter provides a simpler way to control reading input lines.

When all lines of input have been read, the END procedure is executed, and we know that our list of commands is complete. Before splitting up the commands into fields, we remove any number of spaces following a comma. Next we remove the final period and any trailing spaces. Finally, we create the array comArray using the split() function. We pass this array as an argument to the sort() function, and then we print the sorted values.

This program generates the following output:

    $ awk -f sorter.awk test
    .SH "Name"
    XSubImage -- create a subimage from part of an image.
    .SH "Related Commands"
    ImageByteOrder,
    XAddPixel,
    XCreateImage,
    XDestroyImage,
    XGetImage,
    XGetPixel,
    XGetSubImage,
    XPutImage,
    XPutPixel.

Once again, the virtue of calling a function to do the sort versus writing or copying the code to do the same task is that the function is a module that's been tested previously and has a standard interface. That is, you know that it works and you know how it works. When you come upon the same sort code in the awk version, which uses different variable names, you have to scan it to verify that it works the same way as other versions. Even if you were to copy the lines into another program, you would have to make changes to accommodate the new circumstances. With a function, all you need to know is what kind of arguments it expects and their calling sequence. Using a function reduces the chance for error by reducing the complexity of the problem that you are solving.

Because this script presumes that the sort() function exists in a separate file, it must be invoked using the multiple -f options:

    $ awk -f sort.awk -f sorter.awk test

where the sort() function is defined in the file sort.awk.


	5.18 The Bottom Drawer

		5.18.1 The getline function 
		is similar to awk's next statement. While both cause the next input line to be read, the next statement passes control back to the top of the script. The getline function gets the next line without changing control in the script. Possible return values are:
1 	

If it was able to read a line.
0 	

If it encounters the end-of-file.
-1 	

If it encounters an error.

    NOTE: Although getline is called a function and it does return a value, its syntax resembles a statement. Do not write getline(); its syntax does not permit parentheses.

In the previous chapter, we used a manual page source file as an example. The -man macros typically place the text argument on the next line. Although the macro is the pattern that you use to find the line, it is actually the next line that you process. For instance, to extract the name of the command from the manpage, the following example matches the heading "Name," reads the next line, and prints the first field of it:

    # getline.awk -- test getline function
    /^\.SH "?Name"?/ { 
    	getline # get next line
    	print $1 # print $1 of new line.
    }

The pattern matches any line with ".SH" followed by "Name," which might be enclosed in quotes. Once this line is matched, we use getline to read the next input line. When the new line is read, getline assigns it $0 and parses it into fields. The system variables NF, NR, and FNR are also set. Thus, the new line becomes the current line, and we are able to refer to "$1" and retrieve the first field. Note that the previous line is no longer available as $0. However, if necessary, you can assign the line read by getline to a variable and avoid changing $0, as we'll see shortly.

Here's an example that shows how the previous script works, printing out the first field of the line following ".SH Name."

    $ awk -f getline.awk test
    XSubImage

The sorter.awk program that we demonstrated at the end of Chapter 9, "Functions", could have used getline to read all the lines after the heading "Related Commands." We can test the return value of getline in a while loop to read a number of lines from the input. The following procedure replaces the first two procedures in the sorter program:

    # Match "Related Commands" and collect them
    /^\.SH "?Related Commands"?/ {
    	print
    	while (getline > 0)
    		commandList = commandList $0
    }

The expression "getline > 0" will be true as long as getline successfully reads an input line. When it gets to the end-of-file, getline returns 0 and the loop is exited

		5.18.1.1 Assigning the Input to a Variable

The getline function allows you to assign the input record to a variable. The name of the variable is supplied as an argument. Thus, the following statement reads the next line of input into the variable input:

    getline input

Assigning the input to a variable does not affect the current input line; that is, $0 is not affected. The new input line is not split into fields, and thus the variable NF is also unaffected. It does increment the record counters, NR and FNR.

The previous example demonstrated how to prompt the user. That example could be written as follows, assigning the user's response to the variable name.

    BEGIN { printf "Enter your name: "
    	getline name < "-"
    	print name
    }

Study the syntax for assigning the input data to a variable because it is a common mistake to instead write:

    name = getline     # wrong

which assigns the return value of getline to the variable name.


		5.18.1.2 Reading Input from a Pipe

You can execute a command and pipe the output into getline. For example, look at the following expression:
    "who am i" | getline

That expression sets "$0" to the output of the who am i command.
    dale       ttyC3        Jul 18 13:37

The line is parsed into fields and the system variable NF is set. Similarly, you can assign the result to a variable:
    "who am i" | getline me

By assigning the output to a variable, you avoid setting $0 and NF, but the line is not split into fields.

The following script is a fairly simple example of piping the output of a command to getline. It uses the output from the who am i command to get the user's name. It then looks up the name in /etc/passwd, printing out the fifth field of that file, the user's full name:
    awk '# getname - print users fullname from /etc/passwd
    BEGIN { "who am i" | getline 
    	name = $1
    	FS = ":"
    }
    name ~ $1 { print $5 }
    ' /etc/passwd

The command is executed from the BEGIN procedure, and it provides us with the name of the user that will be used to find the user's entry in /etc/passwd. As explained above, who am i outputs a single line, which getline assigns to $0. $1, the first field of that output, is then assigned to name.

The field separator is set to a colon (:) to allow us to access individual fields in entries in the /etc/passwd file. Notice that FS is set after getline or else the parsing of the command's output would be affected.

Finally, the main procedure is designed to test that the first field matches name. If it does, the fifth field of the entry is printed. For instance, when Dale runs this script, it prints "Dale Dougherty."

When the output of a command is piped to getline and it contains multiple lines, getline reads a line at a time. The first time getline is called it reads the first line of output. If you call it again, it reads the second line. To read all the lines of output, you must set up a loop that executes getline until there is no more output. For instance, the following example uses a while loop to read each line of output and assign it to the next element of the array, who_out:

    while ("who" | getline)
    	who_out[++i] = $0

Each time the getline function is called, it reads the next line of output. The who command, however, is executed only once.

The next example looks for "@date" in a document and replaces it with today's date:
    # subdate.awk -- replace @date with todays date
    /@date/ {
    	"date +'%a., %h %d, %Y'" | getline today
    	gsub(/@date/, today)
    }
    { print }

The date command, using its formatting options,[67] provides the date and getline assigns it to the variable today. The gsub() function replaces each instance of "@date" with today's date.

This script might be used to insert the date in a form letter:

    To: Peabody
    From: Sherman 
    Date: @date

    I am writing you on @date to 
    remind you about our special offer.

All lines of the input file would be passed through as is, except the lines containing "@date", which are replaced with today's date:

    $ awk -f subdate.awk subdate.test
    To: Peabody
    From: Sherman
    Date: Sun., May 05, 1996

    I am writing you on Sun., May 05, 1996 to
    remind you about our special offer.

		5.18.2 The close() Function
The close function allows you to close open files and pipes. There are a number of reasons you should use it.

    * You can only have so many pipes open at a time. (See Section 10.8 below, which describes how such limitations can differ from system to system.) In order to open as many pipes in a program as you wish, you must use the close function to close a pipe when you are done with it (ordinarily, when getline returns 0 or -1). It takes a single argument, the same expression used to create the pipe. Here's an example:

          close("who")

    * Closing a pipe allows you to run the same command twice. For example, you can use date twice to time a command.
    * Using close may be necessary in order to get an output pipe to finish its work. For example:

          { some processing of $0 | "sort > tmpfile" }
          END {
          	close("sort > tmpfile")
          	while ((getline < "tmpfile") > 0) {
          		do more work
          	}
          }

    * Closing open files is necessary to keep you from exceeding your system's limit on simultaneously open files.

		5.18.3 The system() Function

The system() function executes a command supplied as an expression.[68] It does not, however, make the output of the command available within the program for processing. It returns the exit status of the command that was executed. The script waits for the command to finish before continuing execution. The following example executes the mkdir command:

    [68]The system() function is modeled after the standard C library function of the same name.

    BEGIN { if (system("mkdir dale") != 0) 
    		print "Command Failed" }

The system() function is called from an if statement that tests for a non-zero exit status. Running the program twice produces one success and one failure:

    $ awk -f system.awk
    $ ls dale
    $ awk -f system.awk
    mkdir: dale: File exists
    Command Failed

The first run creates the new directory and system() returns an exit status of 0 (success). The second time the command is executed, the directory already exists, so mkdir fails and produces an error message. The "Command Failed" message is produced by awk.

The Berkeley UNIX command set has a small but useful command for troff users named soelim, named because it "eliminates" ".so" lines from a troff input file. (.so is a request to include or "source" the contents of the named file.) If you have an older System V system that does not have soelim, you can use the following awk script to create it:

    /^\.so/ { gsub(/"/, "", $2)
    		system("cat " $2)
    		next
    		}
    { print }

This script looks for ".so" at the beginning of a line, removes any quotation marks, and then uses system() to execute the cat command and output the contents of the file. This output merges with the rest of the lines in the file, which are simply printed to standard output, as in the following example.

    $ cat soelim.test
    This is a test
    .so test1
    This is a test
    .so test2
    This is a test.
    $ awk -f soelim.awk soelim.test
    This is a test
    first:second
    one:two
    This is a test
    three:four
    five:six
    This is a test.

We don't explicitly test the exit status of the command. Thus, if the file does not exist, the error messages merge with the output:

    $ awk -f soelim.awk soelim.test
    This is a test
    first:second
    one:two
    This is a test
    cat: cannot open test2
    This is a test.

We might want to test the return value of the system() function and generate an error message for the user. This program is also very simplistic: it does not handle instances of ".so" nested in the included file. Think about how you might implement a version of this program that did handle nested ".so" requests.

This example is a function prompting you to enter a filename. It uses the system() function to execute the test command to verify the file exists and is readable:

    # getFilename function -- prompts user for filename,
    #   verifies that file exists and returns absolute pathname. 
    function getFilename(	file) { 
        while (! file) {
    	printf "Enter a filename: "
    	getline < "-" # get response
    	file = $0
    	# check that file exists and is readable
    	# test returns 1 if file does not exist.
    	if (system("test -r " file)) {
    		print file " not found"
    		file = ""
    	}
        }
        if (file !~ /^\//) {
    	"pwd" | getline # get current directory 
     	close("pwd")
    	file = $0 "/" file
        }
        return file
    }

This function returns the absolute pathname of the file specified by the user. It places the prompting and verification sequence inside a while loop in order to allow the user to make a different entry if the previous one is invalid.

The test -r command returns 0 if the file exists and is readable, and 1 if not. Once it is determined that the filename is valid, then we test the filename to see if it begins with a "/", which would indicate that the user supplied an absolute pathname. If that test fails, we use the getline function to get the output of the pwd command and prepend it to the filename. (Admittedly, the script makes no attempt to deal with "./" or "../" entries, although tests can be easily devised to match them.) Note the two uses of the getline function: the first gets the user's response and the second executes the pwd command.

		5.18.4 A Menu-Based Command Generator

In this section, we look at a general use of the system() and getline functions to implement a menu-based command generator. The object of this program is to give unsophisticated users a simple way to execute long or complex UNIX commands. A menu is used to prompt the user with a description of the task to be performed, allowing the user to choose by number any selection of the menu to execute.

This program is designed as a kind of interpreter that reads from a file the descriptions that appear in the menu and the actual command lines that are executed. That way, multiple menu-command files can be used, and they can be easily modified by awk-less users without changing the program.

The format of a menu-command file contains the menu title as the first line in the file. Subsequent lines contain two fields: the first is the description of the action to be performed and the second is the command line that performs it. An example is shown below:

    $ cat uucp_commands
    UUCP Status Menu
    Look at files in PUBDIR:find /var/spool/uucppublic -print
    Look at recent status in LOGFILE:tail /var/spool/uucp/LOGFILE
    Look for lock files:ls /var/spool/uucp/*.LCK

The first step in implementing the menu-based command generator is to read the menu-command file. We read the first line of this file and assign it to a variable named title. The rest of the lines contain two fields and are read into two arrays, one for the menu items and one for the commands to be executed. A while loop is used, along with getline, to read one line at a time from the file.

    BEGIN { FS = ":"
    if ((getline < CMDFILE) > 0)
    	title = $1
    else
    	exit 1
    while ((getline < CMDFILE) > 0) {
    	# load array
    	++sizeOfArray
    	# array of menu items
    	menu[sizeOfArray] = $1
    	# array of commands associated with items
    	command[sizeOfArray] = $2
    	} 
    ...
    }

Look carefully at the syntax of the expression tested by the if statement and the while loop.

    (getline < CMDFILE) > 0

The variable CMDFILE is the name of the menu-command file, which is passed as a command-line parameter. The two angle-bracket symbols have completely different functions. The "<" symbol is interpreted by getline as the input redirection operator. Then the value returned by getline is tested to see if it is greater than (">") 0. It is parenthesized on purpose, in order to make this clear. In other words, "getline < CMDFILE" is evaluated first and then its return value is compared to 0.

This procedure is placed in the BEGIN pattern. However, there is one catch. Because we intended to pass the name of the menu file as a command-line parameter, the variable CMDFILE would not normally be defined and available in the BEGIN pattern. In other words, the following command will not work:

    awk script CMDFILE="uucp_commands"  -

because CMDFILE variable won't be defined until the first line of input is read.

Fortunately, awk provides the -v option to handle just such a case. Using the -v option makes sure that the variable is set immediately and thus available in the BEGIN pattern.

    awk -v CMDFILE="uucp_commands"  script -

If your version of awk doesn't have the -v option, you can pass the value of CMDFILE as a shell variable. Create a shell script to execute awk and in it define CMDFILE. Then change the line that reads CMDFILE in the invoke script (see below) as follows:

    while ((getline < '"$CMDFILE"') > 0 ) {

Once the menu-command file is loaded, the program must display the menu and prompt the user. This is implemented as a function because we need to call it in two places: from the BEGIN pattern to prompt the user initially, and after we have processed the user's response so that another choice can be made. Here's the display_menu() function:

    function display_menu() {
    	# clear screen -- comment out if clear does not work
    	system("clear")
    	# print title, list of items, exit item, and prompt
    	print "\t" title
    	for (i = 1; i <= sizeOfArray; ++i)
    		printf "\t%d. %s\n", i, menu[i]
    	printf "\t%d. Exit\n", i
    	printf("Choose one: ")
    }

The first thing we do is use the system() function to call a command to clear the screen. (On my system, clear does this; on others it may be cls or some other command. Comment out the line if you cannot find such a command.) Then we print the title and each of the items in a numbered list. The last item is always "Exit." Finally, we prompt the user for a choice.

The program will take standard input so that the user's answer to the prompt will be the first line of input. Our reading of the menu-command file was done within the program and not as part of the input stream. Thus, the main procedure of the program is to respond to the user's choice and execute a command. Here's that part of the program:

    # Applies the user response to prompt
    {
       # test value of user response
       if ($1 > 0 && $1 <= sizeOfArray) {
    	# print command that is executed
    	printf("Executing ... %s\n", command[$1]) 
    	# then execute it. 
    	system(command[$1])
    	printf("<Press RETURN to continue>")
     	# wait for input before displaying menu again
    	getline
       }
       else 
    	exit	
       # re-display menu 
       display_menu()
    }

First, we test the range of the user's response. If the response falls outside the range, we simply exit the program. If it is a valid response, then we retrieve the command from the array command, display it, and then execute it using the system() function. The user sees the result of the command on the screen followed by the message "<Press RETURN to continue>." The purpose of this message is to wait for the user to finish before clearing the screen and redisplaying the menu. The getline function causes the program to wait for a response. Note that we don't do anything with the response. The display_menu() function is called at the end of this procedure to redisplay the menu and prompt for another line of input.

Here's the invoke program in full:

    awk -v CMDFILE="uucp_commands"  '# invoke -- menu-based
                                      # command generator
    # first line in CMDFILE is the title of the menu
    # subsequent lines contain: $1 - Description;
    # $2 Command to execute
    BEGIN { FS = ":" 
    # process CMDFILE, reading items into menu array 
      if ((getline < CMDFILE) > 0)
    	title = $1
      else
    	exit 1
      while ((getline < CMDFILE) > 0) {
    	# load array
    	++sizeOfArray
    	# array of menu items
    	menu[sizeOfArray] = $1
    	# array of commands associated with items
    	command[sizeOfArray] = $2
      } 
      # call function to display menu items and prompt
      display_menu()
    }
    # Applies the user response to prompt
    {
       # test value of user response
       if ($1 > 0 && $1 <= sizeOfArray) {
    	# print command that is executed
    	printf("Executing ... %s\n", command[$1]) 
    	# then execute it. 
    	system(command[$1])
    	printf("<Press RETURN to continue>")
     	# wait for input before displaying menu again
    	getline
       }
       else 
    	exit	
       # re-display menu 
       display_menu()
    }

    function display_menu() {
    	# clear screen -- if clear does not work, try "cls"
    	system("clear")
    	# print title, list of items, exit item, and prompt
    	print "\t" title
    	for (i = 1; i <= sizeOfArray; ++i)
    		printf "\t%d. %s\n", i, menu[i]
    	printf "\t%d. Exit\n", i
    	printf("Choose one: ")
    }' -

When a user runs the program, the following output is displayed:

    UUCP Status Menu
            a. Look at files in PUBDIR
            b. Look at recent status in LOGFILE
            c. Look for lock files
            d. Exit
    Choose one:

The user is prompted to enter the number of a menu selection. Anything other than a number between 1 and 3 exits the menu. For instance, if the user enters "1" to see a list of files in uucp's public directory, then the following result is displayed on the screen:

    Executing ...find /var/spool/uucppublic -print
    /var/spool/uucppublic
    /var/spool/uucppublic/dale
    /var/spool/uucppublic/HyperBugs
    <Press RETURN to continue>

When the user presses the RETURN key, the menu is redisplayed on the screen. The user can quit from the program by choosing "4".

This program is really a shell for executing commands. Any sequence of commands (even other awk programs) can be executed by modifying the menu-command file. In other words, the part of the program that might change the most is extracted from the program itself and maintained in a separate file. This allows the menu list to be changed and extended very easily by a nontechnical user.

		5.18.5 Directing Output to Files and Pipes

The output of any print or printf statement can be directed to a file, using the output redirection operators ">" or ">>". For example, the following statement writes the current record to the file data.out:

    print > "data.out"

The filename can be any expression that evaluates to a valid filename. A file is opened by the first use of the redirection operator, and subsequent uses append data to the file. The difference between ">" and ">>" is the same as between the shell redirection operators. A right-angle bracket (">") truncates the file when opening it while ">>" preserves whatever the file contains and appends data to it.

Because the redirection operator ">" is the same as the relational operator, there is the potential for confusion when you specify an expression as an argument to the print command. The rule is that ">" will be interpreted as a redirection operator when it appears in an argument list for any of the print statements. To use ">" as a relational operator in an expression that appears in the argument list, put either the expression or the argument list in parentheses. For example, the following example uses parentheses around the conditional expression to make sure that the relational expression is evaluated properly:

    print "a =", a, "b =", b, "max =", (a > b ? a : b) > "data.out"

The conditional expression evaluates whether a is greater than b; if it is, then the value of a is printed as the maximum value; otherwise, b's value is used.

		5.18.5.1 Directing Output to a Pipe

You can also direct output to a pipe. The command

    print | command

opens a pipe the first time it is executed and sends the current record as input to that command. In other words, the command is only invoked once, but each execution of the print command supplies another line of input.

The following script strips troff macros and requests from the current input line and then sends the line as input to wc to determine how many words are in the file:

    {# words.awk - strip macros then get word count
    sub(/^\.../,"")
    print | "wc -w" 
    }

By removing formatting codes, we get a truer word count.

In most cases, we prefer to use a shell script to pipe the output of the awk command to another command rather than do it inside the awk script. For instance, we'd write the previous example as a shell script invoking awk and piping its output to wc:

    awk '{ # words -- strip macros 
    sub(/^\.../,"")
    print 
    }' $* | 
    # get word count
    wc -w

This method seems simpler and easier to understand. Nonetheless, the other method has the advantage of accomplishing the same thing without creating a shell script.

Remember that you can only have so many pipes open at a time. Use the close() function to close the pipe when you are done with it.

			5.18.5.2 Working with Multiple Files

A file is opened whenever you read from or write to a file. Every operating system has some limit on the number of files a running program may have open. Furthermore, each implementation of awk may have an internal limit on the number of open files; this number could be smaller than the system's limit.[69] So that you don't run out of open files, awk provides a close() function that allows you to close an open file. Closing files that you have finished processing allows your program to open more files later on.

    [69] Gawk will attempt to appear to have more files open than the system limit by closing and reopening files as needed. Even though gawk is "smart," it is still more efficient to close your files when you're done with them.

A common use for directing output to files is to split up a large file into a number of smaller files. Although UNIX provides utilities, split and csplit, that do a similar job, they do not have the ability to give the new file a useful filename.

Similarly, sed can be used to write to a file, but you must specify a fixed filename. With awk, you can use a variable to specify the filename and pick up the value from a pattern in the file. For instance, if $1 provided a string that could be used as a filename, you could write a script to output each record to its own file:

    print $0 > $1

You should perhaps test the filename, either to determine its length or to look for characters that cannot be used in a filename.

If you don't close your files, such a program would eventually run out of available open files, and have to give up. The example we are going to look at works because it uses the close() function so that you will not run into any open-file limitations.

The following script was used to split up a large file containing dozens of manpages. Each manual page began by setting a number register and ended with a blank line:

    .nr X 0

(Although they used the -man macros for the most part, the beginning of a manpage was strangely coded, making things a little harder.) The line that provides the filename looks like this:

    .if \nX=0 .ds x}  XDrawLine "" "Xlib - Drawing Primitives"

The fifth field on this line, "XDrawLine," contains the filename. Perhaps the only difficulty in writing the script is that the first line is not the one that provides the filename. Therefore, we collect the lines in an array until we get a filename. Once we get the filename, we output the array, and from that point on we simply write each input line to the new file. Here's the man.split script:

    # man.split -- split up a file containing X manpages. 
    BEGIN { file = 0; i = 0; filename = "" }

    # First line of new manpage is ".nr X 0"
    # Last line is blank
    /^\.nr X 0/,/^$/ {
    	# this conditional collects lines until we get a filename.
    	if (file == 0)
    		line[++i] = $0
    	else
    		print $0 > filename

    	# this matches the line that gives us the filename
    	if ($4 == "x}") {
    		# now we have a filename
    		filename = $5 
    		file = 1
    		# output name to screen 
    		print filename 
    		# print any lines collected
    		for (x = 1; x <= i; ++x){
    			print line[x] > filename
    		}
    		i = 0
    	}

    	# close up and clean up for next one
    	if ($0 ~ /^$/) {
    		close(filename)
    		filename = ""
    		file = 0
    		i = 0
    	}
    }

As you can see, we use the variable file as a flag to convey whether or not we have a valid filename and can write to the file. Initially, file is 0, and the current input line is stored in an array. The variable i is a counter used to index the array. When we encounter the line that sets the filename, then we set file to 1. The name of the new file is printed to the screen so that the user can get some feedback on the progress of the script. Then we loop through the array and output it to the new file. When the next input line is read, file will be set to 1 and the print statement will output it to the named file.

		5.18.6 Generating Columnar Reports

This section describes a small-scale business application that produces reports with dollar amounts. While this application doesn't introduce any new material, it does emphasize the data processing and reporting capabilities of awk. (Surprisingly, some people do use awk to write small business applications.)

It is presumed that a script exists for data entry. The data-entry script has two jobs: the first is to enter the customer's name and mailing address for later use in building a mailing list; the second is to record the customer's order of any of seven items, the number of items ordered, and the price per item. The data collected for the mailing list and the customer order were written to separate files.

Here are two sample customer records from the customer order file:

    Charlotte Webb 
    P.O  N61331 97 Y 045 	Date: 03/14/97
    #1 3  7.50
    #2 3  7.50
    #3 1  7.50
    #4 1  7.50
    #7 1  7.50 

    Martin S. Rossi 
    P.O  NONE 	Date: 03/14/97
    #1 2  7.50
    #2 5  6.75

Each order covers multiple lines, and a blank line separates one order from another. The first two lines supply the customer's name, purchase order number and the date of the order. Each subsequent line identifies an item by number, the number ordered, and the price of the item.

Let's write a simple program that multiplies the number of items by the price. The script can ignore the first two lines of each record. We only want to read the lines where an item is specified, as in the following example.

    awk '/^#/ {
    		amount = $2 * $3
    		printf "%s %6.2f\n", $0, amount
    		next
    	 }
    { print }' $*

The main procedure only affects lines that match the pattern. It multiplies the second field by the third field, assigning the value to the variable amount. The printf conversion %f is used to print a floating-point number; "6.2" specifies a minimum field width of six and a precision of two. Precision is the number of digits to the right of the decimal point; the default for %f is six. We print the current record along with the value of the variable amount. If a line is printed within this procedure, the next line is read from standard input. Lines not matching the pattern are simply passed through. Let's look at how addem works:

    $ addem orders
    Charlotte Webb 
    P.O  N61331 97 Y 045 	Date: 03/14/97
    #1 3  7.50  22.50
    #2 3  7.50  22.50
    #3 1  7.50   7.50
    #4 1  7.50   7.50
    #7 1  7.50   7.50

    Martin S. Rossi 
    P.O  NONE 	Date: 03/14/97
    #1 2  7.50  15.00
    #2 5  6.75  33.75

This program did not need to access the customer record as a whole; it simply acted on the individual item lines. Now, let's design a program that reads multiline records and accumulates order information for display in a report. This report should display for each item the total number of copies and the total amount. We also want totals reflecting all copies ordered and the sum of all orders.

Our new script will begin by setting the field and record separators:

    BEGIN { FS = "\n"; RS = "" }

Each record has a variable number of fields, depending upon how many items have been ordered. First, we check that the input record has at least three fields. Then a for loop is built to read all of the fields beginning with the third field.

    NF >= 3 {
    for (i = 3; i <= NF; ++i) {

In database terms, each field has a value and each value can be further broken up as subvalues. That is, if the value of a field in a multiline record is a single line, subvalues are the words that are on that line. We can use the split() function to divide a field into subvalues.

The following part of the script splits each field into subvalues. $i will supply the value of the current field that will be divided into elements of the array order:

    sv = split($i, order, " ")
    if (sv == 3) {
           procedure
    } else
           print "Incomplete Record"
    } # end for loop

The number of elements returned by the function is saved in a variable sv. This allows us to test that there are three subvalues. If there are not, the else statement is executed, printing the error message to the screen.

Next we assign each individual element of the array to a specific variable. This is mainly to make it easier to remember what each element represents:

    title = order[1] 
    copies = order[2] 
    price = order[3]

Then we perform a group of arithmetic operations on these values:

    amount = copies * price  
    total_vol += copies
    total_amt += amount
    vol[title] += copies
    amt[title] += amount

We accumulate these values until the last input record is read. The END procedure prints the report.

Here's the complete program:

    $ cat addemup
    #! /bin/sh
    # addemup -- total customer orders 
    awk 'BEGIN { FS = "\n"; RS = "" }
    NF >= 3 {
    	for (i = 3; i <= NF; ++i) { 	
    		sv = split($i, order, " ")
    		if (sv == 3) {
    			title = order[1] 
    			copies = order[2] 
    			price = order[3]
    			amount = copies * price  
    			total_vol += copies
    			total_amt += amount
    			vol[title] += copies
    			amt[title] += amount
    		} else
    			print "Incomplete Record"
    	}
    }

    END { 
       printf "%5s\t%10s\t%6s\n\n", "TITLE", "COPIES SOLD", "TOTAL"
       for (title in vol)
           printf "%5s\t%10d\t$%7.2f\n", title, vol[title], amt[title] 
       printf "%s\n", "-------------"
       printf "\t%s%4d\t$%7.2f\n", "Total ", total_vol, total_amt
    }' $*

We have defined two arrays that have the same subscript. We only need to have one for loop to read both arrays.

addemup, an order report generator, produces the following output:

    $ addemup orders
    TITLE     COPIES SOLD      TOTAL

       #1              5     $  37.50
       #2              8     $  56.25
       #3              1     $   7.50
       #4              1     $   7.50
       #7              1     $   7.50
    -------------
         Total        16     $ 116.25

		5.18.7 Debugging

No aspect of programming is more frustrating or more essential than debugging. In this section, we'll look at ways to debug awk scripts and offer advice on how to correct an awk program that fails to do what it is supposed to do.

Modern versions of awk do a pretty good job of reporting syntax errors. But even with good error detection, it is often difficult to isolate the problem. The techniques for discovering the source of the problem are a modest few and are fairly obvious. Unfortunately, most awk implementations come with no debugging tools or extensions.

There are two classes of problems with a program. The first is really a bug in the program's logic. The program runs--that is, it finishes without reporting any error messages, but it does not produce the result you wanted. For instance, perhaps it does not create any output. This bug could be caused by failing to use a print statement to output the result of a calculation. Program errors are mental errors, if you will.

The second class of error is one in which the program fails to execute or complete execution. This could result from a syntax error and cause awk to spit code at you that it is unable to interpret. Many syntax errors are the result of a typo or a missing brace or parenthesis. Syntax errors usually generate error messages that help direct you to the problem. Sometimes, however, a program may cause awk to fail (or "core dump") without producing any reasonable error message.[70] This may also be caused by a syntax error, but there could be problems specific to the machine. We have had a few larger scripts that dumped core on one machine while they ran without a problem on another. You could, for instance, be running up against limitations set for awk for that particular implementation. See Section 10.8, later in this chapter.

    [70]This indicates that the awk implementation is poor. Core dumps are very rare in modern versions of awk.

You should be clear in your mind which type of program bug you are trying to find: an error in the script's logic or an error in its syntax.

			5.18.7.1 Make a Copy

Before you begin debugging a program, make a copy of it. This is extremely important. To debug an awk script, you have to change it. These modifications may point you to the error but many changes will have no effect or may introduce new problems. It's good to be able to restore changes that you make. However, it is bothersome to restore each change that you make, so I like to continue making changes until I have found the problem. When I know what it is, I go back to the original and make the change. In effect, that restores all the other inconsequential changes that were made in the copy.

It is also helpful to view the process of creating a program as a series of stages. Look at a core set of features as a single stage. Once you have implemented these features and tested them, make a copy of the program before going to the next stage to develop new features. That way, you can always return to the previous stage if you have problems with the code that you add.

We would recommend that you formalize this process, and go so far as to use a source code management system, such as SCCS (Source Code Control System), RCS (Revision Control System), or CVS (Concurrent Versioning System, which is compatible with RCS). The latter two are freely available from any GNU FTP mirror site.-

			5.18.7.2 Before and After Photos

What is difficult in debugging awk is that you don't always know what is happening during the course of the program. You can inspect the input and the output, but there is no way to stop the program in mid-course and examine its state. Thus, it is difficult to know which part of the program is causing a problem.

A common problem is determining when or where in the program the assignment of a variable takes place. The first method of attack is to use the print statement to print the value of the variable at various points in the program. For instance, it is common to use a variable as a flag to determine that a certain condition has occurred. At the beginning of the program, the flag might be set to 0. At one or more points in the program, the value of this flag might be set to 1. The problem is to find where the change actually occurs. If you want to check the flag at a particular part of the program, use print statements before and after the assignment. For instance:

    print flag, "before"
    if (! $1) {
    	.
    	.
    	.
    	flag = 1
    }
    print flag, "after"

If you are unsure about the result of a substitution command or any function, print the string before and after the function is called:

    print $2
    sub(/ *\(/, "(", $2)
    print $2

The value of printing the value before the substitution command is to make sure that the command sees the value that you think should be there. A previous command might have changed that variable. The problem may turn out to be that the format of the input record is not as you thought. Checking the input carefully is a very important step in debugging. In particular, use print statements to verify that the sequence of fields is as you expect. When you find that input is causing the problem, you can either fix the input or write new code to accommodate it.

			5.18.7.3 Finding Out Where the Problem Is

The more modular a script is--that is, the more it can be broken down into separate parts--the easier it is to test and debug the program. One of the advantages of writing functions is that you can isolate what is going on inside the function and test it without affecting other parts of the program. You can omit an entire action and see what happens.

If a program has a number of branching constructs, you might find that an input line falls through one of branches. Test that the input reaches part of a program. For instance, when debugging the masterindex program, described in Chapter 12, "Full-Featured Applications", we wanted to know if an entry containing the word "retrieving" was being handled in a particular part of the program. We inserted the following line in the part of the program where we thought it should be encountered:

    if ($0 ~ /retrieving/) print ">> retrieving" > "/dev/tty"

When the program runs, if it encounters the string "retrieving," it will print the message. (">>" is used as a pair of characters that will instantly call attention to the output; "!!" is also a good one.)

Sometimes you might not be sure which of several print statements are causing a problem. Insert identifiers into the print statement that will alert you to the print statement being executed. In the following example, we simply use the variable name to identify what is printed with a label:

    if (PRIMARY)
    	print (">>PRIMARY:", PRIMARY)
    else
    	if (SECONDARY)
    		print (">>SECONDARY:", SECONDARY)
    	else
    		print (">>TERTIARY:", TERTIARY)

This technique is also useful for investigating whether or not parts of the program are executed at all. Some programs get to be like remodeled homes: a room is added here, a wall is taken down there. Trying to understand the basic structure can be difficult. You might wonder if each of the parts is truly needed or indeed if it is ever executed at all.

If an awk program is part of a pipeline of several programs, even other awk programs, you can use the tee command to redirect output to a file, while also piping the output to the next command. For instance, look at the shell script for running the masterindex program, as shown in Chapter 12, "Full-Featured Applications":

    $INDEXDIR/input.idx $FILES |
    sort -bdf -t:  +0 -1 +1 -2 +3 -4 +2n -3n | uniq |
    $INDEXDIR/pagenums.idx | tee page.tmp |
    $INDEXDIR/combine.idx |
    $INDEXDIR/format.idx

By adding "tee page.tmp", we are able to capture the output of the pagenums.idx program in a file named page.tmp. The same output is also piped to combine.idx.

			5.18.7.4 Commenting Out Loud

Another technique is simply commenting out a series of lines that may be causing problems to see whether they really are. We recommend developing a consistent two-character symbol such as "#%" to comment out lines temporarily. Then you will notice them on subsequent editing and remember to deal with them. It also becomes easier to remove the symbols and restore the lines with a single editing command that does not affect program comments:

    #% if ( thisFails )
    	print "I give up"

Using the comment here eliminates the conditional, so the print statement is executed unconditionally.

			5.18.7.5 Slash and Burn

When all else fails, arm yourself with your editor's delete command and begin deleting portions of the program until the error disappears. Of course, make a copy of the program and delete lines from the temporary copy. This is a very crude technique, but an effective one to use before giving up altogether or starting over from scratch. It is sometimes the only way to discover what is wrong when the only result you get is that the program dumps core. The idea is the same as above, to isolate the problem code. Remove a function, for instance, or a for loop to see if it is the cause of the problem. Be sure to cut out complete units: for instance, all the statements within braces and the matching braces. If the problem persists--the program continues to break--then cut out another large section of the program. Sooner or later, you will find the part that is causing the problem.

You can use "slash and burn" to learn how a program works. First, run the original program on sample input, saving the output. Begin by removing a part of the program that you don't understand. Then run the modified program on sample input and compare the output to the original. Look to see what changed.

			5.18.7.5 Getting Defensive About Your Script

There are all types of input errors and inconsistencies that will turn up bugs in your script. You probably didn't consider that user errors will be pointed to as problems with your program. Therefore, it is a good idea to surround your core program with "defensive" procedures designed to trap inconsistent input records and prevent the program from failing unexpectedly. For instance, you might want to verify each input record before processing it, making sure that the proper number of fields exist or that the kind of data that you expect is found in a particular field.

Another aspect of incorporating defensive techniques is error handling. In other words, what do you want to have happen once the program detects an error? While in some cases you can have the program continue, in other cases it may be preferable that the program print an error message and/or halt.

It is also appropriate to recognize that awk scripts are typically confined to the realm of quick fixes, programs that solve a particular problem rather than solving a class of problems encountered by many different users. Because of the nature of these programs, it is not really necessary that they be professional quality. Thus, it is not necessary to write 100% user-proof programs. For one thing, defensive programming is quite time-consuming and frequently tedious. Secondly, as amateurs, we are at liberty to write programs that perform the way we expect them to; a professional has to write for an audience and must account for their expectations. In brief, if you are writing the script for others to use, consider how it may be used and what problems its users may encounter before considering the program complete. If not, maybe the fact that the script works--even for a very narrow set of circumstances--is good enough and all there is time for.

		5.18.8 Limitations

There are fixed limits within any awk implementation. The only trouble is that the documentation seldom reports them. Table 10.1 lists the limitations as described in The AWK Programming Language. These limitations are implementation-specific but they are good ballpark figures for most systems.

Table 10.1. Limitations
Item 	Limit
Number of fields per record 	100
Characters per input record 	3000
Characters per output record 	3000
Characters per field 	1024
Characters per printf string 	3000
Characters in literal string 	400
Characters in character class 	400
Files open 	15
Pipes open 	1

    NOTE: Despite the number in Table 10.1, experience has shown that most awks allow you to have more than one open pipe.
In terms of numeric values, awk uses double-precision, floating-point numbers that are limited in size by the machine's architecture
Running into these limits can cause unanticipated problems with scripts. In developing examples for the first edition of this book, Dale thought he'd write a search program that could look for a word or sequence of words in a single paragraph. The idea was to read a document as a series of multiline records and if any of the fields contained the search term, print the record, which was a paragraph. It could be used to search through mail files where blank lines delimit paragraphs. The resulting program worked for small test files. However, when tried on larger files, the program dumped core because it encountered a paragraph that was longer than the maximum input record size, which is 3000 characters. (Actually, the file contained an included mail message where blank lines within the message were prefixed by ">".) Thus, when reading multiple lines as a single record, you better be sure that you don't anticipate records longer than 3000 characters. By the way, there is no particular error message that alerts you to the fact that the problem is the size of the current record.

Fortunately, gawk and mawk (see Chapter 11, "A Flock of awks") don't have such small limits; for example, the number of fields in a record is limited in gawk to the maximum value that can be held in a C long, and certainly records can be longer than 3000 characters. These versions allow you to have more open files and pipes.

Recent versions of the Bell Labs awk have two options, -mf N and -mr N, that allow you to set the maximum number of fields and the maximum record size on the command line, as an emergency way to get around the default limits.

		5.18.9 Invoking awk Using the #!
The "#!" syntax is an alternative syntax for invoking awk from a shell script. It has the advantage of allowing you to specify awk parameters and filenames on the shell-script command line. The "#!" syntax is recognized on modern UNIX systems, but is not typically found in older System V systems. The best way to use this syntax is to put the following line as the first line[71] of the shell script
    [71]Note that the pathname to use is system-specific.

    #!/bin/awk -f

"#!" is followed by the pathname that locates your version of awk and then the -f option. After this line, you specify the awk script:

    #!/bin/awk -f
    { print $1 }

Note that no quotes are necessary around the script. All lines in the file after the first one will be executed as though they were specified in a separate script file

A few years ago, there was an interesting discussion on the Net about the use of the "#!" syntax that clarified how it works. The discussion was prompted by a 4.2BSD user's observation that the shell script below fails:

    #!/bin/awk
    { print $1 }

while the one below works:

    #!/bin/sh
    /bin/awk '{ print $1 }'

The two responses that we saw were by Chris Torek and Guy Harris and we will try to summarize their explanation. The first script fails because it passes the filename of the script as the first parameter (argv[1] in C) and awk interprets it as the input file and not the script file. Because no script has been supplied, awk produces a syntax error message. In other words, if the name of the shell script is "myscript," then the first script executes as:


If the script was changed to add the -f option, it looks like this:

    #!/bin/awk -f
    { print $1 }

Then you enter the following command:

    $ myscript myfile

It then executes as though you had typed:

    /bin/awk -f myscript myfile

    NOTE: You can put only one parameter on the "#!" line. This line is processed directly by the UNIX kernel; it is not processed by the shell and thus cannot contain arbitrary shell constructs.

The "#!" syntax allows you to create shell scripts that pass command-line parameters transparently to awk. In other words, you can pass awk parameters from the command line that invokes the shell script.

For instance, we demonstrate passing parameters by changing our sample awk script to expect a parameter n:

    { print $1*n }

Assuming that we have a test file in which the first field contains a number that can be multiplied by n, we can invoke the program, as follows:

This spares us from having to pass "$1" as a shell variable and assigning it to n as an awk parameter inside the shell script.

The masterindex, described in Chapter 12, "Full-Featured Applications", uses the "#!" syntax to invoke awk. If your system does not support this syntax, you can change the script by removing the "#!", placing single quotes around the entire script, and ending the script with "$*", which expands to all shell command-line parameters.

		5.18.10 Full-Featured Applications


			5.18.10.1 An Interactive Spelling Checker

The UNIX spell program does an adequate job of catching spelling errors in a document. For most people, however, it only does half the job. It doesn't help you correct the misspelled words. First-time users of spell find themselves jotting down the misspelled words and then using the text editor to change the document. More skilled users build a sed script to make the changes automatically.

The spellcheck program offers another way--it shows you each word that spell has found and asks if you want to correct the word. You can change each occurrence of the word after seeing the line on which it occurs, or you can correct the spelling error globally. You can also choose to add any word that spell turns up to a local dictionary file.

Before describing the program, let's have a demonstration of how it works. The user enters spellcheck, a shell script that invokes awk, and the name of the document file.

    $ spellcheck ch00
    Use local dict file? (y/n)y

If a dictionary file is not specified on the command line, and a file named dict exists in the current directory, then the user is asked if the local dictionary should be used. spellcheck then runs spell using the local dictionary.

    Running spell checker ...

Using the list of "misspelled" words turned up by spell, spellcheck prompts the user to correct them. Before the first word is displayed, a list of responses is shown that describes what actions are possible.

    Responses: 
    	Change each occurrence, 
    	Global change, 
    	Add to Dict, 
    	Help, 
    	Quit 
    	CR to ignore: 
    1 - Found SparcStation (C/G/A/H/Q/):a

The first word found by spell is "SparcStation." A response of "a" (followed by a carriage return) adds this word to a list that will be used to update the dictionary. The second word is clearly a misspelling and a response of "g" is entered to make the change globally:

    2 - Found languauge (C/G/A/H/Q/):g
    Globally change to:language
    Globally change languauge to language? (y/n):y
    > and a full description of its scripting language. 
    1 lines changed. Save changes? (y/n)y

After prompting the user to enter the correct spelling and confirming the entry, the change is made and each line affected is displayed, preceded by a ">". The user is then asked to approve these changes before they are saved. The third word is also added to the dictionary:

    3 - Found nawk (C/G/A/H/Q/):a

The fourth word is a misspelling of "utilities."

    4 - Found utlitities (C/G/A/H/Q/):c
    These utlitities have many things in common, including
          ^^^^^^^^^^
    Change to:utilities
    Change utlitities to utilities? (y/n):y
    Two other utlitities that are found on the UNIX system
              ^^^^^^^^^^
    Change utlitities to utilities? (y/n):y
    >These utilities have many things in common, including
    >Two other utilities that are found on the UNIX system
    2 lines changed. Save changes? (y/n)y

The user enters "c" to change each occurrence. This response allows the user to see the line containing the misspelling and then make the change. After the user has made each change, the changed lines are displayed and the user is asked to confirm saving the changes.

It is unclear whether the fifth word is a misspelling or not, so the user enters "c" to view the line.

    5 - Found xvf (C/G/A/H/Q/):c
    tar xvf filename
        ^^^
    Change to:RETURN

After determining that it is not a misspelling, the user enters a carriage return to ignore the word. Generally, spell turns up a lot of words that are not misspellings so a carriage return means to ignore the word.

After all the words in the list have been processed, or if the user quits before then, the user is prompted to save the changes made to the document and the dictionary.

    Save corrections in ch00 (y/n)? y
    Make changes to dictionary (y/n)? y

If the user answers "n," the original file and the dictionary are left unchanged.

Now let's look at the spellcheck.awk script, which can be divided into four sections:

    * The BEGIN procedure, that processes the command-line arguments and executes the spell command to create a word list.
    * The main procedure, that reads one word at a time from the list and prompts the user to make a correction.
    * The END procedure, that saves the working copy of the file, overwriting the original. It also appends words from the exception list to the current dictionary.
    * Supporting functions, that are called to make changes in the file.

We will look at each of these sections of the program.

			5.18.10.1.1 BEGIN Procedure

The BEGIN procedure for spellcheck.awk is large. It is also somewhat unusual.

    # spellcheck.awk -- interactive spell checker
    #
    # AUTHOR: Dale Dougherty
    #
    # Usage: nawk -f spellcheck.awk [+dict] file 
    # (Use spellcheck as name of shell program) 
    # SPELLDICT = "dict" 
    # SPELLFILE = "file"

    # BEGIN actions perform the following tasks: 
    #	1) process command-line arguments
    #	2) create temporary filenames
    #	3) execute spell program to create wordlist file
    #	4) display list of user responses

    BEGIN { 
    # Process command-line arguments
    # Must be at least two args -- nawk and filename
    	if (ARGC > 1) {
    	# if more than two args, second arg is dict 
    		if (ARGC > 2) {
    		# test to see if dict is specified with "+"  
    		# and assign ARGV[1] to SPELLDICT
    			if (ARGV[1] ~ /^\+.*/) 
    				SPELLDICT = ARGV[1]
    			else 
    				SPELLDICT = "+" ARGV[1]
    		# assign file ARGV[2] to SPELLFILE 
    			SPELLFILE = ARGV[2]
    		# delete args so awk does not open them as files
    			delete ARGV[1]
    			delete ARGV[2]
    		}
    	# not more than two args
    		else {
    		# assign file ARGV[1] to SPELLFILE 
    			SPELLFILE = ARGV[1]
    		# test to see if local dict file exists
    			if (! system ("test -r dict")) {
    			# if it does, ask if we should use it
    				printf ("Use local dict file? (y/n)")	
    				getline reply < "-"
    			# if reply is yes, use "dict" 
    				if (reply ~ /[yY](es)?/){
    					SPELLDICT = "+dict"
    				}
    			}
    		}
    	} # end of processing args > 1 
    	# if args not > 1, then print shell-command usage 
    	else {
    		print "Usage: spellcheck [+dict] file"
    		exit 1
    	}
    # end of processing command line arguments

    # create temporary file names, each begin with sp_
    	wordlist = "sp_wordlist"
    	spellsource = "sp_input"
    	spellout = "sp_out"

    # copy SPELLFILE to temporary input file
    	system("cp " SPELLFILE " " spellsource)

    # now run spell program; output sent to wordlist
    	print "Running spell checker ..."
    	if (SPELLDICT)
    		SPELLCMD = "spell " SPELLDICT " "
    	else
    		SPELLCMD = "spell "
    	system(SPELLCMD spellsource " > " wordlist )

    # test wordlist to see if misspelled words turned up
    	if ( system("test -s " wordlist ) ) {
    	# if wordlist is empty (or spell command failed), exit
    		print "No misspelled words found."
    		system("rm " spellsource " " wordlist)
    		exit
    	}	

    # assign wordlist file to ARGV[1] so that awk will read it.	
    	ARGV[1] = wordlist

    # display list of user responses 
    	responseList = "Responses: \n\tChange each occurrence," 
    	responseList = responseList "\n\tGlobal change," 
    	responseList = responseList "\n\tAdd to Dict,"  
    	responseList = responseList "\n\tHelp," 
    	responseList = responseList "\n\tQuit" 
    	responseList = responseList "\n\tCR to ignore: "
    	printf("%s", responseList)

    } # end of BEGIN procedure

The first part of the BEGIN procedure processes the command-line arguments. It checks that ARGC is greater than one for the program to continue. That is, in addition to "nawk," a filename must be specified. This file specifies the document that spell will analyze. An optional dictionary filename can be specified as the second argument. The spellcheck script follows the command-line interface of spell, although none of the obscure spell options can be invoked from the spellcheck command line. If a dictionary is not specified, then the script executes a test command to see if the file dict exists. If it does, the prompt asks the user to approve using it as the dictionary file.

Once we've processed the arguments, we delete them from the ARGV array. This is to prevent their being interpreted as filename arguments.

The second part of the BEGIN procedure sets up some temporary files, because we do not want to work directly with the original file. At the end of the program, the user will have the option of saving or discarding the work done in the temporary files. The temporary files all begin with "sp_" and are removed before exiting the program.

The third part of the procedure executes spell and creates a word list. We test to see that this file exists and that there is something in it before proceeding. If for some reason the spell program fails, or there are no misspelled words found, the wordlist file will be empty. If this file does exist, then we assign the filename as the second element in the ARGV array. This is an unusual but valid way of supplying the name of the input file that awk will process. Note that this file did not exist when awk was invoked! The name of the document file, which was specified on the command line, is no longer in the ARGV array. We will not read the document file using awk's main input loop. Instead, a while loop reads the file to find and correct misspelled words.

The last task in the BEGIN procedure is to define and display a list of responses that the user can enter when a misspelled word is displayed. This list is displayed once at the beginning of the program as well as when the user enters "Help" at the main prompt. Putting this list in a variable allows us to access it from different points in the program, if necessary, without maintaining duplicates. The assignment of responseList could be done more simply, but the long string would not be printable in this book. (You can't break a string over two lines.)

				5.18.10.1.2 Main Procedure

The main procedure is rather small, merely displaying a misspelled word and prompting the user to enter an appropriate response. This procedure is executed for each misspelled word.

One reason this procedure is short is because the central action--correcting a misspelled word--is handled by two larger user-defined functions, which we'll see in the last section.

    # main procedure, executed for each line in wordlist.
    #	Purpose is to show misspelled word and prompt user
    #	for appropriate action.

    {
    # assign word to misspelling
    	misspelling = $1 
    	response = 1
    	++word

    # print misspelling and prompt for response
    	while (response !~ /(^[cCgGaAhHqQ])|^$/ ) {
    		printf("\n%d - Found %s (C/G/A/H/Q/):", word, misspelling)
    		getline response < "-"
    	}
    # now process the user's response
    # CR - carriage return ignores current word 
    # Help
    	if (response ~ /[Hh](elp)?/) {
    	# Display list of responses and prompt again.
    		printf("%s", responseList)
    		printf("\n%d - Found %s (C/G/A/Q/):", word, misspelling)
    		getline response < "-"
    	}
    # Quit
    	if (response ~ /[Qq](uit)?/) exit
    # Add to dictionary
    	if ( response ~ /[Aa](dd)?/) { 
    		dict[++dictEntry] = misspelling
    	}
    # Change each occurrence
    	if ( response ~ /[cC](hange)?/) {
    	# read each line of the file we are correcting
    		newspelling = ""; changes = ""
    		while( (getline < spellsource) > 0){
    		# call function to show line with misspelled word
    		# and prompt user to make each correction 
    			make_change($0)
    		# all lines go to temp output file
    			print > spellout
    		}	
    	# all lines have been read 
    	# close temp input and temp output file
    		close(spellout)
    		close(spellsource)
    	# if change was made
    		if (changes){ 
    		# show changed lines
    			for (j = 1; j <= changes; ++j)
    				print changedLines[j]
    			printf ("%d lines changed. ", changes) 
    		# function to confirm before saving changes
    			confirm_changes()
    		}
    	}
    # Globally change
    	if ( response ~ /[gG](lobal)?/) {
    	# call function to prompt for correction
    	# and display each line that is changed.
    	# Ask user to approve all changes before saving.
    		make_global_change()
    	}	
    } # end of Main procedure

The first field of each input line from wordlist contains the misspelled word and it is assigned to misspelling. We construct a while loop inside which we display the misspelled word to the user and prompt for a response. Look closely at the regular expression that tests the value of response:

    while (response !~ /(^[cCgGaAhHqQ])|^$/)

The user can only get out of this loop by entering any of the specified letters or by entering a carriage return--an empty line. The use of regular expressions for testing user input helps tremendously in writing a simple but flexible program. The user can enter a single letter "c" in lower- or uppercase or a word beginning with "c" such as "Change."

The rest of the main procedure consists of conditional statements that test for a specific response and perform a corresponding action. The first response is "help," which displays the list of responses again and then redisplays the prompt.

The next response is "quit." The action associated with quit is exit, which drops out of the main procedure and goes to the END procedure.

If the user enters "add," the misspelled word is put in the array dict and will be added as an exception in a local dictionary.

The "Change" and "Global" responses cause the program's real work to begin. It's important to understand how they differ. When the user enters "c" or "change," the first occurrence of the misspelled word in the document is displayed. Then the user is prompted to make the change. This happens for each occurrence in the document. When the user enters "g" or "global," the user is prompted to make the change right away, and all the changes are made at once without prompting the user to confirm each one. This work is largely handled by two functions, make_change() and make_global_change(), which we'll look at in the last section. These are all the valid responses, except one. A carriage return means to ignore the misspelled word and get the next word in the list. This is the default action of the main input loop, so no conditional need be set up for it.

				5.18.10.1.3 END Procedure

The END procedure, of course, is reached in one of the following circumstances:

    *

      The spell command failed or did not turn up any misspellings.
    *

      The list of misspelled words is exhausted.
    *

      The user has entered "quit" at a prompt.

The purpose of the END procedure is to allow the user to confirm any permanent change to the document or the dictionary.

    # END procedure makes changes permanent.
    # It overwrites the original file, and adds words
    # to the dictionary.
    # It also removes the temporary files.

    END {
    # if we got here after reading only one record, 
    # no changes were made, so exit.
    	if (NR <= 1) exit
    # user must confirm saving corrections to file
    	while (saveAnswer !~ /([yY](es)?)|([nN]o?)/ ) {
    		printf "Save corrections in %s (y/n)? ", SPELLFILE
    		getline saveAnswer < "-"
    	}
    # if answer is yes then mv temporary input file to SPELLFILE
    # save old SPELLFILE, just in case
    	if (saveAnswer ~ /^[yY]/) {
    		system("cp " SPELLFILE " " SPELLFILE ".orig")
    		system("mv " spellsource " " SPELLFILE)
    	}
    # if answer is no then rm temporary input file
    	if (saveAnswer ~ /^[nN]/)
    		system("rm " spellsource) 

    # if words have been added to dictionary array, then prompt
    # to confirm saving in current dictionary. 
    	if (dictEntry) {
    		printf "Make changes to dictionary (y/n)? "
    		getline response < "-"
    		if (response ~ /^[yY]/){
    		# if no dictionary defined, then use "dict"
    			if (! SPELLDICT) SPELLDICT = "dict"
    		
    		# loop through array and append words to dictionary
    			sub(/^\+/, "", SPELLDICT)
    			for ( item in dict )
    				print dict[item] >> SPELLDICT
    			close(SPELLDICT)
    		# sort dictionary file 
    			system("sort " SPELLDICT "> tmp_dict")
    			system("mv " "tmp_dict " SPELLDICT)
    		}
    	}
    # remove word list
    	system("rm sp_wordlist")
    } # end of END procedure

The END procedure begins with a conditional statement that tests that the number of records is less than or equal to 1. This occurs when the spell program does not generate a word list or when the user enters "quit" after seeing just the first record. If so, the END procedure is exited as there is no work to save.

Next, we create a while loop to ask the user about saving the changes made to the document. It requires the user to respond "y" or "n" to the prompt. If the answer is "y," the temporary input file replaces the original document file. If the answer is "n," the temporary file is removed. No other responses are accepted.

Next, we test to see if the dict array has something in it. Its elements are the words to be added to the dictionary. If the user approves adding them to the dictionary, these words are appended to the current dictionary, as defined above, or if not, to a local dict file. Because the dictionary must be sorted to be read by spell, a sort command is executed with the output sent to a temporary file that is afterwards copied over the original file.
12.1.4. Supporting Functions

There are three supporting functions, two of which are large and do the bulk of the work of making changes in the document. The third function supports that work by confirming that the user wants to save the changes that were made.

When the user wants to "Change each occurrence" in the document, the main procedure has a while loop that reads the document one line at a time. (This line becomes $0.) It calls the make_change() function to see if the line contains the misspelled word. If it does, the line is displayed and the user is prompted to enter the correct spelling of the word.

    # make_change -- prompt user to correct misspelling 
    #		 for current input line.  Calls itself
    # 		 to find other occurrences in string.
    # 	stringToChange -- initially $0; then unmatched substring of $0
    # 	len -- length from beginning of $0 to end of matched string 
    # Assumes that misspelling is defined. 

    function make_change (stringToChange, len,	# parameters
    	line, OKmakechange, printstring, carets)	# locals
    {
    # match misspelling in stringToChange; otherwise do nothing 
      if ( match(stringToChange, misspelling) ) {
      # Display matched line 
    	printstring = $0
    	gsub(/\t/, " ", printstring)
    	print printstring
    	carets = "^"
    	for (i = 1; i < RLENGTH; ++i)
    		carets = carets "^"
    	if (len)
    		FMT = "%" len+RSTART+RLENGTH-2 "s\n"
    	else
    		FMT = "%" RSTART+RLENGTH-1 "s\n"
    	printf(FMT, carets)

      # Prompt user for correction, if not already defined
    	if (! newspelling) {
    		printf "Change to:"
    		getline newspelling < "-"
    	}
      # A carriage return falls through
      # If user enters correction, confirm  
    	while (newspelling && ! OKmakechange) {
    		printf ("Change %s to %s? (y/n):", misspelling, newspelling)
    		getline OKmakechange < "-"
    		madechg = ""
    	# test response
    		if (OKmakechange ~ /[yY](es)?/ ) {
    		# make change (first occurrence only)
    			madechg = sub(misspelling, newspelling, stringToChange)
    		}
    		else if ( OKmakechange ~ /[nN]o?/ ) {
    			# offer chance to re-enter correction 
    			printf "Change to:"
    			getline newspelling < "-"
    			OKmakechange = ""
    		}
    	} # end of while loop

       # if len, we are working with substring of $0
    	if (len) {
    	# assemble it
    		line = substr($0,1,len-1)
    		$0 = line stringToChange
    	}
    	else {
    		$0 = stringToChange
    		if (madechg) ++changes
    	}

       # put changed line in array for display
    	if (madechg) 
    		changedLines[changes] = ">" $0

       # create substring so we can try to match other occurrences
    	len += RSTART + RLENGTH
    	part1 = substr($0, 1, len-1)
    	part2 = substr($0, len)
       # calls itself to see if misspelling is found in remaining part 
    	make_change(part2, len) 

      } # end of if

    } # end of make_change()

If the misspelled word is not found in the current input line, nothing is done. If it is found, this function shows the line containing the misspelling and asks the user if it should be corrected. Underneath the display of the current line is a row of carets that indicates the misspelled word.

    Two other utlitities that are found on the UNIX system
              ^^^^^^^^^^

The current input line is copied to printstring because it is necessary to change the line for display purposes. If the line contains any tabs, each tab in this copy of the line is temporarily replaced by a single space. This solves a problem of aligning the carets when tabs were present. (A tab counts as a single character when determining the length of a line but actually occupies greater space when displayed, usually five to eight characters long.)

After displaying the line, the function prompts the user to enter a correction. It then follows up by displaying what the user has entered and asks for confirmation. If the correction is approved, the sub() function is called to make the change. If not approved, the user is given another chance to enter the correct word.

Remember that the sub() function only changes the first occurrence on a line. The gsub() function changes all occurrences on a line, but we want to allow the user to confirm each change. Therefore, we have to try to match the misspelled word against the remaining part of the line. And we have to be able to match the next occurrence regardless of whether or not the first occurrence was changed.

To do this, make_change() is designed as a recursive function; it calls itself to look for additional occurrences on the same line. In other words, the first time make_change() is called, it looks at all of $0 and matches the first misspelled word on that line. Then it splits the line into two parts--the first part contains the characters up to the end of the first occurrence and the second part contains the characters that immediately follow up to the end of the line. Then it calls itself to try and match the misspelled word in the second part. When called recursively, the function takes two arguments.

    make_change(part2, len)

The first is the string to be changed, which is initially $0 when called from the main procedure but each time thereafter is the remaining part of $0. The second argument is len or the length of the first part, which we use to extract the substring and reassemble the two parts at the end.

The make_change() function also collects an array of lines that were changed.

    # put changed line in array for display
            if (madechg)
                    changedLines[changes] = ">" $0

The variable madechg will have a value if the sub() function was successful. $0 (the two parts have been rejoined) is assigned to an element of the array. When all of the lines of the document have been read, the main procedure loops through this array to display all the changed lines. Then it calls the confirm_changes() function to ask if these changes should be saved. It copies the temporary output file over the temporary input file, keeping intact the corrections made for the current misspelled word.

If a user decides to make a "Global change," the make_global_change() function is called to do it. This function is similar to the make_change() function, but is simpler because we can make the change globally on each line.

    # make_global_change --
    #		prompt user to correct misspelling 
    #		for all lines globally.  
    #		Has no arguments
    # Assumes that misspelling is defined. 

    function make_global_change(    newspelling, OKmakechange, changes)
    {
    # prompt user to correct misspelled word
       printf "Globally change to:"
       getline newspelling < "-"

    # carriage return falls through
    # if there is an answer, confirm 
       while (newspelling && ! OKmakechange) {
    		printf ("Globally change %s to %s? (y/n):", misspelling,
    				newspelling)
    		getline OKmakechange < "-"
    	# test response and make change
    		if (OKmakechange ~ /[yY](es)?/ ) {
    		# open file, read all lines 
    			while( (getline < spellsource) > 0){
    			# if match is found, make change using gsub
    			# and print each changed line.
    				if ($0 ~ misspelling) {
    					madechg = gsub(misspelling, newspelling)
    					print ">", $0
    					changes += 1  # counter for line changes
    				}
    			# write all lines to temp output file
    				print > spellout
    			} # end of while loop for reading file

    		# close temporary files
    			close(spellout)
    			close(spellsource)
    		# report the number of changes	
    			printf ("%d lines changed. ", changes) 

    		# function to confirm before saving changes
    			confirm_changes()
    		} # end of if (OKmakechange ~ y) 

    	# if correction not confirmed,  prompt for new word
    		else if ( OKmakechange ~ /[nN]o?/ ){
    			printf "Globally change to:"
    			getline newspelling < "-"
    			OKmakechange = ""
    		}

      } # end of while loop for prompting user for correction

    } # end of make_global_change()

This function prompts the user to enter a correction. A while loop is set up to read all the lines of the document and apply the gsub() function to make the changes. The main difference is that all the changes are made at once--the user is not prompted to confirm them. When all lines have been read, the function displays the lines that were changed and calls confirm_changes() to get the user to approve this batch of changes before saving them.

The confirm_changes() function is a routine called to get approval of the changes made when the make_change() or make_global_change() function is called.

    # confirm_changes --  
    #		confirm before saving changes

    function confirm_changes(  savechanges) {
    # prompt to confirm saving changes
    	while (! savechanges ) {
    		printf ("Save changes? (y/n)")
    		getline savechanges < "-"
    	}
    # if confirmed, mv output to input
    	if (savechanges ~ /[yY](es)?/)
    		system("mv " spellout " " spellsource) 
    }

The reason for creating this function is to prevent the duplication of code. Its purpose is simply to require the user to acknowledge the changes before replacing the old version of the document file (spellsource) with the new version (spellout).

				5.18.10.1.4 he spellcheck Shell Script

To make it easy to invoke this awk script, we create the spellcheck shell script (say that three times fast). It contains the following lines:

    AWKLIB=/usr/local/awklib
    nawk -f $AWKLIB/spellcheck.awk $*

This script sets up a shell variable AWKLIB that specifies the location of the spellcheck.awk script. The symbol "$*" expands to all command-line parameters following the name of the script. These parameters are then available to awk.

One of the interesting things about this spell checker is how little is done in the shell script.[85] All of the work is done in the awk programming language, including executing 10 UNIX commands. We're using a consistent syntax and the same constructs by doing it all in awk. When you have to do some of your work in the shell and some in awk, it can get confusing. For instance, you have to remember the differences in the syntax of if conditionals and how to reference variables. Modern versions of awk provide a true alternative to the shell for executing commands and interacting with a user.

			This is the full script:
			Full Listing of spellcheck.awk

    # spellcheck.awk -- interactive spell checker
    #
    # AUTHOR: Dale Dougherty
    #
    # Usage: nawk -f spellcheck.awk [+dict] file 
    # (Use spellcheck as name of shell program) 
    # SPELLDICT = "dict" 
    # SPELLFILE = "file"

    # BEGIN actions perform the following tasks: 
    #      1) process command line arguments
    #      2) create temporary filenames
    #      3) execute spell program to create wordlist file
    #      4) display list of user responses

    BEGIN { 
    # Process command line arguments
    # Must be at least two args -- nawk and filename
          if (ARGC > 1) {
          # if more than two args, second arg is dict 
                if (ARGC > 2) {
                # test to see if dict is specified with "+"  
                # and assign ARGV[1] to SPELLDICT
                      if (ARGV[1] ~ /^\+.*/) 
                            SPELLDICT = ARGV[1]

                      else 
                            SPELLDICT = "+" ARGV[1]
                # assign file ARGV[2] to SPELLFILE 
                      SPELLFILE = ARGV[2]
                # delete args so awk does not open them as files
                      delete ARGV[1]
                      delete ARGV[2]
                }
          # not more than two args
                else {
                # assign file ARGV[1] to SPELLFILE 
                      SPELLFILE = ARGV[1]
                # test to see if local dict file exists
                      if (! system ("test -r dict")) {
                      # if it does, ask if we should use it
                            printf ("Use local dict file? (y/n)")   
                            getline reply < "-"
                      # if reply is yes, use "dict" 
                            if (reply ~ /[yY](es)?/){
                                  SPELLDICT = "+dict"
                            }
                      }
                }
          } # end of processing args > 1 
          # if args not > 1, then print shell-command usage 
          else {
                print "Usage: spellcheck [+dict] file"
                exit 1
          }
    # end of processing command line arguments

    # create temporary file names, each begin with sp_
          wordlist = "sp_wordlist"
          spellsource = "sp_input"
          spellout = "sp_out"

    # copy SPELLFILE to temporary input file
          system("cp " SPELLFILE " " spellsource)

    # now run spell program; output sent to wordlist
          print "Running spell checker ..."
          if (SPELLDICT)
                SPELLCMD = "spell " SPELLDICT " "
          else
                SPELLCMD = "spell "
          system(SPELLCMD spellsource " > " wordlist )

    # test wordlist to see if misspelled words turned up
          if ( system("test -s " wordlist ) ) {
          # if wordlist is empty, (or spell command failed), exit
                print "No misspelled words found."
                system("rm " spellsource " " wordlist)
                exit
          }   

    # assign wordlist file to ARGV[1] so that awk will read it.   
          ARGV[1] = wordlist

    # display list of user responses 
          responseList = "Responses: \n\tChange each occurrence," 
          responseList = responseList "\n\tGlobal change," 
          responseList = responseList "\n\tAdd to Dict,"  
          responseList = responseList "\n\tHelp," 
          responseList = responseList "\n\tQuit" 
          responseList = responseList "\n\tCR to ignore: "
          printf("%s", responseList)

    } # end of BEGIN procedure

    # main procedure, executed for each line in wordlist.
    #     Purpose is to show misspelled word and prompt user
    #     for appropriate action.

    {
    # assign word to misspelling
          misspelling = $1 
          response = 1
          ++word
    # print misspelling and prompt for response
          while (response !~ /(^[cCgGaAhHqQ])|^$/ ) {
                printf("\n%d - Found %s (C/G/A/H/Q/):", word, misspelling)
                getline response < "-"
          }
    # now process the user's response
    # CR - carriage return ignores current word 
    # Help
          if (response ~ /[Hh](elp)?/) {
          # Display list of responses and prompt again.
                printf("%s", responseList)
                printf("\n%d - Found %s (C/G/A/Q/):", word, misspelling)
                getline response < "-"
          }
    # Quit
          if (response ~ /[Qq](uit)?/) exit
    # Add to dictionary
          if ( response ~ /[Aa](dd)?/) { 
                dict[++dictEntry] = misspelling
          }
    # Change each occurrence
          if ( response ~ /[cC](hange)?/) {
          # read each line of the file we are correcting
                newspelling = ""; changes = ""
                while( (getline < spellsource) > 0){
                # call function to show line with misspelled word
                # and prompt user to make each correction 
                      make_change($0)
                # all lines go to temp output file
                      print > spellout
                }   
          # all lines have been read 
          # close temp input and temp output file
                close(spellout)
                close(spellsource)
          # if change was made
                if (changes){ 
                # show changed lines
                      for (j = 1; j <= changes; ++j)
                            print changedLines[j]
                      printf ("%d lines changed. ", changes) 
                # function to confirm before saving changes
                      confirm_changes()
                }
          }
    # Globally change
          if ( response ~ /[gG](lobal)?/) {
          # call function to prompt for correction
          # and display each line that is changed.
          # Ask user to approve all changes before saving.
                make_global_change()
          }   
    } # end of Main procedure

    # END procedure makes changes permanent.
    # It overwrites the original file, and adds words
    # to the dictionary.
    # It also removes the temporary files.

    END {
    # if we got here after reading only one record, 
    # no changes were made, so exit.
          if (NR <= 1) exit
    # user must confirm saving corrections to file
          while (saveAnswer !~ /([yY](es)?)|([nN]o?)/ ) {
                printf "Save corrections in %s (y/n)? ", SPELLFILE
                getline saveAnswer < "-"
          }
    # if answer is yes then mv temporary input file to SPELLFILE
    # save old SPELLFILE, just in case
          if (saveAnswer ~ /^[yY]/) {
                system("cp " SPELLFILE " " SPELLFILE ".orig")
                system("mv " spellsource " " SPELLFILE)
          }
    # if answer is no then rm temporary input file
          if (saveAnswer ~ /^[nN]/)
                system("rm " spellsource) 

    # if words have been added to dictionary array, then prompt
    # to confirm saving in current dictionary. 
          if (dictEntry) {
                printf "Make changes to dictionary (y/n)? "
                getline response < "-"
                if (response ~ /^[yY]/){
                # if no dictionary defined, then use "dict"
                      if (! SPELLDICT) SPELLDICT = "dict"
          
                # loop through array and append words to dictionary
                      sub(/^\+/, "", SPELLDICT)
                      for ( item in dict )
                            print dict[item] >> SPELLDICT
                      close(SPELLDICT)
                # sort dictionary file 
                      system("sort " SPELLDICT "> tmp_dict")
                      system("mv " "tmp_dict " SPELLDICT)
                }
          }
    # remove word list
          system("rm sp_wordlist")
    } # end of END procedure

    # function definitions

    # make_change -- prompt user to correct misspelling 
    #                for current input line.  Calls itself
    #                to find other occurrences in string.
    #     stringToChange -- initially $0; then unmatched substring of $0
    #     len -- length from beginning of $0 to end of matched string 
    # Assumes that misspelling is defined. 

    function make_change (stringToChange, len,   # parameters
          line, OKmakechange, printstring, carets)   # locals
    {
    # match misspelling in stringToChange; otherwise do nothing 
       if ( match(stringToChange, misspelling) ) {
       # Display matched line 
             printstring = $0
             gsub(/\t/, " ", printstring)
             print printstring
             carets = "^"
             for (i = 1; i < RLENGTH; ++i)
                   carets = carets "^"
             if (len)
                   FMT = "%" len+RSTART+RLENGTH-2 "s\n"
             else
                   FMT = "%" RSTART+RLENGTH-1 "s\n"
             printf(FMT, carets)
       # Prompt user for correction, if not already defined
             if (! newspelling) {
                   printf "Change to:"
                   getline newspelling < "-"
             }
       # A carriage return falls through
       # If user enters correction, confirm  
             while (newspelling && ! OKmakechange) {
                   printf ("Change %s to %s? (y/n):", misspelling, newspelling)
                   getline OKmakechange < "-"
                   madechg = ""
             # test response
                   if (OKmakechange ~ /[yY](es)?/ ) {
                   # make change (first occurrence only)
                         madechg = sub(misspelling, newspelling, stringToChange)
                   }
                   else if ( OKmakechange ~ /[nN]o?/ ) {
                         # offer chance to re-enter correction 
                         printf "Change to:"
                         getline newspelling < "-"
                         OKmakechange = ""
                   }
             } # end of while loop

       # if len, we are working with substring of $0
             if (len) {
             # assemble it
                   line = substr($0,1,len-1)
                   $0 = line stringToChange
             }
             else {
                   $0 = stringToChange
                   if (madechg) ++changes
             }

       # put changed line in array for display
             if (madechg) 
                   changedLines[changes] = ">" $0


       # create substring so we can try to match other occurrences
             len += RSTART + RLENGTH
             part1 = substr($0, 1, len-1)
             part2 = substr($0, len)
       # calls itself to see if misspelling is found in remaining part 
             make_change(part2, len) 

       } # end of if

    } # end of make_change()

    # make_global_change --
    #        prompt user to correct misspelling 
    #        for all lines globally.  
    #        Has no arguments
    # Assumes that misspelling is defined. 

    function make_global_change(    newspelling, OKmakechange, changes)
    {
    # prompt user to correct misspelled word
       printf "Globally change to:"
       getline newspelling < "-"

    # carriage return falls through
    # if there is an answer, confirm 
       while (newspelling && ! OKmakechange) {
                   printf ("Globally change %s to %s? (y/n):", misspelling,
                         newspelling)
                   getline OKmakechange < "-"
             # test response and make change
                   if (OKmakechange ~ /[yY](es)?/ ) {
                   # open file, read all lines 
                         while( (getline < spellsource) > 0){
                         # if match is found, make change using gsub
                         # and print each changed line.
                               if ($0 ~ misspelling) {
                                        madechg = gsub(misspelling, newspelling)
                                        print ">", $0
                                        changes += 1  # counter for line changes
                               }
                         # write all lines to temp output file
                               print > spellout
                         } # end of while loop for reading file

                   # close temporary files
                         close(spellout)
                         close(spellsource)
                   # report the number of changes   
                         printf ("%d lines changed. ", changes) 
                   # function to confirm before saving changes
                         confirm_changes()
                   } # end of if (OKmakechange ~ y) 

       # if correction not confirmed,  prompt for new word
                   else if ( OKmakechange ~ /[nN]o?/ ){
                         printf "Globally change to:"
                         getline newspelling < "-"
                         OKmakechange = ""
                   }

       } # end of while loop for prompting user for correction

    } # end of make_global_change()

    # confirm_changes --  
    #        confirm before saving changes

    function confirm_changes(  savechanges) {
    # prompt to confirm saving changes
          while (! savechanges ) {
                printf ("Save changes? (y/n)")
                getline savechanges < "-"
          }
    # if confirmed, mv output to input
          if (savechanges ~ /[yY](es)?/)
                system("mv " spellout " " spellsource) 
          }


			5.18.10.2 A Miscellany of Scripts

				5.18.10.2.1  uutot.awk--Report UUCP Statistics

Contributed by Roger A. Cornelius

Here's something I wrote in nawk in response to all the C versions of the same thing which were posted to alt.sources awhile back. Basically, it summarizes statistics of uucp connections (connect time, throughput, files transmitted, etc.). It only supports HDB-style log files, but will show statistics on a site-by-site, or on an overall (all sites), basis. [It also works with /usr/spool/uucp/SYSLOG.]

I use a shell wrapper which calls "awk -f" to run this, but it's not necessary. Usage information is in the header. (Sorry about the lack of comments.)

    # @(#) uutot.awk - display uucp statistics - requires new awk
    # @(#) Usage:awk -f uutot.awk [site ...] /usr/spool/uucp/.Admin/xferstats
    # Author: Roger A. Cornelius (rac@sherpa.uucp)

    #       dosome[];               # site names to work for - all if not set
    #       remote[];               # array of site names
    #       bytes[];                # bytes xmitted by site
    #       time[];	               # time spent by site
    #       files[];                # files xmitted by site
    BEGIN {
    	doall = 1;
    	if (ARGC > 2) {
    		doall = 0;
    		for (i = 1; i < ARGC-1; i++) {
    			dosome[ ARGV[i] ];
    			ARGV[i] = "";
    		}
    	}

    	kbyte = 1024	# 1000 if you're not picky
    	bang = "!";
    	sending = "->";
    	xmitting = "->" "|" "<-";

    	hdr1 = "Remote     K-Bytes   K-Bytes   K-Bytes " \
    		"Hr:Mn:Sc Hr:Mn:Sc AvCPS AvCPS    #    #\n";
    	hdr2 = "SiteName      Recv      Xmit     Total     " \
    		"Recv     Xmit  Recv  Xmit Recv Xmit\n";

    	hdr3 = "-------- --------- --------- --------- -------- " \
    		"-------- ----- ----- ---- ----";
    	fmt1 = "%-8.8s %9.3f %9.3f %9.3f %2d:%02d:%02.0f " \
    		"%2d:%02d:%02.0f %5.0f %5.0f %4d %4d\n";
    	fmt2 = "Totals   %9.3f %9.3f %9.3f %2d:%02d:%02.0f " \
    		"%2d:%02d:%02.0f %5.0f %5.0f %4d %4d\n";
    }
    {
    	if ($6 !~ xmitting)		# should never be
    		next;
    	direction = ($6 == sending ? 1 : 2)

    	site = substr($1,1,index($1,bang)-1);
    	if (site in dosome || doall) {
    		remote[site];
    		bytes[site,direction] += $7;
    		time[site,direction] += $9;
    		files[site,direction]++;
    	}
    }
    END {
    	print hdr1 hdr2 hdr3;
    	for (k in remote) {
    		rbyte += bytes[k,2];	sbyte += bytes[k,1];
    		rtime += time[k,2];	stime += time[k,1];
    		rfiles += files[k,2];	sfiles += files[k,1];
    		printf(fmt1, k, bytes[k,2]/kbyte, bytes[k,1]/kbyte,
    			(bytes[k,2]+bytes[k,1])/kbyte,
    			time[k,2]/3600, (time[k,2]%3600)/60, time[k,2]%60,
    			time[k,1]/3600, (time[k,1]%3600)/60, time[k,1]%60,
    			bytes[k,2] && time[k,2] ? bytes[k,2]/time[k,2] : 0,
    			bytes[k,1] && time[k,1] ? bytes[k,1]/time[k,1] : 0,
    			files[k,2], files[k,1]);
    	}

    	print hdr3
    	printf(fmt2, rbyte/kbyte, sbyte/kbyte, (rbyte+sbyte)/kbyte,
    		rtime/3600, (rtime%3600)/60, rtime%60,
    		stime/3600, (stime%3600)/60, stime%60,
    		rbyte && rtime ? rbyte/rtime : 0,
    		sbyte && stime ? sbyte/stime : 0,
    		rfiles, sfiles);
    }

A test file was generated to test Cornelius' program. Here are a few lines extracted from /usr/spool/uucp/.Admin/xferstats (because each line in this file is too long to print on a page, we have broken the line following the directional arrow for display purposes only):

    isla!nuucp S (8/3-16:10:17) (C,126,25) [ttyi1j] ->
                         1131/4.880 secs, 231 bytes/sec
    isla!nuucp S (8/3-16:10:20) (C,126,26) [ttyi1j] ->
                         149/0.500 secs, 298 bytes/sec

    isla!sue S (8/3-16:10:49) (C,126,27) [ttyi1j] ->
                         646/25.230 secs, 25 bytes/sec
    isla!sue S (8/3-16:10:52) (C,126,28) [ttyi1j] ->
                         145/0.510 secs, 284 bytes/sec
    uunet!uisla M (8/3-16:15:50) (C,951,1) [cui1a] ->
                         1191/0.660 secs, 1804 bytes/sec
    uunet!uisla M (8/3-16:15:53) (C,951,2) [cui1a] ->
                         148/0.080 secs, 1850 bytes/sec
    uunet!uisla M (8/3-16:15:57) (C,951,3) [cui1a] ->
                         1018/0.550 secs, 1850 bytes/sec
    uunet!uisla M (8/3-16:16:00) (C,951,4) [cui1a] ->
                         160/0.070 secs, 2285 bytes/sec
    uunet!daemon M (8/3-16:16:06) (C,951,5) [cui1a] <-
                         552/2.740 secs, 201 bytes/sec
    uunet!daemon M (8/3-16:16:09) (C,951,6) [cui1a] <-
                         102/1.390 secs, 73 bytes/sec

Note that there are 12 fields; however, the program really only uses fields 1, 6, 7, and 9. Running the program on the sample input produces the following results:

    $ nawk -f uutot.awk uutot.test
    Remote     K-Bytes   K-Bytes   K-Bytes Hr:Mn:Sc Hr:Mn:Sc AvCPS AvCPS    #    #
    SiteName      Recv      Xmit     Total     Recv     Xmit  Recv  Xmit Recv Xmit
    -------- --------- --------- --------- -------- -------- ----- ----- ---- ----
    uunet        0.639     2.458     3.097  0:04:34  2:09:49     2     0    2    4
    isla         0.000     2.022     2.022  0:00:00  0:13:58     0     2    0    4
    -------- --------- --------- --------- -------- -------- ----- ----- ---- ----
    Totals       0.639     4.480     5.119  0:04:34  2:23:47     2     1    2    8

    This nawk application is an excellent example of a clearly written awk program. It is also a typical example of using awk to change a rather obscure UNIX log into a useful report.

Although Cornelius apologizes for the lack of comments that explain the logic of the program, the usage of the program is clear from the initial comments. Also, he uses variables to define search patterns and the report's layout. This helps to simplify conditional and print statements in the body of the program. It also helps that the variables have names which aid in immediately recognizing their purpose.

				5.18.10.2.2  phonebill--Track Phone Usage

Contributed by Nick Holloway

The problem is to calculate the cost of phone calls made. In the United Kingdom, charges are made for the number of "units" used during the duration of the call (no free local calls). The length of time a "unit" lasts depends on the charge band (linked to distance) and the charge rate (linked to time of day). You get charged a whole unit as soon as the time period begins.

The input to the program is four fields. The first field is the date (not used). The second field is "band/rate" and is used to look up the length a unit will last. The third field is the length of the call. This can either be "ss," "mm:ss," or "hh:mm:ss". The fourth field is the name of the caller. We keep a stopwatch (old cheap digital), a book, and a pen. Come bill time this is fed through my awk script. This only deals with the cost of the calls, not the standing charge.

The aim of the program was to enable the minimum amount of information to be entered by the callers, and the program could be used to collect together the call costs for each user in one report. It is also written so that if British Telecom changes its costs, these can be done easily in the top of the source (this has been done once already). If more charge bands or rates are added, the table can be simply expanded (wonders of associative arrays). There are no real sanity checks done on the input data. The usage is:

    phonebill [ file ... ]

Here is a (short) sample of input and output.

Input:

    29/05   b/p      5:35   Nick
    29/05   L/c   1:00:00   Dale
    01/06   L/c     30:50   Nick

Output:

    Summary for Dale:
    	29/05   L/c  1:00:00  11 units
    Total: 11 units @ 5.06 pence per unit = $0.56
    Summary for Nick:
    	29/05   b/p     5:35  19 units
    	01/06   L/c    30:50   6 units
    Total: 25 units @ 5.06 pence per unit = $1.26

The listing for phonebill follows:

    #!/bin/awk -f
    #------------------------------------------------------------------
    #   Awk script to take in phone usage - and calculate cost for each
    #   person
    #------------------------------------------------------------------
    #   Author: N.Holloway (alfie@cs.warwick.ac.uk)
    #   Date  : 27 January 1989
    #   Place : University of Warwick
    #------------------------------------------------------------------
    #   Entries are made in the form
    #	Date   Type/Rate   Length  Name
    #
    #   Format:
    #	Date		: "dd/mm"		- one word
    #	Type/Rate	: "bb/rr"  (e.g. L/c)
    #	Length		: "hh:mm:ss", "mm:ss", "ss"
    #	Name		: "Fred"		- one word (unique)
    #------------------------------------------------------------------
    #   Charge information kept in array 'c', indexed by "type/rate",
    #   and the cost of a unit is kept in the variable 'pence_per_unit'
    #   The info is stored in two arrays, both indexed by the name. The
    #   first 'summary' has the lines that hold input data, and number 
    #   of units, and 'units' has the cumulative total number of units
    #   used by name.
    #------------------------------------------------------------------

    BEGIN \
        {	
    	# --- Cost per unit
    	pence_per_unit  = 4.40		# cost is 4.4 pence per unit
    	pence_per_unit *= 1.15		# VAT is 15%

    	# --- Table of seconds per unit for different bands/rates
    	#     [ not applicable have 0 entered as value ]
    	c ["L/c"] = 330 ;  c ["L/s"] = 85.0;  c ["L/p"] = 60.0;
    	c ["a/c"] =  96 ;  c ["a/s"] = 34.3;  c ["a/p"] = 25.7;
    	c ["b1/c"]= 60.0;  c ["b1/s"]= 30.0;  c ["b1/p"]= 22.5;
    	c ["b/c"] = 45.0;  c ["b/s"] = 24.0;  c ["b/p"] = 18.0;
    	c ["m/c"] = 12.0;  c ["m/s"] = 8.00;  c ["m/p"] = 8.00;
    	c ["A/c"] = 9.00;  c ["A/s"] = 7.20;  c ["A/p"] = 0   ;
    	c ["A2/c"]= 7.60;  c ["A2/s"]= 6.20;  c ["A2/p"]= 0   ;
    	c ["B/c"] = 6.65;  c ["B/s"] = 5.45;  c ["B/p"] = 0   ;
    	c ["C/c"] = 5.15;  c ["C/s"] = 4.35;  c ["C/p"] = 3.95;
    	c ["D/c"] = 3.55;  c ["D/s"] = 2.90;  c ["D/p"] = 0   ;
    	c ["E/c"] = 3.80;  c ["E/s"] = 3.05;  c ["E/p"] = 0   ;
    	c ["F/c"] = 2.65;  c ["F/s"] = 2.25;  c ["F/p"] = 0   ;
    	c ["G/c"] = 2.15;  c ["G/s"] = 2.15;  c ["G/p"] = 2.15;
        }

        {
    	spu = c [ $2 ]				# look up charge band
    	if ( spu == "" || spu == 0 ) {
    	    summary [ $4 ] = summary [ $4 ] "\n\t" \
    			    sprintf ( "%4s  %4s  %7s   ? units",\
    	                          $1, $2, $3 ) \
    			    " - Bad/Unknown Chargeband"
    	} else {
    	    n = split ( $3, t, ":" )  # calculate length in seconds
    	    seconds = 0
    	    for ( i = 1; i <= n; i++ )
    		seconds = seconds*60 + t[i]
    	    u = seconds / spu   # calculate number of seconds
    	    if ( int( u ) == u )   # round up to next whole unit
    		u = int( u )
    	    else
    		u = int( u ) + 1
    	    units [ $4 ] += u   # store info to output at end
    	    summary [ $4 ] = summary [ $4 ] "\n\t" \
    			    sprintf ( "%4s  %4s  %7s %3d units",\
    	                         $1, $2, $3, u )
    	}
        }

    END \
        {
    	for ( i in units ) {		# for each person
    	    printf ( "Summary for %s:", i ) # newline at start
                                                # of summary
    	    print summary [ i ]			# print summary details
    	    # calc cost
    	    total = int ( units[i] * pence_per_unit + 0.5 )
    	    printf ( \
    		"Total: %d units @ %.2f pence per unit = $%d.%02d\n\n", \
    			    units [i], pence_per_unit, total/100, \
                                                   total%100 )
    	}
        }

    This program is another example of generating a report that consolidates information from a simple record structure.

This program also follows the three-part structure. The BEGIN procedure defines variables that are used throughout the program. This makes it easy to change the program, as phone companies are known to "upwardly revise" their rates. One of the variables is a large array named c in which each element is the number of seconds per unit, using the band over the rate as the index to the array.

The main procedure reads each line of the user log. It uses the second field, which identifies the band/rate, to get a value from the array c. It checks that a positive value was returned and then processes that value by the time specified in $3. The number of units for that call is then stored in an array named units, indexed by the name of the caller ($4). This value accumulates for each caller.

Finally, the END routine prints out the values in the units array, producing the report of units used per caller and the total cost of the calls.

				5.18.10.2.3 combine--Extract Multipart uuencoded Binaries

Contributed by Rahul Dhesi

Of all the scripts I have ever written, the one I am most proud of is the "combine" script.

While I was moderating comp.binaries.ibm.pc, I wanted to provide users a simple way of extracting multipart uuencoded binaries. I added BEGIN and END headers to each part to enclose the uuencoded part and provided users with the following script:

    cat $* | sed '/^END/,/^BEGIN/d' | uudecode

This script will accept a list of filenames (in order) provided as command-line arguments. It will also accept concatenated articles as standard input.

This script invokes cat in a very useful way that is well known to expert shell script users but not enough used by most others. This allows the user the choice of either providing command-line arguments or standard input.

The script invokes sed to strip out superfluous headers and trailers, except for headers in the first input file and trailers in the last input file. The final result is that the uuencoded part of the multiple input files is extracted and uudecoded. Each input file (see postings in comp.binaries.ibm.pc) has the following form:

    headers
    BEGIN
    uuencoded text
    END

I have lots of other shell stuff, but the above is simplest and has proved useful to several thousand comp.binaries.ibm.pc readers.

This one is pretty obvious but accomplishes a lot. For those who might not understand the use of this command, here is the explanation. A Usenet newsgroup such as comp.binaries.ibm.pc distributes public-domain programs and such. Binaries, the object code created by the compiler, cannot be distributed as news articles unless they are "encoded." A program named uuencode converts the binary to an ASCII representation that can be easily distributed. Furthermore, there are limits on the size of news articles and large binaries are broken up into a series of articles (1 of 3, 2 of 3, 3 of 3, for example). Dhesi would break up the encoded binary into manageable chunks, and then add the BEGIN and END lines to delimit the text that contained encoded binary.

A reader of these articles might save each article in a file. Dhesi's script automates the process of combining these articles and removing extraneous information such as the article header as well as the extra BEGIN and END headers. His script removes lines from the first END up to and including the next BEGIN pattern. It combines all the separate encoded parcels and directs them to uudecode, which converts the ASCII representation to binary.

One has to appreciate the amount of manual editing work avoided by a simple one-line script.

				5.18.10.2.4  mailavg--Check Size of Mailboxes

Contributed by Wes Morgan

While tuning our mail system, we needed to take a "snapshot" of the users' mailboxes at regular intervals over a 30-day period. This script simply calculates the average size and prints the arithmetic distribution of user mailboxes.

    #! /bin/sh
    #
    # mailavg - average size of files in /usr/mail
    #
    # Written by Wes Morgan, morgan@engr.uky.edu, 2 Feb 90
    ls -Fs /usr/mail | awk '
       { if(NR != 1) {
           total += $1; 
           count += 1;
           size = $1 + 0; 
           if(size == 0) zercount+=1;
           if(size > 0 && size <= 10) tencount+=1;
           if(size > 10 && size <= 19) teencount+=1;
           if(size > 20 && size <= 50) uptofiftycount+=1;
           if(size > 50) overfiftycount+=1;
           }
       }
       END { printf("/usr/mail has %d mailboxes using %d blocks,",
                count,total) 
             printf("average is %6.2f blocks\n", total/count)
             printf("\nDistribution:\n")
             printf("Size      Count\n")
             printf(" O           %d\n",zercount)
             printf("1-10         %d\n",tencount)
             printf("11-20        %d\n",teencount)
             printf("21-50        %d\n",uptofiftycount)
             printf("Over 50      %d\n",overfiftycount)
           }'
    exit 0

Here's a sample output from mailavg:

    $ mailavg
    /usr/mail has 47 mailboxes using 5116 blocks,
    average is 108.85 blocks
    Distribution:
    Size      Count
     O           1
    1-10         13
    11-20        1
    21-50        5
    Over 50      27

    This administrative program is similar to the filesum program
    The conditional expression "NR != 1" could have been put outside the main procedure as a pattern. While the logic is the same, using the expression as a pattern clarifies how the procedure is accessed, making the program easier to understand.

In that procedure, Morgan uses a series of conditionals that allow him to collect distribution statistics on the size of each user's mailbox.

				5.18.10.2.5 adj--Adjust Lines for Text Files

Contributed by Norman Joseph

[Because the author used his program to format his mail message before sending it, we're preserving the linebreaks and indented paragraphs in presenting it here as the program's example. This program is similar to the BSD fmt program.]

            Well, I decided to take you up on your offer.  I'm sure there are
    more sophisticated gurus out there than me, but I do have a nawk script
    that I'm kind of fond of, so I'm sending it in.

            Ok, here's the low down.  When I'm writing e-mail, I often make a
    lot of changes to the text (especially if I'm going to post on the net).
    So what starts out as a nicely adjusted letter or posting usually ends up
    looking pretty sloppy by the time I'm done adding and deleting lines.  So
    I end up spending a lot of time joining and breaking lines all through my
    document so as to get a nice right-hand margin.  So I say to myself,
    "This is just the kind of tedious work a program would be good for."

            Now, I know I can use nroff to filter my document through and
    adjust the lines, but it has lousy defaults (IMHO) for simple text like
    this.  So, with a view to sharpening my nawk skills I wrote adj.nawk
    and the accompanying shell script wrapper adj.  

    Here's the syntax for the nawk filter adj:


    adj [-l|c|r|b] [-w n] [-i n] [files ...]


    The options are:

-l
    Lines are left adjusted, right ragged (default).

-c
    Lines are centered.

-r
    Lines are right adjusted, left ragged.

-b
    Lines are left and right adjusted.

-w n
    Sets line width to n characters (default is 70).

-i n
    Sets initial indent to n characters (default is 0).

            So, whenever I'm finished with this letter (I'm using vi) I will
    give the command :%!adj -w73 (I like my lines a little longer) and
    all the breaking and joining will be done by a program (the way the Good
    Lord intended :-).  Indents and blank lines are preserved, and two spaces
    are given after any end-of-sentence punctuation.

            The program is naive about tabs, and when computing line lengths,
    it considers a tab character to be one space wide.

            The program is notable for its use of command-line parameter
    assignment, and some of the newer features of awk (nawk), such as the
    match and split built-in functions, and for its use of support functions.

    #! /bin/sh
    #
    # adj - adjust text lines
    #
    # usage: adj [-l|c|r|b] [-w n] [-i n] [files ...]
    #
    # options:
    #    -l    - lines are left adjusted, right ragged (default)
    #    -c    - lines are centered
    #    -r    - lines are right adjusted, left ragged
    #    -b    - lines are left and right adjusted
    #    -w n  - sets line width to <n> characters (default: 70)
    #    -i n  - sets initial indent to <n> characters (default: 0)
    #
    # note:
    #    output line width is -w setting plus -i setting
    #
    # author:
    #    Norman Joseph (amanue!oglvee!norm)

    adj=l
    wid=70
    ind=0

    set -- `getopt lcrbw:i: $*`
    if test $? != 0
    then
        printf 'usage: %s [-l|c|r|b] [-w n] [-i n] [files ...]' $0
        exit 1
    fi

    for arg in $*
    do
        case $arg in
        -l) adj=l;  shift;;
        -c) adj=c;  shift;;
        -r) adj=r;  shift;;
        -b) adj=b;  shift;;
        -w) wid=$2;  shift 2;;
        -i) ind=$2;  shift 2;;
        --) shift;  break;;
        esac
    done

    exec nawk -f adj.nawk type=$adj linelen=$wid indent=$ind $*

Here's the adj.nawk script that's called by the shell script adj.

    # adj.nawk -- adjust lines of text per options
    #
    # NOTE:  this nawk program is called from the shell script "adj"
    #    see that script for usage & calling conventions
    #
    # author:
    #    Norman Joseph (amanue!oglvee!norm)

    BEGIN  {
        FS = "\n"
        blankline  = "^[ \t]*$"
        startblank = "^[ \t]+[^ \t]+"
        startwords = "^[^ \t]+"
    }

    $0 ~ blankline {
        if ( type == "b" )
            putline( outline "\n" )
        else
            putline( adjust( outline, type ) "\n" )
        putline( "\n" )
        outline = ""
    }

    $0 ~ startblank {
        if ( outline != "" ) {
            if ( type == "b" )
                putline( outline "\n" )
            else
                putline( adjust( outline, type ) "\n" )
        }

        firstword = ""
        i = 1
        while ( substr( $0, i, 1 ) ~ "[ \t]" ) {
            firstword = firstword substr( $0, i, 1 )
            i++
        }
        inline = substr( $0, i )
        outline = firstword

        nf = split( inline, word, "[ \t]+" )

        for ( i = 1;  i <= nf;  i++ ) {
            if ( i == 1 ) {
                testlen = length( outline word[i] )
            } else {
                testlen = length( outline " " word[i] )
                if ( match( ".!?:;", "\\" substr( outline,
                        length( outline ), 1 )) )
                    testlen++
            }

            if ( testlen > linelen ) {
                putline( adjust( outline, type ) "\n" )
                outline = ""
            }

            if ( outline == "" )
                outline = word[i]
            else if ( i == 1 )
                outline = outline word[i]
            else {
                if ( match( ".!?:;", "\\" substr( outline,
                       length( outline ), 1 )) )
                    outline = outline "  " word[i]     # 2 spaces
                else
                    outline = outline " " word[i]      # 1 space
            }
        }
    }

    $0 ~ startwords  {
        nf = split( $0, word, "[ \t]+" )

        for ( i = 1;  i <= nf;  i++ ) {
            if ( outline == "" )
                testlen = length( word[i] )
            else {
                testlen = length( outline " " word[i] )
                if ( match( ".!?:;", "\\" substr( outline,
                       length( outline ), 1 )) )
                    testlen++
            }

            if ( testlen > linelen ) {
                putline( adjust( outline, type ) "\n" )
                outline = ""
            }

            if ( outline == "" )
                outline = word[i]
            else {
                if ( match( ".!?:;", "\\" substr( outline,
                       length( outline ), 1 )) )
                    outline = outline "  " word[i]     # 2 spaces
                else
                    outline = outline " " word[i]      # 1 space
            }
        }
    }

    END  {
        if ( type == "b" )
            putline( outline "\n" )
        else
            putline( adjust( outline, type ) "\n" )
    }


    #
    # -- support functions --
    #

    function putline( line,    fmt )
    {
        if ( indent ) {
            fmt = "%" indent "s%s"
            printf( fmt, " ", line )
        } else
            printf( "%s", line )
    }


    function adjust( line, type,    fill, fmt )
    {
        if ( type != "l" )
            fill = linelen - length( line )

        if ( fill > 0 ) {
            if        ( type == "c" ) {
                fmt = "%" (fill+1)/2 "s%s"
                line = sprintf( fmt, " ", line )
            } else if ( type == "r" ) {
                fmt = "%" fill "s%s"
                line = sprintf( fmt, " ", line )
            } else if ( type == "b" ) {
                line = fillout( line, fill )
            }
        }

        return line
    }


    function fillout( line, need,    i, newline, nextchar, blankseen )
    {
        while ( need ) {
            newline = ""
            blankseen = 0

            if ( dir == 0 ) {
                for ( i = 1;  i <= length( line );  i++ ) {
                    nextchar = substr( line, i, 1 )
                    if ( need ) {
                        if ( nextchar == " " ) {
                            if ( ! blankseen ) {
                                newline = newline " "
                                need--
                                blankseen = 1
                            }
                        } else {
                            blankseen = 0
                        }
                    }
                    newline = newline nextchar
                }

            } else if ( dir == 1 ) {
                for ( i = length( line );  i >= 1;  i-- ) {
                    nextchar = substr( line, i, 1 )
                    if ( need ) {
                        if ( nextchar == " " ) {
                            if ( ! blankseen ) {
                                newline = " " newline
                                need--
                                blankseen = 1
                            }
                        } else {
                            blankseen = 0
                        }
                    }
                    newline = nextchar newline
                }
            }

            line = newline

            dir = 1 - dir
        }

        return line
    }

    This small text formatter is a nifty program for those of us who use text editors. It allows you to set the maximum line width and justify paragraphs and thus can be used to format mail messages or simple letters.

The adj shell script does all the option setting, although it could have been done by reading ARGV in the BEGIN action. Using the shell to establish command-line parameters is probably easier for those who are already familiar with the shell.

The lack of comments in the adj.awk script makes this script more difficult to read than some of the others. The BEGIN procedure assigns three regular expressions to variables: blankline, startblank, startwords. This is a good technique (one that you'll see used in lex specifications) because regular expressions can be difficult to read and the name of the variable makes it clear what it matches. Remember that modern awks lets you supply a regular expression as a string, in a variable.

There are three main procedures, which can be named by the variable they match. The first is blankline, a procedure which handles collected text once a blank line is encountered. The second is startblank, which handles lines that begin with whitespace (spaces or tabs). The third is startwords, which handles a line of text. The basic procedure is to read a line of text and determine how many of the words in that line will fit, given the line width, outputting those that will fit and saving those that will not in the variable outline. When the next input line is read, the contents of outline must be output before that line is output.

The adjust() function does the work of justifying the text based on a command-line option specifying the format type. All types except "l" (left-adjusted, right-ragged) need to be filled. Therefore, the first thing this function does is figure out how much "fill" is needed by subtracting the length of the current line from the specified line length. It makes excellent use of the sprintf() function to actually do the positioning of the text. For instance, to center text, the value of fill (plus 1) is divided by 2 to determine the amount of padding needed on each side of the line. This amount is passed through the fmt variable as the argument to sprintf():

    fmt = "%" (fill+1)/2 "s%s"
                line = sprintf( fmt, " ", line )

Thus, the space will be used to pad a field that is the length of half the amount of fill needed.

If text is right-justified, the value of fill itself is used to pad the field. Finally, if the format type is "b" (block), then the function fillout is called to determine where to add spaces that will fill out the line.

In looking over the design of the program, you can see, once again, how the use of functions helps to clarify what a program is doing. It helps to think of the main procedure as controlling the flow of input through the program while procedures handle the operations performed on the input. Separating the "operations" from the flow control makes the program readable and more easily maintained.

In passing, we're not sure why FS, the field separator, is set to newline in the BEGIN procedure. This means that the field and record separators are the same (i.e., $0 and $1 are the same). The split() function is called to break the line into fields using tabs or spaces as the delimiter.

    nf = split( $0, word, "[ \t]+" )

It would seem that the field separator could have been set to the same regular expression, as follows:

    FS = "[ \t]+"

It would be more efficient to use the default field parsing.

Finally, using the match() function to find punctuation is inefficient; it would have been better to use index().

				5.18.10.2.6 lpr--lpr Preprocessor

Contributed by Tom Van Raalte

I thought you might want to use the following script around the office. It is a preprocessor for lpr that sends output to the "best" printer. [This shell script is written for a BSD or Linux system and you would use this command in place of lpr. It reads the output of the lpq command to determine if a specific printer is available. If not, it tries a list of printers to see which one is available or which is the least busy. Then it invokes lpr to send the job to that printer.]

    #!/bin/sh
    #
    #set up temp file
    TMP=/tmp/printsum.$$
    LASERWRITER=${LASERWRITER-ps6}
    #Check to see if the default printer is free?
    #
    #
    FREE=`lpq -P$LASERWRITER | awk '
    { if ($0 == "no entries") 
      {
    	val=1
    	print val
    	exit 0
      }
      else
      {
    	val=0
    	print val
    	exit 0
      }
    }'`
    #echo Free is $FREE
    #
    #If the default is free then $FREE is set, and we print and exit.
    #
    if [ $FREE -eq 1 ] 
    then
    	SELECT=$LASERWRITER
    #echo selected $SELECT
    	lpr -P$SELECT $*
    	exit 0
    fi
    #echo Past the exit
    #
    #Now we go on to see if any of the printers in bank are free.  
    #
    BANK=${BANK-$LASERWRITER}
    #echo bank is $BANK
    #
    #If BANK is the same as LASERWRITER, then we have no choice.
    #otherwise, we print on the one that is free, if any are free.
    #
    if [ "$BANK" =  "$LASERWRITER" ] 
    then
    	SELECT=$LASERWRITER
    	lpr -P$SELECT $*
    	exit 0
    fi
    #echo past the check bank=laserprinter
    #
    #Now we check for a free printer.
    #Note that $LASERWRITER is checked again in case it becomes free
    #during the check.
    #
    #echo now we check the other for a free one
    for i in $BANK $LASERWRITER
    do
    FREE=`lpq -P$i | awk '
    { if ($0 == "no entries") 
      {
    	val=1
    	print val
    	exit 0
      }
      else
      {
    	val=0
    	print val
    	exit 0
      }
    }'`
    if [ $FREE -eq 1 ]
    then
    #   echo in loop for $i
    	SELECT=$i
    #   echo select is $SELECT
    #   if [ "$FREE" != "$LASERWRITER" ]
    #   then
    #          echo "Output redirected to printer $i"
    #   fi
    	lpr -P$SELECT $*
    	exit 0
    fi
    done
    #echo done checking for a free one
    # 
    #If we make it here then no printers are free.  So we 
    #print on the printer with the least bytes queued.
    #
    #
    for i in $BANK $LASERWRITER
    do
    val=`lpq -P$i | awk ' BEGIN {
    	start=0;
    }
    /^Time/ {
    	start=1; 
    	next;
    }
    (start == 1){
    	test=substr($0,62,20);
    	print test;
    } ' | awk '
    BEGIN {
    	summ=0;
    }
    {
    	summ=summ+$1;
    }
    END {
    	print summ;
    }'`
    echo "$i $val" >> $TMP
    done

    SELECT=`awk '(NR==1) {
    	select=$1;
    	best=$2
    }
    ($2 < best) {
    	select=$1; 
    	best=$2} 
    END {
    	print select
    }
    ' $TMP `
    #echo $SELECT
    #
    rm $TMP
    #Now print on the selected printer
    #if [ $SELECT != $LASERWRITER ]
    #then
    #   echo "Output redirected to printer $i"
    #fi
    lpr -P$SELECT $*
    trap 'rm -f $TMP; exit 99' 2 3 15

    For the most part, we've avoided scripts like these in which most of the logic is coded in the shell script. However, such a minimalist approach is representative of a wide variety of uses of awk. Here, awk is called to do only those things that the shell script can't do (or do as easily). Manipulating the output of a command and performing numeric comparisons is an example of such a task.

As a side note, the trap statement at the end should be at the top of the script, not at the bottom.

				5.18.10.2.7 transpose--Perform a Matrix Transposition

Contributed by Geoff Clare

transpose performs a matrix transposition on its input. I wrote this when I saw a script to do this job posted to the Net and thought it was horribly inefficient. I posted mine as an alternative with timing comparisons. If I remember rightly, the original one stored all the elements individually and used a nested loop with a printf for each element. It was immediately obvious to me that it would be much faster to construct the rows of the transposed matrix "on the fly."

My script uses ${1+"$@"} to supply file names on the awk command line so that if no files are specified awk will read its standard input. This is much better than plain $* which can't handle filenames containing whitexspace.

    #! /bin/sh
    # Transpose a matrix: assumes all lines have same number
    # of fields

    exec awk '
    NR == 1 {
    	n = NF
    	for (i = 1; i <= NF; i++)
    		row[i] = $i
    	next
    }
    {
    	if (NF > n)
    		n = NF
    	for (i = 1; i <= NF; i++)
    		row[i] = row[i] " " $i
    }
    END {
    	for (i = 1; i <= n; i++)
    		print row[i]
    }' ${1+"$@"}

Here's a test file:

    1 2 3 4
    5 6 7 8
    9 10 11 12

Now we run transpose on the file.

    $ transpose test
    1 5 9
    2 6 10
    3 7 11
    4 8 12

    This is a very simple but interesting script. It creates an array named row and appends each field into an element of the array. The END procedure outputs the array.

				5.18.10.2.8 m1--Simple Macro Processor

Contributed by Jon Bentley

The m1 program is a "little brother" to the m4 macro processor found on UNIX systems. It was originally published in the article m1: A Mini Macro Processor, in Computer Language, June 1990, Volume 7, Number 6, pages 47-61. This program was brought to my attention by Ozan Yigit. Jon Bentley kindly sent me his current version of the program, as well as an early draft of his article (I was having trouble getting a copy of the published one). A PostScript version of this paper is included with the example programs, available from O'Reilly's FTP server (see the Preface). I wrote these introductory notes, and the program notes below. [A.R.]

A macro processor copies its input to its output, while performing several jobs. The tasks are:

   a.  Define and expand macros. Macros have two parts, a name and a body. All occurrences of a macro's name are replaced with the macro's body.
   b.  Include files. Special include directives in a data file are replaced with the contents of the named file. Includes can usually be nested, with one included file including another. Included files are processed for macros.
   c.  Conditional text inclusion and exclusion. Different parts of the text can be included in the final output, often based upon whether a macro is or isn't defined.
   d.  Depending on the macro processor, comment lines can appear that will be removed from the final output.

If you're a C or C++ programmer, you're already familiar with the built-in preprocessor in those languages. UNIX systems have a general-purpose macro processor called m4. This is a powerful program, but somewhat difficult to master, since macro definitions are processed for expansion at definition time, instead of at expansion time. m1 is considerably simpler than m4, making it much easier to learn and to use.

Here is Jon's first cut at a very simple macro processor. All it does is define and expand macros. We can call it m0a. In this and the following programs, the "at" symbol (@) distinguishes lines that are directives, and also indicates the presence of macros that should be expanded.

    /^@define[ \t]/ {
    	name = $2
    	$1 = $2 = ""; sub(/^[ \t]+/, "")
    	symtab[name] = $0
    	next
    }
    {
    	for (i in symtab)
    		gsub("@" i "@", symtab[i])
    	print
    }

This version looks for lines beginning with "@define." This keyword is $1 and the macro name is taken to be $2. The rest of the line becomes the body of the macro. The next input line is then fetched using next. The second rule simply loops through all the defined macros, performing a global substitution of each macro with its body in the input line, and then printing the line. Think about the tradeoffs in this version of simplicity versus program execution time.

The next version (m0b) adds file inclusion:

    function dofile(fname) {
    	while (getline <fname > 0) {
    		if (/^@define[ \t]/) {		# @define name value
    			name = $2
    			$1 = $2 = ""; sub(/^[ \t]+/, "")
    			symtab[name] = $0
    		} else if (/^@include[ \t]/)	# @include filename
    			dofile($2)
    		else {				# Anywhere in line @name@
    			for (i in symtab)
    				gsub("@" i "@", symtab[i])
    			print
    		}
    	}
    	close(fname)
    }
    BEGIN {
    	if (ARGC == 2)
    		dofile(ARGV[1])
    	else
    		dofile("/dev/stdin")
    }

Note the way dofile() is called recursively to handle nested include files.

With all of that introduction out of the way, here is the full-blown m1 program.

    #! /bin/awk -f
    # NAME
    #
    # m1
    #
    # USAGE
    #
    # awk -f m1.awk [file...]
    #
    # DESCRIPTION
    #
    # M1 copies its input file(s) to its output unchanged except as modified by
    # certain "macro expressions."  The following lines define macros for
    # subsequent processing:
    #
    #     @comment Any text
    #     @@                     same as @comment
    #     @define name value
    #     @default name value    set if name undefined
    #     @include filename
    #     @if varname            include subsequent text if varname != 0
    #     @unless varname        include subsequent text if varname == 0
    #     @fi                    terminate @if or @unless
    #     @ignore DELIM          ignore input until line that begins with DELIM
    #     @stderr stuff          send diagnostics to standard error
    #
    # A definition may extend across many lines by ending each line with
    # a backslash, thus quoting the following newline.
    #
    # Any occurrence of @name@ in the input is replaced in the output by
    # the corresponding value.
    #
    # @name at beginning of line is treated the same as @name@.
    #
    # BUGS
    #
    # M1 is three steps lower than m4.  You'll probably miss something
    # you have learned to expect.
    #
    # AUTHOR
    #
    # Jon L. Bentley, jlb@research.bell-labs.com
    #

    function error(s) {
    	print "m1 error: " s | "cat 1>&2"; exit 1
    }

    function dofile(fname,  savefile, savebuffer, newstring) {
    	if (fname in activefiles)
    		error("recursively reading file: " fname)
    	activefiles[fname] = 1
    	savefile = file; file = fname
    	savebuffer = buffer; buffer = ""
    	while (readline() != EOF) {
    		if (index($0, "@") == 0) {
    			print $0
    		} else if (/^@define[ \t]/) {
    			dodef()
    		} else if (/^@default[ \t]/) {
    			if (!($2 in symtab))
    				dodef()
    		} else if (/^@include[ \t]/) {
    			if (NF != 2) error("bad include line")
    			dofile(dosubs($2))
    		} else if (/^@if[ \t]/) {
    			if (NF != 2) error("bad if line")
    			if (!($2 in symtab) || symtab[$2] == 0)
    				gobble()
    		} else if (/^@unless[ \t]/) {
    			if (NF != 2) error("bad unless line")
    			if (($2 in symtab) && symtab[$2] != 0)
    				gobble()
    		} else if (/^@fi([ \t]?|$)/) { # Could do error checking here
    		} else if (/^@stderr[ \t]?/) { 
    			print substr($0, 9) | "cat 1>&2"
    		} else if (/^@(comment|@)[ \t]?/) {
    		} else if (/^@ignore[ \t]/) { # Dump input until $2
    			delim = $2
    			l = length(delim)
    			while (readline() != EOF)
    				if (substr($0, 1, l) == delim)
    					break
    		} else {
    			newstring = dosubs($0)
    			if ($0 == newstring || index(newstring, "@") == 0)
    				print newstring
    			else
    				buffer = newstring "\n" buffer
    		}
    	}
    	close(fname)
    	delete activefiles[fname]
    	file = savefile
    	buffer = savebuffer
    }

    # Put next input line into global string "buffer"
    # Return "EOF" or "" (null string)

    function readline(  i, status) {
    	status = ""
    	if (buffer != "") {
    		i = index(buffer, "\n")
    		$0 = substr(buffer, 1, i-1)
    		buffer = substr(buffer, i+1)
    	} else {
    		# Hume: special case for non v10: if (file == "/dev/stdin")
    		if (getline <file <= 0)
    			status = EOF
    	}
    	# Hack: allow @Mname at start of line w/o closing @
    	if ($0 ~ /^@[A-Z][a-zA-Z0-9]*[ \t]*$/)
    		sub(/[ \t]*$/, "@")
    	return status
    }

    function gobble(  ifdepth) {
    	ifdepth = 1
    	while (readline() != EOF) {
    		if (/^@(if|unless)[ \t]/)
    			ifdepth++
    		if (/^@fi[ \t]?/ && --ifdepth <= 0)
    			break
    	}
    }

    function dosubs(s,  l, r, i, m) {
    	if (index(s, "@") == 0)
    		return s
    	l = ""	# Left of current pos; ready for output
    	r = s	# Right of current; unexamined at this time
    	while ((i = index(r, "@")) != 0) {
    		l = l substr(r, 1, i-1)
    		r = substr(r, i+1)	# Currently scanning @
    		i = index(r, "@")
    		if (i == 0) {
    			l = l "@"
    			break
    		}
    		m = substr(r, 1, i-1)
    		r = substr(r, i+1)
    		if (m in symtab) {
    			r = symtab[m] r
    		} else {
    			l = l "@" m
    			r = "@" r
    		}
    	}
    	return l r
    }

    function dodef(fname,  str, x) {
    	name = $2
    	sub(/^[ \t]*[^ \t]+[ \t]+[^ \t]+[ \t]*/, "")  # OLD BUG: last * was +
    	str = $0
    	while (str ~ /\\$/) {
    		if (readline() == EOF)
    			error("EOF inside definition")
    		x = $0
    		sub(/^[ \t]+/, "", x)
    		str = substr(str, 1, length(str)-1) "\n" x
    	}
    	symtab[name] = str
    }

    BEGIN {	EOF = "EOF"
    	if (ARGC == 1)
    		dofile("/dev/stdin")
    	else if (ARGC >= 2) {
    		for (i = 1; i < ARGC; i++)
    			dofile(ARGV[i])
    	} else
    		error("usage: m1 [fname...]")
    }
The program is nicely modular, with an error() function similar to the one presented in Chapter 11, "A Flock of awks", and each task cleanly divided into separate functions.

The main program occurs in the BEGIN procedure at the bottom. It simply processes either standard input, if there are no arguments, or all of the files named on the command line.

The high-level processing happens in the dofile() function, which reads one line at a time, and decides what to do with each line. The activefiles array keeps track of open files. The variable fname indicates the current file to read data from. When an "@include" directive is seen, dofile() simply calls itself recursively on the new file, as in m0b. Interestingly, the included filename is first processed for macros. Read this function carefully--there are some nice tricks here.

The readline() function manages the "pushback." After expanding a macro, macro processors examine the newly created text for any additional macro names. Only after all expanded text has been processed and sent to the output does the program get a fresh line of input.

The dosubs() function actually performs the macro substitution. It processes the line left-to-right, replacing macro names with their bodies. The rescanning of the new line is left to the higher-level logic that is jointly managed by readline() and dofile(). This version is considerably more efficient than the brute-force approach used in the m0 programs.

Finally, the dodef() function handles the defining of macros. It saves the macro name from $2, and then uses sub() to remove the first two fields. The new value of $0 now contains just (the first line of) the macro body. The Computer Language article explains that sub() is used on purpose, in order to preserve whitespace in the macro body. Simply assigning the empty string to $1 and $2 would rebuild the record, but with all occurrences of whitespace collapsed into single occurrences of the value of OFS (a single blank). The function then proceeds to gather the rest of the macro body, indicated by lines that end with a "\". This is an additional improvement over m0: macro bodies can be more than one line long.

The rest of the program is concerned with conditional inclusion or exclusion of text; this part is straightforward. What's nice is that these conditionals can be nested inside each other.

m1 is a very nice start at a macro processor. You might want to think about how you could expand upon it; for instance, by allowing conditionals to have an "@else" clause; processing the command line for macro definitions; "undefining" macros, and the other sorts of things that macro processors usually do.


	5.19 Quick Reference for awk

		5.19.1 Command-Line Syntax

The syntax for invoking awk has two basic forms:

    awk [-v var=value] [-Fre] [--] 'pattern { action }' var=value datafile(s)
    awk [-v var=value] [-Fre] -f scriptfile [--] var=value datafile(s)

An awk command line consists of the command, the script and the input filename. Input is read from the file specified on the command line. If there is no input file or "-" is specified, then standard input is read. The -F option sets the field separator (FS) to re.

The -v option sets the variable var to value before the script is executed. This happens even before the BEGIN procedure is run. (See the discussion below on command-line parameters.)

Following POSIX argument parsing conventions, the "--" option marks the end of command-line options. Using this option, for instance, you could specify a datafile that begins with "-", which would otherwise be confused with a command-line option.

You can specify a script consisting of pattern and action on the command line, surrounded by single quotes. Alternatively, you can place the script in a separate file and specify the name of the scriptfile on the command line with the -f option.

Parameters can be passed into awk by specifying them on the command line after the script. This includes setting system variables such as FS, OFS, and RS. The value can be a literal, a shell variable ($var) or the result of a command (`cmd`); it must be quoted if it contains spaces or tabs. Any number of parameters can be specified.

Command-line parameters are not available until the first line of input is read, and thus cannot be accessed in the BEGIN procedure. (Older implementations of awk and nawk would process leading command-line assignments before running the BEGIN procedure. This was contrary to how things were documented in The AWK Programming Language, which says that they are processed when awk would go to open them as filenames, i.e., after the BEGIN procedure. The Bell Labs awk was changed to correct this, and the -v option was added at the same time, in early 1989. It is now part of POSIX awk.) Parameters are evaluated in the order in which they appear on the command line up until a filename is recognized. Parameters appearing after that filename will be available when the next filename is recognized.

			5.19.1.1 Shell Wrapper for Invoking awk

Typing a script at the system prompt is only practical for simple, one-line scripts. Any script that you might invoke as a command and reuse can be put inside a shell script. Using a shell script to invoke awk makes the script easy for others to use.

You can put the command line that invokes awk in a file, giving it a name that identifies what the script does. Make that file executable (using the chmod command) and put it in a directory where local commands are kept. The name of the shell script can be typed on the command line to execute the awk script. This is preferred for easily used and reused scripts.

On modern UNIX systems, including Linux, you can use the #! syntax to create self-contained awk scripts:

    #! /usr/bin/awk -f
    script

Awk parameters and the input filename can be specified on the command line that invokes the shell script. Note that the pathname to use is system-dependent.

		5.19.2 Language Summary for awk
	
		This section summarizes how awk processes input records and describes the various syntactic elements that make up an awk program.
			5.19.2.1. Records and Fields

Each line of input is split into fields. By default, the field delimiter is one or more spaces and/or tabs. You can change the field separator by using the -F command-line option. Doing so also sets the value of FS. The following command-line changes the field separator to a colon:

    awk -F: -f awkscr /etc/passwd

You can also assign the delimiter to the system variable FS. This is typically done in the BEGIN procedure, but can also be passed as a parameter on the command line.

    awk -f awkscr FS=: /etc/passwd

Each input line forms a record containing any number of fields. Each field can be referenced by its position in the record. "$1" refers to the value of the first field; "$2" to the second field, and so on. "$0" refers to the entire record. The following action prints the first field of each input line:

    { print $1 }

The default record separator is a newline. The following procedure sets FS and RS so that awk interprets an input record as any number of lines up to a blank line, with each line being a separate field.

    BEGIN { FS = "\n"; RS = "" }

It is important to know that when RS is set to the empty string, newline always separates fields, in addition to whatever value FS may have. This is discussed in more detail in both The AWK Programming Language and Effective AWK Programming.
			5.19.2.2. Format of a Script

An awk script is a set of pattern-matching rules and actions:

    pattern { action }

An action is one or more statements that will be performed on those input lines that match the pattern. If no pattern is specified, the action is performed for every input line. The following example uses the print statement to print each line in the input file:

    { print }

If only a pattern is specified, then the default action consists of the print statement, as shown above.

Function definitions can also appear:

    function name (parameter list) { statements }

This syntax defines the function name, making available the list of parameters for processing in the body of the function. Variables specified in the parameter-list are treated as local variables within the function. All other variables are global and can be accessed outside the function. When calling a user-defined function, no space is permitted between the name of the function and the opening parenthesis. Spaces are allowed in the function's definition. User-defined functions are described in Chapter 9, "Functions".
			5.19.2.2.1. Line termination

A line in an awk script is terminated by a newline or a semicolon. Using semicolons to put multiple statements on a line, while permitted, reduces the readability of most programs. Blank lines are permitted between statements.

Program control statements (do, if, for, or while) continue on the next line, where a dependent statement is listed. If multiple dependent statements are specified, they must be enclosed within braces.

    if (NF > 1) {
    	name = $1
    	total += $2
    }

You cannot use a semicolon to avoid using braces for multiple statements.

You can type a single statement over multiple lines by escaping the newline with a backslash (\). You can also break lines following any of the following characters:

    , { && ||

Gawk also allows you to continue a line after either a "?" or a ":". Strings cannot be broken across a line (except in gawk, using "\" followed by a newline).
			5.19.2.2.2. Comments

A comment begins with a "#" and ends with a newline. It can appear on a line by itself or at the end of a line. Comments are descriptive remarks that explain the operation of the script. Comments cannot be continued across lines by ending them with a backslash.
			5.19.2.3. Patterns

A pattern can be any of the following:

    /regular expression/
    relational expression
    BEGIN
    END
    pattern, pattern

   a.

      Regular expressions use the extended set of metacharacters and must be enclosed in slashes. For a full discussion of regular expressions, see Chapter 3, "Understanding Regular Expression Syntax".
   b.

      Relational expressions use the relational operators listed under "Expressions" later in this chapter.
   c.

      The BEGIN pattern is applied before the first line of input is read and the END pattern is applied after the last line of input is read.
   d.

      Use ! to negate the match; i.e., to handle lines not matching the pattern.
   e.

      You can address a range of lines, just as in sed:

          pattern, pattern

      Patterns, except BEGIN and END, can be expressed in compound forms using the following operators:
      && 	Logical And
      || 	Logical Or
      Sun's version of nawk (SunOS 4.1.x) does not support treating regular expressions as parts of a larger Boolean expression. E.g., "/cute/ && /sweet/" or "/fast/ || /quick/" do not work.

      In addition the C conditional operator ?: (pattern ? pattern : pattern) may be used in a pattern.
   f.

      Patterns can be placed in parentheses to ensure proper evaluation.
   g.

      BEGIN and END patterns must be associated with actions. If multiple BEGIN and END rules are written, they are merged into a single rule before being applied.

			5.19.2.4. Regular Expressions

Table B.1 summarizes the regular expressions as described in Chapter 3, "Understanding Regular Expression Syntax". The metacharacters are listed in order of precedence.
Table B.1. Regular Expression Metacharacters
Special 	
Characters 	Usage
c 	Matches any literal character c that is not a metacharacter.
\ 	Escapes any metacharacter that follows, including itself.
^ 	Anchors following regular expression to the beginning of string.
$ 	Anchors preceding regular expression to the end of string.
. 	Matches any single character, including newline.
[...] 	Matches any one of the class of characters enclosed between the brackets. A circumflex (^) as the first character inside brackets reverses the match to all characters except those listed in the class. A hyphen (-) is used to indicate a range of characters. The close bracket (]) as the first character in a class is a member of the class. All other metacharacters lose their meaning when specified as members of a class, except \, which can be used to escape ], even if it is not first.
r1|r2 	Between two regular expressions, r1 and r2, it allows either of the regular expressions to be matched.
(r1)(r2) 	Used for concatenating regular expressions.
r* 	Matches any number (including zero) of the regular expression that immediately precedes it.
r+ 	Matches one or more occurrences of the preceding regular expression.
r? 	Matches 0 or 1 occurrences of the preceding regular expression.
(r) 	Used for grouping regular expressions.

Regular expressions can also make use of the escape sequences for accessing special characters, as defined in Section 			5.19.2.5.2 later in this appendix.

Note that ^ and $ work on strings; they do not match against newlines embedded in a record or string.

Within a pair of brackets, POSIX allows special notations for matching non-English characters. They are described in Table 			5.19.2.
Table 			5.19.2. POSIX Character List Facilities
Notation 	Facility
[.symbol.] 	Collating symbols. A collating symbol is a multi-character sequence that should be treated as a unit.
[=equiv=] 	Equivalence classes. An equivalence class lists a set of characters that should be considered equivalent, such as "e" and "".
[:class:] 	Character classes. Character class keywords describe different classes of characters such as alphabetic characters, control characters, and so on.
[:alnum:] 	Alphanumeric characters
[:alpha:] 	Alphabetic characters
[:blank:] 	Space and tab characters
[:cntrl:] 	Control characters
[:digit:] 	Numeric characters
[:graph:] 	Printable and visible (non-space) characters
[:lower:] 	Lowercase characters
[:print:] 	Printable characters
[:punct:] 	Punctuation characters
[:space:] 	Whitespace characters
[:upper:] 	Uppercase characters
[:xdigit:] 	Hexadecimal digits

Note that these facilities (as of this writing) are still not widely implemented.
			5.19.2.5. Expressions

An expression can be made up of constants, variables, operators and functions. A constant is a string (any sequence of characters) or a numeric value. A variable is a symbol that references a value. You can think of it as a piece of information that retrieves a particular numeric or string value.
			5.19.2.5.1. Constants

There are two types of constants, string and numeric. A string constant must be quoted while a numeric constant is not.
			5.19.2.5.2. Escape sequences

The escape sequences described in Table B.3 can be used in strings and regular expressions.
Table B.3. Escape Sequences
Sequence 	Description
\a 	Alert character, usually ASCII BEL character
\b 	Backspace
\f 	Formfeed
\n 	Newline
\r 	Carriage return
\t 	Horizontal tab
\v 	Vertical tab
\ddd 	Character represented as 1 to 3 digit octal value
\xhex 	Character represented as hexadecimal value[91]
\c 	

Any literal character c (e.g., \" for ")[92]

[91]POSIX does not provide "\x", but it is commonly available.

    [92]Like ANSI C, POSIX leaves it purposely undefined what you get when you put a backslash before any character not listed in the table. In most awks, you just get that character.

			5.19.2.5.3. Variables

There are three kinds of variables: user-defined, built-in, and fields. By convention, the names of built-in or system variables consist of all capital letters.

The name of a variable cannot start with a digit. Otherwise, it consists of letters, digits, and underscores. Case is significant in variable names.

A variable does not need to be declared or initialized. A variable can contain either a string or numeric value. An uninitialized variable has the empty string ("") as its string value and 0 as its numeric value. Awk attempts to decide whether a value should be processed as a string or a number depending upon the operation.

The assignment of a variable has the form:

    var = expr

It assigns the value of the expression to var. The following expression assigns a value of 1 to the variable x.

    x = 1

The name of the variable is used to reference the value:

    { print x }

prints the value of the variable x. In this case, it would be 1.

See the later Section 2.2.5.5 for information on built-in variables. A field variable is referenced using $n, where n is any number 0 to NF, that references the field by position. It can be supplied by a variable, such as $NF meaning the last field, or constant, such as $1 meaning the first field.
			5.19.2.5.4. Arrays

An array is a variable that can be used to store a set of values. The following statement assigns a value to an element of an array:

    array[index] = value

In awk, all arrays are associative arrays. What makes an associative array unique is that its index can be a string or a number.

An associative array makes an "association" between the indices and the elements of an array. For each element of the array, a pair of values is maintained: the index of the element and the value of the element. The elements are not stored in any particular order as in a conventional array.

You can use the special for loop to read all the elements of an associative array.

    for (item in array)

The index of the array is available as item, while the value of an element of the array can be referenced as array[item].

You can use the operator in to test that an element exists by testing to see if its index exists.

    if (index in array)

tests that array[index] exists, but you cannot use it to test the value of the element referenced by array[index].

You can also delete individual elements of the array using the delete statement.
			5.19.2.5.5. System variables

Awk defines a number of special variables that can be referenced or reset inside a program, as shown in Table B.4 (defaults are listed in parentheses).
Table B.4. Awk System Variables
Variable 	Description
ARGC 	Number of arguments on command line
ARGV 	An array containing the command-line arguments
CONVFMT 	String conversion format for numbers (%.6g). (POSIX)
ENVIRON 	An associative array of environment variables
FILENAME 	Current filename
FNR 	Like NR, but relative to the current file
FS 	Field separator (a blank)
NF 	Number of fields in current record
NR 	Number of the current record
OFMT 	Output format for numbers (%.6g)
OFS 	Output field separator (a blank)
ORS 	Output record separator (a newline)
RLENGTH 	Length of the string matched by match() function
RS 	Record separator (a newline)
RSTART 	First position in the string matched by match() function
SUBSEP 	Separator character for array subscripts (\034)

			5.19.2.5.6. Operators

Table B.5 lists the operators in the order of precedence (low to high) that are available in awk.
Table B.5. Operators
Operators 	Description
= += -= *= /= %= ^= **= 	Assignment
?: 	C conditional expression
|| 	Logical OR
&& 	Logical AND
~ !~ 	Match regular expression and negation
< <= > >= != == 	Relational operators
(blank) 	Concatenation
+ - 	Addition, subtraction
* / % 	Multiplication, division, and modulus
+ - ! 	Unary plus and minus, and logical negation
^ ** 	Exponentiation
++ -- 	Increment and decrement, either prefix or postfix
$ 	Field reference

    NOTE: While "**" and "**=" are common extensions, they are not part of POSIX awk.

			5.19.2.6. Statements and Functions

An action is enclosed in braces and consists of one or more statements and/or expressions. The difference between a statement and a function is that a function returns a value, and its argument list is specified within parentheses. (The formal syntactical difference does not always hold true: printf is considered a statement, but its argument list can be put in parentheses; getline is a function that does not use parentheses.)

Awk has a number of predefined arithmetic and string functions. A function is typically called as follows:

    return = function(arg1,arg2)

where return is a variable created to hold what the function returns. (In fact, the return value of a function can be used anywhere in an expression, not just on the right-hand side of an assignment.) Arguments to a function are specified as a comma-separated list. The left parenthesis follows after the name of the function. (With built-in functions, a space is permitted between the function name and the parentheses.)


		5.19.3  Command Summary for awk

The following alphabetical list of statements and functions includes all that are available in POSIX awk, nawk, or gawk. See Chapter 11, "A Flock of awks", for extensions available in different implementations.

atan2()
    atan2(y, x)

    Returns the arctangent of y/x in radians.
break
    Exit from a while, for, or do loop.

close()
    close(filename-expr)

    close(command-expr)

    In most implementations of awk, you can only have a limited number of files and/or pipes open simultaneously. Therefore, awk provides a close() function that allows you to close a file or a pipe. It takes as an argument the same expression that opened the pipe or file. This expression must be identical, character by character, to the one that opened the file or pipe--even whitespace is significant.
continue
    Begin next iteration of while, for, or do loop.

cos()
    cos(x)

    Return cosine of x in radians.
delete
    delete array[element]

    Delete element of an array.
do
    do

     body

    while (expr)

    Looping statement. Execute statements in body then evaluate expr and if true, execute body again.
exit
    exit [expr]

    Exit from script, reading no new input. The END rule, if it exists, will be executed. An optional expr becomes awk's return value.
exp()
    exp(x)

    Return exponential of x (e ^ x).
for
    for (init-expr; test-expr; incr-expr) statement

    C-style looping construct. init-expr assigns the initial value of the counter variable. test-expr is a relational expression that is evaluated each time before executing the statement. When test-expr is false, the loop is exited. incr-expr is used to increment the counter variable after each pass.

    for (item in array) statement

    Special loop designed for reading associative arrays. For each element of the array, the statement is executed; the element can be referenced by array[item].
getline
    Read next line of input.

    getline [var] [<file]

    command | getline [var]

    The first form reads input from file and the second form reads the output of command. Both forms read one line at a time, and each time the statement is executed it gets the next line of input. The line of input is assigned to $0 and it is parsed into fields, setting NF, NR, and FNR. If var is specified, the result is assigned to var and the $0 is not changed. Thus, if the result is assigned to a variable, the current line does not change. getline is actually a function and it returns 1 if it reads a record successfully, 0 if end-of-line is encountered, and -1 if for some reason it is otherwise unsuccessful.
gsub()
    gsub(r, s, t)

    Globally substitute s for each match of the regular expression r in the string t. Return the number of substitutions. If t is not supplied, defaults to $0.
if
    if (expr) statement1

    [ else statement2 ]

    Conditional statement. Evaluate expr and, if true, execute statement1; if else clause is supplied, execute statement2 if expr is false.
index()
    index(str, substr)

    Return position (starting at 1) of substring in string.
int()
    int(x)

    Return integer value of x by truncating any digits following a decimal point.
length()
    length(str)

    Return length of string, or the length of $0 if no argument.
log()
    log(x)

    Return natural logarithm (base e) of x.
match()
    match(s, r)

    Function that matches the pattern, specified by the regular expression r, in the string s and returns either the position in s where the match begins, or 0 if no occurrences are found. Sets the values of RSTART and RLENGTH to the start and length of the match, respectively.
next
    Read next input line and begin executing script at first rule.

print
    print [ output-expr ] [ dest-expr ]

    Evaluate the output-expr and direct it to standard output followed by the value of ORS. Each output-expr is separated by the value of OFS. dest-expr is an optional expression that directs the output to a file or pipe. "> file" directs the output to a file, overwriting its previous contents. ">> file" appends the output to a file, preserving its previous contents. In both of these cases, the file will be created if it does not already exist. "| command" directs the output as the input to a system command.
printf
    printf (format-expr [, expr-list ]) [ dest-expr ]

    An alternative output statement borrowed from the C language. It has the ability to produce formatted output. It can also be used to output data without automatically producing a newline. format-expr is a string of format specifications and constants; see next section for a list of format specifiers. expr-list is a list of arguments corresponding to format specifiers. See the print statement for a description of dest-expr.
rand()
    rand()

    Generate a random number between 0 and 1. This function returns the same series of numbers each time the script is executed, unless the random number generator is seeded using the srand() function.
return
    return [expr]

    Used at end of user-defined functions to exit function, returning value of expression.
sin()
    sin(x)

    Return sine of x in radians.
split()
    split(str, array, sep)

    Function that parses string into elements of array using field separator, returning number of elements in array. Value of FS is used if no field separator is specified. Array splitting works the same as field splitting.
sprintf()
    sprintf (format-expr [, expr-list ] )

    Function that returns string formatted according to printf format specification. It formats data but does not output it. format-expr is a string of format specifications and constants; see the next section for a list of format specifiers. expr-list is a list of arguments corresponding to format specifiers.
sqrt()
    sqrt(x)

    Return square root of x.
srand()
    srand(expr)

    Use expr to set a new seed for random number generator. Default is time of day. Return value is the old seed.
sub()
    sub(r, s, t)

    Substitute s for first match of the regular expression r in the string t. Return 1 if successful; 0 otherwise. If t is not supplied, defaults to $0.
substr()
    substr(str, beg, len)

    Return substring of string str at beginning position beg, and the characters that follow to maximum specified length len. If no length is given, use the rest of the string.
system()
    system(command)

    Function that executes the specified command and returns its status. The status of the executed command typically indicates success or failure. A value of 0 means that the command executed successfully. A non-zero value, whether positive or negative, indicates a failure of some sort. The documentation for the command you're running will give you the details. The output of the command is not available for processing within the awk script. Use "command | getline" to read the output of a command into the script.
tolower()
    tolower(str)

    Translate all uppercase characters in str to lowercase and return the new string.[93]

        [93]Very early versions of nawk, such as that in SunOS 4.1.x, don't support tolower() and toupper(). However, they are now part of the POSIX specification for awk.

toupper()
    toupper(str)

    Translate all lowercase characters in str to uppercase and return the new string.
while
    while (expr) statement

    Looping construct. While expr is true, execute statement.

			5.19.3.1. Format Expressions Used in printf and sprintf

A format expression can take three optional modifiers following "%" and preceding the format specifier:

    %-width.precision format-specifier

The width of the output field is a numeric value. When you specify a field width, the contents of the field will be right-justified by default. You must specify "-" to get left-justification. Thus, "%-20s" outputs a string left-justified in a field 20 characters wide. If the string is less than 20 characters, the field will be padded with spaces to fill.

The precision modifier, used for decimal or floating-point values, controls the number of digits that appear to the right of the decimal point. For string formats, it controls the number of characters from the string to print.

You can specify both the width and precision dynamically, via values in the printf or sprintf argument list. You do this by specifying asterisks, instead of specifying literal values.

    printf("%*.*g\n", 5, 3, myvar);

In this example, the width is 5, the precision is 3, and the value to print will come from myvar. Older versions of nawk may not support this.

Note that the default precision for the output of numeric values is "%.6g." The default can be changed by setting the system variable OFMT. This affects the precision used by the print statement when outputting numbers. For instance, if you are using awk to write reports that contain dollar values, you might prefer to change OFMT to "%.2f."

The format specifiers, shown in Table B.6, are used with printf and sprintf statements.
Table B.6. Format Specifiers Used in printf
Character 	Description
c 	ASCII character.
d 	Decimal integer.
i 	Decimal integer. Added in POSIX.
e 	Floating-point format ([-]d.precisione[+-]dd).
E 	Floating-point format ([-]d.precisionE[+-]dd).
f 	Floating-point format ([-]ddd.precision).
g 	e or f conversion, whichever is shortest, with trailing zeros removed.
G 	E or f conversion, whichever is shortest, with trailing zeros removed.
o 	Unsigned octal value.
s 	String.
x 	Unsigned hexadecimal number. Uses a-f for 10 to 15.
X 	Unsigned hexadecimal number. Uses A-F for 10 to 15.
% 	Literal %.

Often, whatever format specifiers are available in the system's sprintf(3) subroutine are available in awk.

The way printf and sprintf() do rounding will often depend upon the system's C sprintf(3) subroutine. On many machines, sprintf rounding is "unbiased," which means it doesn't always round a trailing ".5" up, contrary to naive expectations. In unbiased rounding, ".5" rounds to even, rather than always up, so 1.5 rounds to 2 but 4.5 rounds to 4. The result is that if you are using a format that does rounding (e.g., "%.0f") you should check what your system does. The following function does traditional rounding; it might be useful if your awk's printf does unbiased rounding.

    # round --- do normal rounding
    #	Arnold Robbins, arnold@gnu.ai.mit.edu
    #	Public Domain
    function round(x,       ival, aval, fraction)
    {
            ival = int(x)	# integer part, int() truncates
    	# see if fractional part
    	if (ival == x)	# no fraction
    		return x
    	if (x < 0) {
    		aval = -x	# absolute value
    		ival = int(aval)
    		fraction = aval - ival
    		if (fraction >= .5)
    			return int(x) - 1		# -2.5 --> -3
    		else
    			return int(x)		# -2.3 --> -2
    	} else {
    		fraction = x - ival
    		if (fraction >= .5)
    			return ival + 1
    		else
    			return ival
    	}
    }



    5.20 awk tutorialspoint , tags: awk tutorialspoint 

        5.20.1  practice

            5.20.1.1  basics

                5.20.1.1.1  add header
[i500695@WYLQRXL9LQ:2024-01-11 18:57:36:~/work/code/awk/practice:]2136$ cat marks.txt 
1)  Amit    Physics  80
2)  Rahul   Maths    90
3)  Shyam   Biology  87
4)  Kedar   English  85
5)  Hari    History  89
[i500695@WYLQRXL9LQ:2024-01-11 18:59:34:~/work/code/awk/practice:]2137$ awk 'BEGIN{printf "Sr  Name    Course\tGrade\n"} {print}' marks.txt 
Sr  Name    Course	Grade
1)  Amit    Physics  80
2)  Rahul   Maths    90
3)  Shyam   Biology  87
4)  Kedar   English  85
5)  Hari    History  89

                5.20.1.1.2 provide command file
[i500695@WYLQRXL9LQ:2024-01-11 19:01:39:~/work/code/awk/practice:]2139$ cat cmd.awk 
{print $1 $2}
[i500695@WYLQRXL9LQ:2024-01-11 19:01:43:~/work/code/awk/practice:]2140$ awk -f cmd.awk marks.txt 
1)Amit
2)Rahul
3)Shyam
4)Kedar
5)Hari

                5.20.1.1.3 awk options

                    5.20.1.1.3.1 The -v option
This option assigns a value to a variable. It allows assignment before the program execution. The following example describes the usage of the -v option.
$ awk -v name=yosi 'BEGIN {printf "my name is %s\n", name}'
my name is yosi

                    5.20.1.1.3.2 The --dump-variables[=file] option
It prints a sorted list of global variables and their final values to file. The default file is awkvars.out.
                5.20.1.1.4 print columns

$0 - whole line
$n - nth column
$NF - last column
/patter/ print only lines matching pattern

[i500695@WYLQRXL9LQ:2024-01-14 17:38:08:~/work/code/awk/practice:]2037$ awk '{print $2 "\t" $4}' marks.txt 
Amit	80
Rahul	90
Shyam	87
Kedar	85
Hari	89
[i500695@WYLQRXL9LQ:2024-01-14 17:38:30:~/work/code/awk/practice:]2039$ awk '/8/ {print $2 "\t" $4}' marks.txt 
Amit	80
Shyam	87
Kedar	85
Hari	89
[i500695@WYLQRXL9LQ:2024-01-14 17:38:44:~/work/code/awk/practice:]2040$ awk '/8/ {print $0}' marks.txt 
1)  Amit    Physics  80
3)  Shyam   Biology  87
4)  Kedar   English  85
5)  Hari    History  89


                5.20.1.1.5 count matches
[i500695@WYLQRXL9LQ:2024-01-14 17:38:56:~/work/code/awk/practice:]2041$ awk '/8/ {counter++} END{print "match 8 count ", counter}' marks.txt 
match 8 count  4

                5.20.1.1.6 print lines longer than X characters
[i500695@WYLQRXL9LQ:2024-01-14 19:16:34:~/work/code/awk/practice:]2045$ awk 'length($0) > 20' marks.txt 
1)  Amit    Physics  80
2)  Rahul   Maths    90
3)  Shyam   Biology  87
4)  Kedar   English  85
5)  Hari    History  89
[i500695@WYLQRXL9LQ:2024-01-14 19:16:38:~/work/code/awk/practice:]2046$ awk 'length($0) > 30' marks.txt 
[i500695@WYLQRXL9LQ:2024-01-14 19:16:44:~/work/code/awk/practice:]2047$ 

                5.20.1.1.7 ARGV and ARGC
[i500695@WYLQRXL9LQ:2024-01-14 19:16:44:~/work/code/awk/practice:]2047$ awk 'BEGIN {
> for (i=0;i<ARGC; ++i) {
> printf "ARGV[%d] = %s\n", i, ARGV[i]
> }
> }' aaa bbb ccc
ARGV[0] = awk
ARGV[1] = aaa
ARGV[2] = bbb
ARGV[3] = ccc

                5.20.1.1.8 built in variables CONVFMT, ENVIRON, FILENAME, FS, NF, NR etc
CONVFMT represents the conversion format for numbers. Its default value is %.6g.

Example
[jerry]$ awk 'BEGIN { print "Conversion Format =", CONVFMT }'

Output
Conversion Format = %.6g

ENVIRON - associative array of all env vars
[i500695@WYLQRXL9LQ:2024-01-14 19:19:12:~/work/code/awk/practice:]2048$ awk 'BEGIN {print ENVIRON["SHELL"]}'
/bin/bash

[i500695@WYLQRXL9LQ:2024-01-14 19:23:08:~/work/code/awk/practice:]2050$ awk 'END {print FILENAME}' marks.txt 
marks.txt

FS
It represents the (input) field separator and its default value is space. You can also change this by using -F command line option.

Example

[jerry]$ awk 'BEGIN {print "FS = " FS}' | cat -vte

Output

FS =  $
NF
It represents the number of fields in the current record. For instance, the following example prints only those lines that contain more than two fields.

Example

[jerry]$ echo -e "One Two\nOne Two Three\nOne Two Three Four" | awk 'NF > 2'

Output

One Two Three
One Two Three Four
NR
It represents the number of the current record. For instance, the following example prints the record if the current record number is less than three.

Example

[jerry]$ echo -e "One Two\nOne Two Three\nOne Two Three Four" | awk 'NR < 3'

Output

One Two
One Two Three
FNR
It is similar to NR, but relative to the current file. It is useful when AWK is operating on multiple files. Value of FNR resets with new file.

OFMT
It represents the output format number and its default value is %.6g.

Example

[jerry]$ awk 'BEGIN {print "OFMT = " OFMT}'

Output

OFMT = %.6g
OFS
It represents the output field separator and its default value is space.

Example

[jerry]$ awk 'BEGIN {print "OFS = " OFS}' | cat -vte

Output

OFS =  $
ORS
It represents the output record separator and its default value is newline.

Example

[jerry]$ awk 'BEGIN {print "ORS = " ORS}' | cat -vte

Output

ORS = $
$
RLENGTH
It represents the length of the string matched by match function. AWK's match function searches for a given string in the input-string.

Example

[jerry]$ awk 'BEGIN { if (match("One Two Three", "re")) { print RLENGTH } }'

Output

2
RS
It represents (input) record separator and its default value is newline.

Example

[jerry]$ awk 'BEGIN {print "RS = " RS}' | cat -vte

Output

RS = $
$
RSTART
It represents the first position in the string matched by match function.

Example

[jerry]$ awk 'BEGIN { if (match("One Two Three", "Thre")) { print RSTART } }'

Output

9
SUBSEP
It represents the separator character for array subscripts and its default value is \034.

Example

[jerry]$ awk 'BEGIN { print "SUBSEP = " SUBSEP }' | cat -vte

Output

SUBSEP = ^\$
$0
It represents the entire input record.

Example

[jerry]$ awk '{print $0}' marks.txt

Output

1) Amit     Physics   80
2) Rahul    Maths     90
3) Shyam    Biology   87
4) Kedar    English   85
5) Hari     History   89
$n
It represents the nth field in the current record where the fields are separated by FS.

Example

[jerry]$ awk '{print $3 "\t" $4}' marks.txt

Output

Physics   80
Maths     90
Biology   87
English   85
History   89

                5.20.1.1.9
        5.20.2 operators

            5.20.2.1  arithmetic

Addition
It is represented by plus (+) symbol which adds two or more numbers. The following example demonstrates this

Example
[jerry]$ awk 'BEGIN { a = 50; b = 20; print "(a + b) = ", (a + b) }'

Output
(a + b) = 70
Subtraction
It is represented by minus (-) symbol which subtracts two or more numbers. The following example demonstrates this

Example
[jerry]$ awk 'BEGIN { a = 50; b = 20; print "(a - b) = ", (a - b) }'

Output
(a - b) = 30
Multiplication
It is represented by asterisk (*) symbol which multiplies two or more numbers. The following example demonstrates this

Example
[jerry]$ awk 'BEGIN { a = 50; b = 20; print "(a * b) = ", (a * b) }'

Output
(a * b) = 1000
Division
It is represented by slash (/) symbol which divides two or more numbers. The following example illustrates this

Example
[jerry]$ awk 'BEGIN { a = 50; b = 20; print "(a / b) = ", (a / b) }'

Output
(a / b) = 2.5
Modulus
It is represented by percent (%) symbol which finds the Modulus division of two or more numbers. The following example illustrates this

Example
[jerry]$ awk 'BEGIN { a = 50; b = 20; print "(a % b) = ", (a % b) }'
On executing this code, you get the following result

Output
(a % b) = 10

            5.20.2.2 increment decrement

pre/post inc/decrement ++a, --a, a++, a-- difference is return value. pre first modified then returns. post vice versa

            5.20.2.3 assignment 

= , += -= /= %= ^= **=

            5.20.2.4 relational
like in C++
== , != , <, <=, >, >=

            5.20.2.5 logical
like in C++
&&, || , !
and or not

            5.20.2.6 ternary
expr? statement1: statement2

            5.20.2.7 unary
            +, -
            multiply by +1 , -1

            5.20.2.8 exponential
^, **
            5.20.2.9 string concat
strange one. space.
example:
$ awk 'BEGIN{s1="aaa"; s2="bbb"; s3=s1 s2; print s3}'
aaabbb

            5.20.2.10 array membership and iteration over array
$ awk 'BEGIN{a[1]="aaa"; a[2]="bbb"; s3=s1 s2; for (idx in a) printf "a[%d]=%s\n", idx,a[idx]}'
a[2]=bbb
a[1]=aaa

unlike python x in arr is index, not element

            5.20.2.11 regular expression
!~, ~
not match and match
[i500695@WYLQRXL9LQ:2024-01-15 22:51:47:~/work/code/awk/practice:]2056$ awk '$0 ~ "A[a-z]"' marks.txt 
1)  Amit    Physics  80

[i500695@WYLQRXL9LQ:2024-01-15 22:51:52:~/work/code/awk/practice:]2057$ awk '$0 !~ "A[a-z]"' marks.txt 
2)  Rahul   Maths    90
3)  Shyam   Biology  87
4)  Kedar   English  85
5)  Hari    History  89


            5.20.2.12
        5.20.3 regular expressions 
run of the mill...
. - any char
^, $ - begin end of line
[], [^] - char set inclusive / exclusive
| - or
? - 0 or 1 occurrences
+ - 1 or more occurrences 
() - grouping
ex:
[i500695@WYLQRXL9LQ:2024-01-16 13:21:07:~/work/code/awk/practice:]2060$ echo -e "Good morning" | awk '/^[^GO]../'
[i500695@WYLQRXL9LQ:2024-01-16 13:21:29:~/work/code/awk/practice:]2061$ echo -e "Good morning" | awk '/^[GO]../'
Good morning

        5.20.4 arrays
AWK has associative arrays and one of the best thing about it is the indexes need not to be continuous set of number; you can use either string or number as an array index. Also, there is no need to declare the size of an array in advance arrays can expand/shrink at runtime

Its syntax is as follows

Syntax
array_name[index] = value
Where array_name is the name of array, index is the array index, and value is any value assigning to the element of the array.

create array
[i500695@WYLQRXL9LQ:2024-01-16 13:25:45:~/work/code/awk/practice:]2065$ awk 'BEGIN{ a["yosi"]="izaq";a["guy"]="hanan"; print a["yosi"] "\n" a["guy"]}'
izaq
hanan


delete array elements:
delete arr[index]

arrays support is for 1 dimension only.
to "use" multi dim array like MxN use arithmetic to access i,j element stored in 1 dim array. e.g. m*i+j 

        5.20.5 control flows
if (cond) 
    action 
or {} for multiple actions
if (cond) {
    action1;
    action2;
    ...
}

also if else and if else if else are supported
e.x.
 awk 'BEGIN {
   a = 30;
   
   if (a==10)
   print "a = 10";
   else if (a == 20)
   print "a = 20";
   else if (a == 30)
   print "a = 30";
}'

        5.20.6 loops
like in C

for (init; cond; inc/decr) {
}
alse while (cond) {} and do {} while (cond)

break and continue are supported

exit(code) - exists with status code


        5.20.7 builtin functions

            5.20.7.1 arithmetic
atan(y,x) - arctangent of y/x
cos, exp, log, int (truncates), rand (random num x 0<=x<1), sin, sqrt, srand(seed) (random with seed, defaults to current time)

            5.20.7.2 string
asort - typographically sort array
ex:
[jerry]$ awk 'BEGIN {
   arr[0] = "Three"
   arr[1] = "One"
   arr[2] = "Two"
   print "Array elements before sorting:"
   
   for (i in arr) {
      print arr[i]
   }
   asort(arr)
   print "Array elements after sorting:"
   
   for (i in arr) {
      print arr[i]
   }
}'

aorti - sort by index
gsub(regex, sub, string)
gsub stands for global substitution. It replaces every occurrence of regex with the given string (sub). The third parameter is optional. If it is omitted, then $0 is used.
ex:
[jerry]$ awk 'BEGIN {
   str = "Hello, World"
   print "String before replacement = " str
   
   gsub("World", "Jerry", str)
   print "String after replacement = " str
}'

index(str, sub)
It checks whether sub is a substring of str or not. On success, it returns the position where sub starts; otherwise it returns 0. The first character of str is at position 1.

[jerry]$ awk 'BEGIN {
   str = "One Two Three"
   subs = "Two"
   ret = index(str, subs)
   
   printf "Substring \"%s\" found at %d location.\n", subs, ret
}'

length(str) - str length

match(str, regex)
It returns the index of the first longest match of regex in string str. It returns 0 if no match found
ex:
 awk 'BEGIN {
   str = "One Two Three"
   subs = "Two"
   ret = match(str, subs)
  
   printf "Substring \"%s\" found at %d location.\n", subs, ret
}'

split(str, arr, regex)
This function splits the string str into fields by regular expression regex and the fields are loaded into the array arr. If regex is omitted, then FS is used.

awk 'BEGIN {
   str = "One,Two,Three,Four"
   split(str, arr, ",")
   print "Array contains following values"
   
   for (i in arr) {
      print arr[i]
   }
}'

printf(format, expr-list)
This function returns a string constructed from expr-list according to format.
awk 'BEGIN {
   param = 1024.0
   result = sqrt(param)
   
   printf "sqrt(%f) = %f\n", param, result
}'

strtonum(str)
This function examines str and return its numeric value. If str begins with a leading 0, it is treated as an octal number. If str begins with a leading 0x or 0X, it is taken as a hexadecimal number. Otherwise, assume it is a decimal number.

sub(regex, sub, string)
This function performs a single substitution. It replaces the first occurrence of the regex pattern with the given string (sub). The third parameter is optional. If it is omitted, $0 is used.


substr(str, start, l)
This function returns the substring of string str, starting at index start of length l. If length is omitted, the suffix of str starting at index start is returned.

tolower(str)
This function returns a copy of string str with all upper-case characters converted to lower-case.

toupper(str)
This function returns a copy of string str with all lower-case characters converted to upper case.




            5.20.7.3 time functions
systime This function returns the current time of the day as the number of seconds since the Epoch (1970-01-01 00:00:00 UTC on POSIX systems).
[jerry]$ awk 'BEGIN {
   print "Number of seconds since the Epoch = " systime()
}'

mktime(datespec)
awk 'BEGIN {
   print "Number of seconds since the Epoch = " mktime("2014 12 14 30 20 10")
}'

strftime([format [, timestamp[, utc-flag]]])
awk 'BEGIN {
   print strftime("Time = %m/%d/%Y %H:%M:%S", systime())
}'

            5.20.7.4 bit manipulation
AND, OR, XOR compl, lshift, rshift , equivalent in C++ to &&, ||, ^, ~, <<, >>

            5.20.7.5 miscellaneous
close(expr)
This function closes file of pipe..

Example
[jerry]$ awk 'BEGIN {
   cmd = "tr [a-z] [A-Z]"
   print "hello, world !!!" |& cmd
   
   close(cmd, "to")
   cmd |& getline out
   print out;
   
   close(cmd);
}'
On executing this code, you get the following result

Output
HELLO, WORLD !!!
Does the script look cryptic? Let us demystify it.

The first statement, cmd = "tr [a-z] [A-Z]" - is the command to which we establish the two way communication from AWK.

The next statement, i.e., the print command, provides input to the tr command. Here &| indicates two-way communication.

The third statement, i.e., close(cmd, "to"), closes the to process after competing its execution.

The next statement cmd |& getline out stores the output into out variable with the aid of getline function.

The next print statement prints the output and finally the close function closes the command.



        5.20.8 functions syntax
# Returns minimum number
function find_min(num1, num2){
   if (num1 < num2)
   return num1
   return num2
}
# Returns maximum number
function find_max(num1, num2){
   if (num1 > num2)
   return num1
   return num2
}
# Main function
function main(num1, num2){
   # Find minimum number
   result = find_min(10, 20)
   print "Minimum =", result
  
   # Find maximum number
   result = find_max(10, 20)
   print "Maximum =", result
}
# Script execution starts here
BEGIN {
   main(10, 20)
}

        5.20.9 redirection operators
>, writes to file 
>>, appends to file
| - pipe one cmd output to another cmd input
|& - 2 way pipe
see explanation by GPT:
The `|&` operator in Linux is used for two-way communication between two processes. It combines the standard output (`stdout`) and standard error (`stderr`) of a command, allowing them to be sent to another command or process. This can be useful when you want to capture both standard output and standard error in a single stream.

Here are a couple of examples demonstrating the usage of the `|&` operator:

### Example 1: Redirecting both stdout and stderr to a file

```bash
# Run a command and redirect both stdout and stderr to a file
command1 |& tee output.txt
```

In this example, the `tee` command is used to redirect both standard output and standard error to a file named `output.txt`. The `tee` command will also display the output on the terminal.

### Example 2: Piping stdout and stderr to another command

```bash
# Run a command and pipe both stdout and stderr to another command
command1 |& grep "error"
```

In this example, the output (both stdout and stderr) of `command1` is piped to the `grep` command, which searches for lines containing the word "error."

### Example 3: Redirecting both stdout and stderr separately

```bash
# Run a command and redirect stdout to file1 and stderr to file2
command1 > file1 2> file2
```

In this example, stdout is redirected to `file1`, and stderr is redirected to `file2`. This is the traditional way of redirecting stdout and stderr separately.

### Note:
- The `|&` operator is available in certain shells like Bash. If your shell does not support it, you can use `2>&1` to achieve similar functionality.
- The `2>&1` notation is an alternative for redirecting stderr to the same location as stdout. It combines stderr (file descriptor 2) with stdout (file descriptor 1).

```bash
command1 2>&1 | grep "error"
```

These examples illustrate different ways to use the `|&` operator for two-way process communication and redirection of both standard output and standard error in Linux command-line environments.

        5.20.10 pretty printing 
https://www.tutorialspoint.com/awk/awk_pretty_printing.htm

        5.20.11
    5.21
6. Tips

	6.1 Count number of files in directory
		$ ls -l | awk ' !/^d/ {n++} END {print n}'
		$ ls -l | grep -v '^d' | wc -l



7. AWK one liners
AWK One Liners

FILE SPACING:

# double space a file
# awk 1;{print }
# awk BEGIN{ORS=\n\n};1
#
# # double space a file which already has blank lines in it. Output file
# # should contain no more than one blank line between lines of text.
# # NOTE: On Unix systems, DOS lines which have only CRLF (\r\n) are
# # often treated as non-blank, and thus NF alone will return TRUE.
# awk NF{print $0 \n}
#
# # triple space a file
# awk 1;{print \n}
#
# NUMBERING AND CALCULATIONS:
#
# # precede each line by its line number FOR THAT FILE (left alignment).
# # Using a tab (\t) instead of space will preserve margins.
# awk {print FNR \t $0} files*
#
# # precede each line by its line number FOR ALL FILES TOGETHER, with tab.
# awk {print NR \t $0} files*
#
# # number each line of a file (number on left, right-aligned)
# # Double the percent signs if typing from the DOS command prompt.
# awk {printf(%5d : %s\n, NR,$0)}
#
# # number each line of file, but only print numbers if line is not blank
# # Remember caveats about Unix treatment of \r (mentioned above)
# awk NF{$0=++a  : $0};{print}
# awk {print (NF? ++a  : :") $0}
#
# # count lines (emulates wc -l)
# awk END{print NR}
#
# # print the sums of the fields of every line
# awk {s=0; for (i=1; i max {max=$1; maxline=$0}; END{ print max, maxline}
#
# # print the number of fields in each line, followed by the line
# awk { print NF : $0 } 
#
# # print the last field of each line
# awk { print $NF }
#
# # print the last field of the last line
# awk { field = $NF }; END{ print field }
#
# # print every line with more than 4 fields
# awk NF > 4
#
# # print every line where the value of the last field is > 4
# awk $NF > 4
#
# TEXT CONVERSION AND SUBSTITUTION:
#
#
# # IN UNIX ENVIRONMENT: convert DOS newlines (CR/LF) to Unix format
# awk {sub(/\r$/,");print} # assumes EACH line ends with Ctrl-M
#
# # IN UNIX ENVIRONMENT: convert Unix newlines (LF) to DOS format
# awk {sub(/$/,\r);print}
#
# # IN DOS ENVIRONMENT: convert Unix newlines (LF) to DOS format
# awk 1
#
# # IN DOS ENVIRONMENT: convert DOS newlines (CR/LF) to Unix format
# # Cannot be done with DOS versions of awk, other than gawk:
# gawk -v BINMODE=w 1 infile >outfile
#
# # Use tr instead.
# tr -d \r outfile # GNU tr version 1.22 or higher
#
# # delete leading whitespace (spaces, tabs) from front of each line
# # aligns all text flush left
# awk {sub(/^[ \t]+/, ); print}
#
# # delete trailing whitespace (spaces, tabs) from end of each line
# awk {sub(/[ \t]+$/, );print}
#
# # delete BOTH leading and trailing whitespace from each line
# awk {gsub(/^[ \t]+|[ \t]+$/,");print}
# awk {$1=$1;print} # also removes extra space between fields
#
# # insert 5 blank spaces at beginning of each line (make page offset)
# awk {sub(/^/,  );print}
#
# # align all text flush right on a 79-column width
# awk {printf %79s\n, $0} file*
#
# # center all text on a 79-character width
# awk {l=length();s=int((79-l)/2); printf %(s+l)s\n,$0} file*
#
# # substitute (find and replace) foo with bar on each line
# awk {sub(/foo/,bar);print} # replaces only 1st instance
# gawk {$0=gensub(/foo/,bar,4);print} # replaces only 4th instance
# awk {gsub(/foo/,bar);print} # replaces ALL instances in a line
#
# # substitute foo with bar ONLY for lines which contain baz
# awk /baz/{gsub(/foo/, bar)};{print}
#
# # substitute foo with bar EXCEPT for lines which contain baz
# awk !/baz/{gsub(/foo/, bar)};{print}
#
# # change scarlet or ruby or puce to red
# awk {gsub(/scarlet|ruby|puce/, red); print}
#
# # reverse order of lines (emulates tac)
# awk {a[i++]=$0} END {for (j=i-1; j>=0;) print a[j] } file*
#
# # if a line ends with a backslash, append the next line to it
# # (fails if there are multiple lines ending with backslash)
# awk /\\$/ {sub(/\\$/,"); getline t; print $0 t; next}; 1 file*
#
# # print and sort the login names of all users
# awk -F : { print $1 | sort } /etc/passwd
#
# # print the first 2 fields, in opposite order, of every line
# awk {print $2, $1} file
#
# # switch the first 2 fields of every line
# awk {temp = $1; $1 = $2; $2 = temp} file
#
# # print every line, deleting the second field of that line
# awk { $2 = ; print }
#
# # print in reverse order the fields of every line
# awk {for (i=NF; i>0; i) printf(%s ,i);printf (\n)} file
#
# # remove duplicate, consecutive lines (emulates uniq)
# awk a !~ $0; {a=$0}
#
# # remove duplicate, nonconsecutive lines
# awk ! a[$0]++ # most concise script
# awk !($0 in a) {a[$0];print} # most efficient script
#
# # concatenate every 5 lines of input, using a comma separator
# # between fields
# awk ORS=%NR%5?,:\n file
#
# SELECTIVE PRINTING OF CERTAIN LINES:
#
# # print first 10 lines of file (emulates behavior of head)
# awk NR 1{exit};1
#
# # print the last 2 lines of a file (emulates tail -2)
# awk {y=x \n $0; x=$0};END{print y}
#
# # print the last line of a file (emulates tail -1)
# awk END{print}
#
# # print only lines which match regular expression (emulates grep)
# awk /regex/
#
# # print only lines which do NOT match regex (emulates grep -v)
# awk !/regex/
#
# # print the line immediately before a regex, but not the line
# # containing the regex
# awk /regex/{print x};{x=$0}
# awk /regex/{print (x==" ? match on line 1 : x)};{x=$0}
#
# # print the line immediately after a regex, but not the line
# # containing the regex
# awk /regex/{getline;print}
#
# # grep for AAA and BBB and CCC (in any order)
# awk /AAA/; /BBB/; /CCC/
#
# # grep for AAA and BBB and CCC (in that order)
# awk /AAA.*BBB.*CCC/
#
# # print only lines of 65 characters or longer
# awk length > 64
#
# # print only lines of less than 65 characters
# awk length 

8. My examples

	8.1 Counters

	    8.1.1 sed and awk to count many things


[yizaq@yizaq-wxp:Tue Nov 30:~:]$ cat stam | sed -e 's_(__' -e 's_)__' -e 's_:_ _g'| awk '{ t+=$4; s1+=$7; s2+=$9; s3+=$11; s4+=$13 } END {print "Total ", t, "S1 ", s1, "S2 ",s2, "S3 ", s3, "S4 ", s4 } '
Total  1368 S1  0 S2  28 S3  766 S4  574
[yizaq@yizaq-wxp:Tue Nov 30:~:]$ cat stam 
project bl-framework: total (278) sev 1:0 sev2:10 sev3:137 sev4:131
project bl-sec: total (18) sev 1:0 sev2:0 sev3:14 sev4:4
project bl-api: total (7) sev 1:0 sev2:0 sev3:5 sev4:2
project gui-framework: total (94) sev 1:0 sev2:0 sev3:62 sev4:32
project gui-app: total (507) sev 1:0 sev2:4 sev3:269 sev4:234
project pi-framework: total (8) sev 1:0 sev2:1 sev3:3 sev4:4
project pi-app: total (35) sev 1:0 sev2:0 sev3:14 sev4:21
project pi-axis: total (4) sev 1:0 sev2:0 sev3:2 sev4:2
project aac: total (25) sev 1:0 sev2:0 sev3:6 sev4:19
project cli-app: total (17) sev 1:0 sev2:0 sev3:6 sev4:11
project im: total (4) sev 1:0 sev2:0 sev3:3 sev4:1
project common: total (16) sev 1:0 sev2:5 sev3:10 sev4:1
project db: total (52) sev 1:0 sev2:6 sev3:40 sev4:6
project audit: total (23) sev 1:0 sev2:0 sev3:12 sev4:11
project logging-logging: total (2) sev 1:0 sev2:0 sev3:2 sev4:0
project messagebus: total (7) sev 1:0 sev2:0 sev3:6 sev4:1
project replication: total (57) sev 1:0 sev2:0 sev3:46 sev4:11
project configloader: total (2) sev 1:0 sev2:0 sev3:1 sev4:1
project dbpassword: total (1) sev 1:0 sev2:0 sev3:1 sev4:0
project distributedmanagement: total (24) sev 1:0 sev2:0 sev3:20 sev4:4
project dbtools: total (1) sev 1:0 sev2:0 sev3:1 sev4:0
project migration: total (93) sev 1:0 sev2:1 sev3:55 sev4:37
project clients-spml: total (10) sev 1:0 sev2:0 sev3:10 sev4:0
project changepassword: total (5) sev 1:0 sev2:1 sev3:4 sev4:0
project internalcli: total (2) sev 1:0 sev2:0 sev3:2 sev4:0
project licensing: total (7) sev 1:0 sev2:0 sev3:4 sev4:3
project transferutils: total (60) sev 1:0 sev2:0 sev3:24 sev4:36
project configrecovery: total (1) sev 1:0 sev2:0 sev3:1 sev4:0
project dbsupport: total (1) sev 1:0 sev2:0 sev3:1 sev4:0
project dynamic-azn: total (2) sev 1:0 sev2:0 sev3:0 sev4:2
project performancemonitoring: total (5) sev 1:0 sev2:0 sev3:5 sev4:0
[yizaq@yizaq-wxp:Tue Nov 30:~:]$ cat stam | sed -e 's_(__' -e 's_)__' -e 's_:_ _g'| awk '{ t+=$4; s1+=$7; s2+=$9; s3+=$11; s4+=$13 } END {print "Total ", t, "S1 ", s1, "S2 ",s2, "S3 ", s3, "S4 ", s4 } '
Total  1368 S1  0 S2  28 S3  766 S4  574

	    8.1.2 count a number in lines matching patter, ex: how many tests failed.
[i500695@C02X632CJGH6:2018-11-13 18:46:20:~/Desktop/work/SAP/CF_LocAL/portal-cf-site-semantic-service:]571$ cat  ~/run_rep | awk ' /[0-9]+ failing/ {f+=$1;t+=202; print $0} END {print "total tests faild=",f," of total=",t} '
...
  1 failing
  1 failing
  1 failing
  1 failing
  1 failing
  4 failing
  1 failing
total tests faild= 120  of total= 10908

	    8.1.3
	8.2 Calculate linux total disk size
[root@acs-e6a ~]# fdisk -l | awk 'BEGIN {s=0} /^Disk \/dev.* GB/ {size+=$3} END {print "Total disk size is " size "GB"} ' ; fdisk -l | awk 'BEGIN {s=0} /^Disk \/dev.* MB/ {size+=$3} END {print "Total disk size is " size "MB"} '
Disk /dev/dm-0 doesn't contain a valid partition table
Disk /dev/dm-1 doesn't contain a valid partition table
Disk /dev/dm-2 doesn't contain a valid partition table
Disk /dev/dm-3 doesn't contain a valid partition table
Disk /dev/dm-4 doesn't contain a valid partition table
Disk /dev/dm-5 doesn't contain a valid partition table
Disk /dev/dm-6 doesn't contain a valid partition table
Disk /dev/dm-7 doesn't contain a valid partition table
Disk /dev/dm-8 doesn't contain a valid partition table
Disk /dev/dm-9 doesn't contain a valid partition table
Disk /dev/dm-10 doesn't contain a valid partition table
Total disk size is 493.9GB
Disk /dev/dm-0 doesn't contain a valid partition table
Disk /dev/dm-1 doesn't contain a valid partition table
Disk /dev/dm-2 doesn't contain a valid partition table
Disk /dev/dm-3 doesn't contain a valid partition table
Disk /dev/dm-4 doesn't contain a valid partition table
Disk /dev/dm-5 doesn't contain a valid partition table
Disk /dev/dm-6 doesn't contain a valid partition table
Disk /dev/dm-7 doesn't contain a valid partition table
Disk /dev/dm-8 doesn't contain a valid partition table
Disk /dev/dm-9 doesn't contain a valid partition table
Disk /dev/dm-10 doesn't contain a valid partition table
Total disk size is 39388MB

	8.3 average csv file column

		8.3.1 on say 4th column

yizaq@yizaq-WS:Wed Sep 12:/cygdrive/c/Documents and Settings/yizaq/Desktop:]$ echo 1,2,3,4 > stam
[yizaq@yizaq-WS:Wed Sep 12:/cygdrive/c/Documents and Settings/yizaq/Desktop:]$ echo 1,2,3,6 >> stam
[yizaq@yizaq-WS:Wed Sep 12:/cygdrive/c/Documents and Settings/yizaq/Desktop:]$ echo 1,2,3,8 >> stam
[yizaq@yizaq-WS:Wed Sep 12:/cygdrive/c/Documents and Settings/yizaq/Desktop:]$ cat stam
1,2,3,4
1,2,3,6
1,2,3,8
[yizaq@yizaq-WS:Wed Sep 12:/cygdrive/c/Documents and Settings/yizaq/Desktop:]$ awk  -F, '{s += $4; l+=$1} END {print "avg is "s/l}' stam
avg is 6

		8.3.2


	8.4
9. My Cookbook 

	9.1 Using awk to print all columns from the nth to the last 

My solution, part of command to get all unique vi edited files (so need to drop first column, command #):
[root@acs-e4t01 bin]# history | grep vi | awk '{$1=""; print $0}' | uniq

Harder way, loop:
awk '{out=$2; for(i=3;i<=NF;i++){out=out" "$i}; print out}'

	9.2
10.  FAQS

	10.1  Round up/down, truncate 

		10.1.1  Via Integer Function

The int() function truncates a numeric value by removing digits to the right of the decimal point. Look at the following two statements:

    print 100/3
    print int(100/3)

The output from these statements is shown below:

$    33.3333
    33

The int() function simply truncates; it does not round up or down. (Use the printf format "%.0f" to perform rounding.)

		10.1.2 Discussion in forum

- > echo "1.2345" | cut -d"." -f1
1
> echo "134.2345" | cut -d"." -f1
> 134
>

- awk '{printf("%.0f", $1*.04) }' infile > outfile

- perl -ne ' print int($_* 3.5), "\n" '  < infile > outfile

- I think what your asking is to truncate the number. Just print it as an integer with printf.

awk '{printf("%d\n",$1)}' file.data


		10.1.3
	10.2

11. Summary

	11.1 Intro This week-of-unix-tools 
	is intended to be a high concentration of information with little fluff. I'll be covering only GNU versions of the tools, for the sake of choosing only one version for sanity sake.
What is awk?
Hands-down, one of *the* most useful filter tools you'll find. Awk is a scripting language, but I find it is best used from the shell in oneliners.
Basic awk(1) usage
awk [-F<field_sep>] [awk_script]
Records and Fields
Awk has two data concepts that come from file input: Records and Fields.
A record is generally a whole line. The default input record separator (RS) is a newline. You can change this at any time.
A field is generally a word split by any number of whitespace (tab or space). The default input field separator (FS) is a single space. FS can be a single character or a regular expression. If FS is a single space, it is treated magically as if you had specified [ \t]+.
Field selection
Fields are accessed using the $ "operator". The following are valid:
$1, $2, $3 ...
(first, second and third fields)
$NF
The last field. Nothing special. NF is a variable holding the total number of fields in the current record, therefore $NF would be the last field
x=1; $(x + 3)
The 4th field. $(x + 3) == $(1 + 3) == $4
Patterns and functions
Awk expressions come in two forms, a function or a pattern. I've never bothered writing functions.
Here's what a pattern looks like: [condition_expressions] { [action_expressions] }
Basically this equates to the folloing psuedocode: if (condition_expressions) { action_expressions }
If no action_expression is defined, the default is 'print' which means 'print $0' which means printthe current record. If no condition is given, the default is to execute the action for all records.
Magic patterns: BEGIN and END
BEGIN and END are magic "conditions". BEGIN is used to execute things before the first record has been parsed, and END is obviously to do things after the last record. These patterns cannot be combined with others.
Sample pattern expressions
length($0) > 72 (From FreeBSD's awk manpage)
Print lines longer than 72 characters
$1 ~ /foo/ { print $2 }
Print the 2nd field of all records where the first field matches /foo/
$5 > 0
Print all records where the 5th field is greater than 0. (Complete with magical number conversion, when possible.
int($5) > 0
Same as above, but force $5 to int before comparing
Variables
Variables are the same syntax as in C. You do not declare variables.
Examples:
$2 == "test" { x++ }; END { print x }
Total records where $2 == "test"
{ $1 = ""; print }
Delete the first field of every record, print the new record
{ $3 = "Hello"; print }
Should be obvious. This one is *super* useful; modifying fields inline is awesome
Arrays
Arrays are magical. You simply start using a variable as an array, and it becomes an array. Arrays are more like dictionaries/hash tables/associative arrays than "real" arrays. Quite useful.
Example: awk '{ a[$1]++ } END { for (i in a) { print i, a[i] } }'
String concatonation
String appending is simple.
x = "foo"; x = x"test";    # x == "footest"

print $1","$2" = "$3;      # if input was "hello there world"
                           # output will be: "hello,there = world"
Example: Open files by user
This example is basically "add things up by a given key, then print them at the end". I use it so often I'm probably just going to write an alias for it in my shell.
% fstat | sed -e 1d \
  | awk '{a[$1]++} END { for (i in a) { print i, a[i] } }' \
  | sort -nk2
smmsp 8
_dhcp 11
www 45
root 328
jls 482
Example: Datestamp input
This particular example is *extremely* useful for long-running programs that output logs or other data without any kind of timestamp. This requires GNU awk.
% (echo hello; sleep 5; echo world) \
  | awk '{ print strftime("%Y/%m/%d %H:%M:%S", systime()), $0 }'
2007/05/22 01:09:47 hello
2007/05/22 01:09:52 world
Example: show non-empty files
% ls -l | awk '$5 > 0'
Example: Date-scan your logs
Let's assume all log entries are syslog format:
May 22 01:12:02 nightfall pptp[860]: anon log ...
Show only log entries between May 10th and May 20th (inclusive)
% cat *.log | awk '$1 == "May" && ($2 >= 10 && $2 <= 20)'
Example: Scrape host(1) output
% host www.google.com | awk '/has address/ { print $4 }'
Example: Find an environment variable
I often login to my workstation remotely and want to use its ssh-agent. So, I need to find the most common value for SSH_AUTH_SOCK on all processes.
% ps aexww \
  | awk '{ for (i = 0; i < NF; i++) { if ($i ~ /^SSH_AUTH_SOCK=/) { print $i } } }' \
  | sort | uniq -c
  24 SSH_AUTH_SOCK=/tmp/ssh-sc4iKR7ZIf/agent.721
Teeth that will bite you
Awk falls to the same problem C does. You can assign in conditions. Here's how you screw up:
% cat *.log | awk '$1 = "May"'
This will replace the first field with "May" for every record, and since "May" is a positive value, it will print your modified $0 with $1 set to "May" now. Ouch.

	11.2
12.
