.........................................Table Of Contents...............................................................
1. grep <URL:#tn=1. grep>
	1. 1. recursive grep on certain type of files. example on all .cc files search for a certain string, also print name of file being searched. <URL:#tn=	1. 1. recursive grep on certain type of files. example on all .cc files search for a certain string, also print name of file being searched.>
	1.2. the same but search in .cc or .h files. put the output in a file. <URL:#tn=	1.2. the same but search in .cc or .h files. put the output in a file.>
	1.3 Use grep as shortcut to clearcase operations <URL:#tn=	1.3 Use grep as shortcut to clearcase operations>
		1.3.1 run mkelem command on a list of checked out files  <URL:#tn=		1.3.1 run mkelem command on a list of checked out files >
	1.4. list only directories do. <URL:#tn=	1.4. list only directories do.>
		1.4.1 $ ls -F | grep "/"  <URL:#tn=		1.4.1 $ ls -F | grep "/" >
	1.5.5 grep regular expression. <URL:#tn=	1.5.5 grep regular expression.>
		1.5.5.1 use pearl refular expression syntax. -P <URL:#tn=		1.5.5.1 use pearl refular expression syntax. -P>
	 1.5.6 grep with context (print lines before and after) <URL:#tn=	 1.5.6 grep with context (print lines before and after)>
	1.5.7 Create a list of attributes from ACS dictionary attr file. <URL:#tn=	1.5.7 Create a list of attributes from ACS dictionary attr file.>
6. Usage <URL:#tn=6. Usage>
	   1.5.8. How can I list just the names of matching files? <URL:#tn=	   1.5.8. How can I list just the names of matching files?>
	   1.5.9. How do I search directories recursively? <URL:#tn=	   1.5.9. How do I search directories recursively?>
	   1.5.10. What if a pattern has a leading `-'? <URL:#tn=	   1.5.10. What if a pattern has a leading `-'?>
	   1.5.11. Word boundary, Suppose I want to search for a whole word, not a part of a word? <URL:#tn=	   1.5.11. Word boundary, Suppose I want to search for a whole word, not a part of a word?>
	   1.5.12. How do I output context around the matching lines? <URL:#tn=	   1.5.12. How do I output context around the matching lines?>
	   1.5.13. How do I force grep to print the name of the file? <URL:#tn=	   1.5.13. How do I force grep to print the name of the file?>
	   1.5.14. Why do people use strange regular expressions on ps output? <URL:#tn=	   1.5.14. Why do people use strange regular expressions on ps output?>
	   1.5.15. Why does grep report "Binary file matches"? <URL:#tn=	   1.5.15. Why does grep report "Binary file matches"?>
	   1.5.16. Why doesn't `grep -lv' print nonmatching file names? <URL:#tn=	   1.5.16. Why doesn't `grep -lv' print nonmatching file names?>
	  1.5.17. I can do OR with `|', but what about AND? <URL:#tn=	  1.5.17. I can do OR with `|', but what about AND?>
	  1.5.18. How can I search in both standard input and in files? <URL:#tn=	  1.5.18. How can I search in both standard input and in files?>
	  1.5.19. How to express palindromes in a regular expression? <URL:#tn=	  1.5.19. How to express palindromes in a regular expression?>
	  1.5.20. Why are my expressions whith the vertical bar fail? <URL:#tn=	  1.5.20. Why are my expressions whith the vertical bar fail?>
	  1.5..17. What do grep, fgrep, egrep stand for ? <URL:#tn=	  1.5..17. What do grep, fgrep, egrep stand for ?>
	1.6  grep or operator <URL:#tn=	1.6  grep or operator>
		1.6.1  Use the OR operator in grep to search for words and phrases <URL:#tn=		1.6.1  Use the OR operator in grep to search for words and phrases>
		1.6.2 <URL:#tn=		1.6.2>
	1.7 <URL:#tn=	1.7>
2. Editors <URL:#tn=2. Editors>
	2.1 emacs: <URL:#tn=	2.1 emacs:>
		2.1.1 Key shortcuts <URL:#tn=		2.1.1 Key shortcuts>
		2.1.2.Emacs tips <URL:#tn=		2.1.2.Emacs tips>
	2.2 VI, for more information see KB_vi  <URL:#tn=	2.2 VI, for more information see KB_vi >
		2.2.1. vi highlights. <URL:#tn=		2.2.1. vi highlights.>
			2.2.1.1 :set list  will show hidden chars. <URL:#tn=			2.2.1.1 :set list  will show hidden chars.>
		2.2.2. Tip. Resume work with VI buffers. <URL:#tn=		2.2.2. Tip. Resume work with VI buffers.>
	2.3 VIM <URL:#tn=	2.3 VIM>
	2.4 Latex. <URL:#tn=	2.4 Latex.>
		2.4.1 To produce a dvi file <URL:#tn=		2.4.1 To produce a dvi file>
		2.4.2 To produce a PDF <URL:#tn=		2.4.2 To produce a PDF>
		2.4.3 To view a dvi file <URL:#tn=		2.4.3 To view a dvi file>
			2.4.3.1 use ghost view  <URL:#tn=			2.4.3.1 use ghost view >
			2.4.3.2 use yap <URL:#tn=			2.4.3.2 use yap>
3. Sed <URL:#tn=3. Sed>
	3.1 use sed to replace every occurence of : with a space. <URL:#tn=	3.1 use sed to replace every occurence of : with a space.>
		3.1.1 $ ps -ef | grep yizaq | awk '{print $8}' | sed 's/\:/ /g' <URL:#tn=		3.1.1 $ ps -ef | grep yizaq | awk '{print $8}' | sed 's/\:/ /g'>
		3.1.2 to run a sed command file do  <URL:#tn=		3.1.2 to run a sed command file do >
		3.1.3 to append a text to every line that contains string "mapped" create a command file: <URL:#tn=		3.1.3 to append a text to every line that contains string "mapped" create a command file:>
		3.1.4 to change a configuration param from yes to no create a command file: <URL:#tn=		3.1.4 to change a configuration param from yes to no create a command file:>
		3.1.5  All the files the contain String include change to nystring include <URL:#tn=		3.1.5  All the files the contain String include change to nystring include>
		3.1.6 make a comand file that contains command <file name> lines. example: <URL:#tn=		3.1.6 make a comand file that contains command <file name> lines. example:>
		3.1.7  use sed to remove all blank lines in file <URL:#tn=		3.1.7  use sed to remove all blank lines in file>
4. System management <URL:#tn=4. System management>
	4.1. crontab, order process or tasks to be performed automatically at given times. <URL:#tn=	4.1. crontab, order process or tasks to be performed automatically at given times.>
		4.1.1 to view: <URL:#tn=		4.1.1 to view:>
		4.1.2 to edit: <URL:#tn=		4.1.2 to edit:>
	4.2. Change hostname. <URL:#tn=	4.2. Change hostname.>
		4.2.1 become root ( su - ) <URL:#tn=		4.2.1 become root ( su - )>
		4.2.2 uname -S <name> <URL:#tn=		4.2.2 uname -S <name>>
		4.2.3 netcfg; protocols->Modify-> now change host name <URL:#tn=		4.2.3 netcfg; protocols->Modify-> now change host name>
		4.2.4 change IP. <URL:#tn=		4.2.4 change IP.>
	4.3 RAID maintenance <URL:#tn=	4.3 RAID maintenance>
		4.3.1. determine whether or not a machine is working with RAID disk. <URL:#tn=		4.3.1. determine whether or not a machine is working with RAID disk.>
			4.3.1.1 Type sdiconfig -l  you'll C that the device attached to c1 is the tape  <URL:#tn=			4.3.1.1 Type sdiconfig -l  you'll C that the device attached to c1 is the tape >
			4.3.1.2 $ prtconf  <URL:#tn=			4.3.1.2 $ prtconf >
		4.3.2. RAID configuration <URL:#tn=		4.3.2. RAID configuration>
			4.3.2.1 0-fastest to 5-slowest. <URL:#tn=			4.3.2.1 0-fastest to 5-slowest.>
			4.3.2.2 memory 128-512 <URL:#tn=			4.3.2.2 memory 128-512>
			4.3.2.3 controller speed. <URL:#tn=			4.3.2.3 controller speed.>
			4.3.2.4 SCSI card speed. should be 160. 29160. 2940 or 2980 is a problem. <URL:#tn=			4.3.2.4 SCSI card speed. should be 160. 29160. 2940 or 2980 is a problem.>
	4.4. determine NIC speed. <URL:#tn=	4.4. determine NIC speed.>
	4.5 Process management <URL:#tn=	4.5 Process management>
		4.5.1 change process priority <URL:#tn=		4.5.1 change process priority>
		4.5.2 Send signal to process (like SIGTERM, or kill) <URL:#tn=		4.5.2 Send signal to process (like SIGTERM, or kill)>
		4.5.3 Zombie process (from wikipedia) <URL:#tn=		4.5.3 Zombie process (from wikipedia)>
		4.5.4 <URL:#tn=		4.5.4>
	4.6 remove user . <URL:#tn=	4.6 remove user .>
	4.7 mounts.  <URL:#tn=	4.7 mounts. >
		4.7.1 /etc/vfstab - mount table.  <URL:#tn=		4.7.1 /etc/vfstab - mount table. >
		4.7.2commands vfstab, mnttab, mount, umount <URL:#tn=		4.7.2commands vfstab, mnttab, mount, umount>
	4.8  `chroot': Run a command with a different root directory <URL:#tn=	4.8  `chroot': Run a command with a different root directory>
	4.9 System information <URL:#tn=	4.9 System information>
		4.9.1 process list  <URL:#tn=		4.9.1 process list >
		4.9.2 memory usage. <URL:#tn=		4.9.2 memory usage.>
		4.9.3 File system. <URL:#tn=		4.9.3 File system.>
		4.9.4 Hardware <URL:#tn=		4.9.4 Hardware>
	4.10 administrator access on Linux <URL:#tn=	4.10 administrator access on Linux>
	4.11 Basic information of system status <URL:#tn=	4.11 Basic information of system status>
		4.11.1 Users online, $ who <URL:#tn=		4.11.1 Users online, $ who>
		4.11.2 User details, $ who am i <URL:#tn=		4.11.2 User details, $ who am i>
		4.11.3 host name: $hostname, ID: $hostid, Data(requires root permission): $hoststat <URL:#tn=		4.11.3 host name: $hostname, ID: $hostid, Data(requires root permission): $hoststat>
5. Shell  <URL:#tn=5. Shell >
	5.1. sh stream redirection <URL:#tn=	5.1. sh stream redirection>
		5.1.1 man sh. search redirect <URL:#tn=		5.1.1 man sh. search redirect>
		5.1.2 example. command that prints to stderr. stderr is redirected to sdtout (2>&1) stdout is appended to $FILE <URL:#tn=		5.1.2 example. command that prints to stderr. stderr is redirected to sdtout (2>&1) stdout is appended to $FILE>
	5.2 sh command status: <URL:#tn=	5.2 sh command status:>
	5.3 Shell rex exp expansion. <URL:#tn=	5.3 Shell rex exp expansion.>
		5.3.1 example. Create 10 files. <URL:#tn=		5.3.1 example. Create 10 files.>
6. Advanced tricks <URL:#tn=6. Advanced tricks>
	6.1. turn off echo of keystrokes. and steal a user's password. <URL:#tn=	6.1. turn off echo of keystrokes. and steal a user's password.>
	6.2. Locate a machine in the lab. Will only work in Unix flavours that enable making this sound (Most Linux won't). <URL:#tn=	6.2. Locate a machine in the lab. Will only work in Unix flavours that enable making this sound (Most Linux won't).>
		6.2.1 login as root. <URL:#tn=		6.2.1 login as root.>
		6.2.2 issue <URL:#tn=		6.2.2 issue>
	6.3. send mail. <URL:#tn=	6.3. send mail.>
		6.3.1  from shell mailx <URL:#tn=		6.3.1  from shell mailx>
		6.3.2  perl send_mail <URL:#tn=		6.3.2  perl send_mail>
	6.4. copy dir tree including symbolic links. <URL:#tn=	6.4. copy dir tree including symbolic links.>
		6.4.1 $find . -follow | cpio -pd <target dir> <URL:#tn=		6.4.1 $find . -follow | cpio -pd <target dir>>
		6.4.2 $tar cvfh tarfile src_dir <URL:#tn=		6.4.2 $tar cvfh tarfile src_dir>
	6.5 view hex. <URL:#tn=	6.5 view hex.>
	6.6 compare directories content. <URL:#tn=	6.6 compare directories content.>
		6.6.1 $ dircmp dir1 dir2 <URL:#tn=		6.6.1 $ dircmp dir1 dir2>
		6.6.2 Compare mainly binary files directoris <URL:#tn=		6.6.2 Compare mainly binary files directoris>
		6.6.3 text diff <URL:#tn=		6.6.3 text diff>
	6.7 Talk to another user on same system. <URL:#tn=	6.7 Talk to another user on same system.>
	6.8 Broadcast message on unix system <URL:#tn=	6.8 Broadcast message on unix system>
	6.9 To send message to a specific user <URL:#tn=	6.9 To send message to a specific user>
	6.10 To create a low level TCP connection do: <URL:#tn=	6.10 To create a low level TCP connection do:>
	6.11 Remove every second (or nth) line from a file. <URL:#tn=	6.11 Remove every second (or nth) line from a file.>
		6.11.1 with awk. <URL:#tn=		6.11.1 with awk.>
		6.11.2 with sed. <URL:#tn=		6.11.2 with sed.>
7. Performance. <URL:#tn=7. Performance.>
	7.1 use ldd to determine the list of so depen.   <URL:#tn=	7.1 use ldd to determine the list of so depen.  >
	7.2 use nm to view the so (or binary) name and list of symbols. <URL:#tn=	7.2 use nm to view the so (or binary) name and list of symbols.>
	7.3 use strings to get the binary strings.  <URL:#tn=	7.3 use strings to get the binary strings. >
	7.4. sar. <URL:#tn=	7.4. sar.>
		7.4.1 to record load system params for 10 hours in a half minute res. <URL:#tn=		7.4.1 to record load system params for 10 hours in a half minute res.>
		7.4.2 to read the recorded input. <URL:#tn=		7.4.2 to read the recorded input.>
		7.4.3 sar flags. <URL:#tn=		7.4.3 sar flags.>
8.  Remote procedures (execute commands copy/move files between remote machines) <URL:#tn=8.  Remote procedures (execute commands copy/move files between remote machines)>
	8.1.1 rsh a complicated command.  <URL:#tn=	8.1.1 rsh a complicated command. >
		8.1.1.1 for example a complicated kill of sfe process on machine doh. <URL:#tn=		8.1.1.1 for example a complicated kill of sfe process on machine doh.>
	8.1.2 ssh <URL:#tn=	8.1.2 ssh>
		8.1.2.1 execute remote command <URL:#tn=		8.1.2.1 execute remote command>
		8.1.2.2 comprehensive guide to SSH usage for remote file operations <URL:#tn=		8.1.2.2 comprehensive guide to SSH usage for remote file operations>
			8.1.2.2.1 PUSH <URL:#tn=			8.1.2.2.1 PUSH>
				8.1.2.2.1.1 push a compressed tar file  <URL:#tn=				8.1.2.2.1.1 push a compressed tar file >
				8.1.2.2.1.2 push one file <URL:#tn=				8.1.2.2.1.2 push one file>
				8.1.2.2.1.3 advanced push commands <URL:#tn=				8.1.2.2.1.3 advanced push commands>
				8.1.2.2.1.4 push one file and compress it on the way <URL:#tn=				8.1.2.2.1.4 push one file and compress it on the way>
			8.1.2.2.2 PULL: <URL:#tn=			8.1.2.2.2 PULL:>
				8.1.2.2.2.1 pull one file <URL:#tn=				8.1.2.2.2.1 pull one file>
				8.1.2.2.2.2 pull one compressed file and uncompress it on the way <URL:#tn=				8.1.2.2.2.2 pull one compressed file and uncompress it on the way>
			8.1.2.2.3 COMPARE: <URL:#tn=			8.1.2.2.3 COMPARE:>
			8.1.2.2.4 Further information <URL:#tn=			8.1.2.2.4 Further information>
				8.1.2.2.4.1 Tip, much more efficient way to do cp -r <URL:#tn=				8.1.2.2.4.1 Tip, much more efficient way to do cp -r>
		8.1.3  FTP <URL:#tn=		8.1.3  FTP>
		8.1.4 Pipes and Redirects <URL:#tn=		8.1.4 Pipes and Redirects>
		8.1.5 Related bash operators   <URL:#tn=		8.1.5 Related bash operators  >
		8.1.6 && and || in bash CLI <URL:#tn=		8.1.6 && and || in bash CLI>
		8.1.7 The dash "-" in bash  <URL:#tn=		8.1.7 The dash "-" in bash >
9. Compile and build code <URL:#tn=9. Compile and build code>
	9.1. Make, gmake <URL:#tn=	9.1. Make, gmake>
		9.1.1. pass compilation value. <URL:#tn=		9.1.1. pass compilation value.>
			9.1.1.1 add the compilation flag at the make file under CXX flags. <URL:#tn=			9.1.1.1 add the compilation flag at the make file under CXX flags.>
			9.1.1.2 or use -d [ flag ]. <URL:#tn=			9.1.1.2 or use -d [ flag ].>
			9.1.1.3 Ask make to print the commands but not execute then (for verification) <URL:#tn=			9.1.1.3 Ask make to print the commands but not execute then (for verification)>
	9.2 Find duplicate inlines in C++ binaries. <URL:#tn=	9.2 Find duplicate inlines in C++ binaries.>
	9.3 Discover source code dependency on header files. <URL:#tn=	9.3 Discover source code dependency on header files.>
	9.4. GNU Makefile <URL:#tn=	9.4. GNU Makefile>
		9.4.1 Basics <URL:#tn=		9.4.1 Basics>
			9.4.1.1 What a Rule Looks Like <URL:#tn=			9.4.1.1 What a Rule Looks Like>
			9.4.1.2 A Simple Makefile <URL:#tn=			9.4.1.2 A Simple Makefile>
			9.4.1.3 A Simple Makefile <URL:#tn=			9.4.1.3 A Simple Makefile>
			9.4.1.4  How make Processes a Makefile <URL:#tn=			9.4.1.4  How make Processes a Makefile>
			9.4.1.5  Variables Make Makefiles Simpler <URL:#tn=			9.4.1.5  Variables Make Makefiles Simpler>
			9.4.1.6  Letting make Deduce the Commands <URL:#tn=			9.4.1.6  Letting make Deduce the Commands>
			9.4.1.7  Another Style of Makefile <URL:#tn=			9.4.1.7  Another Style of Makefile>
			9.4.1.8   Rules for Cleaning the Directory <URL:#tn=			9.4.1.8   Rules for Cleaning the Directory>
		9.4.2 Writing Makefiles <URL:#tn=		9.4.2 Writing Makefiles>
			9.4.2.1 What Makefiles Contain <URL:#tn=			9.4.2.1 What Makefiles Contain>
			9.4.2.2 What Name to Give Your Makefile <URL:#tn=			9.4.2.2 What Name to Give Your Makefile>
			9.4.2.3  Including Other Makefiles <URL:#tn=			9.4.2.3  Including Other Makefiles>
			9.4.2.4 The Variable MAKEFILES <URL:#tn=			9.4.2.4 The Variable MAKEFILES>
			9.4.2.5 The Variable MAKEFILE_LIST <URL:#tn=			9.4.2.5 The Variable MAKEFILE_LIST>
			9.4.2.6 Other Special Variables <URL:#tn=			9.4.2.6 Other Special Variables>
			9.4.2.7  <URL:#tn=			9.4.2.7 >
		9.4.3 Short tutorial on writing makefiles: <URL:#tn=		9.4.3 Short tutorial on writing makefiles:>
	9.5 Make man <URL:#tn=	9.5 Make man>
	9.6 <URL:#tn=	9.6>
10. Clearcase, see clear_case_KB.txt for full information <URL:#tn=10. Clearcase, see clear_case_KB.txt for full information>
	10.1. add mvobs elements. new dir with new files. <URL:#tn=	10.1. add mvobs elements. new dir with new files.>
		10.1.1 $ct co -nc [root dir] <URL:#tn=		10.1.1 $ct co -nc [root dir]>
		10.1.2 $ct mkdir -nc [new dir] <URL:#tn=		10.1.2 $ct mkdir -nc [new dir]>
		10.1.3 $cp [files] [new dir] <URL:#tn=		10.1.3 $cp [files] [new dir]>
		10.1.4 $cd [new dir] <URL:#tn=		10.1.4 $cd [new dir]>
		10.1.5 $ct mkelem -nc * <URL:#tn=		10.1.5 $ct mkelem -nc *>
	10.2. remove version from integ. one that is hyperlinked from a private branch version. <URL:#tn=	10.2. remove version from integ. one that is hyperlinked from a private branch version.>
	10.3 find all checked out files recursively. <URL:#tn=	10.3 find all checked out files recursively.>
11. Shell scripting. <URL:#tn=11. Shell scripting.>
	11.1 sh <URL:#tn=	11.1 sh>
		11.1.1. sh variable condition example.  <URL:#tn=		11.1.1. sh variable condition example. >
12. Debug processes, programs <URL:#tn=12. Debug processes, programs>
	12.1. Execution report in terms of syscalls. <URL:#tn=	12.1. Execution report in terms of syscalls.>
	12.2. GDB <URL:#tn=	12.2. GDB>
		12.2.1 examine memory <URL:#tn=		12.2.1 examine memory>
13. Viewers <URL:#tn=13. Viewers>
	13.1. less. <URL:#tn=	13.1. less.>
		13.1.1 use like tail -f to display information stream. <URL:#tn=		13.1.1 use like tail -f to display information stream.>
		13.1.2 ignore case  <URL:#tn=		13.1.2 ignore case >
14. awk, AWK, see kb_awk for full information. <URL:#tn=14. awk, AWK, see kb_awk for full information.>
	14.1 awk seperator <URL:#tn=	14.1 awk seperator>
	14.2 Simple arithmetic calculation.  <URL:#tn=	14.2 Simple arithmetic calculation. >
		14.2.1 Simple example <URL:#tn=		14.2.1 Simple example>
		14.2.2. Advanced example. <URL:#tn=		14.2.2. Advanced example.>
	14.3 Use awk to filter many files from one file. <URL:#tn=	14.3 Use awk to filter many files from one file.>
	14.4 tee <URL:#tn=	14.4 tee>
15. Useful, Common procedures and shortcuts <URL:#tn=15. Useful, Common procedures and shortcuts>
	15.1 issue command on list of files. <URL:#tn=	15.1 issue command on list of files.>
		15.1.1 in sh: <URL:#tn=		15.1.1 in sh:>
		15.1.2 Process the contents of a file and then run a list of commands: <URL:#tn=		15.1.2 Process the contents of a file and then run a list of commands:>
	15.2 list directories. <URL:#tn=	15.2 list directories.>
	15.3 process tree. <URL:#tn=	15.3 process tree.>
	15.4. List open files in system <URL:#tn=	15.4. List open files in system>
	15.5. Remove repeated lines from file: <URL:#tn=	15.5. Remove repeated lines from file:>
	15.6 Encryption. <URL:#tn=	15.6 Encryption.>
		15.6.1 Don't use crypt!!! It is based on 56 bit DES and is easy to break. <URL:#tn=		15.6.1 Don't use crypt!!! It is based on 56 bit DES and is easy to break.>
		15.6.2 Can use ccrypt (based on AES) <URL:#tn=		15.6.2 Can use ccrypt (based on AES)>
	15.7 Create tarball backup of files according to file lists in a file. <URL:#tn=	15.7 Create tarball backup of files according to file lists in a file.>
	15.8 Create/modify a UNIX file with an arbitrary timestamp <URL:#tn=	15.8 Create/modify a UNIX file with an arbitrary timestamp>
	15.2 Open remote graphical display <URL:#tn=	15.2 Open remote graphical display>
		15.2.1 Directlym configure Manually, in may time won't work due to proxy or network configuration <URL:#tn=		15.2.1 Directlym configure Manually, in may time won't work due to proxy or network configuration>
		15.2.2 Using SSH with X forwarding for untrusted X11 display forwarding <URL:#tn=		15.2.2 Using SSH with X forwarding for untrusted X11 display forwarding>
		15.2.3 ssh -Y <URL:#tn=		15.2.3 ssh -Y>
		15.2.4 <URL:#tn=		15.2.4>
	15.3 nohup <URL:#tn=	15.3 nohup>
16. Debug <URL:#tn=16. Debug>
	16.1 create core file. <URL:#tn=	16.1 create core file.>
		16.1.1 gcore -o <core name> -pid <pid> <URL:#tn=		16.1.1 gcore -o <core name> -pid <pid>>
		16.1.2 or kill -7 <pid>  <URL:#tn=		16.1.2 or kill -7 <pid> >
	16.2. list .so an executable uses: <URL:#tn=	16.2. list .so an executable uses:>
17. Plot, print, display, pretty print etc. <URL:#tn=17. Plot, print, display, pretty print etc.>
	17.1. print a ascii banner  <URL:#tn=	17.1. print a ascii banner >
	17.2. gnuplot <URL:#tn=	17.2. gnuplot>
	49. sh functions. <URL:#tn=	49. sh functions.>
18. Source navigation. <URL:#tn=18. Source navigation.>
	18.1 cscope is a very good source code navigator based on curse (acsii gui) lib. <URL:#tn=	18.1 cscope is a very good source code navigator based on curse (acsii gui) lib.>
	18.2 source navigator <URL:#tn=	18.2 source navigator>
	18.3 cflow <URL:#tn=	18.3 cflow>
	18.4 Understand. <URL:#tn=	18.4 Understand.>
	18.5 hypersrc, pypersrc <URL:#tn=	18.5 hypersrc, pypersrc>
		18.5.1 hypersrc installtion for cygwin. <URL:#tn=		18.5.1 hypersrc installtion for cygwin.>
		18.5.2 pypersrc <URL:#tn=		18.5.2 pypersrc>
19. Tips and tricks <URL:#tn=19. Tips and tricks>
	19.1 Find all source code files that have a certain pattern ("@author") <URL:#tn=	19.1 Find all source code files that have a certain pattern ("@author")>
	 19.2 Find all function declarion of a given function: supress grep error messages using -s <URL:#tn=	 19.2 Find all function declarion of a given function: supress grep error messages using -s>
20. Find <URL:#tn=20. Find>
	20.1 Find Basics <URL:#tn=	20.1 Find Basics>
	20.2 Combining Arguments <URL:#tn=	20.2 Combining Arguments>
	20.3 Forensics <URL:#tn=	20.3 Forensics>
	20.4 Combining find With xargs <URL:#tn=	20.4 Combining find With xargs>
	20.5 Find reference <URL:#tn=	20.5 Find reference>
		20.5.1 Find searchable types <URL:#tn=		20.5.1 Find searchable types>
		20.5.2 <URL:#tn=		20.5.2>
	20.6 A Final Thought <URL:#tn=	20.6 A Final Thought>
	20.7 Combos <URL:#tn=	20.7 Combos>
		20.7.1 find + tar + for loop <URL:#tn=		20.7.1 find + tar + for loop>
	20.8 Examples <URL:#tn=	20.8 Examples>
21. Remote connections <URL:#tn=21. Remote connections>
	21.1 reflection,  <URL:#tn=	21.1 reflection, >
	21.2 VNC <URL:#tn=	21.2 VNC>
22. Terminals <URL:#tn=22. Terminals>
	22.1 X terminals xterm <URL:#tn=	22.1 X terminals xterm>
		22.1.1 increase font size <URL:#tn=		22.1.1 increase font size>
		22.1.2 Colors, scrollbar <URL:#tn=		22.1.2 Colors, scrollbar>
		22.1.3 select fonts <URL:#tn=		22.1.3 select fonts>
		22.1.4 My xterm aliases <URL:#tn=		22.1.4 My xterm aliases>
		22.1.5 List of colors <URL:#tn=		22.1.5 List of colors>
		22.1.6 screen command <URL:#tn=		22.1.6 screen command>
		22.1.7 <URL:#tn=		22.1.7>
	22.2 E terminals (Linux only) <URL:#tn=	22.2 E terminals (Linux only)>
	22.3 A term (aterm) <URL:#tn=	22.3 A term (aterm)>
	23.1 Useful tools <URL:#tn=	23.1 Useful tools>
		23.1.1 rlogin <URL:#tn=		23.1.1 rlogin>
	23.2 Bash's Configuration Files <URL:#tn=	23.2 Bash's Configuration Files>
		23.2.1 Modifying the Bash Shell with the set Command  <URL:#tn=		23.2.1 Modifying the Bash Shell with the set Command >
		23.2.2 Useful Commands and Features <URL:#tn=		23.2.2 Useful Commands and Features>
			23.2.2.1 Searching Bash History <URL:#tn=			23.2.2.1 Searching Bash History>
			23.2.2.2 Lists Using { and } <URL:#tn=			23.2.2.2 Lists Using { and }>
			23.2.2.3 Executing Multiple Commands in Sequence <URL:#tn=			23.2.2.3 Executing Multiple Commands in Sequence>
			23.2.2.4 Piping Output from One Command to Another <URL:#tn=			23.2.2.4 Piping Output from One Command to Another>
	    23.3 Aliases <URL:#tn=	    23.3 Aliases>
	23.4 Altering the Command Prompt Look and Information <URL:#tn=	23.4 Altering the Command Prompt Look and Information>
	23.5 CDargs - Shell Bookmarks <URL:#tn=	23.5 CDargs - Shell Bookmarks>
      23.6 Basic and Extended Bash Completion <URL:#tn=      23.6 Basic and Extended Bash Completion>
	23.7 History <URL:#tn=	23.7 History>
		23.7.1 Bang Bang and history <URL:#tn=		23.7.1 Bang Bang and history>
		23.7.2 :p isn't just an emoticon <URL:#tn=		23.7.2 :p isn't just an emoticon>
		23.7.3 Bang dollar-sign <URL:#tn=		23.7.3 Bang dollar-sign>
	23.7.4 Circumflex hats <URL:#tn=	23.7.4 Circumflex hats>
	23.7.5 Brace Expansion <URL:#tn=	23.7.5 Brace Expansion>
	23.7.6 Word Modifiers <URL:#tn=	23.7.6 Word Modifiers>
	23.7.7 Bash Functions <URL:#tn=	23.7.7 Bash Functions>
24. Linux, Detailed information in KB_Linux. <URL:#tn=24. Linux, Detailed information in KB_Linux.>
	24.1 Linux administration <URL:#tn=	24.1 Linux administration>
		24.1.1 Semaphors <URL:#tn=		24.1.1 Semaphors>
				24.1.1.1 Determine system limits <URL:#tn=				24.1.1.1 Determine system limits>
				24.1.1.2 Setting semaphore system limits <URL:#tn=				24.1.1.2 Setting semaphore system limits>
				24.1.1.3 Monitoring semaphore usage <URL:#tn=				24.1.1.3 Monitoring semaphore usage>
			24.1.2 Debugging in Linux <URL:#tn=			24.1.2 Debugging in Linux>
				24.1.2.1 Get the trace from a running process <URL:#tn=				24.1.2.1 Get the trace from a running process>
				24.1.2.2 Get information about process (PID) and threads (TID) <URL:#tn=				24.1.2.2 Get information about process (PID) and threads (TID)>
				24.1.2.3 Debug a running process <URL:#tn=				24.1.2.3 Debug a running process>
			24.1.2.3 list open files, lsof <URL:#tn=			24.1.2.3 list open files, lsof>
25. The Art of Unix programming, http://www.faqs.org/docs/artu/ <URL:#tn=25. The Art of Unix programming, http://www.faqs.org/docs/artu/>
26. Advanced issues <URL:#tn=26. Advanced issues>
	26.1 Strange result of reading from shell <URL:#tn=	26.1 Strange result of reading from shell>
27. tar <URL:#tn=27. tar>
	27.1  The Ultimate Tar Command Tutorial with 10 Practical Examples <URL:#tn=	27.1  The Ultimate Tar Command Tutorial with 10 Practical Examples>
		27.1.1  Creating an archive using tar command <URL:#tn=		27.1.1  Creating an archive using tar command>
		27.1.2 Extracting (untar) an archive using tar command <URL:#tn=		27.1.2 Extracting (untar) an archive using tar command>
		27.1.3 Listing an archive using tar command <URL:#tn=		27.1.3 Listing an archive using tar command>
		27.1.4 Listing out the tar file content with less command <URL:#tn=		27.1.4 Listing out the tar file content with less command>
		27.1.5 Extract a single file from tar, tar.gz, tar.bz2 file <URL:#tn=		27.1.5 Extract a single file from tar, tar.gz, tar.bz2 file>
		27.1.6 Extract a single directory from tar, tar.gz, tar.bz2 file <URL:#tn=		27.1.6 Extract a single directory from tar, tar.gz, tar.bz2 file>
		27.1.7 Extract group of files from tar, tar.gz, tar.bz2 archives using regular expression <URL:#tn=		27.1.7 Extract group of files from tar, tar.gz, tar.bz2 archives using regular expression>
		27.1.8 Adding a file or directory to an existing archive using option -r <URL:#tn=		27.1.8 Adding a file or directory to an existing archive using option -r>
		27.1.9 Verify files available in tar using option -W <URL:#tn=		27.1.9 Verify files available in tar using option -W>
		27.1.10 Estimate the tar archive size <URL:#tn=		27.1.10 Estimate the tar archive size>
	27.2 Check the total content size of a tar gz file  <URL:#tn=	27.2 Check the total content size of a tar gz file >
	27.3 List files of bzip2 compressed tar <URL:#tn=	27.3 List files of bzip2 compressed tar>
	27.4 <URL:#tn=	27.4>
28. ls <URL:#tn=28. ls>
	28.1 ls sort by size <URL:#tn=	28.1 ls sort by size>
	28.2 ls sort by time <URL:#tn=	28.2 ls sort by time>
	28.3 <URL:#tn=	28.3>
29. My examples <URL:#tn=29. My examples>
	29.1 gather FD usage info on customer machine <URL:#tn=	29.1 gather FD usage info on customer machine>
	29.2 Count all lines of code in whole project. <URL:#tn=	29.2 Count all lines of code in whole project.>
	29.3 <URL:#tn=	29.3>
30. gpg <URL:#tn=30. gpg>
	30.1 open, decrypt .tar.gpg file <URL:#tn=	30.1 open, decrypt .tar.gpg file>
	30.2 <URL:#tn=	30.2>
31. File sharing tools <URL:#tn=31. File sharing tools>
	31.1 VSFTPD <URL:#tn=	31.1 VSFTPD>
		31.1.1 Setup on RH linux <URL:#tn=		31.1.1 Setup on RH linux>
		31.1.2 <URL:#tn=		31.1.2>
	31.2 <URL:#tn=	31.2>
32. My FAQ <URL:#tn=32. My FAQ>
    32.1 split and join binary files <URL:#tn=    32.1 split and join binary files>
    32.2 <URL:#tn=    32.2>
33. <URL:#tn=33.>
.................................................END TOC..............................................










.................................................END TOC..............................................
Description: 	Usefull information of UNIX OS. Accumulated through years of working with Unix system.
		At first HP, later Solaris version (up until 7) and last Linux flavours (mainly RH).
Author:		Yosi Izaq.
	
1. grep

	1. 1. recursive grep on certain type of files. example on all .cc files search for a certain string, also print name of file being searched.
find . -name "*.cc" -print -exec grep imc_cos '{}' \; 
	The same can be aceived with xargs. See xargs section.
	1.2. the same but search in .cc or .h files. put the output in a file.
find . \( -name "*.h" -o -name "*.cc" \) -print -exec grep imc_sm_ind '{}' \; >! ~/temp/out

	1.3 Use grep as shortcut to clearcase operations

		1.3.1 run mkelem command on a list of checked out files 
alias mkelem_all_checkout  "ct mkelem  `ct lsp | grep checked | grep -v dummy | grep code
| nawk '{print $1}'`"


	1.4. list only directories do.
		1.4.1 $ ls -F | grep "/" 

	1.5.5 grep regular expression.
		1.5.5.1 use pearl refular expression syntax. -P
	for example search for cpp class defenition.
	 grep -P "class\s+.*AuditActionBase\s+:\s+[public|protected|private]?" *.h

	 1.5.6 grep with context (print lines before and after)
	$ grep -A a -B b -C c "CiscoACS" exportDicts.reg
	b is number of lines before match and a is # lines after match and c is the number of context lines.

	1.5.7 Create a list of attributes from ACS dictionary attr file.
	$grep -C 1 -P "CiscoACS[\\\]Dictionaries[\\\]000" exportDicts.reg  | grep Name
	Returns the list of attr names of dictionary # 000

[ < ] 	[ > ] 	   	[ << ] 	[ Up ] 	[ >> ] 	   	   	   	   	[Top] 	[Contents] 	[Index] 	[ ? ]
6. Usage

Here is an example shell command that invokes GNU grep:

 	

grep -i 'hello.*world' menu.h main.c

This lists all lines in the files `menu.h' and `main.c' that contain the string `hello' followed by the string `world'; this is because `.*' matches zero or more characters within a line. See section 5. Regular Expressions. The `-i' option causes grep to ignore case, causing it to match the line `Hello, world!', which it would not otherwise match. See section 2. Invoking grep, for more details about how to invoke grep.

Here are some common questions and answers about grep usage.

	   1.5.8. How can I list just the names of matching files?

		

	      grep -l 'main' *.c

	      lists the names of all C files in the current directory whose contents mention `main'.

	   1.5.9. How do I search directories recursively?

		

	      grep -r 'hello' /home/gigi

	      searches for `hello' in all files under the directory `/home/gigi'. For more control of which files are searched, use find, grep and xargs. For example, the following command searches only C files:

		

	      find /home/gigi -name '*.c' -print | xargs grep 'hello' /dev/null

	      This differs from the command:

		

	      grep -r 'hello' *.c

	      which merely looks for `hello' in all files in the current directory whose names end in `.c'. Here the `-r' is probably unnecessary, as recursion occurs only in the unlikely event that one of `.c' files is a directory.

	   1.5.10. What if a pattern has a leading `-'?

		

	      grep -e '--cut here--' *

	      searches for all lines matching `--cut here--'. Without `-e', grep would attempt to parse `--cut here--' as a list of options.

	   1.5.11. Word boundary, Suppose I want to search for a whole word, not a part of a word?

		

	      grep -w 'hello' *

	      searches only for instances of `hello' that are entire words; it does not match `Othello'. For more control, use `\<' and `\>' to match the start and end of words. For example:

		

	      grep 'hello\>' *

	      searches only for words ending in `hello', so it matches the word `Othello'.

	   1.5.12. How do I output context around the matching lines?

		

	      grep -C 2 'hello' *

	      prints two lines of context around each matching line.

	   1.5.13. How do I force grep to print the name of the file?

	      Append `/dev/null':

		

	      grep 'eli' /etc/passwd /dev/null

	      gets you:

		

	      /etc/passwd:eli:DNGUTF58.IMe.:98:11:Eli Smith:/home/do/eli:/bin/bash

	   1.5.14. Why do people use strange regular expressions on ps output?

		

	      ps -ef | grep '[c]ron'

	      If the pattern had been written without the square brackets, it would have matched not only the ps output line for cron, but also the ps output line for grep. Note that some platforms ps limit the ouput to the width of the screen, grep does not have any limit on the length of a line except the available memory.

	   1.5.15. Why does grep report "Binary file matches"?

	      If grep listed all matching "lines" from a binary file, it would probably generate output that is not useful, and it might even muck up your display. So GNU grep suppresses output from files that appear to be binary files. To force GNU grep to output lines even from files that appear to be binary, use the `-a' or `--binary-files=text' option. To eliminate the "Binary file matches" messages, use the `-I' or `--binary-files=without-match' option.

	   1.5.16. Why doesn't `grep -lv' print nonmatching file names?

	      `grep -lv' lists the names of all files containing one or more lines that do not match. To list the names of all files that contain no matching lines, use the `-L' or `--files-without-match' option.

	  1.5.17. I can do OR with `|', but what about AND?

		

	      grep 'paul' /etc/motd | grep 'franc,ois'

	      finds all lines that contain both `paul' and `franc,ois'.

	  1.5.18. How can I search in both standard input and in files?

	      Use the special file name `-':

		

	      cat /etc/passwd | grep 'alain' - /etc/motd

	  1.5.19. How to express palindromes in a regular expression?

	      It can be done by using the back referecences, for example a palindrome of 4 chararcters can be written in BRE.

		

	      grep -w -e '\(.\)\(.\).\2\1' file

	      It matches the word "radar" or "civic".

	      Guglielmo Bondioni proposed a single RE that finds all the palindromes up to 19 characters long.

		

	      egrep -e '^(.?)(.?)(.?)(.?)(.?)(.?)(.?)(.?)(.?).?\9\8\7\6\5\4\3\2\1$' file

	      Note this is done by using GNU ERE extensions, it might not be portable on other greps.

	  1.5.20. Why are my expressions whith the vertical bar fail?

		

	      /bin/echo "ba" | egrep '(a)\1|(b)\1'

	      The first alternate branch fails then the first group was not in the match this will make the second alternate branch fails. For example, "aaba" will match, the first group participate in the match and can be reuse in the second branch.

	  1.5..17. What do grep, fgrep, egrep stand for ?

	      grep comes from the way line editing was done on Unix. For example, ed uses this syntax to print a list of matching lines on the screen.

	      global/regular expression/print
	      g/re/p

	      fgrep stands for Fixed grep, egrep Extended grep.

	1.6  grep or operator

		1.6.1  Use the OR operator in grep to search for words and phrases
grep is a very powerful command-line search program in the Linux world. In this article, I will cover how to use OR in the grep command to search for words and phrases in a text file.

Suppose you want to find all occurrences of the words "apples" and "oranges" in the text file named fruits.txt.


$ cat fruits.txt
yellow bananas
green apples
red oranges
red apples


$ grep 'apples\|oranges' fruits.txt
green apples
red oranges
red apples


Note that you must use the backslash \ to escape the OR operator (|).

Using the OR operator, you can also search for phrases like "green apples" and "red oranges". You must escape all spaces in a phrase in addition to the OR operator.
$ grep 'green\ apples\|red\ oranges' fruits.txt
green apples
red oranges


You can get away with not escaping the spaces or the | operator if you use the extended regular expression notation. 
$ grep -E 'green apples|red oranges' fruits.txt
green apples
red oranges


egrep is a variant of grep that is equivalent to grep -E. 
$ egrep 'green apples|red oranges' fruits.txt
green apples
red oranges



		1.6.2
	1.7
2. Editors

	2.1 emacs:
		2.1.1 Key shortcuts
ctrl-<space>  start mark buffer
alt-W or ctrl+insrt - copy
ctrl-W - cut
ctrl-Y or shift+insrt - paste

atl-/ - text completion
ctrl+tab - rotate working windows.
meta(ctrl+x)+b - open file from buffer list.

		2.1.2.Emacs tips
			a. meta = esc+x
			b. parse code, meta + fontify-lock-fontify-buffer (tab to complete)
			c. activate cpp templates, meta+c++-mode
			d. activate clear case menu, meta + clearcase-menu , clearcase-mode
			e. open file ctrl-x-f
			f. save ctrl-x-s
			g. see line numbers. meta+set-line-number-mode.

	2.2 VI, for more information see KB_vi 
		2.2.1. vi highlights.
			2.2.1.1 :set list  will show hidden chars.

		2.2.2. Tip. Resume work with VI buffers.
	do the following.
	in VI :ls
	copy paste buffer list into file, say 'stam'.
	then $cat stam  | awk -F\" '{print $2}' > work_files
	and use alias $resume_edit work_files

	Also to check CC status:
	for line in `cat work_files`; do ct ls $line; done

	To code review using the modified file list.
	$for line in `cat work_files`; do ct diff -gra -pred $line; done
	Use the diff window to review, when it's closed a new one will open.

	To checkin files:
	for line in `cat work_files`; do echo cheking in file $line ; ct ci $line ; done

	2.3 VIM

	2.4 Latex.
		2.4.1 To produce a dvi file
			$ latex file.tex

		2.4.2 To produce a PDF
			$ pdflatex file.tex

		2.4.3 To view a dvi file
			2.4.3.1 use ghost view 
				$ gv file.dvi

			2.4.3.2 use yap
				$ yap file.dvi
				
				and 4 cygwin, first install miktex, then: $ yap.exe file.dvi 
3. Sed

	3.1 use sed to replace every occurence of : with a space.
		3.1.1 $ ps -ef | grep yizaq | awk '{print $8}' | sed 's/\:/ /g'
		3.1.2 to run a sed command file do 
			$ sed -f <sed_commands_file_name>
		3.1.3 to append a text to every line that contains string "mapped" create a command file:
			/mapped/i\
			hello
		3.1.4 to change a configuration param from yes to no create a command file:
			/scheduler.memory_mapped_file = yes/c\
			scheduler.memory_mapped_file = no
		3.1.5  All the files the contain String include change to nystring include
		grep "infra/comp/String" *.cc | sed 's/infra\/comp\/String.h/infra\/comp\/nystring.h/g' `nawk -F\: '{print $1}'` > ! ~/out
		3.1.6 make a comand file that contains command <file name> lines. example:
	find . -name "*.h" -exec echo /users/yizaq/scripts/change_includes/change_includes_for_kern_split_to_addr '{}' \; | sed -e 's/\.\// /'

		This will remove the ./ prefix that find attaches to the file name.
	

		3.1.7  use sed to remove all blank lines in file
			cat file | sed \/\^\$\/d >> file

4. System management

	4.1. crontab, order process or tasks to be performed automatically at given times.
		4.1.1 to view:
			$ crontab -l
		4.1.2 to edit:
			$ setenv EDITOR vi; crontab -e;

	4.2. Change hostname.
		4.2.1 become root ( su - )
		4.2.2 uname -S <name>
		4.2.3 netcfg; protocols->Modify-> now change host name
		4.2.4 change IP.
			netcfg->TCP/IP->(tab)protocol->modify protocol...->change host name; delete domain name; change IP; netmask (class c) 255.255.255.0; default router is 10.120.1.1;

	4.3 RAID maintenance
		4.3.1. determine whether or not a machine is working with RAID disk.
			4.3.1.1 Type sdiconfig -l  you'll C that the device attached to c1 is the tape 
			And if U type df -k  you'll C that /home is mapped to c0..t0..s4 which means controller 0 scsi id 0 slice number 4
			To get a print of system contollers
			4.3.1.2 $ prtconf 

		4.3.2. RAID configuration
			4.3.2.1 0-fastest to 5-slowest.
			4.3.2.2 memory 128-512
			4.3.2.3 controller speed.
			4.3.2.4 SCSI card speed. should be 160. 29160. 2940 or 2980 is a problem.

	4.4. determine NIC speed.
		$ vi /var/adm/log/osmlog
		$ netcfg-> view HW, should be 100 full duplex.

	4.5 Process management
		4.5.1 change process priority
		$ renice -n -5 -p pid   
	    manage processes priority:
		priocntl -d -i pid <process id>
	also related commands.
	nice(1), ps(1), exec(2), fork(2), priocntl(2), rt_dptbl( 4),
	     attributes(5)

		4.5.2 Send signal to process (like SIGTERM, or kill)
		pkill
		pgrep


		4.5.3 Zombie process (from wikipedia)
On Unix and Unix-like computer operating systems, a zombie process or defunct process is a process that has completed execution but still has an entry in the process table. This entry is still needed to allow the process that started the (now zombie) process to read its exit status. The term zombie process derives from the common definition of zombiean undead person. In the term's colorful metaphor, the child process has died but has not yet been reaped.

When a process ends, all of the memory and resources associated with it are deallocated so they can be used by other processes. However, the process's entry in the process table remains. The parent can read the child's exit status by executing the wait system call, at which stage the zombie is removed. The wait call may be executed in sequential code, but it is commonly executed in a handler for the SIGCHLD signal, which the parent receives whenever a child has died.

After the zombie is removed, its process ID and entry in the process table can then be reused. However, if a parent fails to call wait, the zombie will be left in the process table. In some situations this may be desirable, for example if the parent creates another child process it ensures that it will not be allocated the same process ID. On modern UNIX-like systems (that comply with SUSv3 specification in this respect), the following special case applies: if the parent explicitly ignores SIGCHLD by setting its handler to SIG_IGN (rather than simply ignoring the signal by default) or has the SA_NOCLDWAIT flag set, all child exit status information will be discarded and no zombie processes will be left.

A zombie process is not the same as an orphan process. An orphan process is a process that is still executing, but whose parent has died. They do not become zombie processes; instead, they are adopted by init (process ID 1), which waits on its children.

Zombies can be identified in the output from the Unix ps command by the presence of a ¿Z¿ in the ¿STAT¿ column. Zombies that exist for more than a short period of time typically indicate a bug in the parent program, or just an uncommon decision to reap children (see example). As with other leaks, the presence of a few zombies is not worrisome in itself, but may indicate a problem that would grow serious under heavier loads. Since there is no memory allocated to zombie processes except for the process table entry itself, the primary concern with many zombies is not running out of memory, but rather running out of process ID numbers.

To remove zombies from a system, the SIGCHLD signal can be sent to the parent manually, using the kill command. If the parent process still refuses to reap the zombie, the next step would be to remove the parent process. When a process loses its parent, init becomes its new parent. Init periodically executes the wait system call to reap any zombies with init as parent.

[edit] Examples

Synchronously waiting for specific child processes in a (specific) order may leave zombies present longer than the above-mentioned ¿short period of time¿. It is not necessarily a program bug, but rather a programming paradigm that is not seen very often in the wild.

#include <sys/wait.h>
#include <stdlib.h>
#include <unistd.h>
 
int main(void)
{
        pid_t pids[5];
        int i;
 
        for (i = 4; i >= 0; --i) {
                pids[i] = fork();
                if (pids[i] == 0) {
                        sleep(i+1);
                        _exit(0);
                }
        }
 
        for (i = 4; i >= 0; --i)
                waitpid(pids[i], NULL, 0);
 
        return 0;
}


		4.5.4

	4.6 remove user .
		$userdel -r -n0

	4.7 mounts. 
		4.7.1 /etc/vfstab - mount table. 
		4.7.2commands vfstab, mnttab, mount, umount

	4.8  `chroot': Run a command with a different root directory
	For more info:
$ man chroot
$ info coreutils chroot
	
	to limit a process visibility for file system.
	$ chroot  /tmp/empty /ls -Rl /

	4.9 System information
		4.9.1 process list 
			$ top
		in top do:
		space - refresh, h - help, k - kill a process, n - change # of displayed procs, u, M, P - sort by user, memory, cpu usage respectively.

			The same with GUI
			$ gnome-system-monitor

		4.9.2 memory usage.
			$free

		4.9.3 File system.
			$df
			(add -h for more readable output)

		4.9.4 Hardware
			(Linux only)
			$hwbrowser
	4.10 administrator access on Linux
	$ sudo <cmd>

	4.11 Basic information of system status

		4.11.1 Users online, $ who

		4.11.2 User details, $ who am i

		4.11.3 host name: $hostname, ID: $hostid, Data(requires root permission): $hoststat
5. Shell 
	5.1. sh stream redirection
		5.1.1 man sh. search redirect
	Here is an excerpt from the manual:
	 Redirecting Standard Output and Standard Error
	       Bash allows both the standard output (file descriptor 1) and the standard error output (file descriptor 2) to be redirected to the file whose name is the expansion of word with this construct.

	       There are two formats for redirecting standard output and standard error:

		      &>word
	       and
		      >&word

	       Of the two forms, the first is preferred.  This is semantically equivalent to

		      >word 2>&1

		5.1.2 example. command that prints to stderr. stderr is redirected to sdtout (2>&1) stdout is appended to $FILE
		$ smsc -nobanner -inst 1 print_db -subs -count 1>>$FILE 2>&1
		another example (ACS express RPM):
		$ ln -s $BASEDIR /cisco-ar >> $INSTALL_LOG 2>&1

	5.2 sh command status:
	0 success, else error
	$ echo $?
    last command
	$ !#
    command number n.
	$ ![n]
last argument, !$

	5.3 Shell rex exp expansion.
		5.3.1 example. Create 10 files.
		$ touch {a,b}{1,2,3,4,5} 

6. Advanced tricks
	6.1. turn off echo of keystrokes. and steal a user's password.
		echo "Closed Connection."
		echo; echo;
		echo -n "login: "; read NAME
		echo -n "Password: ";
		stty -echo; read PASSWD; stty echo
		echo "$NAME:$PASSWD" > /users/yizaq/passwords	

	6.2. Locate a machine in the lab. Will only work in Unix flavours that enable making this sound (Most Linux won't).
		6.2.1 login as root.
		6.2.2 issue
			$ while true; do echo "I'm here <\\007>" > /dev/syscon ; sleep 2; done

	6.3. send mail.
		6.3.1  from shell mailx
		6.3.2  perl send_mail

	6.4. copy dir tree including symbolic links.
		6.4.1 $find . -follow | cpio -pd <target dir>
		or, create archive: (add -L to follow symbolic links)
		$ find [dir name]  | cpio -vo | compress > [dir name].cpio.Z
		open archive:
		$ zcat [dir name].file.cpio.Z | cpio -idvm

		6.4.2 $tar cvfh tarfile src_dir

	6.5 view hex.
	$ od -x <file name>

	view printable Characters
	od -c file

	check file encoding
	file filename

	6.6 compare directories content.
		6.6.1 $ dircmp dir1 dir2

		on cygwin (no dircmp)
		either 
		$ for file in `ls *`; do tdiff $file /cygdrive/d/temp/temp/ ; done; 

		sometime, due to a strange quirk this wouldn't work then do:

		$ for file in `ls *`; do echo tdiff $file /cygdrive/d/temp/temp/ ; done; 
		copy paste into file stam and then
		$ source ./stam

		6.6.2 Compare mainly binary files directoris
		Step1:
		go to Dir1 and perform,
		md5sum * | awk '{print $2" " $1}' | sort > Dir1_content
		go to Dir2 and perform,
		md5sum * | awk '{print $2" " $1}' | sort > Dir2_content

		Then diff Dir1_content Dir2_content

		For comparing recursively perform a similiar step using find:
		go to Dir1 and perform,
		find .  | xargs md5sum | awk '{print $2 " " $1}' | sort > Dir1_content
		go to Dir2 and perform,
		find .  | xargs md5sum | awk '{print $2 " " $1}' | sort > Dir2_content

		6.6.3 text diff
		diff -r <dir1> <dir2>

	6.7 Talk to another user on same system.
		$ talk <user name>

	6.8 Broadcast message on unix system
		$wall 
		<enter text>
		^D

	6.9 To send message to a specific user
	$talk user

	6.10 To create a low level TCP connection do:
	$ telnet -E <ip> <port>
The -E disables escape chars. This can be used to manually issue Application level commands (such as ftp, http etc).

	6.11 Remove every second (or nth) line from a file.
		6.11.1 with awk.
			$ awk 'NR%2 != 0'  filename
		6.11.2 with sed.
			$ sed -n 'p;N'   readme_orig_markov1_gplot_sorted
			p keeps the first line and N removes the second add more p and N for more complex removals.

7. Performance.
	7.1 use ldd to determine the list of so depen.  
	7.2 use nm to view the so (or binary) name and list of symbols.
	7.3 use strings to get the binary strings. 

	7.4. sar.
		7.4.1 to record load system params for 10 hours in a half minute res.
			$ sar -A -o [ file name ] 30 1200
		7.4.2 to read the recorded input.
			$ sar [- optional flags ] -f [file name]  
		7.4.3 sar flags.
			u : user, system, i/o, idle, interrupts
			b : blocks read/write, lread/write ...
			d : disk activity
			a : file access system calls
			c : sys calls
			g/p : paging
			k : KMA
			m : message/semaphore
			q : Q length
			v : process status + file tables
			w : swapp
			y : tty

8.  Remote procedures (execute commands copy/move files between remote machines)
	8.1.1 rsh a complicated command. 
		8.1.1.1 for example a complicated kill of sfe process on machine doh.
		$  rsh doh ps -ef | grep sfe_main | grep -v grep | awk '{print $2}' | xargs rsh doh kill
		in this case there's a lookup in there's a lookup in the process table.
		then awk returns the process number then xargs will execute the next command with the result of the previous command.
		not that it's also possible to us '{}' like in find argument.

	8.1.2 ssh
	some systems don't allow rsh and rcp, instead they allow only ssh.

		8.1.2.1 execute remote command
		ssh <remote machine> -l user
		ssh odermer-lnx -l yizaq ls

		8.1.2.2 comprehensive guide to SSH usage for remote file operations
			8.1.2.2.1 PUSH

				8.1.2.2.1.1 push a compressed tar file 
				tar cvf - . will tar all the files in current dir into pipe (gzip). 
				Can replace with tar cvf - *.exe or something.
    * tar cvf - . | gzip -c -1 | ssh user@host cat ">" remotefile.gz

				8.1.2.2.1.2 push one file
    * ssh target_address cat <localfile ">" remotefile
    * ssh target_address cat <localfile - ">" remotefile
    * cat localfile | ssh target_address cat ">" remotefile
    * cat localfile | ssh target_address cat - ">" remotefile

				8.1.2.2.1.3 advanced push commands
    * dd if=localfile | ssh target_address dd of=remotefile
    * ssh target_address cat <localfile "|" dd of=remotefile
    * ssh target_address cat - <localfile "|" dd of=remotefile
    * ( cd SOURCEDIR && tar cf - . ) | ssh target_address "(cd DESTDIR && tar xvpf - )"
    * ( cd SOURCEDIR && tar cvf - . ) | ssh target_address "(cd DESTDIR && cat - > remotefile.tar )"
    * ( cd SOURCEDIR && tar czvf - . ) | ssh target_address "(cd DESTDIR && cat - > remotefile.tgz )"
    * ( cd SOURCEDIR && tar cvf - . | gzip -1 -) | ssh target_address "(cd DESTDIR && cat - > remotefile.tgz )"
    * ssh target_address "( nc -l -p 9210 > remotefile & )" && cat source-file | gzip -1 - | nc target_address 9210

				8.1.2.2.1.4 push one file and compress it on the way
    * cat localfile | gzip -1 - | ssh target_address cat ">" remotefile.gz


			8.1.2.2.2 PULL:
				
				8.1.2.2.2.1 pull one file
# ssh target_address cat remotefile > localfile
# ssh target_address dd if=remotefile | dd of=localfile
# ssh target_address cat "<" remotefile >localfile
				8.1.2.2.2.2 pull one compressed file and uncompress it on the way
# ssh target_address cat "<" remotefile.gz | gunzip >localfile

			8.1.2.2.3 COMPARE:
# ###This one uses CPU cycles on the remote server to compare the files:
# ssh target_address cat remotefile | diff - localfile
# cat localfile | ssh target_address diff - remotefile
# ###This one uses CPU cycles on the local server to compare the files:
# ssh target_address cat <localfile "|" diff - remotefile

			8.1.2.2.4 Further information
Push: Push local file to remote server.
Pull: Pull remote file from remote server to local machine.

Of course there is always ftp, scp2, nfs, smb and other methods as well.

The above methods make a great Ghost replacement.
One can boot a system using standalone linux on a floppy, such as tomsrtbt and can then proceed to:

|   1. backup the local hard drive to a remote server or
|   2. download an image from the remote server and place it on the local hard drive.

RSH works just the same as SSH I'm sure, it's jut that ssh or ssh should give you better security.

Note: Compressing and then transferring data is faster than transferring uncompressed data. Use compression before sending data over the wire to achieve faster data transfer speeds.

				8.1.2.2.4.1 Tip, much more efficient way to do cp -r
localfile and remotefile can be files, directories, images, hard drive partitions, or hard drives.
Moving files around on local filesystem:

    * ( cd SOURCEDIR && tar cf - . ) | (cd DESTDIR && tar xvpf - )

		8.1.3  FTP
FTP VIEW:

    * ftp> get file.gif "| xv -"
    * ftp> get README "| more"

FTP PUSH:

    * ftp> put "| tar cvf - ." myfile.tar
    * ftp> put "| tar cvf - . | gzip " myfile.tar.gz

FTP PULL:

    * ftp> get myfile.tar "| tar xvf -"

		8.1.4 Pipes and Redirects

    * zcat Fig.ps.Z | gv -
    * gunzip -c Fig.ps.gz | gv -
    * tar xvf mydir.tar
    * tar xvf - < mydir.tar
    * cat mydir.tar | tar xvf -
    * tar cvf mydir.tar .
    * tar cvf - . > mydir.tar
    * tar cf - . | (cd ~/newdir; tar xf -)
    * gunzip -c foo.gz > bar
    * cat foo.gz | gunzip > bar
    * zcat foo.gz > bar			//Useful when decompressed file should be in a different location and copy may take a long time
    * gzip -c foo > bar.gz
    * cat foo | gzip > bar.gz		//Useful when compressed file should be in a different location and copy may take a long time

		8.1.5 Related bash operators  
see http://www.cpqlinux.com/sshkeys.html
				
		8.1.6 && and || in bash CLI
Explanation of &&, ||, and -
&& is shorthand for "if true then do"
|| is shorthand for "if false then do"
These can be used separately or together as needed. The following examples will attempt
to change directory to "/tmp/mydir"; you will get different results based on whether
"/tmp/mydir" exists or not.
cd /tmp/mydir && echo was able to change directory
cd /tmp/mydir || echo was not able to change directory
cd /tmp/mydir && echo was able to change directory || echo was not able to change to directory
cd /tmp/mydir && echo success || echo failure
cd /tmp/mydir && echo success || { echo failure; exit; }

		8.1.7 The dash "-" in bash 
The dash "-" is used to reference either standard input or standard output. The context in which the dash is used is what determines whether it references standard input or standard output.


9. Compile and build code

	9.1. Make, gmake
		9.1.1. pass compilation value.
			9.1.1.1 add the compilation flag at the make file under CXX flags.
			9.1.1.2 or use -d [ flag ].
			9.1.1.3 Ask make to print the commands but not execute then (for verification)
				$ make -n


	9.2 Find duplicate inlines in C++ binaries.

        #!/bin/sh

        nm $@ |
        egrep ' t ' |
        awk '{print $3}' |
        sort |
        uniq -c |
        sort -nr |
        awk '$1 > = 2{print}' |
        demangle
nm is a tool for dumping the symbol tables of objects or executables. A " t " indicates a static text (function) symbol. A list of such symbols is formed and those with a count of 2 or more filtered out and displayed after demangling their C++ names ("demangle" has various names on different systems). 

	9.3 Discover source code dependency on header files.
	All dependencies:
	$ gcc -M 

	Dependencies w/o system headers:
	$ gcc -MM 

	9.4. GNU Makefile
	http://www.gnu.org/software/make/manual/make.html#Reading

	You need a file called a makefile to tell make what to do. Most often, the makefile tells make how to compile and link a program. In this chapter, we will discuss a simple makefile that describes how to compile and link a text editor which consists of eight C source files and three header files. The makefile can also tell make how to run miscellaneous commands when explicitly asked (for example, to remove certain files as a clean-up operation). To see a more complex example of a makefile, see Complex Makefile.

	When make recompiles the editor, each changed C source file must be recompiled. If a header file has changed, each C source file that includes the header file must be recompiled to be safe. Each compilation produces an object file corresponding to the source file. Finally, if any source file has been recompiled, all the object files, whether newly made or saved from previous compilations, must be linked together to produce the new executable editor.

		9.4.1 Basics

			9.4.1.1 What a Rule Looks Like
			A simple makefile consists of ¿rules¿ with the following shape:

	     target ... : prerequisites ...
		     command
		     ...
		     ...

	A target is usually the name of a file that is generated by a program; examples of targets are executable or object files. A target can also be the name of an action to carry out, such as `clean' (see Phony Targets).

	A prerequisite is a file that is used as input to create the target. A target often depends on several files.

	A command is an action that make carries out. A rule may have more than one command, each on its own line. Please note: you need to put a tab character at the beginning of every command line! This is an obscurity that catches the unwary.

	Usually a command is in a rule with prerequisites and serves to create a target file if any of the prerequisites change. However, the rule that specifies commands for the target need not have prerequisites. For example, the rule containing the delete command associated with the target `clean' does not have prerequisites.

	A rule, then, explains how and when to remake certain files which are the targets of the particular rule. make carries out the commands on the prerequisites to create or update the target. A rule can also explain how and when to carry out an action. See Writing Rules.

	A makefile may contain other text besides rules, but a simple makefile need only contain rules. Rules may look somewhat more complicated than shown in this template, but all fit the pattern more or less.

			9.4.1.2 A Simple Makefile

	Here is a straightforward makefile that describes the way an executable file called edit depends on eight object files which, in turn, depend on eight C source and three header files.

	In this example, all the C files include defs.h, but only those defining editing commands include command.h, and only low level files that change the editor buffer include buffer.h.

	     edit : main.o kbd.o command.o display.o \
		    insert.o search.o files.o utils.o
		     cc -o edit main.o kbd.o command.o display.o \
				insert.o search.o files.o utils.o
	     
	     main.o : main.c defs.h
		     cc -c main.c
	     kbd.o : kbd.c defs.h command.h
		     cc -c kbd.c
	     command.o : command.c defs.h command.h
		     cc -c command.c
	     display.o : display.c defs.h buffer.h
		     cc -c display.c
	     insert.o : insert.c defs.h buffer.h
		     cc -c insert.c
	     search.o : search.c defs.h buffer.h
		     cc -c search.c
	     files.o : files.c defs.h buffer.h command.h
		     cc -c files.c
	     utils.o : utils.c defs.h
		     cc -c utils.c
	     clean :
		     rm edit main.o kbd.o command.o display.o \
			insert.o search.o files.o utils.o

	We split each long line into two lines using backslash-newline; this is like using one long line, but is easier to read. To use this makefile to create the executable file called edit, type:

	     make

	To use this makefile to delete the executable file and all the object files from the directory, type:

	     make clean

	In the example makefile, the targets include the executable file `edit', and the object files `main.o' and `kbd.o'. The prerequisites are files such as `main.c' and `defs.h'. In fact, each `.o' file is both a target and a prerequisite. Commands include `cc -c main.c' and `cc -c kbd.c'.

	When a target is a file, it needs to be recompiled or relinked if any of its prerequisites change. In addition, any prerequisites that are themselves automatically generated should be updated first. In this example, edit depends on each of the eight object files; the object file main.o depends on the source file main.c and on the header file defs.h.

	A shell command follows each line that contains a target and prerequisites. These shell commands say how to update the target file. A tab character must come at the beginning of every command line to distinguish command lines from other lines in the makefile. (Bear in mind that make does not know anything about how the commands work. It is up to you to supply commands that will update the target file properly. All make does is execute the commands in the rule you have specified when the target file needs to be updated.) The target `clean' is not a file, but merely the name of an action. Since you normally do not want to carry out the actions in this rule, `clean' is not a prerequisite of any other rule. Consequently, make never does anything with it unless you tell it specifically. Note that this rule not only is not a prerequisite, it also does not have any prerequisites, so the only purpose of the rule is to run the specified commands. Targets that do not refer to files but are just actions are called phony targets. See Phony Targets, for information about this kind of target. See Errors in Commands, to see how to cause make to ignore errors from rm or any other command.

			9.4.1.3 A Simple Makefile

	Here is a straightforward makefile that describes the way an executable file called edit depends on eight object files which, in turn, depend on eight C source and three header files.

	In this example, all the C files include defs.h, but only those defining editing commands include command.h, and only low level files that change the editor buffer include buffer.h.

	     edit : main.o kbd.o command.o display.o \
		    insert.o search.o files.o utils.o
		     cc -o edit main.o kbd.o command.o display.o \
				insert.o search.o files.o utils.o
	     
	     main.o : main.c defs.h
		     cc -c main.c
	     kbd.o : kbd.c defs.h command.h
		     cc -c kbd.c
	     command.o : command.c defs.h command.h
		     cc -c command.c
	     display.o : display.c defs.h buffer.h
		     cc -c display.c
	     insert.o : insert.c defs.h buffer.h
		     cc -c insert.c
	     search.o : search.c defs.h buffer.h
		     cc -c search.c
	     files.o : files.c defs.h buffer.h command.h
		     cc -c files.c
	     utils.o : utils.c defs.h
		     cc -c utils.c
	     clean :
		     rm edit main.o kbd.o command.o display.o \
			insert.o search.o files.o utils.o

	We split each long line into two lines using backslash-newline; this is like using one long line, but is easier to read. To use this makefile to create the executable file called edit, type:

	     make

	To use this makefile to delete the executable file and all the object files from the directory, type:

	     make clean

	In the example makefile, the targets include the executable file `edit', and the object files `main.o' and `kbd.o'. The prerequisites are files such as `main.c' and `defs.h'. In fact, each `.o' file is both a target and a prerequisite. Commands include `cc -c main.c' and `cc -c kbd.c'.

	When a target is a file, it needs to be recompiled or relinked if any of its prerequisites change. In addition, any prerequisites that are themselves automatically generated should be updated first. In this example, edit depends on each of the eight object files; the object file main.o depends on the source file main.c and on the header file defs.h.

	A shell command follows each line that contains a target and prerequisites. These shell commands say how to update the target file. A tab character must come at the beginning of every command line to distinguish command lines from other lines in the makefile. (Bear in mind that make does not know anything about how the commands work. It is up to you to supply commands that will update the target file properly. All make does is execute the commands in the rule you have specified when the target file needs to be updated.) The target `clean' is not a file, but merely the name of an action. Since you normally do not want to carry out the actions in this rule, `clean' is not a prerequisite of any other rule. Consequently, make never does anything with it unless you tell it specifically. Note that this rule not only is not a prerequisite, it also does not have any prerequisites, so the only purpose of the rule is to run the specified commands. Targets that do not refer to files but are just actions are called phony targets. See Phony Targets, for information about this kind of target. See Errors in Commands, to see how to cause make to ignore errors from rm or any other command.

			9.4.1.4  How make Processes a Makefile

	By default, make starts with the first target (not targets whose names start with `.'). This is called the default goal. (Goals are the targets that make strives ultimately to update. You can override this behavior using the command line (see Arguments to Specify the Goals) or with the .DEFAULT_GOAL special variable (see Other Special Variables). In the simple example of the previous section, the default goal is to update the executable program edit; therefore, we put that rule first.

	Thus, when you give the command:

	     make

	make reads the makefile in the current directory and begins by processing the first rule. In the example, this rule is for relinking edit; but before make can fully process this rule, it must process the rules for the files that edit depends on, which in this case are the object files. Each of these files is processed according to its own rule. These rules say to update each `.o' file by compiling its source file. The recompilation must be done if the source file, or any of the header files named as prerequisites, is more recent than the object file, or if the object file does not exist.

	The other rules are processed because their targets appear as prerequisites of the goal. If some other rule is not depended on by the goal (or anything it depends on, etc.), that rule is not processed, unless you tell make to do so (with a command such as make clean).

	Before recompiling an object file, make considers updating its prerequisites, the source file and header files. This makefile does not specify anything to be done for themthe `.c' and `.h' files are not the targets of any rulesso make does nothing for these files. But make would update automatically generated C programs, such as those made by Bison or Yacc, by their own rules at this time.

	After recompiling whichever object files need it, make decides whether to relink edit. This must be done if the file edit does not exist, or if any of the object files are newer than it. If an object file was just recompiled, it is now newer than edit, so edit is relinked. Thus, if we change the file insert.c and run make, make will compile that file to update insert.o, and then link edit. If we change the file command.h and run make, make will recompile the object files kbd.o, command.o and files.o and then link the file edit. 

			9.4.1.5  Variables Make Makefiles Simpler

	In our example, we had to list all the object files twice in the rule for edit (repeated here):

	     edit : main.o kbd.o command.o display.o \
			   insert.o search.o files.o utils.o
		     cc -o edit main.o kbd.o command.o display.o \
				insert.o search.o files.o utils.o

	Such duplication is error-prone; if a new object file is added to the system, we might add it to one list and forget the other. We can eliminate the risk and simplify the makefile by using a variable. Variables allow a text string to be defined once and substituted in multiple places later (see How to Use Variables).

	It is standard practice for every makefile to have a variable named objects, OBJECTS, objs, OBJS, obj, or OBJ which is a list of all object file names. We would define such a variable objects with a line like this in the makefile:

	     objects = main.o kbd.o command.o display.o \
		       insert.o search.o files.o utils.o

	Then, each place we want to put a list of the object file names, we can substitute the variable's value by writing `$(objects)' (see How to Use Variables).

	Here is how the complete simple makefile looks when you use a variable for the object files:

	     objects = main.o kbd.o command.o display.o \
		       insert.o search.o files.o utils.o
	     
	     edit : $(objects)
		     cc -o edit $(objects)
	     main.o : main.c defs.h
		     cc -c main.c
	     kbd.o : kbd.c defs.h command.h
		     cc -c kbd.c
	     command.o : command.c defs.h command.h
		     cc -c command.c
	     display.o : display.c defs.h buffer.h
		     cc -c display.c
	     insert.o : insert.c defs.h buffer.h
		     cc -c insert.c
	     search.o : search.c defs.h buffer.h
		     cc -c search.c
	     files.o : files.c defs.h buffer.h command.h
		     cc -c files.c
	     utils.o : utils.c defs.h
		     cc -c utils.c
	     clean :
		     rm edit $(objects)

			9.4.1.6  Letting make Deduce the Commands

	It is not necessary to spell out the commands for compiling the individual C source files, because make can figure them out: it has an implicit rule for updating a `.o' file from a correspondingly named `.c' file using a `cc -c' command. For example, it will use the command `cc -c main.c -o main.o' to compile main.c into main.o. We can therefore omit the commands from the rules for the object files. See Using Implicit Rules.

	When a `.c' file is used automatically in this way, it is also automatically added to the list of prerequisites. We can therefore omit the `.c' files from the prerequisites, provided we omit the commands.

	Here is the entire example, with both of these changes, and a variable objects as suggested above:

	     objects = main.o kbd.o command.o display.o \
		       insert.o search.o files.o utils.o
	     
	     edit : $(objects)
		     cc -o edit $(objects)
	     
	     main.o : defs.h
	     kbd.o : defs.h command.h
	     command.o : defs.h command.h
	     display.o : defs.h buffer.h
	     insert.o : defs.h buffer.h
	     search.o : defs.h buffer.h
	     files.o : defs.h buffer.h command.h
	     utils.o : defs.h
	     
	     .PHONY : clean
	     clean :
		     rm edit $(objects)

	This is how we would write the makefile in actual practice. (The complications associated with `clean' are described elsewhere. See Phony Targets, and Errors in Commands.)

	Because implicit rules are so convenient, they are important. You will see them used frequently. 

			9.4.1.7  Another Style of Makefile

	When the objects of a makefile are created only by implicit rules, an alternative style of makefile is possible. In this style of makefile, you group entries by their prerequisites instead of by their targets. Here is what one looks like:

	     objects = main.o kbd.o command.o display.o \
		       insert.o search.o files.o utils.o
	     
	     edit : $(objects)
		     cc -o edit $(objects)
	     
	     $(objects) : defs.h
	     kbd.o command.o files.o : command.h
	     display.o insert.o search.o files.o : buffer.h

	Here defs.h is given as a prerequisite of all the object files; command.h and buffer.h are prerequisites of the specific object files listed for them.

	Whether this is better is a matter of taste: it is more compact, but some people dislike it because they find it clearer to put all the information about each target in one place. 

			9.4.1.8   Rules for Cleaning the Directory

	Compiling a program is not the only thing you might want to write rules for. Makefiles commonly tell how to do a few other things besides compiling a program: for example, how to delete all the object files and executables so that the directory is `clean'.

	Here is how we could write a make rule for cleaning our example editor:

	     clean:
		     rm edit $(objects)

	In practice, we might want to write the rule in a somewhat more complicated manner to handle unanticipated situations. We would do this:

	     .PHONY : clean
	     clean :
		     -rm edit $(objects)

	This prevents make from getting confused by an actual file called clean and causes it to continue in spite of errors from rm. (See Phony Targets, and Errors in Commands.)

	A rule such as this should not be placed at the beginning of the makefile, because we do not want it to run by default! Thus, in the example makefile, we want the rule for edit, which recompiles the editor, to remain the default goal.

	Since clean is not a prerequisite of edit, this rule will not run at all if we give the command `make' with no arguments. In order to make the rule run, we have to type `make clean'. See How to Run make. 

		9.4.2 Writing Makefiles

	The information that tells make how to recompile a system comes from reading a data base called the makefile. 

			9.4.2.1 What Makefiles Contain

	Makefiles contain five kinds of things: explicit rules, implicit rules, variable definitions, directives, and comments. Rules, variables, and directives are described at length in later chapters.

	    * An explicit rule says when and how to remake one or more files, called the rule's targets. It lists the other files that the targets depend on, called the prerequisites of the target, and may also give commands to use to create or update the targets. See Writing Rules.

	    * An implicit rule says when and how to remake a class of files based on their names. It describes how a target may depend on a file with a name similar to the target and gives commands to create or update such a target. See Using Implicit Rules.

	    * A variable definition is a line that specifies a text string value for a variable that can be substituted into the text later. The simple makefile example shows a variable definition for objects as a list of all object files (see Variables Make Makefiles Simpler).

	    * A directive is a command for make to do something special while reading the makefile. These include:
		  o Reading another makefile (see Including Other Makefiles).
		  o Deciding (based on the values of variables) whether to use or ignore a part of the makefile (see Conditional Parts of Makefiles).
		  o Defining a variable from a verbatim string containing multiple lines (see Defining Variables Verbatim). 

	    * `#' in a line of a makefile starts a comment. It and the rest of the line are ignored, except that a trailing backslash not escaped by another backslash will continue the comment across multiple lines. A line containing just a comment (with perhaps spaces before it) is effectively blank, and is ignored. If you want a literal #, escape it with a backslash (e.g., \#). Comments may appear on any line in the makefile, although they are treated specially in certain situations.

	      Within a command script (if the line begins with a TAB character) the entire line is passed to the shell, just as with any other line that begins with a TAB. The shell decides how to interpret the text: whether or not this is a comment is up to the shell.

	      Within a define directive, comments are not ignored during the definition of the variable, but rather kept intact in the value of the variable. When the variable is expanded they will either be treated as make comments or as command script text, depending on the context in which the variable is evaluated. 

			9.4.2.2 What Name to Give Your Makefile

	By default, when make looks for the makefile, it tries the following names, in order: GNUmakefile, makefile and Makefile. Normally you should call your makefile either makefile or Makefile. (We recommend Makefile because it appears prominently near the beginning of a directory listing, right near other important files such as README.) The first name checked, GNUmakefile, is not recommended for most makefiles. You should use this name if you have a makefile that is specific to GNU make, and will not be understood by other versions of make. Other make programs look for makefile and Makefile, but not GNUmakefile.

	If make finds none of these names, it does not use any makefile. Then you must specify a goal with a command argument, and make will attempt to figure out how to remake it using only its built-in implicit rules. See Using Implicit Rules.

	If you want to use a nonstandard name for your makefile, you can specify the makefile name with the `-f' or `--file' option. The arguments `-f name' or `--file=name' tell make to read the file name as the makefile. If you use more than one `-f' or `--file' option, you can specify several makefiles. All the makefiles are effectively concatenated in the order specified. The default makefile names GNUmakefile, makefile and Makefile are not checked automatically if you specify `-f' or `--file'.


			9.4.2.3  Including Other Makefiles

	The include directive tells make to suspend reading the current makefile and read one or more other makefiles before continuing. The directive is a line in the makefile that looks like this:

	     include filenames...

	filenames can contain shell file name patterns. If filenames is empty, nothing is included and no error is printed. Extra spaces are allowed and ignored at the beginning of the line, but a tab is not allowed. (If the line begins with a tab, it will be considered a command line.) Whitespace is required between include and the file names, and between file names; extra whitespace is ignored there and at the end of the directive. A comment starting with `#' is allowed at the end of the line. If the file names contain any variable or function references, they are expanded. See How to Use Variables.

	For example, if you have three .mk files, a.mk, b.mk, and c.mk, and $(bar) expands to bish bash, then the following expression

	     include foo *.mk $(bar)

	is equivalent to

	     include foo a.mk b.mk c.mk bish bash

	When make processes an include directive, it suspends reading of the containing makefile and reads from each listed file in turn. When that is finished, make resumes reading the makefile in which the directive appears.

	One occasion for using include directives is when several programs, handled by individual makefiles in various directories, need to use a common set of variable definitions (see Setting Variables) or pattern rules (see Defining and Redefining Pattern Rules).

	Another such occasion is when you want to generate prerequisites from source files automatically; the prerequisites can be put in a file that is included by the main makefile. This practice is generally cleaner than that of somehow appending the prerequisites to the end of the main makefile as has been traditionally done with other versions of make. See Automatic Prerequisites. If the specified name does not start with a slash, and the file is not found in the current directory, several other directories are searched. First, any directories you have specified with the `-I' or `--include-dir' option are searched (see Summary of Options). Then the following directories (if they exist) are searched, in this order: prefix/include (normally /usr/local/include 1) /usr/gnu/include, /usr/local/include, /usr/include.

	If an included makefile cannot be found in any of these directories, a warning message is generated, but it is not an immediately fatal error; processing of the makefile containing the include continues. Once it has finished reading makefiles, make will try to remake any that are out of date or don't exist. See How Makefiles Are Remade. Only after it has tried to find a way to remake a makefile and failed, will make diagnose the missing makefile as a fatal error.

	If you want make to simply ignore a makefile which does not exist and cannot be remade, with no error message, use the -include directive instead of include, like this:

	     -include filenames...

	This acts like include in every way except that there is no error (not even a warning) if any of the filenames do not exist. For compatibility with some other make implementations, sinclude is another name for -include. 


			9.4.2.4 The Variable MAKEFILES

	If the environment variable MAKEFILES is defined, make considers its value as a list of names (separated by whitespace) of additional makefiles to be read before the others. This works much like the include directive: various directories are searched for those files (see Including Other Makefiles). In addition, the default goal is never taken from one of these makefiles and it is not an error if the files listed in MAKEFILES are not found.

	The main use of MAKEFILES is in communication between recursive invocations of make (see Recursive Use of make). It usually is not desirable to set the environment variable before a top-level invocation of make, because it is usually better not to mess with a makefile from outside. However, if you are running make without a specific makefile, a makefile in MAKEFILES can do useful things to help the built-in implicit rules work better, such as defining search paths (see Directory Search).

	Some users are tempted to set MAKEFILES in the environment automatically on login, and program makefiles to expect this to be done. This is a very bad idea, because such makefiles will fail to work if run by anyone else. It is much better to write explicit include directives in the makefiles. See Including Other Makefiles. 


			9.4.2.5 The Variable MAKEFILE_LIST

	As make reads various makefiles, including any obtained from the MAKEFILES variable, the command line, the default files, or from include directives, their names will be automatically appended to the MAKEFILE_LIST variable. They are added right before make begins to parse them.

	This means that if the first thing a makefile does is examine the last word in this variable, it will be the name of the current makefile. Once the current makefile has used include, however, the last word will be the just-included makefile.

	If a makefile named Makefile has this content:

	     name1 := $(lastword $(MAKEFILE_LIST))
	     
	     include inc.mk
	     
	     name2 := $(lastword $(MAKEFILE_LIST))
	     
	     all:
		     @echo name1 = $(name1)
		     @echo name2 = $(name2)

	then you would expect to see this output:

	     name1 = Makefile
	     name2 = inc.mk

	See Text Functions, for more information on the word and words functions used above. See The Two Flavors of Variables, for more information on simply-expanded (:=) variable definitions. 


			9.4.2.6 Other Special Variables

	GNU make also supports other special variables. Unless otherwise documented here, these values lose their special properties if they are set by a makefile or on the command line.

	.DEFAULT_GOAL
	    Sets the default goal to be used if no targets were specified on the command line (see Arguments to Specify the Goals). The .DEFAULT_GOAL variable allows you to discover the current default goal, restart the default goal selection algorithm by clearing its value, or to explicitly set the default goal. The following example illustrates these cases:

		      # Query the default goal.
		      ifeq ($(.DEFAULT_GOAL),)
			$(warning no default goal is set)
		      endif
		      
		      .PHONY: foo
		      foo: ; @echo $@
		      
		      $(warning default goal is $(.DEFAULT_GOAL))
		      
		      # Reset the default goal.
		      .DEFAULT_GOAL :=
		      
		      .PHONY: bar
		      bar: ; @echo $@
		      
		      $(warning default goal is $(.DEFAULT_GOAL))
		      
		      # Set our own.
		      .DEFAULT_GOAL := foo
		 

	    This makefile prints:

		      no default goal is set
		      default goal is foo
		      default goal is bar
		      foo
		 

	    Note that assigning more than one target name to .DEFAULT_GOAL is illegal and will result in an error.


	MAKE_RESTARTS
	    This variable is set only if this instance of make has restarted (see How Makefiles Are Remade): it will contain the number of times this instance has restarted. Note this is not the same as recursion (counted by the MAKELEVEL variable). You should not set, modify, or export this variable.


	.VARIABLES
	    Expands to a list of the names of all global variables defined so far. This includes variables which have empty values, as well as built-in variables (see Variables Used by Implicit Rules), but does not include any variables which are only defined in a target-specific context. Note that any value you assign to this variable will be ignored; it will always return its special value.


	.FEATURES
	    Expands to a list of special features supported by this version of make. Possible values include:

	    `archives'
		Supports ar (archive) files using special filename syntax. See Using make to Update Archive Files.
	    `check-symlink'
		Supports the -L (--check-symlink-times) flag. See Summary of Options.
	    `else-if'
		Supports ¿else if¿ non-nested conditionals. See Syntax of Conditionals.
	    `jobserver'
		Supports ¿job server¿ enhanced parallel builds. See Parallel Execution.
	    `second-expansion'
		Supports secondary expansion of prerequisite lists.
	    `order-only'
		Supports order-only prerequisites. See Types of Prerequisites.
	    `target-specific'
		Supports target-specific and pattern-specific variable assignments. See Target-specific Variable Values. 


	.INCLUDE_DIRS
	    Expands to a list of directories that make searches for included makefiles (see Including Other Makefiles). 


			9.4.2.7 
		9.4.3 Short tutorial on writing makefiles:
		Separate Compilation
	One of the features of C and C++ that's considered a strength is the idea of "separate compilation". Instead of writing all the code in one file, and compiling that one file, C/C++ allows you to write many .cpp files and compile them separately.

	With few exceptions, most .cpp files have a corresponding .h file. A .cpp usually consists of:

	    * the implementations of all methods in a class,
	    * standalone functions (functions that aren't part of any class),
	    * and global variables (usually avoided). 

	The corresponding .h file contains

	    * class declarations,
	    * function prototypes,
	    * and extern variables (again, for global variables). 

	The purpose of the .h files is to export "services" to other .cpp files.

	For example, suppose you wrote a Vector class. You would have a .h file which included the class declaration. Suppose you needed a vector in a MovieTheater class. Then, you would #include "Vector.h.
	Compiling a .cpp file
	What does the compiler do when it sees #include of a .h file in a .cpp file? For example, suppose you are compiling a MovieTheater class, which contains the line #include "Vector.h".

	The preprocessor will "insert" the Vector.h file into a copy of the MovieTheater.cpp source code. After the preprocessing phase has completed, the modified copy is sent to the compiler.

	What information does the compiler have about Vector when compiling MovieTheater.cpp? Itn only knows the data members of Vector.h and the methods of Vector.h. It does NOT know how the methods are implemented, since this information is not stored in the .h file.

	Fortunately, that's all it needs to know to create an object file. To create an object, you need to know how big it is, and what the data members are. You get that information from Vector.h.

	The compiler also need to know what are valid methods of Vector, what the parameters are and the return types. Again, this information is also in Vector.h. This is necessary to make sure that methods called on Vector objects are called in a valid manner.

	Surprisingly enough, the compiler doesn't need to know how any of Vector's methods are implemented. When compiling MovieTheatern.cpp, the compiler never looks at Vector.cpp.

	However, eventually, if the MovieTheater class is to run, there must be a time when the object code (i.e. the .o file) for Vector is integrated with the object code for MovieTheater (and whatever object file contains main()). That process occurs later on, during the linking phase. We'll talk about linking momentarily.

	When you compile MovieTheater.cpp, you only create a MovieTheater.o file. This is called an object file, and is also called a .o file. The object file doesn't have enough information to run by itself. It is not an executable.

	Typically, when you have many .cpp files, you will end up creating a corresponding .o file. In g++ and cxx (and most UNIX C++ compilers), this is done with the -c option.

	g++ -Wall -c MovieTheater.cpp
	cxx -w0 -std strict_ansi -c MovieTheater.cpp

	The -c option can appear anywhere in the command.

	When the compiler sees this option, it assumes you will create a .o file from the .cpp file. When compiling a .cpp file, the compiler does not require a main() function to appear in the file, although you can have a main(). Also, any implementations of classes that are NOT part of the .cpp file, but appear in the #include files are not compiled. For example, if Movie.cpp includes Vector.h, the code for Vector.cpp is not compiled when Movie.cpp is being compiled.

	Instead, the compiler merely checks that the Vector methods called correctly. It can check that the calls are correct, even without the implementation. All this information that is needed by the compiler appears in the .h files.
	Common Misconceptions
	There are some common misconceptions.

	    "When you compile a .cpp file using -c option, it compiles all other .cpp files mentioned in the #include .h files" 

	Untrue. It only compiles the single .cpp file being referred to, and only refers to the .h for purposes of creating objects (making sure it's the correct size, etc) and checking that methods are being called correctly.

	Even if you remove the -c option, it does not look for all other .cpp files. In fact, if you remove the option, it assumes that the .cpp file is a stand alone file, with everything defined, and includes a main().

	    "If you write something like:

	    g++ -Wall foo.cpp bar.cpp baz.cpp

	    the compiler will look at "bar.cpp" and "baz.cpp" when compiling "foo.cpp". 

	That's not quite true. The compiler compiles each .cpp file separately. If you don't use the -c option, then effectively, it creates internal versions of .o files (they aren't made into real files), and then linked.

	However, this process is basically the same as if you had compiled each .cpp file individually and then linked together at the end.
	Now, makefiles
	Why all the talk about how .cpp files get compiled in C++? Because of the way C++ compiles files, makefiles can take advantage of the fact that when you have many .cpp files, it's not necessary to recompile all the files when you make changes. You only need to recompile a small subset of the files.

	Back in the old days, a makefile was very convenient. Compiling was slow, and therefore, having to avoid recompiling every single file meant saving a lot of time.

	Although it's much faster to compile now, it's still not very fast. If you begin to work on projects with hundreds of files, where recompiling the entire code can take many hours, you will still want a makefile to avoid having to recompile everything.

	A makefile is based on a very simple concept. A makefile typically consists of many entries. Each entry has:

	    * a target (usually a file)
	    * the dependencies (files which the target depends on)
	    * and commands to run, based on the target and dependencies. 

	Let's look at a simple example.

	Movie.o: Movie.cpp Movie.h Vector.h
	   g++ -Wall -c Movie.cpp

	The basic syntax of an entry looks like:

	<target>: [ <dependency > ]*
	   [ <TAB> <command> <endl> ]+

	A makefile typically consists of many entries. A target appears before the left of a colon. A target is usually a file, but not always. The entry tells you how to construct a target, and more importantly, when to construct the target.

	After the target and the colon, you list out the dependencies. The dependencies are (usually) a list of files which the target file depends on. The dependencies are used by the makefile to detemine when when the target needs to be reconstructed (usually by recompiling code). The make utility decides when to reconstruct the target based on something very simple: a timestamp.

	Each file has a timestamp that indicates when the file was last modified. make looks at the timestamp of the file, and then the timestamp of the dependencies (which are also files). The idea is this. If the dependencies have changed, then perhaps the target needs to be updated. More precisely, if the dependent files have a more recent timestamp than the target, then the command lines are run. If written correctly, the commands should update the target.

	The updates are done recursively. When checking the dependencies, the makefiles checks if any of the dependent files are targets, and if so, it looks at those dependencies, and, based on the timestamp, it may run commands to update the dependent files, and then update the target.
	Making .o files
	The most common target you will make is a .o file. This is created when you compile a .cpp file using the -c option.

	Here's an example:

	Movie.o: Movie.cpp Movie.h Vector.h
	   g++ -Wall -c Movie.cpp

	The target is Movie.o. The dependencies of a .o file are almost always:

	    * The corresponding .cpp file.
	    * The corresponding .h file.
	    * Other .h files 

	It's a common mistake to assume that other .cpp files are ever dependencies. They shouldn't be. Why?

	If you recall, when you compile a .cpp file to an object file, the compiler never needs to refer to other .cpp files. That's the benefit of separate compilation. You only need to consider the .cpp file and other .h files. That way, if the .h files that a .cpp depend on don't change, but the corresponding .cpp files do, there's no need to recompile the original .cpp file.

	Let's illustrate with an example. Suppose Movie.cpp depends on Vector.h. Perhaps it contains a Vector object in the class. Now, if Vector.cpp changes but Vector.h does not, then there's no need to recompile Movie.cpp. Movie.cpp doesn't need to see the code for Vector.cpp. That's handled later at the linking phase.
	Deciding on dependencies
	OK, so it makes sense to have Movie.o depend on Movie.cpp and Movie.h. After all, if either change, then Movie.o needs to be recompiled (at least, usually). But what should you put for the other .h dependencies?

	That should be easy. Look in the corresponding .h and .cpp files and look for any #include files in double quotes (the ones in angle brackets ought to be files like "iostream", etc., which basically never change), and put those down. Many people forget to put these dependencies down, and therefore, they don't recompile the object file when they should.

	However, it's a little more complicated than that. Suppose the Vector.h depends on other .h files. You ought to add the dependencies there too. Why? Suppose Vector has a Foo object as a data member. Now, Movie might include a Vector object as a data member. Even if Vector.h doesn't change, Foo might change (let's say a new data member was added). This makes Foo bigger (since it has a new data member), which makes Vector bigger, which makes Movie bigger. Thus, one needs to recompile Movie to reflect that change.

	It can be tough to track all the .h dependencies, but you can write the dependencies in the makefile.

	Vector.h: Foo.h

	In this case, you don't need any targets. It just allows the compiler to determine that a dependency exists (dependencies are transitive, that is, if X is dependent on Y, and Y is dependent on Z, then X is dependent on Z).
	Who Determines the Dependencies?
	You do. Yes, it would be nice if a program could determine the dependencies for you. After all, doesn't it simply require that you look at the .h files and then do this recursively?

	As it turns out, the answer is that this can be done automatically (i.e., by a program). That program is called makedepend. However, we won't discuss makedepend in this tutorial.
	Commands
	Once make determines that there is at least one dependency with a more recent timestamp than the target, it will run the commands that appear just after the dependency line (the line containing the target and the target's dependencies). There can be more than one command after the dependency line, but all commands must immediately follow the dependency line, and the first character of each commandn line must be a TAB.

	This is, alas, one of the more common errors in making a makefile. The problem, unfortunately, lies not with the user of the makefile, but the person who originally programmed it. That person must have thought that it would be convenient to have each command start with a TAB, because the command would be indented in.

	However, the person failed to realize that several spaces look like a TAB, and even a few spaces followed by a TAB, look like a TAB. It's just too hard to see any mistakes, and the person should never have depended on TABs.

	Alas, like many UNIX utilities, the original version has managed to hang around forever, despite its flaws. The one advantage of Microsoft programs is that they are always being updated, and that it's hard to find programs that are nearly the same as the original. On the other hand, tools like make and tar with obvious ways to make it better still exists in pretty much the old format.

	As a user, you must be careful with TABs.

	Ah, back to business. The commands are supposed to generate a new version of the target. For example,

	Movie.o: Movie.cpp Movie.h Vector.h
	   g++ -Wall -c Movie.cpp

	The g++ command does indeed create a Movie.o when done. Keep that in mind. The commands you list should generate the target. You don't want to have a command like:

	Movie.o: Movie.cpp Movie.h Vector.h
	   g++ -Wall -c Vector.cpp

	This command does not produce Movie.o as a result. This isn't a good command.
	Exectuables as targets
	Usually, the purpose of the makefile is to create an executable. That target is often the first target in the makefile. For example,

	p1 : MovieList.o Movie.o NameList.o Name.o Iterator.o
	    g++ -Wall MoveList.o Movie.o NameList.o Name.o Iterator.o -o p1

	If p1 is the first target in your makefile, then when you type "make", it will run the commands for p1. "make" always runs the commands from the first target in the makefile. It won't run any of the commands from other targets.

	Notice that the command to compile will create an executable called p1. It uses the -o option to create an executable with a name other than a.out.
	What to name the makefile
	The "make" utility uses the following rules to determine which makefile to run.

	   a. If you type make and there's a file called makefile, then it will run the commands from the first target of that file, provided the dependent files are more recent than the target.
	   b. If you type make and there's a file called Makefile, but no file called makefile, then it will run the commands from the first target of that file, provided the dependent files are more recent than the target.
	   c. If you type make -f <filename> then it will run the commands from the first target of <filename>, provided the dependent files are more recent than the target. 

	You will be told in class or in a webpage what you should name your file.
	Running make on the command line
	There are several ways to run make.

	   a. Type make and hit return. It will look for a makefile in the current directory and run the commands of the first target, assuming that it has to based on the dependencies.
	   b. Type make -f <filename> and hit return. It will look for a makefile with the name <filename> in the current directory and run the commands of the first target, assuming that it has to based on the dependencies.
	   c. Type make <target>. It will again look for a file called makefile (and if that's not there, it looks for Makefile) and locate the target. This does not have to be the first target. It will run the commands provided the dependencies are more recent than the target. 

	Macro definitions for flexibility
	You should use macros. You SHOULD use macros! YOU SHOULD USE MACROS!!!

	Use MACROS!!!! (We mean it).

	Macros allow you to define "variables" which are substituted in. Here's an example of how to use them:

	OBJS = MovieList.o Movie.o NameList.o Name.o Iterator.o
	CC = g++
	DEBUG = -g
	CFLAGS = -Wall -c $(DEBUG)
	LFLAGS = -Wall $(DEBUG)

	p1 : $(OBJS)
	    $(CC) $(LFLAGS) $(OBJS) -o p1

	Macros are usually put at the beginning of a makefile.

	A macro has the following syntax

	<macro_name> = <macro_string>

	On the left hand side of the equal sign is the macro name. Basically, you should follow the convention of only using upper case letters and underscores.

	On the right hand side is some string, which will be substituted. A macro is very similar to a #define in C/C++. It allows for substitution. It looks throughout the file for anything starting with a dollar sign and surrounded by either parentheses or braces.

	In particular, it looks for things taht look like $(CC) or ${CC}--- both versions are acceptable, although I will only use the one with parentheses.

	If the macro string is too long for one line, you can put a backslash, followed immediately by a return (newline). "make" will assume that the next line is a continuation of the line that has the backslash followed by the newline.

	In fact, you can always use the backslash followed by a newline to indicate a continuing line. It doesn't just appear in macros. It can also apply to dependencies, commands, etc.
	Why use macos?
	You want to use macros to make it easy to make changes. For example, if you use macros, it's easy to change the compiler and compiler options between g++ and cxx. It's easy to turn on and of debug options. Without macros, you would use a lot of search and replace.

	You use macros in makefiles for the same reason you define constants in programs. It's easier to update the files and make it more flexible.
	Common macros for C++ programming
	The example in the previous section shows common examples of macros.

	    * CC The name of the compiler
	    * DEBUG The debugging flag. This is -g in both g++ and cxx. The purpose of the flag is to include debugging information into the executable, so that you can use utilities such as gdb to debug the code.
	    * LFLAGS The flags used in linking. As it turns out, you don't need any special flags for linking. The option listed is "-Wall" which tells the compiler to print all warnings. But that's fine. We can use that.
	    * CFLAGS The flags used in compiling and creating object files. This includes both "-Wall" and "-c". The "-c" option is needed to create object files, i.e. .o files. 

	You will notice that once a macro is defined, it can be used to define subsequent macros.
	Dummy targets
	There are times when you don't want to create a target. Instead, you want to run a few commands. You can do so with dummy targets. A dummy target is a target which is not a file. However, the make utility doesn't know it's not a file, and will run commands as if it was a file that had to be created.

	There are three popular dummy targets used in makefiles.
	make clean
	Occasionally, you will want to remove a directory of all .o files and executables (and possibly emacs backup files). The reason? If you have written your makefile incorrectly, it's possible that there is a .o file that is not being updated when it should.

	By removing all such files, you force the makefile to recompile all .o files, thus guaranteeing the most recent rebuild. Of course, this is not something that should be necessary if you've created a makefile correctly. It defeats the purpose of creating a makefile in the first place!

	Nevertheless, if you find that the code isn't working the way it should, and you suspect it's due to a buggy makefile, you can run make clean. This is how it usually looks:

	clean:
	     \rm *.o *~ p1

	The backslash prevents "rm" from complaining if you remove the files (especially, if it's been aliased with the -i option). Normally, you remove all .o files, all files ending in ~ (which are emacs backup files), and the name of the executable (or executables, as the case may be). In the example above, the executable is called p1.

	Notice that "clean" does not cause any files to be created. In particular, clean is not created. There are also no dependencies. Basically, make notices that there is no such file as clean and begins to run the commands in the command lines to create the file (which won't be created).

	The effect is that running make clean causes the commands to always run. That's fine, because it's used to remove files, we don't want. Usually, you run make after running make clean to rebuild the executable.
	make tar
	When you submit files, you will often have to tar many files together. It's easy to forget to tar all files, and it's easy to make mistakes on the tar syntax. By adding a dummy target that creates a tar file, you can make a tarfile without errors (presuming you "debug" it long before you need to submit).

	Here's an example:

	tar:
	     tar cfv p1.tar Movie.h Movie.cpp Name.h Name.cpp NameList.h \
		     NameList.cpp  Iterator.cpp Iterator.h

	Again, there are no dependencies. However, this time a file does get created (p1.tar). You could make p1.tar the target if you want, but this way, it always recreates p1.tar since tar should not be a file that exists in your directory.
	make all
	Finally, there are a few occasions when you may need to create more than one executable or do more than one task. You can do this with an all target. For example, suppose you want to create three executables: p1, p2, and p3.

	You can write

	all: p1 p2 p3

	p1: Foo.o main1.o
	   g++ -Wall Foo.o main1.o -o p1

	p2: Bar.o main2.o
	   g++ -Wall Bar.o main2.o -o p2

	p3: Baz.o main3.o
	   g++ -Wall Baz.o main3.o -o p3

	Notice that the all target has no command lines. However, it lists the dependencies as p1, p2, and p3. It attempts to build the most up-to-date versions of those files (if they are up-to-date, it will let you know). Having an all target is convenient for making a bunch of executables, without having to type make on each target separately.
	Common errors in makefiles
	Perhaps the most common error in writing a makefile is failing to put a TAB at the beginning of commands. The second most common error is to put a TAB at the beginning of blank lines. The first error causes the commands not to run. The second causes the "make" utility to complain that there is a "blank" command.

	Unfortuantely, it's hard to see TABs. You need to go into the editor, and go to the beginning of command lines, and move the cursor forward. If it jumps several spaces, you know there is a TAB at the beginning. If it doesn't, you know there isn't.

	If you have a command that continues on the next line with a backslash (followed immediately by a newline), then the continuation line does not need to start with a TAB (it makes sense, after all, it is treated as a continuation, not as a new command).

	Another common error is not hitting return just after the backslash, should you choose to use it.

	Of course, another error is not getting the dependencies correct, but that's not exactly a makefile error, per se.
	Putting it all together
	Here's a sample makefile for creating an executable called p1.

	OBJS = MovieList.o Movie.o NameList.o Name.o Iterator.o
	CC = g++
	DEBUG = -g
	CFLAGS = -Wall -c $(DEBUG)
	LFLAGS = -Wall $(DEBUG)

	p1 : $(OBJS)
	    $(CC) $(LFLAGS) $(OBJS) -o p1

	MovieList.o : MovieList.h MovieList.cpp Movie.h NameList.h Name.h Iterator.h
	    $(CC) $(CFLAGS) MovieList.cpp

	Movie.o : Movie.h Movie.cpp NameList.h Name.h
	    $(CC) $(CFLAGS) Movie.cpp

	NameList.o : NameList.h NameList.cpp Name.h 
	    $(CC) $(CFLAGS) NameList.cpp

	Name.o : Name.h Name.cpp 
	    $(CC) $(CFLAGS) Name.cpp

	Iterator.o : Iterator.h Iterator.cpp MovieList.h
	    $(CC) $(CFLAGS) Iterator.cpp

	clean:
	    \rm *.o *~ p1

	tar:
	    tar cfv p1.tar Movie.h Movie.cpp Name.h Name.cpp NameList.h \
		    NameList.cpp  Iterator.cpp Iterator.h




	9.5 Make man
NAME

       make - GNU make utility to maintain groups of programs

SYNOPSIS

       make [ -f makefile ] [ options ] ... [ targets ] ...

WARNING

       This  man  page	is an extract of the documentation of GNU make.  It is
       updated only occasionally, because the GNU project does not use	nroff.
       For  complete,  current documentation, refer to the Info file make.info
       which is made from the Texinfo source file make.texi.

DESCRIPTION

       The purpose of the make utility is  to  determine  automatically  which
       pieces of a large program need to be recompiled, and issue the commands
       to recompile them.  The manual  describes  the  GNU  implementation  of
       make,  which was written by Richard Stallman and Roland McGrath, and is
       currently maintained by Paul Smith.   Our  examples  show  C  programs,
       since  they  are most common, but you can use make with any programming
       language whose compiler can be run with a shell command.  In fact, make
       is  not limited to programs.  You can use it to describe any task where
       some files must be updated automatically from others whenever the  oth-
       ers change.

       To  prepare to use make, you must write a file called the makefile that
       describes the relationships among files in your program, and the states
       the  commands for updating each file.  In a program, typically the exe-
       cutable file is updated from object files, which are in	turn  made  by
       compiling source filese

       Once  a	suitable  makefile  exists,  each  time you change some source
       files, this simple shell command:

	      make

       suffices to perform all necessary  recompilations.   The  make  program
       uses  the  makefile  data  base	and the last-modification times of the
       files to decide which of the files need to be  updated.	 For  each  of
       those files, it issues the commands recorded in the data base.

       make  executes  commands  in  the makefile to update one or more target
       names, where name is typically a program.  If no -f option is  present,
       make  will  look for the makefiles GNUmakefile, makefile, and Makefile,
       in that order.

       Normally you should call your makefile  either  makefile  or  Makefile.
       (We  recommend  Makefile because it appears prominently near the begin-
       ning of a directory listing, right near other important files  such  as
       README.)   The  first name checked, GNUmakefile, is not recommended for
       most makefiles.	You should use this name if you have a	makefile  that
       is  specific  to GNU make, and will not be understood by other versions
       of make.  If makefile is `-', the standard input is read.

       make updates a target if it depends on  prerequisite  files  that  have
       been modified since the target was last modified, or if the target does
       not exist.

OPTIONS

       -b, -m
	    These options are ignored for compatibility with other versions of
	    make.

       -B, --always-make
	    Unconditionally make all targets.

       -C dir, --directory=dir
	    Change to directory dir before reading the makefiles or doing any-
	    thing else.  If multiple -C options are specified, each is	inter-
	    preted  relative to the previous one: -C / -C etc is equivalent to
	    -C /etc.  This is typically used  with  recursive  invocations  of
	    make.

       -d   Print debugging information in addition to normal processing.  The
	    debugging information says which files are	being  considered  for
	    remaking,  which  file-times  are  being  compared	and  with what
	    results, which files actually need to be  remade,  which  implicit
	    rules  are considered and which are applied---everything interest-
	    ing about how make decides what to do.

       --debug[=FLAGS]
	    Print debugging information in addition to normal processing.   If
	    the  FLAGS are omitted, then the behavior is the same as if -d was
	    specified.	FLAGS may be a for all debugging output (same as using
	    -d),  b for basic debugging, v for more verbose basic debugging, i
	    for showing implicit rules, j for details on  invocation  of  com-
	    mands, and m for debugging while remaking makefiles.

       -e, --environment-overrides
	    Give  variables  taken  from the environment precedence over vari-
	    ables from makefiles.

       +-f file, --file=file, --makefile=FILE
	    Use file as a makefile.

       -i, --ignore-errors
	    Ignore all errors in commands executed to remake files.

       -I dir, --include-dir=dir
	    Specifies a directory dir to search for  included  makefiles.   If
	    several  -I  options  are used to specify several directories, the
	    directories are searched in the order specified.  Unlike the argu-
	    ments  to other flags of make, directories given with -I flags may
	    come directly after the flag: -Idir is allowed, as well as -I dir.
	    This syntax is allowed for compatibility with the C preprocessor's
	    -I flag.

       -j [jobs], --jobs[=jobs]
	    Specifies the number of jobs (commands) to run simultaneously.  If
	    there  is  more than one -j option, the last one is effective.  If
	    the -j option is given without an argument, make  will  not  limit
	    the number of jobs that can run simultaneously.

       -k, --keep-going
	    Continue  as  much	as  possible after an error.  While the target
	    that failed, and those that depend on it, cannot  be  remade,  the
	    other dependencies of these targets can be processed all the same.

       -l [load], --load-average[=load]
	    Specifies that no new jobs (commands) should be started  if  there
	    are  others  jobs running and the load average is at least load (a
	    floating-point number).  With no argument, removes a previous load
	    limit.

       -L, --check-symlink-times
	    Use the latest mtime between symlinks and target.

       -n, --just-print, --dry-run, --recon
	    Print  the	commands  that	would  be executed, but do not execute
	    them.

       -o file, --old-file=file, --assume-old=file
	    Do not remake the file file even if it is older than its dependen-
	    cies,  and	do  not remake anything on account of changes in file.
	    Essentially the file is treated as very  old  and  its  rules  are
	    ignored.

       -p, --print-data-base
	    Print  the data base (rules and variable values) that results from
	    reading the makefiles; then execute as usual or as otherwise spec-
	    ified.   This  also prints the version information given by the -v
	    switch (see below).  To print the  data  base  without  trying  to
	    remake any files, use make -p -f/dev/null.

       -q, --question
	    ``Question	mode''.   Do  not run any commands, or print anything;
	    just return an exit status that is zero if the  specified  targets
	    are already up to date, nonzero otherwise.

       -r, --no-builtin-rules
	    Eliminate  use of the built-in implicit rules.  Also clear out the
	    default list of suffixes for suffix rules.

       -R, --no-builtin-variables
	    Don't define any built-in variables.

       -s, --silent, --quiet
	    Silent operation; do not print the commands as they are  executed.

       -S, --no-keep-going, --stop
	    Cancel  the  effect  of  the  -k  option.  This is never necessary
	    except in a recursive make where -k might be  inherited  from  the
	    top-level make via MAKEFLAGS or if you set -k in MAKEFLAGS in your
	    environment.

       -t, --touch
	    Touch files (mark them up to date without  really  changing  them)
	    instead  of  running their commands.  This is used to pretend that
	    the commands were done, in order to  fool  future  invocations  of
	    make.

       -v, --version
	    Print  the version of the make program plus a copyright, a list of
	    authors and a notice that there is no warranty.

       -w, --print-directory
	    Print a message containing the working directory before and  after
	    other  processing.	 This  may  be useful for tracking down errors
	    from complicated nests of recursive make commands.

       --no-print-directory
	    Turn off -w, even if it was turned on implicitly.

       -W file, --what-if=file, --new-file=file, --assume-new=file
	    Pretend that the target file has just been	modified.   When  used
	    with  the -n flag, this shows you what would happen if you were to
	    modify that file.  Without -n, it is almost the same as running  a
	    touch  command  on the given file before running make, except that
	    the modification time is changed only in the imagination of  make.

       --warn-undefined-variables
	    Warn when an undefined variable is referenced.

	9.6

10. Clearcase, see clear_case_KB.txt for full information
	10.1. add mvobs elements. new dir with new files.
		10.1.1 $ct co -nc [root dir]
		10.1.2 $ct mkdir -nc [new dir]
		10.1.3 $cp [files] [new dir]
		10.1.4 $cd [new dir]
		10.1.5 $ct mkelem -nc *

	10.2. remove version from integ. one that is hyperlinked from a private branch version.
	example:
	|1. $  ct lsvtree burst_global.h to get the correct version to remove. 
	|2. $ ct describe -long burst_global.h@@/main/integ_sfe_3.0.0/2 
	|3. result:
	version "burst_global.h@@/main/integ_sfe_3.0.0/2"
	  created 08-Jan-04.11:55:05 by ClearCase user (integ_sfe.esadm@rocky)
	  "Change that is part of the ability to switch from SM, SME FullInv to PermInv. "
	  Element Protection:
	    User : leonidb  : r--
	    Group: users    : r--
	    Other:          : r--
	  element type: h_source
	  predecessor version: /main/integ_sfe_3.0.0/1
	  Hyperlinks:
	    Merge@1222270@/mvobs/SMSC <- /mvobs/SMSC/sfe/code/kern/inc/burst_global.h@@/main/integ_sfe_3.0.0/p_yizaq_sfe_3.0.0/1
	|4. $ ct rmhlink -nc Merge@1222270

	10.3 find all checked out files recursively.
		$ cd project/code
		$ find -type f -exec /usr/atria/bin/cleartool ls '{}' \; | grep OUT


11. Shell scripting.

	11.1 sh
		11.1.1. sh variable condition example. 
		This one checks if variable contains value. if so it prints an error message to stderr.
			if [ -n "${NETOLOGY_INST}" ]
		 then
		   echo "Run with -noinst" >&2
		   exit 1
		fi

12. Debug processes, programs
	12.1. Execution report in terms of syscalls.
		in older solaris 
		$ truss <command>

	truss <process args>
	or truss -pid <pid>

		For linux and current solaris
		#strace <command>

	12.2. GDB

	    12.2.1 tutorial
check icloud notes 

		12.2.2 examine memory
		(gdb) h x
	Examine memory: x/FMT ADDRESS.
	ADDRESS is an expression for the memory address to examine.
	FMT is a repeat count followed by a format letter and a size letter.
	Format letters are o(octal), x(hex), d(decimal), u(unsigned decimal),
	  t(binary), f(float), a(address), i(instruction), c(char) and s(string).
	Size letters are b(byte), h(halfword), w(word), g(giant, 8 bytes).
	The specified number of objects of the specified size are printed
	according to the format.

	Defaults for format and size letters are those previously used.
	Default count is 1.  Default address is following last thing printed
	with this command or "print".
	Example:
	(gdb) x/1g authResponse
	0xbfffe7f0:     0x0000005200000000

13. Viewers

	13.1. less.
		13.1.1 use like tail -f to display information stream.
		$ less <file name>
			then press SHIFT+f
		13.1.2 ignore case 
		$ less -i <file name>


14. awk, AWK, see kb_awk for full information.

	14.1 awk seperator
		example seperator /
		cat ~/temp/merge_cmd | awk -F \/ '{print $5}'


	14.2 Simple arithmetic calculation. 
		14.2.1 Simple example
		Say I want to sum the size of all.obj and .pch files in a visuall c++ project dir. I run the next command from the projects root dir.
$ find . \( -name "*.obj" -o -name "*.pch" \) -exec ls -l '{}' \; | awk ' {size+= $5}  END {print "size: " size "kb" }'

		14.2.2. Advanced example.
		Use awk for simple calculation. For example find all files and directories larger than 100MB. do
	$  du -k . |  awk '{if ($1 > 100000) print $2, " is bigger than 100MB, size: ",$1}'

	or
	$ du -k . > usage_report
	$ awk '{if ($1 > 100000) print $2, " is bigger than 100MB, size: ",$1}' usage_report

	or possible to use find

	14.3 Use awk to filter many files from one file.
	This will split file basic1 to as many files as specified in parameter number 3.
	 awk '{print $0 >> sprintf("aaa%s.log", $3)}' basic1
	 awk '{print $0 >> sprintf("aaa%s.log", $3)}' basic1

	14.4 tee
	tee is a command that writed from standard IO to files. It can be used for example to debug awk scripts that work with pipes.
	If an awk program is part of a pipeline of several programs, even other awk programs, you can use the tee command to redirect output to a file, while also piping the output to the next command. For instance, look at the shell script for running the masterindex program, as shown in Chapter 12, "Full-Featured Applications":

    $INDEXDIR/input.idx $FILES |
    sort -bdf -t:  +0 -1 +1 -2 +3 -4 +2n -3n | uniq |
    $INDEXDIR/pagenums.idx | tee page.tmp |
    $INDEXDIR/combine.idx |
    $INDEXDIR/format.idx

By adding "tee page.tmp", we are able to capture the output of the pagenums.idx program in a file named page.tmp. The same output is also piped to combine.idx.

15. Useful, Common procedures and shortcuts

	15.1 issue command on list of files.
		15.1.1 in sh:
			for file in *.c ; do wc -l $file ; done
			
			extract class name from java source files and grep it for usage in list of files
			for class in `grep class *.java | awk -F. '{print $1}'`; do echo $class; grep $class  ../../../data/brl/* ; done

			//ignore */
		15.1.2 Process the contents of a file and then run a list of commands:
			stam file contains a list of files in windows format, the files are converted to unix format and then the commands are ran.
			stam format is: "H:\ismg_israel_acs\Acs\Vendors\DzAttr.H@@\main\yytzhak.int.acs4_1\CHECKEDOUT.359737"
			$ for file in `cat stam | awk -F"\\\@" '{print $1}' | sed 's/H:\\\ismg_israel_acs//' | sed 's_\\\_/_g' `; do echo $file; done

			$ for file in `cat stam | awk -F"\\\@" '{print $1}' | sed 's/H:\\\ismg_israel_acs\\\Acs//' | sed 's_\\\_/_g' `; do tkdiff /cygdrive/j/ismg_israel_acs/Acs$file /cygdrive/h/ismg_israel_acs/Acs$file ; done


			e\\\Acs//' | sed 's_\\\_/_g' `; do ls /cygdrive/h/ismg_israel_acs/Acs$file ; done
			extract links to local files from web page, change the path to be cygwin compatible, then copy the files to given location
	 for file in `grep "file:" Presentation.html | awk -F"\"" '{print $2}' | sed -e 's/^file.*d:/\/cygdrive\/d/g'`; do \cp $file ./presentation_doc/ ; done
		in tcsh
			foreach file ( * ) 
			foreach? echo $file
			foreach? end
	   on all lines of file.
		in sh:
			for line in `cat <file name>` ; do "echo line is: $line"; done
		in tcsh:
			foreach line ( `cat sm_mng.h.bak` )
			foreach? echo $line
			foreach? end

		example, change file JF_somesuffix to TC_lmsq_somesuffix
			foreach file ( `ls | grep JF | awk -F"_" '{print$2}' ` )
			foreach? mv JF_$file TC_lmsq_$file
			foreach? end


	15.2 list directories.
		$ ls -d *

	15.3 process tree.
	$pstree

	15.4. List open files in system
	$lsof 


	15.5. Remove repeated lines from file:
	$ uniq [file name]

		Usually this goes hand in hand with sort. sort a file then run uniq to remove repeated lines.
		like:
		awk '/pattern/ {print "$1"} file | sort | uniq

	15.6 Encryption.
		15.6.1 Don't use crypt!!! It is based on 56 bit DES and is easy to break.

		15.6.2 Can use ccrypt (based on AES)
			To encrypt
			$ ccrypt -e [file]

			To decrypt

			$ ccrypt -d [file].cpt
	15.7 Create tarball backup of files according to file lists in a file.
	$ tar cvf backup_files.tar `cat hijacked_files ; cat co_files ` ; bzip2 backup_files.tar

	15.8 Create/modify a UNIX file with an arbitrary timestamp
	The touch command in UNIX creates a file if it doesn't exist or updates the modification time of an existing file to the current time. An option of the touch command allows the modification timestamp to be set to any arbitrary time.

To change the modification time of a file (the time displayed in a long listing of the file) of a file called testfile to November 18, 2000, 2:30 PM, use the following command:

touch -t 200011181430


This will alter the modification time of an existing testfile or, if not present, will create an empty file with that timestamp.

The format of the timestamp is
[[CC]YY]MMDDhhmm [.SS]



	15.2 Open remote graphical display

		15.2.1 Directlym configure Manually, in may time won't work due to proxy or network configuration
	export DISPLAY=64.103.112.242:0.0 (my IP)
	xhost+ on 64.103.112.242

		15.2.2 Using SSH with X forwarding for untrusted X11 display forwarding

		The short story, use ssh -X <machine> and ssh will automatically set the display to the source machine.

		The details:
X11 DISPLAY variables and X forwarding

Assuming we are logged in via "ssh" from WOMBATNET to chinookfe, what are some of the possible problems we can encounter with X11 forwarding ? The first clue, in the following example, is the DISPLAY environment variable.

    [chinookfe:/home/chinook/wombat]
    $ xterm
    xterm Xt error: Can't open display: :0

Trying to execute a simple "xterm" on chinookfe through "ssh" yields the above error message, which shows that the DISPLAY environment variable is not set correctly. We can confirm this as follows:

    [chinookfe:/home/chinook/wombat]
    $ echo $DISPLAY
    :0

The immediate inclination is to manually set the DISPLAY variable on chinookfe using either "export" or "setenv", depending on your shell. This is not correct, in that the proper operation of "ssh" with X forwarding allows us to avoid setting or trying to set the DISPLAY variable manually, and it even bypasses the need to run the "xhost" command for the chinookfe session on your workstation. A correct DISPLAY setting, assuming a simple X forwarding operation between chinookfe and our hypothetical workstation WOMBATNET, would yield a value such as:

    [chinookfe:/home/chinook/wombat]
    $ echo $DISPLAY
    chinookfe:11.0

This varies from what you may have seen in the past, in that often users will place command lines into their login scripts on remote systems that set the DISPLAY variable upon login. Lines like:

      setenv DISPLAY myworkstation:0.0
                   or
      export DISPLAY=myworkstation:0.0

should be removed from your dot files (.cshrc, .kshrc, .login, etc.) on NCAR computer systems, as "ssh" will automatically handle forwarding X11 connections for us. This includes setting the DISPLAY environment variable on the remote host, in this case chinookfe.

Assuming we have removed the line(s) setting our DISPLAY variable from our dot files on chinookfe, how do we get "ssh" to do the work for us? Our recommended approach is to use the "-X" option of "ssh" to establish the connection from our hypothetical example workstation WOMBATNET to chinookfe, as follows:

    [WOMBATNET:/home/wombat]
    $ ssh -X chinookfe.ucar.edu

    wombat@chinookfe's password:

After entering our password, we receive the MOTD and are logged into the system.

    Last login: Wed Jul 17 10:34:08 2002 from WOMBATNET.net
    =======================================================================
    chinookfe: IRIX64 6.5.16, SGI Origin2000 (8 cpus, 8 GB's memory)
    =======================================================================
    THIS SYSTEM IS FOR THE USE OF AUTHORIZED USERS ONLY.  INDIVIDUALS USING
    THIS COMPUTER SYSTEM WITHOUT AUTHORITY, OR IN EXCESS OF THEIR AUTHORITY,
    ARE SUBJECT TO HAVING ALL THEIR ACTIVITIES ON THIS SYSTEM MONITORED AND
    RECORDED BY SYSTEM PERSONNEL.  IN THE COURSE OF MONITORING INDIVIDUALS
    IMPROPERLY USING THIS SYSTEM, OR IN THE COURSE OF SYSTEM MAINTENANCE,
    THE ACTIVITIES OF AUTHORIZED USERS MAY ALSO BE MONITORED.
    =========================================================================

    [chinookfe:/home/chinook/wombat]
    $

We should now have the correct DISPLAY variable set:

    [chinookfe:/home/chinook/wombat]
    $ echo $DISPLAY
    chinookfe:10.0

Note that the form of the DISPLAY variable will always be "HOSTNAME:x.y" where "HOSTNAME" is the host we just logged in to, in this case "chinookfe", "x" is a small integer, in this case "10", and "y" is usually "0". What this does, in general terms, is enable X11 forwarding across the established ssh tunnel. The upside is this means we don't have to set DISPLAY through either files or after login, and back on the workstation we don't have to issue a command like "xhost +chinookfe.ucar.edu". In addition, all of the X11 traffic is now encrypted. This effect provides better security than the more traditional techniques, and if your facility has a firewall, this may be the only way, because of the tunneling, to get X11 applications to display back to your workstation from the system at NCAR.

You should now be able to execute a simple X11 application like "xterm" and get the window back at your workstation:

    [chinookfe:/home/chinook/wombat]
    $ xterm &

If this fails to display the "xterm" window, then we need to employ remedial "ssh" diagnostics to try to determine where the X11 authorization problem may exist. With the OpenSSH client software, we can use the same verbose stream debugging we did in our previous example. 

		15.2.3 ssh -Y
e.g. [212680136@G9VK2GH2E:Sun Dec 31:~:]$ ssh -Y de680136@3.87.209.39

		15.2.4

	15.3 nohup
nohup - run a command immune to hangups
nohup command [argument...]

The nohup utility invokes the named command with  the  argu-
     ments supplied.  When the command is invoked, nohup arranges
     for the SIGHUP signal to be ignored by the process.

     When invoked with the -p or -g  flags,  nohup  arranges  for
     processes already running as identified by a list of process
     IDs or a list of process group IDs to become immune to hang-
     ups.

     The nohup utility can be used when it is known that  command
     will  take  a long time to run and the user wants to log out
     of the terminal. When a shell exits, the  system  sends  its
     children  SIGHUP  signals, which by default cause them to be
     killed. All  stopped,  running,  and  background  jobs  will
     ignore  SIGHUP  and continue running, if their invocation is
     preceded by the nohup command or if the process programmati-
     cally has chosen to ignore SIGHUP.


16. Debug
	16.1 create core file.
		16.1.1 gcore -o <core name> -pid <pid>
		16.1.2 or kill -7 <pid> 
			7 is SIGQUIT

	16.2. list .so an executable uses:
	$ ldd <exec>
	$ ldd -v, verbose info, recursive dependency in libs

17. Plot, print, display, pretty print etc.
	17.1. print a ascii banner 
		$ banner <string>

	17.2. gnuplot
		gnuplot>  plot sin(x)   
		gnuplot>  splot sin(x*y/20)
		gnuplot>  plot sin(x) title 'Sine Function', tan(x) title 'Tangent'	

	      gnuplot>  plot  "force.dat" using 1:2 title 'Column', \
			      "force.dat" using 1:3 title 'Beam'

			      instead of letter coordinated use numbers and then give letter names with label.
		gnuplot> set title "The letters frequency in the mutated file"
		gnuplot> set xlabel "letters" 
		gnuplot> set ylabel "frequency"
		gnuplot> set terminal postscript eps color
		gnuplot> set output "gplot_markov0.eps"
		gnuplot> plot "readme_markov0_gplot" w l, "readme_markov0_orig_gplot" w l
		It is possible to manualy edit the PS file and change keys lables and so forth.
		For example, to change x-tics (X indices) search for ( n) and replace the number n with the value of choice.

	Plot to ps file:
		gnuplot> plot "t1" w l
	gnuplot> plot "t1" w l,"t2" w l
	gnuplot> set term postscript portrait mono "Times-Roman" 15
	Terminal type set to 'postscript'
	Options are 'portrait noenhanced monochrome blacktext \
	  dashed dashlength 1.0 linewidth 1.0 defaultplex \
	  palfuncparam 2000,0.003 \
	  butt "Times-Roman" 15'
	gnuplot> set output "gp.ps"
	gnuplot> plot "t1" w l,"t2" w l
	gnuplot>
	49. sh functions.
	since not possible to pass parameters to aliases, one way to achieve that
	is to define a function that take parameters and create and alias to it.
	example, I want a CD <drive letter> alias to does cd /cygdrive/<drive letter>
	I added the following function and alias to my .bashrc:
	_cd () {
		cd /cygdrive/"$1"
	   }
	alias CD="_cd "

18. Source navigation.
	18.1 cscope is a very good source code navigator based on curse (acsii gui) lib.
	Also possible to use freescope.
	$ cd <my_project_path>/<my_project>
	$  find . \( -name "*.[chCH]" -o -name "*.cpp" -o -name "*.hpp" \) > freescope.files
	$ freescope

	18.2 source navigator

	18.3 cflow
	$  cflow --main HCAP_InitClient  hcap.cpp

	reverse call graph
	$ cflow --reverse --main HCAP_InitClient  hcap.cpp

	nicer graph
	$ cflow --tree   --brief --main HCAP_InitClient  hcap.cpp

	18.4 Understand.

	18.5 hypersrc, pypersrc
		18.5.1 hypersrc installtion for cygwin.
			-> Install the gnome dev libs.
			-> If you get an error of "GNU make" not defined, modify configure.sh line with uname -a check to treat cygwin like linux.
			-> run make all install. you may get path errors if cygwin install path contains spaces. in that cast modify GNUmakefile
			and wrap $INSTALL_DIR with "".
			-> run hypersrc.pl.
				You may get an error something like hypersrc not in path add it gybrish. 
				Just edit hypersrc.pl and make sure there are no paths that contains spaces (or else "protect" them with \)

		18.5.2 pypersrc










19. Tips and tricks
	19.1 Find all source code files that have a certain pattern ("@author")
	$ grep -sR '@author' *  | awk -F":" '{print $1}' | uniq

	Then check them all out:
	 ct co -nc `grep -sR '@author' *  | awk -F":" '{print $1}' | uniq`

	 19.2 Find all function declarion of a given function: supress grep error messages using -s
	 search function
		 find . \( -name *.cpp -o -name *.h \) -print -exec grep -s -C 5 '\<getValueAsString\>[ \t]*('  '{}' \;  > stam
	 search method
		find . \( -name *.cpp -o -name *.h \) -print -exec grep -s -C 5 '\w\+[ \t]*::[ \t]*\<getValueAsString\>[ \t]*('  '{}' \;  > stam

"
20. Find
The find command (taken from http://dmiessler.com/study/find/)

find is one of the most useful Linux/Unix tools around, but most people use only a fraction of its power. Many Linux/Unix questions seen online can be solved using the find command alone; it's simply a matter of becoming familiar with its options.

    The power of find lets you do anything from finding all your .jpg files to seeing "all of Michael's files that have the execute bit set and have been modified since yesterday." When combined with xargs, a properly wielded find command can make many common tasks ten times easier.


	20.1 Find Basics

Let's start simple and get progressively more advanced. To begin with we'll look at finding things by name. Remember that the first argument you give find is where to look.
Find all files with something in their name:
find . -name "*.jpg"

...
./Pictures/iPhoto Library/Data/2006/Roll 20/00697_bluewaters_1440x900.jpg
./Pictures/iPhoto Library/Data/2006/Roll 20/00705_cloudyday_1440x900.jpg
./Pictures/iPhoto Library/Data/2006/Roll 20/00710_fragile_1600x1200.jpg
./Pictures/iPhoto Library/Data/2006/Roll 20/00713_coolemoticon_1440x900.jpg
./Pictures/iPhoto Library/Data/2006/Roll 20/00714_cloudyday_1440x900.jpg
...

Note that by default when you give a location to start from (in our case "."), the find command starts there and drills all the way down during its search. So in this case I started from my home directory and it found the files all the way down in "~/Pictures/iPhoto Library/Data/2006/Roll 20" as well.

[ Placing quotes around the search criteria avoids issues with wildcard characters and is probably a good habit to get into. You can also use -iname instead of -name; it's the same but it's case insensitive ]
Find all files that belong to a certain user:
find . -user daniel

...
./Music/iTunes/iTunes Music/Tool/Undertow/01 Intolerance.m4a
./Music/iTunes/iTunes Music/Tool/Undertow/02 Prison Sex.m4a
./Music/iTunes/iTunes Music/Tool/Undertow/03 Sober.m4a
./Music/iTunes/iTunes Music/Tool/Undertow/04 Bottom.m4a
./Music/iTunes/iTunes Music/Tool/Undertow/05 Crawl Away.m4a
./Music/iTunes/iTunes Music/Tool/Undertow/06 Swamp Song.m4a
./Music/iTunes/iTunes Music/Tool/Undertow/07 Undertow.m4a
./Music/iTunes/iTunes Music/Tool/Undertow/08 4 Degrees.m4a
./Music/iTunes/iTunes Music/Tool/Undertow/09 Flood.m4a
./Music/iTunes/iTunes Music/Tool/Undertow/69 Disgustipated.m4a
...

[ Also works for groups (-group) ]
Find only directories, regular files, links, or sockets:
find . -type d

...
./Development/envelope
./Development/mhp
./Development/mservers
./Development/mservers/fortune100
./Development/mst
./Development/mst/nmap
./Development/mst/services
...

Those are all directories, and to look for the others (files, links, or sockets), just substitute f, l, s for the d in the command above.
Find files that are over a gigabyte in size
find ~/Movies -size +1024M

...
/Movies/Comedy/Funny.mpg
/Movies/Drama/Sad.mpg
...


	20.2 Combining Arguments

You can also combine arguments using and, or, and not. By default if you use two different arguments you're and'ing them. If you want to use or you give the -o option, and if you want to get everything except something, you use the ! option.
Find only regular files, owned by daniel, that are also jpg images
find . -user daniel -type f -name *.jpg

...
./Pictures/iPhoto Library/autumn_woods.jpg
./Pictures/iPhoto Library/blue_forest.jpg
./Pictures/iPhoto Library/brothers.jpg
...

Now do the same, but exclude anything named autumn
find . -user daniel -type f -name *.jpg ! -name autumn*

...
./Pictures/iPhoto Library/blue_forest.jpg
./Pictures/iPhoto Library/brothers.jpg
...


	20.3 Forensics

find also has a number of options that help one answer forensics-oriented questions such as when a file was last changed or what files have had their permissions modified recently.
Find all files in /etc owned by root that have been modified within the last day
find /etc -user root -mtime -1

...
/etc/passwd
...

The checks you can use here are:

    * -atime: when the file was last accessed
    * -ctime: when the file's permissions were last changed
    * -mtime: when the file's data was last modified

These searches are done in 24 hour increments and followed by a number n. If you want to match the exact 24 hour period you use n by itself. More frequently, however, you'll want to say everything since yesterday, or everything "more than 3 days ago." This is accomplished using the -n and +n options respectively.

There are also minute versions of the atime, ctime, and mtime arguments:

    * -amin: when (in minutes) the file was last accessed
    * -cmin: when (in minutes) the file's permissions were last changed
    * -mmin: when (in minutes) the file's data was last modified

Show me all files in /etc owned by root that have been accessed within the last two minutes
find /etc -user root -amin -2

...
/etc/hosts
/etc/resolv.conf
...

A list of a few other forensics-oriented options:

    * -nouser: shows output that's not associated with an existing userid
    * -nogroup: shows output not associated with an existing groupid
    * -links n: file has n links
    * -newer file: file was modified more recently than file.
    * -perm mode: file has mode permissions.

Show me all files in ~ with wide open permissions
find ~ -perm 777

...
~/testfile.txt
~/lab.rtf
...


	20.4 Combining find With xargs

This is the piece that we've been leading up to -- performing an action on the stuff that we find with find. So while it's interesting to say, "Show me this stuff", it's far more useful to say, "Take every text file owned by Jason that's hasn't been accessed in 60 days and move it to the backup folder."

Cookbook Examples Of find in action

[ Be sure to test these on your system before using them for important files ]
Find all files on your system that are world writable. The 0002 denotes a 2 in the "other" field in the file permissions, which is the write bit
find / -perm -0002

Collect files that are not owned by valid users and delete them
find / -nouser -print0 | xargs -0 rm

Clean the images off of your *nix desktop
find ~/Desktop -name "*.jpg" -o -name "*.gif" -o -name "*.png" -print0 | xargs -0 mv --target-directory ~/Pictures

[ The -print0 option terminates results with a null character instead of the default newline, making it cleaner and less likely to balk in many cases ]
Correct the permissions on your web directory
find /your/webdir/ -type d -print0 | xargs -0 chmod 755
find /your/webdir -type f | xargs chmod 644

Show a list of files in /etc that have been modified since last month
find /etc -mtime -30

	20.5 Find reference

		20.5.1 Find searchable types
       -type c
	      File is of type c:

	      b      block (buffered) special

	      c      character (unbuffered) special

	      d      directory

	      p      named pipe (FIFO)

	      f      regular file

	      l      symbolic link; this is never true if the -L option or the
		     -follow  option is in effect, unless the symbolic link is
		     broken.  If you want to search for symbolic links when -L
		     is in effect, use -xtype.

	      s      socket

	      D      door (Solaris)

		20.5.2


	20.6 A Final Thought

There is a bit of a debate in some circles about using xargs vs. the -exec option that's built into find itself. To me, however, it's not much of a debate; -exec isn't nearly as good as xargs for what I use find for. I tend to use it to do big jobs involving many files. "Move all these files there", "copy all those directories there", "Delete these links.", etc.

    This is where -exec breaks down and xargs stands up. Whe you use -exec you run a seperate instance of the called program for each input. With xargs, you build up the input into bundles and run them through the called command as few times as possible, which is usually just once. When dealing with hundreds or thousands of elements this is a big win for xargs.

Don't believe me? Well, let's run some numbers. Below is a listing of 5,310 .jpg files on my OS X system using both -exec and xargs:
time find . -name "*.jpg" -exec ls {} \;

real    0m23.548s
user    0m3.913s
sys     0m15.167s

Hmm, that's not bad. 23 seconds for over five thousand files, right? Let's try it with xargs.
time find . -name "*.jpg" -print0 | xargs -0 ls

real    0m1.526s
user    0m0.667s
sys     0m0.864s

That's 2 seconds vs 24 seconds. Seriously; find and xargs is the combination you want to use.


	20.7 Combos

		20.7.1 find + tar + for loop
Ex: create a tarball of all xml files in dirs that are enumarated by find
a. create the archive, empty or with one element
$ tar cvf rt_ut_reports_07_10_10.tar ./runtime/infrastructure/configManager/test/cppunit-reports/test-ConfigManager.xml

b. use the combo to add all the xml files:
$ for dir in `find . -name 'cppunit-reports'` ; do tar rvf rt_ut_reports_07_10_10.tar $dir/*.xml ; done

list content:
[yizaq@yizaq-lnx:Thu Oct 07:/view/yizaq1__int.acs5_0.lx/vob/nm_acs/acs]$ for dir in `find . -name 'cppunit-reports'` ; do echo "UT report for $dir"; cat  $dir/*.xml ; done > rt_ut_report_xml 

	20.8 Examples
Find all filew newer than 2 days:
[yizaq@yizaq-WS:Wed Nov 02:/cygdrive/c/work/KB:]$ find -type f -mtime -2
./ACS/kb_acs_5_3
./ACS/kb_acs_5_4
./awk/kb_awk
./iPhoneKB
./linux/KB_Linux

21. Remote connections
	
	21.1 reflection, 
	Allows to connext using xindoes

	21.2 VNC
		First start vncserver, $vncserver
		Then (if first time), $vncpasswd
		set password

		use vnc viewer to connect to server.

		This method is better than reflection since its faster, graphical and keeps the session alive w/o dependency on connection to machine (meaning resume is supported)

	If you are using VNC to access your linux and would like to have clipboard shared between your PC and linux, do the following.
From your linux session run:
    vncconfig &
UltraVNC will then pass clipboard text between you windows and linux.

22. Terminals
	22.1 X terminals xterm

		22.1.1 increase font size
		shift & + (of numpad) to increase
		shift & - (of numpad) to decrease

		22.1.2 Colors, scrollbar
		xterm -sb -bg brown -fg yellow

		And, with large fonts:
		 xterm -font -*-fixed-medium-r-*-*-20-*-*-*-*-*-iso8859-*  -geometry 70x24  -bg brown -fg yellow  -sb 

		22.1.3 select fonts
		$  xfontsel
This will open an X window where you can decide on the font type and size. Once you have decided, note down the values as shown in the xfontsel dialog box. It will look something like this ...

-*-fixed-medium-r-*-*-18-*-*-*-*-*-iso8859-* 

...if you have selected a fixed, medium 18pt regular font with iso8859 support.These values will be used while starting xterm. Also to specify the size and position of the xterm window, you can use the -geometry option. Now to test your settings, execute xterm. I have executed xterm with the following settings:

$ xterm -font -*-fixed-medium-r-*-*-18-*-*-*-*-*-iso8859-*
          -geometry 70x24

You will find that it opens with better font clarity. But it is very tedious to run the above command each time you want to open xterm. So you create an alias for it and enter the following line into your .bashrc or .bash_profile file. I have entered it in the .bashrc file in my home directory.

#File: ~.bashrc
alias xterm='xterm -font -*-fixed-medium-r-*-*-18-*-*-*-*-*-iso8859-* -geometry 70x24'


The next time you type xterm, your system will execute the program with the options in your .bashrc file in your home directory.
There is another more powerful way to achieve the same. That is to pass the parameters to your X server so that it will know how to display your xterm when you execute it. This is as follows:
Create a '.Xresources' file in your home directory and enter the values that you want to set. My .Xresources file is as follows:


# File : .Xresources
xterm*font: -*-fixed-medium-r-*-*-18-*-*-*-*-*-iso8859-*
xterm*font1: -*-*-*-*-*-*-2-*-*-*-*-*-*-*
xterm*font2: -misc-fixed-*-r-normal-*-8-*-*-*-*-*-iso8859-*
xterm*font3: -b&h-lucidatypewriter-bold-*-*-*-12-*-*-*-*-*-*-*
xterm*font4: -*-screen-bold-r-normal-*-16-*-*-*-*-*-iso8859-*
xterm*font5: -*-lucidatypewriter-medium-*-*-*-18-*-*-*-*-*-*-*
xterm*font6: -*-lucidatypewriter-medium-*-*-*-20-*-*-*-*-*-*-*
xterm*font7: -dec-terminal-bold-r-normal-*-14-*-*-*-*-*-iso8859-*

XTerm*background: white
XTerm*foreground: black
XTerm*pointerColor: red
XTerm*pointerColorBackground: black
XTerm*cursorColor: navy
XTerm*internalBorder: 3
XTerm*loginShell: true
XTerm*scrollBar: false
XTerm*scrollKey: true
XTerm*saveLines: 1000
XTerm*multiClickTime: 250

Another method to increase the font size of the xterm is by pressing [Ctrl] key and the right mouse button at the same time while you have focus in xterm window. Then a pop-up menu will come up which can be used to set the font size to your taste. By pressing [Ctrl] key and the middle mouse button, you get a pop-up menu which helps you set/unset a lot of other features of your xterm window like enabling/disabling the scroll bars and so on.
Here I have explained some of the methods by which you can configure xterm to look good. Ofcourse, GNU/Linux is all about the freedom of choice. So xterm is not the only light weight x terminal around. Two other x terminals which are light weight and having even more features like support for transparency that come to mind are eterm and aterm. But the advantage of xterm is that you can be sure of having it in all linux distributions by default where as if you want to use aterm or eterm, you may have to download them from their site and install it in your machine.


		22.1.4 My xterm aliases

#alias xt1_small='xterm -font -*-fixed-bold-r-*-*-12-*-*-*-*-*-iso8859-*  -geometry 140x48 -sb -bg brown -fg yellow '
#alias xt2_small='xterm -font -*-fixed-bold-r-*-*-12-*-*-*-*-*-iso8859-*  -geometry 140x48 -sb -bg blue -fg orange '
#alias xt3_small='xterm -font -*-fixed-bold-r-*-*-12-*-*-*-*-*-iso8859-*  -geometry 140x48 -sb -bg lightyellow -fg purple'
#alias xt4_small='xterm -font -*-fixed-bold-r-*-*-12-*-*-*-*-*-iso8859-*  -geometry 140x48 -sb -bg grey -fg blue' 
alias xt1_small='xterm -font -*-fixed-bold-r-*-*-12-*-*-*-*-*-iso8859-*  -geometry 140x48 -sb -bg AntiqueWhite -fg DarkBlue'
alias xt2_small='xterm -font -*-fixed-bold-r-*-*-12-*-*-*-*-*-iso8859-*  -geometry 140x48 -sb -bg grey0  -fg DarkGoldenrod1'
alias xt3_small='xterm -font -*-fixed-bold-r-*-*-12-*-*-*-*-*-iso8859-*  -geometry 140x48 -sb -bg DarkCyan  -fg DarkOliveGreen2'
alias xt4_small='xterm -font -*-fixed-bold-r-*-*-12-*-*-*-*-*-iso8859-*  -geometry 140x48 -sb -bg sienna  -fg chartreuse2' 

alias xts_small="xt1_small& xt2_small& xt3_small& xt4_small&"

		22.1.5 List of colors
http://mkaz.com/solog/system/xterm-colors.html

Xterm Colors
Date: Apr 4, 2010

An update to my previous xterm colors page, removed duplicates and fixed issues with two word colors. These colors are from the command showrgb and can be used for various X11 things. The primary spot I use them is for coloring the foreground/background in xterm.

Example: xterm -fg LightGoldenrod1 -bg CornflowerBlue

You can View Source or use Firebug or Color Picker to get RGB values.
XTerm RGB Colors



255 250 250		snow
248 248 255		ghost white
248 248 255		GhostWhite
245 245 245		white smoke
245 245 245		WhiteSmoke
220 220 220		gainsboro
255 250 240		floral white
255 250 240		FloralWhite
253 245 230		old lace
253 245 230		OldLace
250 240 230		linen
250 235 215		antique white
250 235 215		AntiqueWhite
255 239 213		papaya whip
255 239 213		PapayaWhip
255 235 205		blanched almond
255 235 205		BlanchedAlmond
255 228 196		bisque
255 218 185		peach puff
255 218 185		PeachPuff
255 222 173		navajo white
255 222 173		NavajoWhite
255 228 181		moccasin
255 248 220		cornsilk
255 255 240		ivory
255 250 205		lemon chiffon
255 250 205		LemonChiffon
255 245 238		seashell
240 255 240		honeydew
245 255 250		mint cream
245 255 250		MintCream
240 255 255		azure
240 248 255		alice blue
240 248 255		AliceBlue
230 230 250		lavender
255 240 245		lavender blush
255 240 245		LavenderBlush
255 228 225		misty rose
255 228 225		MistyRose
255 255 255		white
  0   0   0		black
 47  79  79		dark slate gray
 47  79  79		DarkSlateGray
 47  79  79		dark slate grey
 47  79  79		DarkSlateGrey
105 105 105		dim gray
105 105 105		DimGray
105 105 105		dim grey
105 105 105		DimGrey
112 128 144		slate gray
112 128 144		SlateGray
112 128 144		slate grey
112 128 144		SlateGrey
119 136 153		light slate gray
119 136 153		LightSlateGray
119 136 153		light slate grey
119 136 153		LightSlateGrey
190 190 190		gray
190 190 190		grey
211 211 211		light grey
211 211 211		LightGrey
211 211 211		light gray
211 211 211		LightGray
 25  25 112		midnight blue
 25  25 112		MidnightBlue
  0   0 128		navy
  0   0 128		navy blue
  0   0 128		NavyBlue
100 149 237		cornflower blue
100 149 237		CornflowerBlue
 72  61 139		dark slate blue
 72  61 139		DarkSlateBlue
106  90 205		slate blue
106  90 205		SlateBlue
123 104 238		medium slate blue
123 104 238		MediumSlateBlue
132 112 255		light slate blue
132 112 255		LightSlateBlue
  0   0 205		medium blue
  0   0 205		MediumBlue
 65 105 225		royal blue
 65 105 225		RoyalBlue
  0   0 255		blue
 30 144 255		dodger blue
 30 144 255		DodgerBlue
  0 191 255		deep sky blue
  0 191 255		DeepSkyBlue
135 206 235		sky blue
135 206 235		SkyBlue
135 206 250		light sky blue
135 206 250		LightSkyBlue
 70 130 180		steel blue
 70 130 180		SteelBlue
176 196 222		light steel blue
176 196 222		LightSteelBlue
173 216 230		light blue
173 216 230		LightBlue
176 224 230		powder blue
176 224 230		PowderBlue
175 238 238		pale turquoise
175 238 238		PaleTurquoise
  0 206 209		dark turquoise
  0 206 209		DarkTurquoise
 72 209 204		medium turquoise
 72 209 204		MediumTurquoise
 64 224 208		turquoise
  0 255 255		cyan
224 255 255		light cyan
224 255 255		LightCyan
 95 158 160		cadet blue
 95 158 160		CadetBlue
102 205 170		medium aquamarine
102 205 170		MediumAquamarine
127 255 212		aquamarine
  0 100   0		dark green
  0 100   0		DarkGreen
 85 107  47		dark olive green
 85 107  47		DarkOliveGreen
143 188 143		dark sea green
143 188 143		DarkSeaGreen
 46 139  87		sea green
 46 139  87		SeaGreen
 60 179 113		medium sea green
 60 179 113		MediumSeaGreen
 32 178 170		light sea green
 32 178 170		LightSeaGreen
152 251 152		pale green
152 251 152		PaleGreen
  0 255 127		spring green
  0 255 127		SpringGreen
124 252   0		lawn green
124 252   0		LawnGreen
  0 255   0		green
127 255   0		chartreuse
  0 250 154		medium spring green
  0 250 154		MediumSpringGreen
173 255  47		green yellow
173 255  47		GreenYellow
 50 205  50		lime green
 50 205  50		LimeGreen
154 205  50		yellow green
154 205  50		YellowGreen
 34 139  34		forest green
 34 139  34		ForestGreen
107 142  35		olive drab
107 142  35		OliveDrab
189 183 107		dark khaki
189 183 107		DarkKhaki
240 230 140		khaki
238 232 170		pale goldenrod
238 232 170		PaleGoldenrod
250 250 210		light goldenrod yellow
250 250 210		LightGoldenrodYellow
255 255 224		light yellow
255 255 224		LightYellow
255 255   0		yellow
255 215   0		gold
238 221 130		light goldenrod
238 221 130		LightGoldenrod
218 165  32		goldenrod
184 134  11		dark goldenrod
184 134  11		DarkGoldenrod
188 143 143		rosy brown
188 143 143		RosyBrown
205  92  92		indian red
205  92  92		IndianRed
139  69  19		saddle brown
139  69  19		SaddleBrown
160  82  45		sienna
205 133  63		peru
222 184 135		burlywood
245 245 220		beige
245 222 179		wheat
244 164  96		sandy brown
244 164  96		SandyBrown
210 180 140		tan
210 105  30		chocolate
178  34  34		firebrick
165  42  42		brown
233 150 122		dark salmon
233 150 122		DarkSalmon
250 128 114		salmon
255 160 122		light salmon
255 160 122		LightSalmon
255 165   0		orange
255 140   0		dark orange
255 140   0		DarkOrange
255 127  80		coral
240 128 128		light coral
240 128 128		LightCoral
255  99  71		tomato
255  69   0		orange red
255  69   0		OrangeRed
255   0   0		red
255 105 180		hot pink
255 105 180		HotPink
255  20 147		deep pink
255  20 147		DeepPink
255 192 203		pink
255 182 193		light pink
255 182 193		LightPink
219 112 147		pale violet red
219 112 147		PaleVioletRed
176  48  96		maroon
199  21 133		medium violet red
199  21 133		MediumVioletRed
208  32 144		violet red
208  32 144		VioletRed
255   0 255		magenta
238 130 238		violet
221 160 221		plum
218 112 214		orchid
186  85 211		medium orchid
186  85 211		MediumOrchid
153  50 204		dark orchid
153  50 204		DarkOrchid
148   0 211		dark violet
148   0 211		DarkViolet
138  43 226		blue violet
138  43 226		BlueViolet
160  32 240		purple
147 112 219		medium purple
147 112 219		MediumPurple
216 191 216		thistle
255 250 250		snow1
238 233 233		snow2
205 201 201		snow3
139 137 137		snow4
255 245 238		seashell1
238 229 222		seashell2
205 197 191		seashell3
139 134 130		seashell4
255 239 219		AntiqueWhite1
238 223 204		AntiqueWhite2
205 192 176		AntiqueWhite3
139 131 120		AntiqueWhite4
255 228 196		bisque1
238 213 183		bisque2
205 183 158		bisque3
139 125 107		bisque4
255 218 185		PeachPuff1
238 203 173		PeachPuff2
205 175 149		PeachPuff3
139 119 101		PeachPuff4
255 222 173		NavajoWhite1
238 207 161		NavajoWhite2
205 179 139		NavajoWhite3
139 121  94		NavajoWhite4
255 250 205		LemonChiffon1
238 233 191		LemonChiffon2
205 201 165		LemonChiffon3
139 137 112		LemonChiffon4
255 248 220		cornsilk1
238 232 205		cornsilk2
205 200 177		cornsilk3
139 136 120		cornsilk4
255 255 240		ivory1
238 238 224		ivory2
205 205 193		ivory3
139 139 131		ivory4
240 255 240		honeydew1
224 238 224		honeydew2
193 205 193		honeydew3
131 139 131		honeydew4
255 240 245		LavenderBlush1
238 224 229		LavenderBlush2
205 193 197		LavenderBlush3
139 131 134		LavenderBlush4
255 228 225		MistyRose1
238 213 210		MistyRose2
205 183 181		MistyRose3
139 125 123		MistyRose4
240 255 255		azure1
224 238 238		azure2
193 205 205		azure3
131 139 139		azure4
131 111 255		SlateBlue1
122 103 238		SlateBlue2
105  89 205		SlateBlue3
 71  60 139		SlateBlue4
 72 118 255		RoyalBlue1
 67 110 238		RoyalBlue2
 58  95 205		RoyalBlue3
 39  64 139		RoyalBlue4
  0   0 255		blue1
  0   0 238		blue2
  0   0 205		blue3
  0   0 139		blue4
 30 144 255		DodgerBlue1
 28 134 238		DodgerBlue2
 24 116 205		DodgerBlue3
 16  78 139		DodgerBlue4
 99 184 255		SteelBlue1
 92 172 238		SteelBlue2
 79 148 205		SteelBlue3
 54 100 139		SteelBlue4
  0 191 255		DeepSkyBlue1
  0 178 238		DeepSkyBlue2
  0 154 205		DeepSkyBlue3
  0 104 139		DeepSkyBlue4
135 206 255		SkyBlue1
126 192 238		SkyBlue2
108 166 205		SkyBlue3
 74 112 139		SkyBlue4
176 226 255		LightSkyBlue1
164 211 238		LightSkyBlue2
141 182 205		LightSkyBlue3
 96 123 139		LightSkyBlue4
198 226 255		SlateGray1
185 211 238		SlateGray2
159 182 205		SlateGray3
108 123 139		SlateGray4
202 225 255		LightSteelBlue1
188 210 238		LightSteelBlue2
162 181 205		LightSteelBlue3
110 123 139		LightSteelBlue4
191 239 255		LightBlue1
178 223 238		LightBlue2
154 192 205		LightBlue3
104 131 139		LightBlue4
224 255 255		LightCyan1
209 238 238		LightCyan2
180 205 205		LightCyan3
122 139 139		LightCyan4
187 255 255		PaleTurquoise1
174 238 238		PaleTurquoise2
150 205 205		PaleTurquoise3
102 139 139		PaleTurquoise4
152 245 255		CadetBlue1
142 229 238		CadetBlue2
122 197 205		CadetBlue3
 83 134 139		CadetBlue4
  0 245 255		turquoise1
  0 229 238		turquoise2
  0 197 205		turquoise3
  0 134 139		turquoise4
  0 255 255		cyan1
  0 238 238		cyan2
  0 205 205		cyan3
  0 139 139		cyan4
151 255 255		DarkSlateGray1
141 238 238		DarkSlateGray2
121 205 205		DarkSlateGray3
 82 139 139		DarkSlateGray4
127 255 212		aquamarine1
118 238 198		aquamarine2
102 205 170		aquamarine3
 69 139 116		aquamarine4
193 255 193		DarkSeaGreen1
180 238 180		DarkSeaGreen2
155 205 155		DarkSeaGreen3
105 139 105		DarkSeaGreen4
 84 255 159		SeaGreen1
 78 238 148		SeaGreen2
 67 205 128		SeaGreen3
 46 139  87		SeaGreen4
154 255 154		PaleGreen1
144 238 144		PaleGreen2
124 205 124		PaleGreen3
 84 139  84		PaleGreen4
  0 255 127		SpringGreen1
  0 238 118		SpringGreen2
  0 205 102		SpringGreen3
  0 139  69		SpringGreen4
  0 255   0		green1
  0 238   0		green2
  0 205   0		green3
  0 139   0		green4
127 255   0		chartreuse1
118 238   0		chartreuse2
102 205   0		chartreuse3
 69 139   0		chartreuse4
192 255  62		OliveDrab1
179 238  58		OliveDrab2
154 205  50		OliveDrab3
105 139  34		OliveDrab4
202 255 112		DarkOliveGreen1
188 238 104		DarkOliveGreen2
162 205  90		DarkOliveGreen3
110 139  61		DarkOliveGreen4
255 246 143		khaki1
238 230 133		khaki2
205 198 115		khaki3
139 134  78		khaki4
255 236 139		LightGoldenrod1
238 220 130		LightGoldenrod2
205 190 112		LightGoldenrod3
139 129  76		LightGoldenrod4
255 255 224		LightYellow1
238 238 209		LightYellow2
205 205 180		LightYellow3
139 139 122		LightYellow4
255 255   0		yellow1
238 238   0		yellow2
205 205   0		yellow3
139 139   0		yellow4
255 215   0		gold1
238 201   0		gold2
205 173   0		gold3
139 117   0		gold4
255 193  37		goldenrod1
238 180  34		goldenrod2
205 155  29		goldenrod3
139 105  20		goldenrod4
255 185  15		DarkGoldenrod1
238 173  14		DarkGoldenrod2
205 149  12		DarkGoldenrod3
139 101   8		DarkGoldenrod4
255 193 193		RosyBrown1
238 180 180		RosyBrown2
205 155 155		RosyBrown3
139 105 105		RosyBrown4
255 106 106		IndianRed1
238  99  99		IndianRed2
205  85  85		IndianRed3
139  58  58		IndianRed4
255 130  71		sienna1
238 121  66		sienna2
205 104  57		sienna3
139  71  38		sienna4
255 211 155		burlywood1
238 197 145		burlywood2
205 170 125		burlywood3
139 115  85		burlywood4
255 231 186		wheat1
238 216 174		wheat2
205 186 150		wheat3
139 126 102		wheat4
255 165  79		tan1
238 154  73		tan2
205 133  63		tan3
139  90  43		tan4
255 127  36		chocolate1
238 118  33		chocolate2
205 102  29		chocolate3
139  69  19		chocolate4
255  48  48		firebrick1
238  44  44		firebrick2
205  38  38		firebrick3
139  26  26		firebrick4
255  64  64		brown1
238  59  59		brown2
205  51  51		brown3
139  35  35		brown4
255 140 105		salmon1
238 130  98		salmon2
205 112  84		salmon3
139  76  57		salmon4
255 160 122		LightSalmon1
238 149 114		LightSalmon2
205 129  98		LightSalmon3
139  87  66		LightSalmon4
255 165   0		orange1
238 154   0		orange2
205 133   0		orange3
139  90   0		orange4
255 127   0		DarkOrange1
238 118   0		DarkOrange2
205 102   0		DarkOrange3
139  69   0		DarkOrange4
255 114  86		coral1
238 106  80		coral2
205  91  69		coral3
139  62  47		coral4
255  99  71		tomato1
238  92  66		tomato2
205  79  57		tomato3
139  54  38		tomato4
255  69   0		OrangeRed1
238  64   0		OrangeRed2
205  55   0		OrangeRed3
139  37   0		OrangeRed4
255   0   0		red1
238   0   0		red2
205   0   0		red3
139   0   0		red4
255  20 147		DeepPink1
238  18 137		DeepPink2
205  16 118		DeepPink3
139  10  80		DeepPink4
255 110 180		HotPink1
238 106 167		HotPink2
205  96 144		HotPink3
139  58  98		HotPink4
255 181 197		pink1
238 169 184		pink2
205 145 158		pink3
139  99 108		pink4
255 174 185		LightPink1
238 162 173		LightPink2
205 140 149		LightPink3
139  95 101		LightPink4
255 130 171		PaleVioletRed1
238 121 159		PaleVioletRed2
205 104 137		PaleVioletRed3
139  71  93		PaleVioletRed4
255  52 179		maroon1
238  48 167		maroon2
205  41 144		maroon3
139  28  98		maroon4
255  62 150		VioletRed1
238  58 140		VioletRed2
205  50 120		VioletRed3
139  34  82		VioletRed4
255   0 255		magenta1
238   0 238		magenta2
205   0 205		magenta3
139   0 139		magenta4
255 131 250		orchid1
238 122 233		orchid2
205 105 201		orchid3
139  71 137		orchid4
255 187 255		plum1
238 174 238		plum2
205 150 205		plum3
139 102 139		plum4
224 102 255		MediumOrchid1
209  95 238		MediumOrchid2
180  82 205		MediumOrchid3
122  55 139		MediumOrchid4
191  62 255		DarkOrchid1
178  58 238		DarkOrchid2
154  50 205		DarkOrchid3
104  34 139		DarkOrchid4
155  48 255		purple1
145  44 238		purple2
125  38 205		purple3
 85  26 139		purple4
171 130 255		MediumPurple1
159 121 238		MediumPurple2
137 104 205		MediumPurple3
 93  71 139		MediumPurple4
255 225 255		thistle1
238 210 238		thistle2
205 181 205		thistle3
139 123 139		thistle4
  0   0   0		gray0
  0   0   0		grey0
  3   3   3		gray1
  3   3   3		grey1
  5   5   5		gray2
  5   5   5		grey2
  8   8   8		gray3
  8   8   8		grey3
 10  10  10		gray4
 10  10  10		grey4
 13  13  13		gray5
 13  13  13		grey5
 15  15  15		gray6
 15  15  15		grey6
 18  18  18		gray7
 18  18  18		grey7
 20  20  20		gray8
 20  20  20		grey8
 23  23  23		gray9
 23  23  23		grey9
 26  26  26		gray10
 26  26  26		grey10
 28  28  28		gray11
 28  28  28		grey11
 31  31  31		gray12
 31  31  31		grey12
 33  33  33		gray13
 33  33  33		grey13
 36  36  36		gray14
 36  36  36		grey14
 38  38  38		gray15
 38  38  38		grey15
 41  41  41		gray16
 41  41  41		grey16
 43  43  43		gray17
 43  43  43		grey17
 46  46  46		gray18
 46  46  46		grey18
 48  48  48		gray19
 48  48  48		grey19
 51  51  51		gray20
 51  51  51		grey20
 54  54  54		gray21
 54  54  54		grey21
 56  56  56		gray22
 56  56  56		grey22
 59  59  59		gray23
 59  59  59		grey23
 61  61  61		gray24
 61  61  61		grey24
 64  64  64		gray25
 64  64  64		grey25
 66  66  66		gray26
 66  66  66		grey26
 69  69  69		gray27
 69  69  69		grey27
 71  71  71		gray28
 71  71  71		grey28
 74  74  74		gray29
 74  74  74		grey29
 77  77  77		gray30
 77  77  77		grey30
 79  79  79		gray31
 79  79  79		grey31
 82  82  82		gray32
 82  82  82		grey32
 84  84  84		gray33
 84  84  84		grey33
 87  87  87		gray34
 87  87  87		grey34
 89  89  89		gray35
 89  89  89		grey35
 92  92  92		gray36
 92  92  92		grey36
 94  94  94		gray37
 94  94  94		grey37
 97  97  97		gray38
 97  97  97		grey38
 99  99  99		gray39
 99  99  99		grey39
102 102 102		gray40
102 102 102		grey40
105 105 105		gray41
105 105 105		grey41
107 107 107		gray42
107 107 107		grey42
110 110 110		gray43
110 110 110		grey43
112 112 112		gray44
112 112 112		grey44
115 115 115		gray45
115 115 115		grey45
117 117 117		gray46
117 117 117		grey46
120 120 120		gray47
120 120 120		grey47
122 122 122		gray48
122 122 122		grey48
125 125 125		gray49
125 125 125		grey49
127 127 127		gray50
127 127 127		grey50
130 130 130		gray51
130 130 130		grey51
133 133 133		gray52
133 133 133		grey52
135 135 135		gray53
135 135 135		grey53
138 138 138		gray54
138 138 138		grey54
140 140 140		gray55
140 140 140		grey55
143 143 143		gray56
143 143 143		grey56
145 145 145		gray57
145 145 145		grey57
148 148 148		gray58
148 148 148		grey58
150 150 150		gray59
150 150 150		grey59
153 153 153		gray60
153 153 153		grey60
156 156 156		gray61
156 156 156		grey61
158 158 158		gray62
158 158 158		grey62
161 161 161		gray63
161 161 161		grey63
163 163 163		gray64
163 163 163		grey64
166 166 166		gray65
166 166 166		grey65
168 168 168		gray66
168 168 168		grey66
171 171 171		gray67
171 171 171		grey67
173 173 173		gray68
173 173 173		grey68
176 176 176		gray69
176 176 176		grey69
179 179 179		gray70
179 179 179		grey70
181 181 181		gray71
181 181 181		grey71
184 184 184		gray72
184 184 184		grey72
186 186 186		gray73
186 186 186		grey73
189 189 189		gray74
189 189 189		grey74
191 191 191		gray75
191 191 191		grey75
194 194 194		gray76
194 194 194		grey76
196 196 196		gray77
196 196 196		grey77
199 199 199		gray78
199 199 199		grey78
201 201 201		gray79
201 201 201		grey79
204 204 204		gray80
204 204 204		grey80
207 207 207		gray81
207 207 207		grey81
209 209 209		gray82
209 209 209		grey82
212 212 212		gray83
212 212 212		grey83
214 214 214		gray84
214 214 214		grey84
217 217 217		gray85
217 217 217		grey85
219 219 219		gray86
219 219 219		grey86
222 222 222		gray87
222 222 222		grey87
224 224 224		gray88
224 224 224		grey88
227 227 227		gray89
227 227 227		grey89
229 229 229		gray90
229 229 229		grey90
232 232 232		gray91
232 232 232		grey91
235 235 235		gray92
235 235 235		grey92
237 237 237		gray93
237 237 237		grey93
240 240 240		gray94
240 240 240		grey94
242 242 242		gray95
242 242 242		grey95
245 245 245		gray96
245 245 245		grey96
247 247 247		gray97
247 247 247		grey97
250 250 250		gray98
250 250 250		grey98
252 252 252		gray99
252 252 252		grey99
255 255 255		gray100
255 255 255		grey100
169 169 169		dark grey
169 169 169		DarkGrey
169 169 169		dark gray
169 169 169		DarkGray
  0   0 139		dark blue
  0   0 139		DarkBlue
  0 139 139		dark cyan
  0 139 139		DarkCyan
139   0 139		dark magenta
139   0 139		DarkMagenta
139   0   0		dark red
139   0   0		DarkRed
144 238 144		light green
144 238 144		LightGreen


		22.1.6 screen command
http://mkaz.com/solog/system/unix-screen.html

Unix Screen Utility
Date: May 1, 2008

screen is a window manager for the terminal. Similar in concept to tabbed browsing in Mozilla, but without the GUI. It allows you to have multiple terminal sessions open while only taking up one on your desktop. Also, the screen utility allows to detach and run in the background. So you could exit the terminal; log back in; and reattach exactly how you left it, running processes and all. Great for remote server work.
Basics

Start a screen: $ screen

Once youre in a screen, you can issue commands using ctrl-a (default), or you can map another key. I use ctrl-a often in the bash shell so I map ctrl-w as my screen command key.

Create ~/.screenrc

  startup_message off
  escape "^Ww"
  defscrollback 5000

Commands
ctrl-w ctrl-c 	Create New Window
ctrl-w ctrl-n 	Next Window
ctrl-w ctrl-p 	Previous Window
ctrl-w 0-9 	Switch to Window #
ctrl-w d 	Detach from current screen (it's still there)
Detach and Reattach

You can detach from a screen session using ctrl-w d ; All your screens will still exist, with all running processes. You can even close your terminal sesssion, ie. disconnect if you were logged in remotely.

You can reattach using: $ screen -r

After reattaching, everything will be as you left it. All screens available and running processes.
Scrolling

Screen is its own window manager and operateas outside of the system windowing, which makes scrolling a bit more difficult. First make sure you define a good chunk of lines to scrollback in your screenrc. Using defscrollback 5000

Next to enter scrollback mode use ctrl-w [ and then to control scrolling it is standard vim commands.

  h,j,k,l - cursor movement
  ctrl-b  - back a page
  ctrl-f  - forward a ag
  /       - search forward
  ?       - search backward

Split Windows
ctrl-w S 	Split screen horizontally
ctrl-w | 	Split screen vertically
ctrl-w tab 	Navigate between splits
ctrl-w X 	Remove current split
ctrl-w Q 	Remove all other splits
ctrl-w :resize 	Resize Screen (prompts for lines)
Tips and Tricks

Sharing Interactive Session

You can use screen to share interactive sessions, allowing people to see and type in the same shared terminal. This could be a nice way to do XP pair-programming remotely.

Try the following:

    Start up to two terminal sessions, A and B, via ssh, telnet or even local.

    In Terminal-A, start up screen: $ screen

    In Terminal-B, connect to screen already established: $ screen -x

    Now whatever you type in either terminal will show up on both. They are actual the same screen shared.

This sharing can be done a few different ways with multiple people. At work, since most of us have admin rights, we simply su to the other person and connect to the screen as them.

Another way is to set the permissions on your pts/tty session, location is usually in /dev somewhere depending on your unix environment. Or use $ mesg y to set your tty session as writable.

		22.1.7
	22.2 E terminals (Linux only)
	Eterm

	22.3 A term (aterm)

23 BASH

	23.1 Useful tools

		23.1.1 rlogin
Don't have a terminal with multiple windows or another terminal close to your desk? You can get the same result by using rlogin or telnet (Section 1.21) to log in to your host again from the same terminal. What I mean is:

    somehost% vi .cshrc
       ...Make edits to the file...
    somehost% rlogin localhost
       ...Logs you in to your same account...
    An error message
    somehost% logout
    Connection closed.
    somehost% vi .cshrc
       ...Edit to fix mistake...

If you don't have rlogin or telnet, the command su - username, where username is your username, will do almost the same thing. Or, if you're testing your login shell configuration, login will do as well. 

	23.2 Bash's Configuration Files
	In your home directory, 3 files have a special meaning to Bash, allowing you to set up your environment automatically when you log in and when you invoke another Bash shell, and allow you to execute commands when you log out.
These files may exist in your home directory, but that depends largely on the Linux distro you're using and how your sysadmin (if not you) has set up your account. If they're missing, Bash defaults to /etc/profile.
You can easily create these files yourself using your favorite texteditor. They are:

    * .bash_profile : read and the commands in it executed by Bash every time you log in to the system
    * .bashrc : read and executed by Bash every time you start a subshell
    * .bash_logout : read and executed by Bash every time a login shell exits 

Bash allows 2 synonyms for .bash_profile : .bash_login and .profile. These are derived from the C shell's file named .login and from the Bourne shell and Korn shell files named .profile. Only one of these files is read when you log in. If .bash_profile isn't there, Bash will look for .bash_login. If that is missing too, it will look for .profile.
.bash_profile is read and executed only when you start a login shell (that is, when you log in to the system). If you start a subshell (a new shell) by typing bash at the command prompt, it will read commands from .bashrc. This allows you to separate commands needed at login from those needed when invoking a subshell.
However, most people want to have the same commands run regardless of whether it is a login shell or a subshell. This can be done by using the source command from within .bash_profile to execute .bashrc. You would then simply place all the commands in .bashrc.

These files are useful for automatically executing commands like: set, alias, unalias, and setting the PS(1-4) variables, which can all be used to modify your bash environment.

Also use the source command to apply the changes that you have just made in a configuration file. For example if you add an alias to /etc/profile to apply the changes to your current session execute:

      $ source /etc/profile

		23.2.1 Modifying the Bash Shell with the set Command 
		Two options that can be set using the set command that will be of some interest to the common user are "-o vi" and "-o emacs". As with all of the environment modifying commands these can be typed at the command prompt or inserted into the appropriate file mentioned above.

      Set Emacs Mode in Bash
            $ set -o emacs 

      This is usually the default editing mode when in the bash environment and means that you are able to use commands like those in Emacs (defined in the Readline library) to move the cursor, cut and paste text, or undo editing.

            Commands to take advantage of bash's Emacs Mode:
            ctrl-a	Move cursor to beginning of line
            ctrl-e	Move cursor to end of line
            meta-b	Move cursor back one word
            meta-f	Move cursor forward one word
            ctrl-w	Cut the last word
            ctrl-u	Cut everything before the cursor 
            ctrl-k	Cut everything after the cursor
            ctrl-y	Paste the last thing to be cut
            ctrl-_	Undo

            NOTE: ctrl- = hold control, meta- = hold meta (where meta is usually the alt or escape key).

      A combination of ctrl-u to cut the line combined with ctrl-y can be very helpful. If you are in middle of typing a command and need to return to the prompt to retrieve more information you can use ctrl-u to save what you have typed in and after you retrieve the needed information ctrl-y will recover what was cut.
      Set Vi Mode in Bash
            $ set -o vi 
	    and for tab completion to work also add
	    bind '"\t":menu-complete'

      Vi mode allows for the use of vi like commands when at the bash prompt. When set to this mode initially you will be in insert mode (be able to type at the prompt unlike when you enter vi). Hitting the escape key takes you into command mode.

            Commands to take advantage of bash's Vi Mode:
            h	Move cursor left
            l	Move cursor right
            A	Move cursor to end of line and put in insert mode
            0	(zero) Move cursor to beginning of line (doesn't put in insert mode) 
            i	Put into insert mode at current position
            a	Put into insert mode after current position
            dd	Delete line (saved for pasting)
            D	Delete text after current cursor position (saved for pasting)
            p	Paste text that was deleted
            j	Move up through history commands
            k	Move down through history commands
            u	Undo

		23.2.2 Useful Commands and Features
		The commands in this section are non-mode specific, unlike the ones listed above.

      Flip the Last Two Characters

      If you type like me your fingers spit characters out in the wrong order on occasion. ctrl-t swaps the order that the last two character appear in.
			23.2.2.1 Searching Bash History

      As you enter commands at the CLI they are saved in a file ~./.bash_history. From the bash prompt you can browse the most recently used commands through the least recently used commands by pressing the up arrow. Pressing the down arrow does the opposite.

      If you have entered a command a long time ago and need to execute it again you can search for it. Type the command 'ctrl-r' and enter the text you want to search for.
      Dealing with Spaces

      First, I will mention a few ways to deal with spaces in directory names, file names, and everywhere else.
            Using the Backslash Escape Sequence

            One option is to use bash's escape character \. Any space following the backslash is treated as being part of the same string. These commands create a directory called "foo bar" and then remove it.
                  $ mkdir foo\ bar
                  $ rm foo\ bar 

            The backslash escape sequence can also be used to decode commands embedded in strings which can be very useful for scripting or modifying the command prompt as discussed later.
            Using Single/Double Quotes with Spaces and Variables

            Single and double quotes can also be used for dealing with spaces.
                  $ touch 'dog poo'
                  $ rm "dog poo" 

            The difference between single and double quotes being that in double quotes the $, \, and ' characters still preserve their special meanings. Single quotes will take the $ and \ literally and regard the ' as the end of the string. Here's an example:
                  $ MY_VAR='This is my text'
                  $ echo $MY_VAR
                  This is my text
                  $ echo "$MY_VAR"
                  This is my text
                  $ echo '$MY_VAR'
                  $MY_VAR 

            The string following the $ character is interpreted as being a variable except when enclosed in single quotes as shown above.

			23.2.2.2 Lists Using { and }

The characters { and } allow for list creation. In other words you can have a command be executed on each item in the list. This is perhaps best explained with examples:

      $ touch {temp1,temp2,temp3,temp4} 

This will create/modify the files temp1, temp2, temp3, and temp4 and as in the example above when the files share common parts of the name you can do:

      $ mv temp{1,2,3,4} ./foo\ bar/ 

This will move all four of the files into a directory 'foo bar'.

			23.2.2.3 Executing Multiple Commands in Sequence

This is a hefty title for a simple task. Consider that you want to run three commands, one right after the other, and you do not want to wait for each to finish before typing the next. You can type all three commands on a line and then start the process:

      $ ./configure; make; make install
      OR
      $ ./configure && make && make install 

With the first if the ./configure fails the other two commands will continue to execute. With the second the commands following the && will only execute if the command previous finishes without error. Thus, the second would be most useful for this example because there is no reason to run 'make' or 'make install' if the configuration fails.

			23.2.2.4 Piping Output from One Command to Another

Piping allows the user to do several fantastic thing by combining utilities. I will cover only very basic uses for piping. I most commonly use the pipe command, |, to pipe text that is output from one command through the grep command to search for text.

      Examples:
      See if a program, centericq, is running:
            $ ps ax | grep centericq
            25824 pts/2 S 0:18 centericq Count the number of files in a directory (nl counts things):
            $ ls | nl
            1 #.emacs#
            2 BitchX
            3 Outcast double cd.lst
            4 bm.shader
            5 bmtexturesbase.pk3
      If my memory serves using RPM to check if a package is installed:
            $ rpm -qa | grep package_name A more advance example:
            $ cat /etc/passwd | awk -F: '{print $1 "\t" $6}' | sort > ./users This sequence takes the information if the file passwd, pipes it to awk, which takes the first and sixth fields (the user name and home directory respectively), pipes these fields separated by a tab ("\t") to sort, which sorts the list alphabetically, and puts it into a file called users.

	    23.3 Aliases
	    If you have used UNIX for a while, you will know that there are many commands available and that some of them have very cryptic names and/or can be invoked with a truckload of options and arguments. So, it would be nice to have a feature allowing you to rename these commands or type something simple instead of a list of options. Bash provides such a feature : the  alias .
Aliasses can be defined on the command line, in .bash_profile, or in .bashrc, using this form :
alias name=command
This means that name is an alias for command. Whenever name is typed as a command, Bash will substitute command in its place. Note that there are no spaces on either side of the equal sign. Quotes around command are necessary if the string being aliassed consists of more than one word. A few examples :
alias ls='ls -aF --color=always' alias ll='ls -l' alias search=grep alias mcd='mount /mnt/cdrom' alias ucd='umount /mnt/cdrom' alias mc='mc -c' alias ..='cd ..' alias ...='cd ../..'
The first example ensures that ls always uses color if available, that dotfiles are listed as well,that directories are marked with a / and executables with a *. To make ls do the same on FreeBSD, the alias would become :
alias ls='/bin/ls -aFG'
To see what aliasses are currently active, simply type alias at the command prompt and all active aliasses will be listed. To "disable" an alias type unalias followed by the alias name.

	23.4 Altering the Command Prompt Look and Information

Bash has the ability to change how the command prompt is displayed in information as well as colour. This is done by setting the PS1 variable. There is also a PS2 variable. It controls what is displayed after a second line of prompt is added and is usually by default '> '. The PS1 variable is usually set to show some useful information by the Linux distribution you are running but you may want to earn style points by doing your own modifications.

Here are the backslash-escape special characters that have meaning to bash:

       \a     an ASCII bell character (07)
       \d     the date  in  "Weekday  Month  Date"  format
              (e.g., "Tue May 26")
       \e     an ASCII escape character (033)
       \h     the hostname up to the first `.'
       \H     the hostname
       \j     the  number of jobs currently managed by the shell
       \l     the basename of the shell's terminal device name
       \n     newline
       \r     carriage return
       \s     the  name  of  the shell, the basename of $0
              (the portion following the final slash)
       \t     the current time in 24-hour HH:MM:SS format
       \T     the current time in 12-hour HH:MM:SS format
       \@     the current time in 12-hour am/pm format
       \u     the username of the current user
       \v     the version of bash (e.g., 2.00)
       \V     the release of bash,  version  +  patchlevel
              (e.g., 2.00.0)
       \w     the current working directory
       \W     the  basename  of the current working direcory
       \!     the history number of this command
       \#     the command number of this command
       \$     if the effective UID is 0, a #, otherwise a $
       \nnn   the character corresponding to the octal number nnn
       \\     a backslash
       \[     begin a sequence of non-printing characters,
              which could be used to embed a terminal control
              sequence into the prompt
       \]     end a sequence of non-printing characters

Colours In Bash:

      Black       0;30     Dark Gray     1;30
      Blue        0;34     Light Blue    1;34
      Green       0;32     Light Green   1;32
      Cyan        0;36     Light Cyan    1;36
      Red         0;31     Light Red     1;31
      Purple      0;35     Light Purple  1;35
      Brown       0;33     Yellow        1;33
      Light Gray  0;37     White         1;37

Here is an example borrowed from the Bash-Prompt-HOWTO:

      PS1="\[\033[1;34m\][\$(date +%H%M)][\u@\h:\w]$\[\033[0m\] " 

This turns the text blue, displays the time in brackets (very useful for not losing track of time while working), and displays the user name, host, and current directory enclosed in brackets. The "\[\033[0m\]" following the $ returns the colour to the previous foreground colour.

How about command prompt modification thats a bit more "pretty":

      PS1="\[\033[1;30m\][\[\033[1;34m\]\u\[\033[1;30m\]@\[\033[0;35m\]\h\[\033[1;30m\]] \[\033[0;37m\]\W \[\033[1;30m\]\$\[\033[0m\] " 

This one sets up a prompt like this: [user@host] directory $

Break down:

      \[\033[1;30m\] - Sets the color for the characters that follow it. Here 1;30 will set them to Dark Gray.
      \u \h \W \$ - Look to the table above
      \[\033[0m\] - Sets the colours back to how they were originally.

Each user on a system can have their own customized prompt by setting the PS1 variable in either the .bashrc or .profile files located in their home directories.

FUN STUFF!

A quick note about bashish. It allows for adding themes to a terminal running under a GUI. Check out the site for some screen-shots of what it can do.

	23.5 CDargs - Shell Bookmarks

Impress your friends and colleagues with lightening fast directory switching using the CDargs bookmarking tool. CDargs is not exclusive to BASH, but is a great addition and works on *nix based systems, including OS X. Download CDargs here in source or rpm.

CDargs allow for setting named marks in directories and moving to them quickly using the cdb command or a ncurses view.
Install

    * Compile / install source
    * Move cdargs-bash.sh to /etc
    * Add this line to your users .bashrc file

            source /etc/cdargs-bash.sh 

    * Relogin or run source ~/.bashrc

Usage

      mark

      Mark a directory that you want to get to quickly in the future. Move to the desired directory and type mark <name> or simply mark to have it take the name of the current directory. You can also mark a directory using the ncurses tool. Run cdargs or cdb to start the ncurses tool. Add a new mark by pressing a.

      cdb

      Now you have a bunch of marked directories. Simply type cdb <name of a mark> to move to the marked directory. Alternatively use cdb and navigate with arrows or number to the desired mark.

      manage

      Start the ncurses tool cdb. Some useful keys to thump:

            a                   add new mark
            d                   delete mark
            e                   edit mark
            right left arrows   move in and out of directories
            l                   list the files in the highlighted directory
            c                   make a copy of a mark
            enter               go to selected directory / mark

      You can also edit the ~/.cdargs text file directly to manage marks


      23.6 Basic and Extended Bash Completion

Basic Bash Completion will work in any bash shell. It allows for completion of:

|   1. File Names
|   2. Directory Names
|   3. Executable Names
|   4. User Names (when they are prefixed with a ~)
|   5. Host Names (when they are prefixed with a @)
|   6. Variable Names (when they are prefixed with a $) 

This is done simply by pressing the tab key after enough of the word you are trying to complete has been typed in. If when hitting tab the word is not completed there are probably multiple possibilities for the completion. Press tab again and it will list the possibilities. Sometimes on my machine I have to hit it a third time.

Extended Programmable Bash Completion is a program that you can install to complete much more than the names of the things listed above. With extended bash completion you can, for example, complete the name of a computer you are trying to connect to with ssh or scp. It achieves this by looking through the known_hosts file and using the hosts listed there for the completion. This is greatly customizable and the package and more information can be found here.

Configuration of Programmable Bash Completion is done in /etc/bash_completion. Here is a list of completions that are in my bash_completion file by default.

	23.7 History
	
		23.7.1 Bang Bang and history

Everyone knows about bash history, right? You'd be surprised. Most modern distributions come with bash history enabled and working. If you've never done so before, try using the up and down arrow keys to scroll through your command history. The up arrow will cycle through your command history from newest to oldest, and the down arrow does, well, the opposite.

As luck would have it, different terminals handle arrow keys differently, so the brilliant minds behind bash came up with additional methods for accessing and making use of the command history. We'll start with history. This command simply gives you a numbered list of the commands you've entered with the oldest command having the smallest number. Simple right?

Here's an example of history output:

  190  ps -axu | grep htt
  191  /www/bin/apachectl start
  192  vi /usr/local/lib/php.ini 
  193  cat /www/logs/error_log 
  194  ps -auxw | grep http
  195  pwd

This brings us to bang-bang or !!. !! tells bash "repeat the last command I entered." But the magic doesn't stop there, if you order now, you'll also receive !xyz. !xyz will allow you to run the last command beginning with xyz that you typed. Be sure to add enough to the abbreviation to make it unique or you could run into problems, for instance: In the example above, ps was ran twice, then pwd. If you typed !p you'd get the output of pwd. Typing !ps is just enough to be unique and will execute the ps -auxw | grep http entry in history. By typing just enough to make your history request unique, give you a much better chance of hitting your targeted command. 

		23.7.2 :p isn't just an emoticon

If you need to be very sure of the command you're targeting, :p can be a huge help. !xyz:p will print the command that would be executed rather than executing it. :p is also clever enough to add the printed command to your history list as the last command executed (even though it didn't execute it) so that, if you decide that you like what was printed, a !! is all you need to make it happen, cap'n.

Bash provides a couple of methods for searching the command history. Both are useful in different situations. The first method is to simply type history, find the number of the command you want and then type !N where "N" is the number of the command you'd like to execute. (:p works here too.) The other method is a tad more complex but also adds flexibilty. ^r (ctrl-r) followed by whatever you type will search the command history for that string. The bonus here is that you're able to edit the command line you've searched for before you send it down the line. While the second method is more powerful, when doing some redundant task, it's much easier to remember !22 than it is to muck with ctrl-r type searches or even the arrow keys. 

		23.7.3 Bang dollar-sign

!$ is the "end" of the previous command. Consider the following example: We start by looking for a word in a file
grep -i joe /some/long/directory/structure/user-lists/list-15
if joe is in that userlist, we want to remove him from it. We can either fire up vi with that long directory tree as the argument, or as simply as
vi !$
Which bash expands to:
vi /some/long/directory/structure/user-lists/list-15

A word of caution: !$ expands to the end word of the previous command. What's a word? The bash man page calls a word "A sequence of characters considered as a single unit by the shell." If you haven't changed anything, chances are good that a word is a quoted string or a white-space delimited group of characters. What is a white-space delimited group of characters ? It's a group of characters that are separated from other characters by some form of white-space (which could be a tab, space, etc.) If you're in doubt, :p works here too.

Another thing to keep in mind when using !$ is that if the previous command had no agruments, !$ will expand to the previous command rather than the most recent argument. This can be handy if, for example, you forget to type vi and you just type the filename. A simple vi !$ and you're in.

Similar to !$ is !*. !* is all of the arguments to the previous command rather than just the last one. As usual, this is useful in many situations. Here's a simple example:
vi cd /stuff
oops!
[exit vi, twice]
!*
Which bash expands to: cd /stuff

	23.7.4 Circumflex hats

Have you ever typed a command, hit return and a micro-second later realized that you made a typo? Seems like I'm always typing mroe filename. Luckily, the folks who wrote bash weren't the greatest typists either. In bash, you can fix typos in the previous command with a circumflex (^) or "hat." Consider the following:
vi /etc/X11/XF86config
oops!
^6c^6C
Which bash turns into:
vi /etc/X11/XF86Config

What happened there? The name of the file that I was trying to edit was /etc/X11/XF86Config (note the capital "C.") I typed a lower-case "c" and vi saw my error as a request for a new file. Once I closed out of vi I was able to fix my mistake with the following formula: ^error^correction.

Hats needn't be only used for errors... Let's say you have a few redundant commands that can't be handled with a wildcard, hats will work great for you. For example:
dd if=kern.flp of=/dev/fd0
^kern^mfsroot
Which bash turns into:
dd if=mfsroot.flp of=/dev/fd0

	23.7.5 Brace Expansion

Everyone has done one of the following to make a quick backup of a file:

$ cp filename filename-old
$ cp filename-old filename

These seem fairly straightforward, what could possibly make them more efficient? Let's look at an example:

$ cp filename{,-old}
$ cp filename{-old,}
$ cp filename{-v1,-v2}

In the first two examples, I'm doing exactly the same thing as I did in the previous set of examples, but with far less typing. The first example takes a file called filename and copies it to filename-old The second example takes a file called filename-old and copies it to simply filename.

The third example might give us a clearer picture of what's actually occuring in the first two. In the third example, I'm copying a file called filename-v1 to a file called filename-v2 The curly brace ({) in this context, tells bash that "brace expansion" is taking place. The preamble (in our case filename,) is prepended to each of the strings in the comma-separated list found within the curly braces, creating a new word for each string. So the third example above expands to:

$ cp filename-v1 filename-v2

Brace expansion can take place anywhere in your command string, can occur multiple times in a line and even be nested. Brace expansion expressions are evaluated left to right. Some examples:

$ touch a{1,2,3}b
$ touch {p2,pv,av,}p
$ ls /usr/{,local/}{,s}bin/jojo

The first example will create three files called a1b, a2b and a3b In this case, the preamble is prepended and the postscript is appended to each string within the curly braces. The second example contains no preamble, so the postscript is appended to each string as before, creating p2p, pvp, avp and simply p The last string in the second example is empty, so p is appended to nothing and becomes just p The third example shows multiple brace expansions on the same line and expands to this:

$ ls /usr/bin/jojo /usr/sbin/jojo /usr/local/bin/jojo /usr/local/sbin/jojo

The following example is an example of nested brace expansion.

$ apt-get remove --purge ppp{,config,oe{,conf}}

The shell will expand it to:

$ apt-get remove --purge ppp pppconfig pppoe pppoeconf

The preamble, "ppp" will be prepended to, (left to right,) nothing ({,), config, then a second expansion will take place and a new preamble, "oe" will be prepended to, first nothing ({,), and then conf which will then each be appended to the original preamble.

For more on brace expansion, including examples of nesting, read the bash man page. 

	23.7.6 Word Modifiers

In the first installment of Advancing in the Bash Shell, we learned about :p which is used to print a command, but not execute it. :p is an example of a "word modifier" and it has several siblings. Here's a shortened list from the bash man page:

    * h Remove a trailing file name component, leaving only the head.
    * t Remove all leading file name components, leaving the tail.
    * r Remove a trailing suffix of the form .xxx, leaving the basename.
    * e Remove all but the trailing suffix.

Let's say I'm reading a file nested deeply in a directory structure. When I finish editing the file, I realize that there are some other operations I want to do in that directory and that they would be more easily accomplished if I were in that directory. I can use :h to help get me there.

$ links /usr/local/share/doc/3dm/3DM_help.htm
$ cd !$:h
$ links !-2$:t

Our old friend !$ is back and is being modified by :h. The second command tells bash to cd to !$ or the last argument of the previous command, modifying it with :h which trims off the file name portion of the string, leaving just the directory. The third command looks pretty crazy, but it is acutally quite simple. !-2 means the command N(in this case 2) commands ago. $ means the last argument of that command and the :t means modify that argument to remove the path from it. So, all told: run links using the last argument of the command preceding the most recent one, trimming the path from that argument, or links 3DM_help.html. No big deal, right?

In our next example, we've downloaded a tar ball from the Internet. We check to see if it is going to create a directory for its files and find out that it will not. Rather than clutter up the current directory, we'll make a directory for it.

$ wget http://www.example.com/path/to/jubby.tgz
$ tar tzvf jubby.tgz
{output}
$ mkdir !$:r

The third command will create a directory called 'jubby'.

Word modifiers can be stacked as well. In the next example, we'll download a file to /tmp, and then create a directory for the contents of that tar file in /usr/local/src.

$ cd /tmp
$ wget http://www.example.com/path/KickassApplicationSuite.tar.gz
$ cd /usr/local/src/
$ mkdir !-2$:t:r:r
{creates directory called 'KickassApplicationSuite'}
$ cd !$
$ tar xvzf /tmp/!-4$:t

The first three commands are fairly common and use no substitution. The fourth command, however, seems like gibberish. We know !-2 means the command prior to the most recent one and that $ indicates the last argument of that command. We even know that :t will strip off the path portion of that argument (in this case, even the "http://".) We even know that :r will remove the file-extension to that argument, but here we call it twice, because there are two extensions (.gz is removed by the first :r and .tar is removed by the second.) We then cd into that directory (!$, again, is the argument to the previous command, in this case the argument to mkdir, which is 'KickassApplicationSuite'.) We then untar the file. !-4$ is the last argument to the command four commands ago, which is then modified by :t to remove the path, because we added the path as /tmp/. So the last command becomes tar xvzf /tmp/KickassApplicationSuite.tar.gz.

There's even a word modifier for substitution. :s can be used similarly to circumflex hats to do simple line substitution.

$ vi /etc/X11/XF86config
$ !!:s/config/Config-4/

We know that !! means the previous command string. :s modifies the previous command, substituting the first argument to :s with the second argument to :s. My example used / to delimit the two arguments, but any non-whitespace character can be used. It's also important to note that, just like circumflex hat substitution, the substitution will only take place on the first instance of the string to be substituted. If you want to affect every instance of the substitution string, you must use the :g word modifier along with :s.

$ mroe file1 ; mroe file2
$ !!:gs/mroe/more

The second command substitutes (:s) more for all (:g) instances of mroe. Hint: :g can be used with circumflex hats too!

The final word modifer we'll look at in this tutorial is &. & means repeat the previous substitution. Let's say we're examining file attributes with the ls command.

$ ls -lh myfile otherfile anotherfile
$ !!:s/myfile/myfile.old/

Seems simple enough. :s steps in and changes myfile to myfile.old so we end up with ls -lh myfile.old myfile2 myfile3. & is just a shortcut that we can use to represent the first argument to :s The following example is equivalent to the example above:

$ ls -lh myfile otherfile anotherfile
$ !!:s/myfile/&.old/ 

	23.7.7 Bash Functions

In the first installment of Advancing in the Bash Shell, we learned a bit about aliases. Aliases are simple, static, substitutions. This isn't to say that one can't have a very advanced and complex alias, but rather to say that no matter how complex the alias, the shell is simply substituting ^x for ^y. Shell functions are like aliases, but they have the ability to contain logic and positional arguments, making them quite powerful.

What is a positional argument? I'm glad you asked. A positional argument is an argument whose position is important. For example, in the following function the directory containing the data to be copied must come first and the destination directory must come second.

function treecp { tar cf - "${1}" | (cd "${2}" ; tar xpf -) ; };

It's certainly possible (and easy) to write functions that can accept their arguments in any order, but in many cases, it just doesn't make sense to do so. Imagine if cp could take its arguments in any order and you had to use switches to designate which file was which!

Let's look at the example function above. To let bash know that you're declaring a function, you start your function with the word function. The first argument to function is the name of the function you want to declare. In this case, treecp. The next character, {, as above, indicates a list to the shell. The list, in this case, is a list of commands. After the curly brace, the logic of the function is defined until the function is closed with a semi-colon followed by a closing curly brace (}.)

The logic of this function is fairly simple, once you understand the two variables that it is using. "${1}" is the first argument to a given command. "${2}" is the second, and so on. As you might infer, "${0}" is the name of the command itself. These are positional arguments. Their number indicates their position.

So, in order to use our treecp function, we must supply it with two arguments, the source tree and the destination tree:

$ treecp dmr ~/public_html

treecp becomes "${0}", dmr becomes "${1}", and ~/public_html is expanded to /home/whomever/public_html which then becomes "${2}".

What happens if the user forgets to add either or both arguments? How can the function know that it shouldn't continue? The function, as above, doesn't. It'll just continue on its merry way no matter how few arguments it receives. Let's add some logic to make sure things are as we expect them before proceeding.

Before we can do that, we need to learn about another variable that is set, (like "${0}",) when a command is run. The "${#}" variable is equal to the number of arguments given to a command. For example:

$ function myfunc { echo "${#}" ; } ;
$ myfunc foo bar taco jojo
{output is '4'}
$ myfunc *
{output is the same as 'ls | wc -l'}
$ myfunc
{output is '0'}

So now that we can discover how many arguments were passed to our command, (in this case a function,) we can determine if we've received the two arguments necessary to make our command work. There's still a chance that these arguments are garbage, containing typos or directories that don't exist, but unfortunately the function can't think for you. :)

function treecp {
    if [ "${#}" != 2 ] ; then
        echo "Usage: treecp source destination";
        return 1;
    else
        tar cf - "${1}" | (cd "${2}" ; tar xpf -) ; 
    fi ;
};

I've made use of the [ (aka test) application to see if the number of arugments is other than the expected two. If there are more or less than two arguments, the function willl echo a usage statement and set the value of "${?}" to 1. "${?}" is called a return code. I'll discuss return codes in a little bit. If there are two arguments, the command runs using the first argument as an argument to tar cf - and the second command as an argument to cd. For more information on [ read its man page (man [.)

Ok, so positional parameters are fun, but what if I don't care about placement and I need to pass all arguments to a command within my function? "${*}" is just what you're looking for.

$ function n { echo "${*}" >> ~/notes; };
$ n do the dumb things I gotta do, touch the puppet head.

No matter how many words are passed to n they'll all end up concatenated to the end of notes in my home directory. Be careful to avoid shell-special characters when entering notes in this manner!

Above, we designated 1 as a return code for an error state. There are no rules about what number should be returned in what case, but there are some commonly used return codes that you may want to use or at least be aware of. 0 (zero) is commonly used to denote successful completion of a task. 1 (one), (or any non-zero number,) is commonly used to denote an error state.

If an function or shell script is quite complex, the author may choose to use any number of error codes to mean different things went wrong. For example, return code 28 might mean your script was unable to create a file in a certain directory, whereas return code 29 might mean that the script received an error code from wget when it tried to download a file. Return codes are more helpful to logic than to people. Don't forget to include good error messages for the humans trying to figure out what's going wrong.

The following is an example of checking a return code:

function err { 
    grep "${*}" /usr/include/*/errno.h; 
    if [ "${?}" != 0 ] ; then
        echo "Not found."
    fi
};

grep will return non-zero if no match was found. We then call test again (as [) to see if the return code from grep was other than zero. If ['s expression evaluates to true, in this case if a non-zero number was returned, the command after then will be run. If grep returns 0, it will output the files/lines that match the expression passed to it, ['s expression will evaluate false and the command after then will not run. 


24. Linux, Detailed information in KB_Linux.

	24.1 Linux administration

		24.1.1 Semaphors

		 Shared memory and semaphores are collectively referred to as "System V IPC". When PostgreSQL exceeds one of the various hard limits of the IPC resources, the postmaster refuses to start up and leaves an error message about which problem was encountered and what needs to be done about it.

Table 1-2. System V IPC parameters
Name	Description							Reasonable Values
SHMMAX	Maximum size of shared memory segment (bytes)			250kB + 8.2kB * shared_buffers + 14.2kB * max_connections or infinity
SHMMIN	Minimum size of shared memory segment (bytes)			1
SHMALL	Total amount of shared memory available (bytes or pages)	if bytes, same as SHMMAX; if pages, ceil(SHMMAX/PAGE_SIZE)
SHMSEG	Maximum number of shared memory segments per process		only 1 segment is needed, but the default is much higher
SHMMNI	Maximum number of shared memory segments system-wide		like SHMSEG plus room for other applications
SEMMNI	Maximum number of semaphore identifiers (that is, sets)		>= ceil(max_connections / 16)
SEMMNS	Maximum number of semaphores system-wide			ceil(max_connections / 16) * 17 + room for other applications
SEMMSL	Maximum number of semaphores per set				>= 17
SEMMAP	Number of entries in semaphore map				see text
SEMVMX	Maximum value of semaphore					>= 255 (The default is often 32767, do not change unless asked to.)

The most important shared memory parameter is SHMMAX, the maximum size, in bytes, that a shared memory segment can have. If you get an error message from shmget along the lines of Invalid argument, it is possible that this limit has been exceeded. The size of the required shared memory segments varies both with the number of requested buffers (-B option) and the number of allowed connections (-N option), although the former is the dominant item. (You can therefore, as a temporary solution, lower these settings to get rid of the failures.) As a rough approximation you can estimate the required segment size as the number of buffers times the block size (8192 KB by default) plus ample overhead (at least half a megabyte). All error messages will contain the size of the failed allocation request.

Less likely to cause problems is the minimum size for shared memory segments (SHMMIN), which should be at most somewhere around 256 KB for PostgreSQL (it is usually just 1). The maximum number of segments system-wide (SHMMNI) or per-process (SHMSEG) should not cause a problem unless your system has them set to zero.

PostgreSQL uses one semaphore per allowed connection (-N option), in sets of 16. Each such set will also contain a 17th semaphore that contains a "magic number", to avoid collision with semaphore sets used by other applications. The maximum number of semaphores in the system is set by SEMMNS, which consequently must be at least as high as the connection setting plus one extra for each 16 allowed connections (see the formula in the section "System V IPC Parameters"). The parameter SEMMNI determines the limit on the number of semaphore sets that can exist on the system at one time. Hence this parameter must be at least ceil(max_connections / 16). Lowering the number of allowed connections is a temporary workaround for failures, which are usually confusingly worded "No space left on device", from the function semget().

In some cases it might also turn out to be necessary to increase SEMMAP to be at least on the order of SEMMNS. This parameter defines the size of the semaphore resource map, in which each contiguous block of available semaphores needs an entry. When a semaphore set is freed it is either added to an existing entry that is adjacent to the freed block or it is registered under a new map entry. If the map is full, the freed semaphores get lost (until reboot). Fragmentation of the semaphore space could therefore over time lead to less available semaphores than there should be.

The SEMMSL parameter, which determines how many semaphores can be in a set, must be at least 17 for PostgreSQL.

Various other settings related to "semaphore undo", such as SEMMNU and SEMUME, are not of concern for PostgreSQL.

The default shared memory limit for SHMMAX is 32 MB and for SHMALL is 2 MB, but it can be changed in the proc file system (without reboot). For example, to allow 128 MB:

$ echo 134217728 >/proc/sys/kernel/shmall
$ echo 134217728 >/proc/sys/kernel/shmmax

You could put these commands into a script run at boot-time.

Alternatively, you can use sysctl(8), if available, to control these parameters. Look for a file called /etc/sysctl.conf and add lines like the following to it:

kernel.shmall = 134217728
kernel.shmmax = 134217728

This file is usually processed at boot time, but sysctl can also be called explicitly later.

Other parameters are sufficiently sized for any application. If you want to see for yourself look into /usr/src/linux-2.4/include/asm-xxx/shmparam.h and /usr/src/linux-2.4/include/linux/sem.h.
Resource Limits

Linux enforces various kinds of resource limits that might interfere with the operation of your PostgreSQL server. Of importance are especially the limits on the number of processes per user, the number of open files per process, and the amount of memory available to a process. Each of these have a "hard" and a "soft" limit. The soft limit is what actually counts but it can be changed by the user up to the hard limit. The hard limit can only be changed by the root user. The system call setrlimit is responsible for setting these parameters. The shell's built-in command ulimit (Bourne shells) or limit (csh) is used to control the resource limits from the command line.

On Linux, /proc/sys/fs/file-max determines the maximum number of files that the kernel will allocate. It can be changed by writing a different number into the file or by adding an assignment in /etc/sysctl.conf. The maximum limit of files per process is fixed at the time the kernel is compiled.

The PostgreSQL server uses one process per connection so you should provide for at least as many processes as allowed connections, in addition to what you need for the rest of your system. This is usually not a problem but, if you run several servers on one machine, things might get tight.

The factory default limit on open files is often set to "socially friendly" values that allow many users to coexist on a machine without using an inappropriate fraction of the system resources. If you run many servers on a machine, this is perhaps what you want, but on dedicated servers you may want to raise this limit.

				24.1.1.1 Determine system limits
				cat /etc/sysctl.conf

				24.1.1.2 Setting semaphore system limits
				Setting Semaphore Parameters 

				To determine the values of the four described semaphore parameters, run: 
				# cat /proc/sys/kernel/sem
				250     32000   32      128
				
				These values represent SEMMSL, SEMMNS, SEMOPM, and SEMMNI. 
				
				Alternatively, you can run: 
				# ipcs -ls
				All four described semaphore parameters can be changed in the proc file system without reboot: 
				# echo 250 32000 100 128 > /proc/sys/kernel/sem
				Alternatively, you can use sysctl(8) to change it: 
				sysctl -w kernel.sem="250 32000 100 128"
				To make the change permanent, add or change the following line in the file /etc/sysctl.conf. This file is used during the boot process. 
				echo "kernel.sem=250 32000 100 128" >> /etc/sysctl.conf

				To make sure initial higher values are not overwritten do:
				$ awk ' ($1 < 1000) ||  ($2 < 32000) || ($3 < 320) || ($4 < 4090) {print "INC_SEM"}'   /proc/sys/kernel/sem  | grep 'INC_SEM' && echo 1000 32000 320 4096 > /proc/sys/kernel/sem


				24.1.1.3 Monitoring semaphore usage
				use ipcs -s to monitor semaphore usage
				use ipcrm -s to remove semaphores of killed processes
				ipcs - to monitor semaphore usage (for example: while [ 1 ]; do ipcs -su ; sleep 2; done )
				lsof - to monitor connections to db (for example: while [ 1 ]; do sudo lsof -i :2638 | grep rt_d | wc -l; sleep 1; done )



			24.1.2 Debugging in Linux

				24.1.2.1 Get the trace from a running process
				ptrace [pid] 
				or dump into file ptrace [pid] > file

				24.1.2.2 Get information about process (PID) and threads (TID)
alias prstat='clear;PS -P `pgrep PROCESS_NAME` -Lo pid,tid,%cpu,vsize,psr,stat | grep PID; while [ 1 ]; do ps -p `pgrep PROCESS_NAME` -Lo pid,tid,%cpu,vsize,psr,stat | grep -v PID | sort -rnk3;tput cup 1 0;sleep 3;done'

				24.1.2.3 Debug a running process
				gdb -p pid
				
			24.1.2.3 list open files, lsof
			for more information: $ man lsof
			
			$lsof


25. The Art of Unix programming, http://www.faqs.org/docs/artu/

26. Advanced issues

	26.1 Strange result of reading from shell
I'm using gvim launched from an linux xterm.

I want to work with all the files in the current directory so I do :r!
ls
the result is full of strange characters that get in the way of
editing the file.
I get something like (ignore the line numbers):
 2 ^[[00mtotal 184
   3 ^[[00m^[[00mcentrifydc.conf^[[00m
   4 ^[[00mdefaults.conf^[[00m
   5 ^[[00mdiff_cfg^[[00m
   6 ^[[00mgid.ignore^[[00m
   7 ^[[00mgroup.ignore^[[00m
   8 ^[[00mgroup.ovr.sample^[[00m
   9 ^[[00;34mold^[[00m/
  10 ^[[00;34mopenldap^[[00m/
  11 ^[[00mpasswd.ovr.sample^[[00m
  12 ^[[00muid.ignore^[[00m
  13 ^[[00mupgradeconf.conf^[[00m
  14 ^[[00muser.ignore^[[00m

Any idea why this happens?- More importantly how to make this go away?

Found away around this outside of VIM by performing:
for file in *; do echo $file; done > list_of_files

Yes.  Those are ANSI escape codes
http://en.wikipedia.org/wiki/Ansi_escape_codes

Your version of ls uses them to visually enhance the output on a xterm.
Usually this means you are using an alias for ls.  Try:

:!r
/bin/ls



27. tar

	27.1  The Ultimate Tar Command Tutorial with 10 Practical Examples
On Unix platform, tar command is the primary archiving utility. Understanding various tar command options will help you master the archive file manipulation.

In this article, let us review various tar examples including how to create tar archives (with gzip and bzip compression), extract a single file or directory, view tar archive contents, validate the integrity of tar archives, finding out the difference between tar archive and file system, estimate the size of the tar archives before creating it etc.,

		27.1.1  Creating an archive using tar command
Creating an uncompressed tar archive using option cvf

This is the basic command to create a tar archive.

$ tar cvf archive_name.tar dirname/

In the above command:

    c – create a new archive
    v – verbosely list files which are processed.
    f – following is the archive file name

Creating a tar gzipped archive using option cvzf

The above tar cvf option, does not provide any compression. To use a gzip compression on the tar archive, use the z option as shown below.

$ tar cvzf archive_name.tar.gz dirname/

    z – filter the archive through gzip

Note: .tgz is same as .tar.gz

Note: I like to keep the ‘cvf’ (or tvf, or xvf) option unchanged for all archive creation (or view, or extract) and add additional option at the end, which is easier to remember. i.e cvf for archive creation, cvfz for compressed gzip archive creation, cvfj for compressed bzip2 archive creation etc., For this method to work properly, don’t give – in front of the options.
Creating a bzipped tar archive using option cvjf

Create a bzip2 tar archive as shown below:

$ tar cvfj archive_name.tar.bz2 dirname/

    j – filter the archive through bzip2

gzip vs bzip2: bzip2 takes more time to compress and decompress than gzip. bzip2 archival size is less than gzip.

Note: .tbz and .tb2 is same as .tar.bz2

		27.1.2 Extracting (untar) an archive using tar command
Extract a *.tar file using option xvf

Extract a tar file using option x as shown below:

$ tar xvf archive_name.tar

    x – extract files from archive

Extract a gzipped tar archive ( *.tar.gz ) using option xvzf

Use the option z for uncompressing a gzip tar archive.

$ tar xvfz archive_name.tar.gz

Extracting a bzipped tar archive ( *.tar.bz2 ) using option xvjf

Use the option j for uncompressing a bzip2 tar archive.

$ tar xvfj archive_name.tar.bz2

Note: In all the above commands v is optional, which lists the file being processed.

		27.1.3 Listing an archive using tar command
View the tar archive file content without extracting using option tvf

You can view the *.tar file content before extracting as shown below.

$ tar tvf archive_name.tar

View the *.tar.gz file content without extracting using option tvzf

You can view the *.tar.gz file content before extracting as shown below.

$ tar tvfz archive_name.tar.gz

View the *.tar.bz2 file content without extracting using option tvjf

You can view the *.tar.bz2 file content before extracting as shown below.

$ tar tvfj archive_name.tar.bz2


		27.1.4 Listing out the tar file content with less command

When the number of files in an archive is more, you may pipe the output of tar to less. But, you can also use less command directly to view the tar archive output, as explained in one of our previous article Open & View 10 Different File Types with Linux Less Command — The Ultimate Power of Less.

		27.1.5 Extract a single file from tar, tar.gz, tar.bz2 file

To extract a specific file from a tar archive, specify the file name at the end of the tar xvf command as shown below. The following command extracts only a specific file from a large tar file.

$ tar xvf archive_file.tar /path/to/file

Use the relevant option z or j according to the compression method gzip or bzip2 respectively as shown below.

$ tar xvfz archive_file.tar.gz /path/to/file

$ tar xvfj archive_file.tar.bz2 /path/to/file


		27.1.6 Extract a single directory from tar, tar.gz, tar.bz2 file

To extract a single directory (along with it’s subdirectory and files) from a tar archive, specify the directory name at the end of the tar xvf command as shown below. The following extracts only a specific directory from a large tar file.

$ tar xvf archive_file.tar /path/to/dir/

To extract multiple directories from a tar archive, specify those individual directory names at the end of the tar xvf command as shown below.

$ tar xvf archive_file.tar /path/to/dir1/ /path/to/dir2/

Use the relevant option z or j according to the compression method gzip or bzip2 respectively as shown below.

$ tar xvfz archive_file.tar.gz /path/to/dir/

$ tar xvfj archive_file.tar.bz2 /path/to/dir/


		27.1.7 Extract group of files from tar, tar.gz, tar.bz2 archives using regular expression

You can specify a regex, to extract files matching a specified pattern. For example, following tar command extracts all the files with pl extension.

$ tar xvf archive_file.tar --wildcards '*.pl'

Options explanation:

    –wildcards *.pl – files with pl extension


		27.1.8 Adding a file or directory to an existing archive using option -r

You can add additional files to an existing tar archive as shown below. For example, to append a file to *.tar file do the following:

$ tar rvf archive_name.tar newfile

This newfile will be added to the existing archive_name.tar. Adding a directory to the tar is also similar,

$ tar rvf archive_name.tar newdir/

Note: You cannot add file or directory to a compressed archive. If you try to do so, you will get “tar: Cannot update compressed archives” error as shown below.

$ tar rvfz archive_name.tgz newfile
tar: Cannot update compressed archives
Try `tar --help' or `tar --usage' for more information.


		27.1.9 Verify files available in tar using option -W

As part of creating a tar file, you can verify the archive file that got created using the option W as shown below.

$ tar cvfW file_name.tar dir/

If you are planning to remove a directory/file from an archive file or from the file system, you might want to verify the archive file before doing it as shown below.

$ tar tvfW file_name.tar
Verify 1/file1
1/file1: Mod time differs
1/file1: Size differs
Verify 1/file2
Verify 1/file3

If an output line starts with Verify, and there is no differs line then the file/directory is Ok. If not, you should investigate the issue.

Note: for a compressed archive file ( *.tar.gz, *.tar.bz2 ) you cannot do the verification.

Finding the difference between an archive and file system can be done even for a compressed archive. It also shows the same output as above excluding the lines with Verify.

Finding the difference between gzip archive file and file system

$ tar dfz file_name.tgz

Finding the difference between bzip2 archive file and file system

$ tar dfj file_name.tar.bz2


		27.1.10 Estimate the tar archive size

The following command, estimates the tar file size ( in KB ) before you create the tar file.

$ tar -cf - /directory/to/archive/ | wc -c
20480

The following command, estimates the compressed tar file size ( in KB ) before you create the tar.gz, tar.bz2 files.

$ tar -czf - /directory/to/archive/ | wc -c
508

$ tar -cjf - /directory/to/archive/ | wc -c
428

	27.2 Check the total content size of a tar gz file 

$ gzip -l compressed.tar.gz
     compressed        uncompressed  ratio uncompressed_name
            132               10240  99.1% compressed.tar


	27.3 List files of bzip2 compressed tar
 tar -tjvf  /cygdrive/c/TEMP/backups/yosi_backup_06_1117.tar.bz2 

	27.4
28. ls

	28.1 ls sort by size
ls -- sort by size

ls -S                # GNU/FSF version. Unsupported in a few unix flavors.

ls -s | sort         # useful even if imperfect
ls -s | sort -n 

ls -al | sort -n +4

	28.2 ls sort by time
	ls -lrt

	28.3
"
29. My examples

	29.1 gather FD usage info on customer machine

cd /proc 
while true ; echo "checking for FDs, time is: "; date; do for proc in `ls  | grep '^[0-9]\+'`; do echo checking process $proc ; echo `ps -e | grep $proc | grep -v grep | awk '{print $4}'` using `ls -l ./$proc/fd/ | wc -l` fds ; echo --- ; done ; sleep 900 ; done   > /tmp/fds_report &

It will gather the info every 15 minutes.
Output ex:
"
checking for FDs, time is: 
Thu Feb 16 16:45:26 IST 2012

---
checking process 245
aio/1 using 1 fds
---
checking process 2497
syslogd using 12 fds
---
checking process 2501
klogd using 3 fds
---
checking process 2510
debugd using 57 fds 

"________

To stop it do
jobs 
kill %[job id] 

	29.2 Count all lines of code in whole project.
- C++ 
[yizaq@csi-pmbu17-lnx:Sun Jun 25:1025:27:/data/trunk/branches/prrt_everglades_br/acs]$ find . -name *.h -o -name *.cpp | xargs wc -l
 161739 total

- C and C++
[yizaq@csi-pmbu17-lnx:Sun Jun 25:1028:30:/data/trunk/branches/adrt_everglades_br/adrt]$ find . -name *.h -o -name *.cpp -o -name *.c | xargs wc -l
478781 total

- java
[yizaq@csi-pmbu17-lnx:Sun Jun 25:1030:32:/data/trunk/branches/ise_everglades_br/cpm]$ find . -name *.java | xargs wc -l
  37527 total

	29.3
30. gpg

	30.1 open, decrypt .tar.gpg file
[yizaq@yizaq-dev01:Thu Dec 03:1024:35:/ws/yizaq-csi/temp]$ cat pass
Bastien123
[yizaq@yizaq-dev01:Thu Dec 03:1023:34:/ws/yizaq-csi/temp]$ gpg -d -q --batch --no-mdc-warning --output ise-support-bundle-ise1-biawifi-1-admin-02-25-2015-11-37.tar  --passphrase-file ./pass ise-support-bundle-ise1-biawifi-1-admin-02-25-2015-11-37.tar.gpg 

	30.2

31. File sharing tools

	31.1 VSFTPD

		31.1.1 Setup on RH linux
-> Install:
yum install vsftpd

-> Verify:
rpm -q vsftpd

-> Start:
  service vsftpd start

-> Configuration
  354  ls /etc/vsftpd/
  355  vi /etc/vsftpd/vsftpd.conf 

-> Anonymous dir: 
  /var/ftp/


		31.1.2


	31.2

32. My FAQ

    32.1 split and join binary files
split to 2 parts:

[212680136@G9VK2GH2E:Tue Nov 07:/cygdrive/c/TEMP/backups:]$ split -n2 -e yosi_backup_07_11_17.tar.bz2 

join:
$ cat x* > test.tarz

    32.2


33.

