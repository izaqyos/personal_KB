.........................................Table Of Contents...............................................................
1. Udemy NodeJS ML Course <URL:#tn=1. Udemy NodeJS ML Course>
    1.1 intro <URL:#tn=    1.1 intro>
        1.1.1 To solve ML problems go through these steps: <URL:#tn=        1.1.1 To solve ML problems go through these steps:>
        1.1.2 plinko  <URL:#tn=        1.1.2 plinko >
            1.1.2.1 plinko getting started <URL:#tn=            1.1.2.1 plinko getting started>
            1.1.2.2 more advanced KNN analysys. <URL:#tn=            1.1.2.2 more advanced KNN analysys.>
            1.1.2.3 core concepts of ML  <URL:#tn=            1.1.2.3 core concepts of ML >
            1.1.2.4 final plinko knn lodash implementation <URL:#tn=            1.1.2.4 final plinko knn lodash implementation>
            1.1.2.5 <URL:#tn=            1.1.2.5>
        1.1.3 TensorFlow-JS <URL:#tn=        1.1.3 TensorFlow-JS>
            1.1.3.1 basics <URL:#tn=            1.1.3.1 basics>
                1.1.3.1.1  install <URL:#tn=                1.1.3.1.1  install>
                1.1.3.1.2 getting started <URL:#tn=                1.1.3.1.2 getting started>
                1.1.3.1.3 <URL:#tn=                1.1.3.1.3>
            1.1.3.2 <URL:#tn=            1.1.3.2>
        1.1.4 <URL:#tn=        1.1.4>
    1.2 TensorFlow Applications <URL:#tn=    1.2 TensorFlow Applications>
        1.2.1 KNN  <URL:#tn=        1.2.1 KNN >
            1.2.1.1 code <URL:#tn=            1.2.1.1 code>
            1.2.1.2 considerations <URL:#tn=            1.2.1.2 considerations>
            1.2.1.3 <URL:#tn=            1.2.1.3>
        1.2.2 <URL:#tn=        1.2.2>
    1.3 linear regression <URL:#tn=    1.3 linear regression>
        1.3.1  Gradient decent <URL:#tn=        1.3.1  Gradient decent>
        1.3.2 code <URL:#tn=        1.3.2 code>
            1.3.2.1 initial arrays based code. <URL:#tn=            1.3.2.1 initial arrays based code.>
            1.3.2.2 vecorized Gradient-Descent tensorflow code <URL:#tn=            1.3.2.2 vecorized Gradient-Descent tensorflow code>
                1.3.2.2.1 source code <URL:#tn=                1.3.2.2.1 source code>
                1.3.2.2.2  <URL:#tn=                1.3.2.2.2 >
            1.3.2.3 coefficient of determination <URL:#tn=            1.3.2.3 coefficient of determination>
            1.3.2.4 multivariant  <URL:#tn=            1.3.2.4 multivariant >
            1.3.2.5 <URL:#tn=            1.3.2.5>
        1.3.3 <URL:#tn=        1.3.3>
    1.4 <URL:#tn=    1.4>
2.  Python  <URL:#tn=2.  Python >
    2.1  vectorized Gradient-Descent  <URL:#tn=    2.1  vectorized Gradient-Descent >
        2.1.1  https://towardsdatascience.com/gradient-descent-in-python-a0d07285742f <URL:#tn=        2.1.1  https://towardsdatascience.com/gradient-descent-in-python-a0d07285742f>
        2.1.2 KNN <URL:#tn=        2.1.2 KNN>
            2.1.2.1 knn scikit iris demo <URL:#tn=            2.1.2.1 knn scikit iris demo>
            2.1.2.2 <URL:#tn=            2.1.2.2>
        2.1.3 <URL:#tn=        2.1.3>
    2.2 <URL:#tn=    2.2>
3. NodeJS <URL:#tn=3. NodeJS>
    3.1 vectorized Gradient-Descent   <URL:#tn=    3.1 vectorized Gradient-Descent  >
    3.2 <URL:#tn=    3.2>
4. My examples <URL:#tn=4. My examples>
    4.1 ML intro presentation <URL:#tn=    4.1 ML intro presentation>
    4.2 <URL:#tn=    4.2>
5. <URL:#tn=5.>
.................................................END TOC..............................................










1. Udemy NodeJS ML Course

    1.1 intro

        1.1.1 To solve ML problems go through these steps:
a. identify independent variables (features) and dependent variables (labels).
ex: if feature1, feature2 changes then label1, label2 r effected

b. assemble a data set
collect data, usually from different sources
assemble it
format it
end result, a csv of sorts with columns for features and labels

c. decide on the output (e.g. classification, regression), which will help choose algorithm
classification - output, does belong to a discreet set
regression - output, does belong to a continuous set (age, weight, year)

d. pick an algorithm that will determine correlation between the features and labels. Ex. linear regression  

e. use a model generated by the algorithm to make a prediction

template:
a. data
features: 
label: 
b. data set
c. output
d. algorithm 
e. model


        1.1.2 plinko 

            1.1.2.1 plinko getting started


        /Users/i500695/work/code/ML/UdemyNodeJS_ML/MLKits
        [i500695@C02X632CJGH6:2020-06-01 17:55:30:~/work/code/ML/UdemyNodeJS_ML/MLKits:]2014$ cd plinko/
[i500695@C02X632CJGH6:2020-06-01 17:56:38:~/work/code/ML/UdemyNodeJS_ML/MLKits/plinko:]2015$ open index.html 

goal, given data where ball is dropped predict in which bucket it will end up.

a. data
features: drop loc, bouncing range, ball size
label: bucket #
b. data set
c. output
d. algorithm 
e. model

JS datasets.
a. array of objects [{f1: 2, f2: 32, ..., l1: 1}, {}, ...]
b. array of arrays [[2,32,...,1],[],...]

/Users/i500695/work/code/ML/UdemyNodeJS_ML/MLKits/plinko/score.js
const outputs = [];

function onScoreUpdate(dropPosition, bounciness, size, bucketLabel) {
  // Ran every time a balls drops into a bucket
  outputs.push([dropPosition, bounciness, size, bucketLabel]);

  console.log(outputs);
}

function runAnalysis() {
  // Write code here to analyze stuff
}

algorithm KNN, K-Nearest-Neighbour. basically:
a. gather data
b. decide on a relevant feature/s or calculation derived from features
c. sort by that
d. decide on K factor 
e. take K first elements in sorted list and return the most frequent value

first implementation very naive. just use 1 feature, distance and arbitrary point and K.
No training/test sets.
e.g.
const predictionPoint = 300;
const dist = point => Math.abs(point - predictionPoint);
const K = 3;

function runAnalysis() {
  // KNN impl
  const bucket = _.chain(outputs)
    .map(row => [dist(row[0]), row[3]]) //produce array [distance, bucket]
    .sortBy(row => row[0]) //sort by distance
    .slice(0, K) //take K neighbors
    .countBy(row => row[1]) //produce {bucket#: frequency} object
    .toPairs() //converts {k:v} obj to [[k1,v1],...[Kn,Vn]] array
    .sortBy(row => row[1]) // sort by frequency 
    .last() //take highest freq array [bucket, freq]
    .first() // bucket (as string since countBy converts to str)
    .parseInt() //convert to int
    .value(); //execute chain

    console.log('predicting ball will fall into bucket ', bucket);
}

After testing it turns out prediction is not good. 
Improve by split data to training and test datasets.
After training KNN on training dataset test accuracy vs. test dataset.
play w/ different params. K. drop point. bounciness, ball size etc to determine best prediction accuracy

            1.1.2.2 more advanced KNN analysys.
here some improvements:
a. split data to training and test sets
b. distance function calculates distance between all features (x dimensions eaclidian dist)
c. track and measure accuracy
d. try different K factors
e. knn gets training set, all features and k as params 

const outputs = [];

function onScoreUpdate(dropPosition, bounciness, size, bucketLabel) {
  // Ran every time a balls drops into a bucket
  outputs.push([dropPosition, bounciness, size, bucketLabel]);

  // console.log(outputs);
}

//// single feature distance
//const dist = (point, predictionPoint) => Math.abs(point - predictionPoint);

// multi feature distance, euclydan/pythogroian distance 
// const dist = (features, prediction) => ( (features[0] - prediction[0])**2 + 
//   (features[1] - prediction[1])**2 +(features[2] - prediction[2])**2)**0.5;
// same using lodash
const dist = (pt1, pt2) => {
  _.chain(pt1)
  .zip(pt2)
  .map( ([p1, p2]) => (p1 - p2)**2 )
  .sum()
  .value() ** 0.5
};

function runAnalysis() {
  console.log('runAnalysis');

  const testSetSize = 50;
  const [testSet, trainSet] = splitDatasets(outputs, testSetSize);


  //pre refactor
  // let correct = 0;
  // _.range(1, 15).forEach(k => {
  //   testSet.forEach(element => {
  //     const bucket = knn(trainSet, element[0], k);

  //     if (bucket === element[3]) correct++;
  //     console.log('Predicting ball dropped from %d to fall into bucket %d. actual bucket %d', element[0], bucket, element[3]);
  //   });
  //   console.log('correct %d times. accuracy %f', correct, correct / testSetSize);

  // })

  //post refactor
  _.range(1, 20).forEach(k => {
    const accuracy = _.chain(testSet)
      .filter(test => knn(trainSet, _.initial(test), k) === test[3])
      //.tap( x => console.log('#test w/ correct prediction %d, %s', x.length, JSON.stringify(x)))
      .size()
      .divide(testSetSize)
      .value();
    console.log('knn k=%d correct accuracy %f', k, accuracy);

  });

}

function knn(data, predictionPoint, k) {
  // KNN impl
  //console.log('knn data %s', JSON.stringify(data));
  //console.log('knn, predictionPoint=%s, k %d', predictionPoint, k);

  //predictionPoint has just features. no labels.
  return _.chain(data)
    .map(row => {
      return [dist(_.initial(row), predictionPoint),
         _.last(row)
         ] //produce array [distance, bucket]
    })
    .sortBy(row => row[0]) //sort by distance
    .slice(0, k) //take K neighbors
    .countBy(row => row[1]) //produce {bucket#: frequency} object
    .toPairs() //converts {k:v} obj to [[k1,v1],...[Kn,Vn]] array
    .sortBy(row => row[1]) // sort by frequency 
    .last() //take highest freq array [bucket, freq]
    .first() // bucket (as string since countBy converts to str)
    .parseInt() //convert to int
    .value(); //execute chain
}

function splitDatasets(data, testCount) {
  const shuff = _.shuffle(data); //shuffle is required, otherwise we risk getting different localized data clusters 
  // in training vs. test sets (like training only has drop positions 0-200 and test 200-300)

  const testSet = _.slice(shuff, 0, testCount);
  const trainSet = _.slice(shuff, testCount);
  return [testSet, trainSet];
}

            1.1.2.3 core concepts of ML 

a. normalize features ( like project them to [0,1) )

To normalize f1 do (f1 - min(f) ) / max(f)-min(f)

another option is to apply standard deviation

b. feature selection
some features effect output more predictably (like drop point distance in plinko)
others effect in an unpredictable manner (like ball bounciness range)

c. common data structures, array maps (objects) , array of arrays

d. Split data for training and test. use test to improve accuracy, for feature selection and algorithm constants fine tune

e. features (variables) vs labels (outcomes)

            1.1.2.4 final plinko knn lodash implementation
const outputs = [];

function onScoreUpdate(dropPosition, bounciness, size, bucketLabel) {
  // Ran every time a balls drops into a bucket
  outputs.push([dropPosition, bounciness, size, bucketLabel]);

  // console.log(outputs);
}

//// single feature distance
//const dist = (point, predictionPoint) => Math.abs(point - predictionPoint);

// multi feature distance, euclydan/pythogroian distance 
// const dist = (features, prediction) => ( (features[0] - prediction[0])**2 + 
//   (features[1] - prediction[1])**2 +(features[2] - prediction[2])**2)**0.5;
// same using lodash
const dist = (pt1, pt2) => {
  _.chain(pt1)
    .zip(pt2)
    .map(([p1, p2]) => (p1 - p2) ** 2)
    .sum()
    .value() ** 0.5
};

function runAnalysis() {
  console.log('runAnalysis');



  //pre refactor
  // let correct = 0;
  // _.range(1, 15).forEach(k => {
  //   testSet.forEach(element => {
  //     const bucket = knn(trainSet, element[0], k);

  //     if (bucket === element[3]) correct++;
  //     console.log('Predicting ball dropped from %d to fall into bucket %d. actual bucket %d', element[0], bucket, element[3]);
  //   });
  //   console.log('correct %d times. accuracy %f', correct, correct / testSetSize);

  // })

  // //post refactor, but w/o feature selection
  // const testSetSize = 50;
  // const [testSet, trainSet] = splitDatasets(normalize(outputs, 3), testSetSize);
  // _.range(1, 20).forEach(k => { //k from 1 to 19
  //   const accuracy = _.chain(testSet)
  //     .filter(test => knn(trainSet, _.initial(test), k) === test[3])
  //     //.tap( x => console.log('#test w/ correct prediction %d, %s', x.length, JSON.stringify(x)))
  //     .size()
  //     .divide(testSetSize)
  //     .value();
  //   console.log('knn k=%d correct accuracy %f', k, accuracy);


  //with feature selection
  _.range(0, 3).forEach(featureIndex => {
    const testSetSize = 50;
    const dataSelectedFeature= outputs.map( r => [ r[featureIndex] , r[r.length-1]]);
    const [testSet, trainSet] = splitDatasets(normalize(dataSelectedFeature, 1), testSetSize);

    _.range(1, 20).forEach(k => { //k from 1 to 19
      const accuracy = _.chain(testSet)
        .filter(test => knn(trainSet, _.initial(test), k) === _.last(test))
        .size()
        .divide(testSetSize)
        .value();
      console.log('knn. feature=%d, k=%d, correct accuracy %f', featureIndex, k, accuracy); 
    }); 
  })
}

function knn(data, predictionPoint, k) {
      // KNN impl
      //console.log('knn data %s', JSON.stringify(data));
      //console.log('knn, predictionPoint=%s, k %d', predictionPoint, k);

      //predictionPoint has just features. no labels.
      return _.chain(data)
        .map(row => {
          return [dist(_.initial(row), predictionPoint),
          _.last(row)
          ] //produce array [distance, bucket]
        })
        .sortBy(row => row[0]) //sort by distance
        .slice(0, k) //take K neighbors
        .countBy(row => row[1]) //produce {bucket#: frequency} object
        .toPairs() //converts {k:v} obj to [[k1,v1],...[Kn,Vn]] array
        .sortBy(row => row[1]) // sort by frequency 
        .last() //take highest freq array [bucket, freq]
        .first() // bucket (as string since countBy converts to str)
        .parseInt() //convert to int
        .value(); //execute chain
    }

function splitDatasets(data, testCount) {
      const shuff = _.shuffle(data); //shuffle is required, otherwise we risk getting different localized data clusters 
      // in training vs. test sets (like training only has drop positions 0-200 and test 200-300)

      const testSet = _.slice(shuff, 0, testCount);
      const trainSet = _.slice(shuff, testCount);
      return [testSet, trainSet];
    }

function normalize(data, featuresCount) {
      const clonedData = _.cloneDeep(data);
      for (let i = 0; i < featuresCount; i++) {
        const clonedDataColumn = clonedData.map(row => row[i]);
        const min = _.min(clonedDataColumn);
        const max = _.max(clonedDataColumn);

        for (let j = 0; j < clonedData.length; j++) {
          clonedData[j][i] = (clonedData[j][i] - min) / (max - min);
        }
      }

      return clonedData;
    }

            1.1.2.5

        1.1.3 TensorFlow-JS

            1.1.3.1 basics
https://www.tensorflow.org/js
https://js.tensorflow.org/api/latest/

tensor - JS object wrapping numbers
dimensions - how many array nesting. 1D - array, 2D- matrix, 3d- [[[]]] etc
shape - # records per dimension.
ex: [1,2,3] -> [3]
ex: [
    [1,2,3],
    [3,4,5]
    ]       -> [2,3] (2 rows, 3 columns)

                1.1.3.1.1  install
                https://www.tensorflow.org/js/tutorials/setup
browser:
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@2.0.0/dist/tf.min.js"></script>

NodeJS:
yarn add @tensorflow/tfjs

or

npm install @tensorflow/tfjs

example:
https://playcode.io/616015/

// Try edit message
const data = {
  message: 'Hello world',
}

$('#msg').html(data.message)

const t1 = tf.tensor([1,2,3]);
const t2 = tf.tensor([4,7,3]);
let t3 = t1.add(t2);
console.log(t3.print())
console.log('t1 shape ', t1.shape)

// from https://www.tensorflow.org/js/guide/tensors_operationshttps://www.tensorflow.org/js/guide/tensors_operations
// Create a rank-2 tensor (matrix) matrix tensor from a multidimensional array.
const a = tf.tensor([[1, 2], [3, 4]]);
console.log('shape:', a.shape);
a.print();

// Or you can create a tensor from a flat array and specify a shape.
const shape = [2, 2];
const b = tf.tensor([1, 2, 3, 4], shape);
console.log('shape:', b.shape);
b.print();

//reshape
const aa = tf.tensor([[1, 2], [3, 4]]);
console.log('aa shape:', aa.shape);
aa.print();

const bb = aa.reshape([4, 1]);
console.log('bb shape:', bb.shape);
bb.print();

//async return actual array/data
 const a1 = tf.tensor([[1, 2], [3, 4]]);
 // Returns the multi dimensional array of values.
 a1.array().then(array => console.log('-> ',array));
 // Returns the flattened data that backs the tensor.
 a1.data().then(data => console.log(data));

 //broadcast, note that t5 shape (1) smaller than t4 so it was 'extended'
 // works only when t5 shape is exactly 1 or same as t4
 const t4 = tf.tensor([1,2,3]);
 const t5=tf.tensor([4]);
 t4.add(t5).print();

 //access via tensor buffer 
 const tfb1 = tf.buffer([1,2,3]);
 const tfb2 = t1.buffer();
 console.log('get(index) ',tfb1.get(0));

 const m1 = tf.tensor([
   [1,2,3],
   [3,4,5],
   [1,2,3],
   [3,4,5],
   [1,2,3],
   [3,4,5]
 ]);

console.log('slices example. extract a column. argument start index [0,1], size and shape [6,1]');
 m1.slice([0,1], [6,1]).print();
 console.log('slices example. extract a column. argument start index [0,1], size and shape [6,0]');
 console.log('note -1 for size or tensor.shape[0] both return max elements in this dimension aka shape...')
 m1.slice([0,1], [-1,0]).print();
 console.log('slices example. extract a column. argument start index [0,1], size and shape [6,2]');
 m1.slice([0,1], [m1.shape[0],2]).print();

 console.log('concat defaults to exteding rows (axis 0)');
  const m2 = tf.tensor([
   [1,2,3],
   [3,4,5],
   [1,2,3],
   [3,4,5],
   [1,2,3],
   [3,4,5]
 ]);
 m1.concat(m2).print()
 console.log('concat defaults to exteding rows, pass (axis) 1 to extend columns');
 m1.concat(m2,1).print()


console:


Yosi Izaq
// Try edit message
const data = {
  message: 'Hello world',
}

$('#msg').html(data.message)

const t1 = tf.tensor([1,2,3]);
const t2 = tf.tensor([4,7,3]);
let t3 = t1.add(t2);
… m1.concat(m2,1).print()
Tensor
    [5, 9, 6]
undefined
t1 shape 
(1) [
3
]
shape:
(2) [
2,
2
]
Tensor
    [[1, 2],
     [3, 4]]
shape:
(2) [
2,
2
]
Tensor
    [[1, 2],
     [3, 4]]
aa shape:
(2) [
2,
2
]
Tensor
    [[1, 2],
     [3, 4]]
bb shape:
(2) [
4,
1
]
Tensor
    [[1],
     [2],
     [3],
     [4]]
Tensor
    [5, 6, 7]
get(index) 0
slices example. extract a column. argument start index [0,1], size and shape [6,1]
Tensor
    [[2],
     [4],
     [2],
     [4],
     [2],
     [4]]
slices example. extract a column. argument start index [0,1], size and shape [6,0]
note -1 for size or tensor.shape[0] both return max elements in this dimension aka shape...
Tensor
    [[],
     [],
     [],
     [],
     [],
     []]
slices example. extract a column. argument start index [0,1], size and shape [6,2]
Tensor
    [[2, 3],
     [4, 5],
     [2, 3],
     [4, 5],
     [2, 3],
     [4, 5]]
concat defaults to exteding rows (axis 0)
Tensor
    [[1, 2, 3],
     [3, 4, 5],
     [1, 2, 3],
     [3, 4, 5],
     [1, 2, 3],
     [3, 4, 5],
     [1, 2, 3],
     [3, 4, 5],
     [1, 2, 3],
     [3, 4, 5],
     [1, 2, 3],
     [3, 4, 5]]
concat defaults to exteding rows, pass (axis) 1 to extend columns
Tensor
    [[1, 2, 3, 1, 2, 3],
     [3, 4, 5, 3, 4, 5],
     [1, 2, 3, 1, 2, 3],
     [3, 4, 5, 3, 4, 5],
     [1, 2, 3, 1, 2, 3],
     [3, 4, 5, 3, 4, 5]]
:1,2,3,4


                1.1.3.1.2 getting started

- sum tensors along axis. pass param. 0 (default) sum along y axis. 1- sum along x axis
- concat works same
- also expandDims
- sort tensors. use unstack which will break a single tensor to an array of tensors which can then be sorted and reconstructed

~/work/code/ML/UdemyNodeJS_ML/MLKits/knn-tf:]2015$ npm i

                1.1.3.1.3


            1.1.3.2


        1.1.4

    1.2 TensorFlow Applications

        1.2.1 KNN 

            1.2.1.1 code


[i500695@C02X632CJGH6:2020-06-18 17:52:00:~/work/code/ML/UdemyNodeJS_ML/MLKits/knn-tf:]2027$ cat index.js 
require('@tensorflow/tfjs-node'); //CPU tf, GPU only supported on linux
const tf = require('@tensorflow/tfjs'); //tensor flow node lib

const loadCSV = require('./load-csv');

function knn(features, labels, predPoint, k)  {
    return features
    .sub(predPoint) // [ [/\X , /\Y] , [], ..., ]
    .pow(2) // [... [ /\X^2 , /\Y^2  ] ]
    .sum(1) // sum on y axis 
    .pow(0.5) //  (/\X^2 + /\Y^2) )^0.5  , pythogrian distance
    .expandDims(1) // we need 2 concat with labels which is 2d #labels,1 ranks. so we change [#features] shape
    // to [#features,1] shape
    .concat( labels, 1) // concat on y axis
    .unstack() // convert the tensor to array of arrays for sorting
    .sort( (a,b) => a.get(0) > b.get(0) ? 1 : -1 ) // a/b[0] - distance
    .slice(0,k) // top k
    .reduce( (acc, elem) => acc + elem.get(1) , 0) / k // use reduce 2 calc sum then average
    
}
let {features, labels, testFeatures, testLabels } = loadCSV('kc_house_data.csv', {
    shuffle: true,
    splitTest: 10,
    dataColumns: ['lat', 'long'],
    labelColumns: ['price']

});

// console.log(testFeatures);
// console.log(testLabels);

//convert to tensors
features = tf.tensor(features)
labels = tf.tensor(labels)

testFeatures.forEach( (element, i) => {
const result = knn( features, labels, tf.tensor(element), 10)
console.log('guess ', i, result, testLabels[i][0]); 
const err =  ((result - testLabels[i][0]) / testLabels[i][0])*(-100);
console.log('error ', i, err);


    [i500695@C02X632CJGH6:2020-06-18 17:51:15:~/work/code/ML/UdemyNodeJS_ML/MLKits/knn-tf:]2027$ node index.js 
2020-06-18 17:51:32.165377: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA
guess  0 1421200 1085000
error  0 -30.98617511520737
guess  1 714001.5 466800
error  1 -52.95661953727506
guess  2 465600 425000
error  2 -9.552941176470588
guess  3 726186 565000
error  3 -28.528495575221243
guess  4 805070 759000
error  4 -6.069828722002635
guess  5 562495 512031
error  5 -9.855653270993358
guess  6 853835 768000
error  6 -11.176432291666668
guess  7 868300 1532500
error  7 43.34094616639478
guess  8 244990 204950
error  8 -19.536472310319592
guess  9 260840 247000
error  9 -5.603238866396762

            1.2.1.2 considerations
-> add more relevant features
-> measure error rate
-> normalization of features or standardtization (std deviation). 
For house sqft_lot feature there can be some extremely large values w/ huge
prices. These can cause bias to higher prices if we were to normalize.
So in such a case (small amount of extremely low/high values) we choose std dev
normalization ( value -average / std_dev)
we use tf.moments which returns mean and variance (which is std_dev^0.5)
-> debugging ML algo
node --inspect-brk index.js

in chrome, chrome://inspect/#devices
inspect
breakpoint in knn
use console log to print 

features.shape
(2) [21602, 3]
features.print()
array_ops.ts:1131 Tensor
    [[47.5200996, -122.3659973, 7936   ],
     [47.3582993, -122.1829987, 9899   ],
     [47.5345001, -122.1790009, 11172  ],
     ...,
     [47.6337013, -122.1539993, 35083  ],
     [47.5089989, -122.3130035, 42070  ],
     [47.6594009, -122.3550034, 1729   ]]

predPoint.print()
array_ops.ts:1131 Tensor
    [47.5610008, -122.2259979, 11019]

mean.print()
array_ops.ts:1131 Tensor
    [47.5600243, -122.2137985, 15100.6171875]

variance.print()
array_ops.ts:1131 Tensor
    [0.0191991, 0.0198359, 1714614784]

scaledPrediction.print()
array_ops.ts:1131 Tensor
    [0.0070479, -0.0866189, -0.098571]

note nice values close to 0

->
            1.2.1.3
        1.2.2
    1.3 linear regression

        1.3.1  Gradient decent
fast, uses trained model

Tries to build a linear realation between independent variables (features) and
dependent variables (labels)
like f(x) = a1*f1 +b1 + a2*f2 +...+ an*f1+bn 

How to measure a guess quality?
one way, compare mean-squared-root. MSE of different guesses.
sigma - summation
MSE = sigma(1,n, (Pred-Value)^2) / n
Pred / Guess = m*X + b

We want MSE to be as low possible but guessing & calculating the MSE brute force is not a good approach.
We don't know the range for the guess, do know the increment and computional weight is heavy.

Better approach calculate the derivative of the prabola when its close to 0 we have a good guess.
so we calclulate the d(MSE(guess)) . We will assumo the m is 0, so our only var is b
(reminder f(x) = (x - value)^2  = x^2 -2*x*value +value^2. f'(x) = 2x -2*value = 2(x-value)  )

Pred = (mx +b)
differential when b is variable
d(MSE(guess)) / db  = 2*sigma(1,n, (Pred-Value) ) / n 

differential when m is variable
d(MSE(guess)) / db  = 2*sigma(1,n, -x(value - Pred) ) / n 

Gradient-Descent algo:
a. pick m,b values
b. Calc slop of MSE when m is variable and another where b is variable
c. if both < threshold (very small) -> return
d. multiply m,b slopes by a small constant called 'learning-rate'
e. substract from m, b. repeat.



        1.3.2 code
~/work/code/ML/UdemyNodeJS_ML/MLKits/regressions$ npm i

cars.csv
MPG = Miles Per Gallon
MPG = m*(Horse-Power) + b

class  LinearRegression
methods:
    gradientDescent() - single Gradient-Descent iteration and update m,b
    train() - iterate gradientDescent() until optimal m,b
    test() - use test set to evalualte the accuracy of m,b
    predict() - make a prediction using m,b

            1.3.2.1 initial arrays based code.

Just the demo the concept.
[i500695@C02X632CJGH6:2020-06-29 16:35:33:~/work/code/ML/UdemyNodeJS_ML/MLKits/regressions:]2034$ cat index.js 
require('@tensorflow/tfjs-node'); //CPU tf, GPU only supported on linux
const tf = require('@tensorflow/tfjs'); //tensor flow node lib
const linearReg = require('./linear-regression');

const loadCSV = require('./load-csv');
const LinearRegression = require('./linear-regression');

let {features, labels, testFeatures, testLabels } = loadCSV('./cars.csv', {
    shuffle: true,
    splitTest: 50,
    dataColumns: ['horsepower'],
    labelColumns: ['mpg']
});

console.log(features, labels);

const regression = new LinearRegression(features, labels, {
    learningRate: 0.001,
    iterations: 100 
});

regression.train();

console.log('m: ', regression.m);
console.log('b: ', regression.b);

[i500695@C02X632CJGH6:2020-06-29 17:08:11:~/work/code/ML/UdemyNodeJS_ML/MLKits/regressions:]2035$ cat linear-regression.js 
const tf = require('@tensorflow/tfjs'); //tensor flow node lib
const _ = require('lodash');

class LinearRegression {

    constructor (features, labels, options) {
        this.features = features;
        this.labels = labels;
        this.options = Object.assign({
             learningRate: 0.1,
             iterations: 1000
            }, options);

        this.m = 0;
        this.b = 0;
    }

// single Gradient-Descent iteration and update m,b
    gradientDescent() {
        this.gradientDescentArrays();

    }

    //Slow impl. using arrays
    gradientDescentArrays() {
        const predictions = this.features.map(row => {
            return this.m*row[0] + this.b;
        });

        //calc b and m slopes
        const bDerivative = _.sum(predictions.map( (pred, i) => {
            return pred - this.labels[i][0]; 
        })) *2 / this.features.length; 

        const mDerivative = _.sum(predictions.map( (pred, i) => {
            return -1 * this.features[i][0] *( this.labels[i][0] - pred ); 
        })) *2 / this.features.length; 

        //mult by learning-rate
        this.m = this.m - mDerivative*this.options.learningRate;
        this.b = this.b - bDerivative*this.options.learningRate;
    };

    //Fast impl. using tensorflow
    gradientDescentTF() {

    }

// iterate gradientDescent() until optimal m,b
    train() {

        for (let index = 0; index < this.options.iterations; index++) {
            this.gradientDescent(); 
        }
    }

// use test set to evalualte the accuracy of m,b
    test() {

    } 

// make a prediction using m,b
    predict(){

    } 

}

module.exports = LinearRegression;

-> run
[i500695@C02X632CJGH6:2020-06-29 17:09:06:~/work/code/ML/UdemyNodeJS_ML/MLKits/regressions:]2036$ node index.js 
2020-06-29 17:09:43.656391: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA
[
  [ 90 ],  [ 150 ], [ 152 ], [ 75 ],  [ 110 ], [ 97 ],  [ 67 ],
  [ 97 ],  [ 49 ],  [ 82 ],  [ 75 ],  [ 155 ], [ 67 ],  [ 46 ],
  [ 85 ],  [ 88 ],  [ 142 ], [ 60 ],  [ 70 ],  [ 83 ],  [ 65 ],
  [ 150 ], [ 95 ],  [ 125 ], [ 190 ], [ 110 ], [ 70 ],  [ 145 ],
  [ 95 ],  [ 68 ],  [ 175 ], [ 90 ],  [ 84 ],  [ 90 ],  [ 90 ],
  [ 88 ],  [ 130 ], [ 100 ], [ 105 ], [ 110 ], [ 92 ],  [ 70 ],
  [ 110 ], [ 76 ],  [ 140 ], [ 115 ], [ 153 ], [ 150 ], [ 62 ],
  [ 88 ],  [ 62 ],  [ 105 ], [ 122 ], [ 71 ],  [ 165 ], [ 105 ],
  [ 85 ],  [ 85 ],  [ 90 ],  [ 83 ],  [ 150 ], [ 165 ], [ 70 ],
  [ 75 ],  [ 95 ],  [ 80 ],  [ 110 ], [ 70 ],  [ 145 ], [ 58 ],
  [ 79 ],  [ 112 ], [ 90 ],  [ 98 ],  [ 85 ],  [ 140 ], [ 79 ],
  [ 160 ], [ 69 ],  [ 95 ],  [ 103 ], [ 78 ],  [ 88 ],  [ 95 ],
  [ 180 ], [ 90 ],  [ 90 ],  [ 150 ], [ 140 ], [ 70 ],  [ 86 ],
  [ 84 ],  [ 98 ],  [ 92 ],  [ 215 ], [ 140 ], [ 70 ],  [ 92 ],
  [ 193 ], [ 90 ],
  ... 242 more items
] [
  [ 22.5 ], [ 16 ],   [ 14.5 ], [ 26 ],   [ 18.6 ], [ 23.9 ], [ 30 ],
  [ 18 ],   [ 29 ],   [ 31 ],   [ 31.3 ], [ 16.9 ], [ 32 ],   [ 26 ],
  [ 20.2 ], [ 34 ],   [ 15.5 ], [ 38.1 ], [ 34.5 ], [ 29 ],   [ 34.4 ],
  [ 15 ],   [ 20.5 ], [ 19.2 ], [ 13 ],   [ 15 ],   [ 33.5 ], [ 19.2 ],
  [ 18 ],   [ 31 ],   [ 14 ],   [ 20 ],   [ 29 ],   [ 19.1 ], [ 20.2 ],
  [ 20.2 ], [ 15 ],   [ 23.7 ], [ 27.9 ], [ 22.4 ], [ 24 ],   [ 30 ],
  [ 25 ],   [ 41.5 ], [ 17.5 ], [ 26.8 ], [ 14 ],   [ 13 ],   [ 37.7 ],
  [ 22.3 ], [ 29.8 ], [ 16 ],   [ 20 ],   [ 29.5 ], [ 13 ],   [ 18 ],
  [ 31 ],   [ 20.8 ], [ 28.4 ], [ 27 ],   [ 13 ],   [ 17.7 ], [ 29 ],
  [ 26 ],   [ 19 ],   [ 25 ],   [ 21 ],   [ 34 ],   [ 13 ],   [ 39.1 ],
  [ 26 ],   [ 19 ],   [ 19 ],   [ 22 ],   [ 21 ],   [ 16 ],   [ 28 ],
  [ 12 ],   [ 37.3 ], [ 25 ],   [ 20.3 ], [ 26 ],   [ 25.1 ], [ 21.1 ],
  [ 16 ],   [ 33.5 ], [ 18 ],   [ 15 ],   [ 19.4 ], [ 36 ],   [ 28 ],
  [ 26.6 ], [ 18.5 ], [ 25.8 ], [ 10 ],   [ 13 ],   [ 39.4 ], [ 28 ],
  [ 9 ],    [ 26 ],
  ... 242 more items
]
m:  -4.852908418956361e+136
b:  -4.101833295159614e+134

Note huge m,b values. reason - learning rate too high.
Too possible fixes.
a. reduce learning rate 
b. normalization or standardtization of input

->

            1.3.2.2 vecorized Gradient-Descent tensorflow code
tensor are matrices.
1st convert the separate MSE derivative calculation to a "bundled" calculation where m,b are in a weights tensor.
Then (consider all terms are tensors)
Slopes = F*(F*W-L) / n

where F=features, W=weights, L=labels

We convert the operations to work on tensors. 
F tesor we add (first or) last column of 1s. (this is for the b multiplier in mf_i +b). This way shape of F is Nf,2 
mulitply by m,b tensor shape 2,1 
When needed we transpose the tensors to enable multiplication...

| Feature1 | Feature2 | 1s
| f11      | f21      | 1
| f21      | f2       | 1

| m |
| b |

To generate the 1s tensor. call tf.ones([rows,columns]). ex:
        tf.ones([this.features.shape[0], 1]).concat(this.features, 1);



                1.3.2.2.1 source code
Two locations, as branch of Udemy code and as my public branch.

~/work/code/ML/UdemyNodeJS_ML/MLKits/regressions, 

~/work/git/myProjects/UdemyMLRegressions, https://github.com/izaqyos/UdemyMLRegressions


                1.3.2.2.2 


            1.3.2.3 coefficient of determination

SSres = Sum of Squares residual = Sum(i,n, (Actual - Predicted)^2)
SStot = Sum of Squares total = Sum(i,n, (Actual - Average)^2)
R^2 = 1 - (SSres / SStot)

R^2 closer to 1 indicates a good guess 

We will apply standardtization on the features

            1.3.2.4 multivariant 
To get better Rsquare we will add more features.
The relation is
MPG = b + sum(1,k, Mi*Fi)

So we do matrix multiplication of 
1, X11, X12, ..., X1n
...
1, Xn1, Xn2, ..., Xnn 

By [b ]
   [M1 ]
   ...
   [Mn ]

This will give the difference matrix (shape [n,1] n is # of features)
Now we calculate MSE
So matrix multiplication. Features.T * Differences / n
.T is transpose

We need a method for adjusting the learning rate. like, ADAM, ADGARD etc.
Basically they are variants of following algorithm.
Keep track of all MSEs during Gradient-Descent iterations.
in each iternation compare current MSE to previous 
If MSE went up (bad) learning-rate = learning-rate /2 
If MSE went down (good) learning-rate = learning-rate * 1.05


            1.3.2.5 Batch and stochastic gradient decent
instead of using the entire training set to update m and b use a batch at a time
If batch size is 1 it is a special case called stochastic-gradient-decent

we will update index.js to pass batchSize as option and the train() and Gradient-Descent() to send and process batches 

Use tf slice method. see, https://www.tensorflow.org/api_docs/python/tf/slice.
tf.slice(
    input_, begin, size, name=None
)
begins is a vector of indices according to shape for begin point.
size is how much to take per dimension

            1.3.2.6
        1.3.3
    1.4 Natural binary classification

Logistic-regression. predict discrete values. used for classification
binary-classification. classify as one of two possible values. like spam/not spam for email.

        1.4.1  problem example

            1.4.1.1  intro


given users age. Does he prefer to read books or watch movies. => Find a mathematical formula that relates age with book/movie prefernce
P = m*age +b 

We gather a data set.   column for age. column for prefernce.
First of we need to convert the prefernce to numbers. so movie->0, book->1.

We can apply linear-regression on data set and get P=m*age +b equation.
But some P values fall out of [0,1] range.These are not good.
This is way a linear function is not good

            1.4.1.1  sigmoid-equation
sigmoid, f(z) = 1/(1+pow(e,-z)), guaranteed range (0,1)
e - euler constant. 2.718
P(1) = 1 / (1 + pow(e, -(m*x+b))) , probablity of the person prefers books, or that the label is 1

Decision boundary. a threshold every probablity higher than boundary is assigned 1. lower is assigned 0.
For instance. 0.5. 

Algo steps:
a. encode labels (change strings to 0/1)
b. guess m,b
c. calc slope of MSE using all observations in training set and m,b
d. Multiply the slope by the learning rate
e. update m,b

            1.4.1.2


            1.4.1.2

        1.4.2 smog test
Problem: given weight, mpg and engine displacement will the car pass a smog emissions test

For Logistic-regression we estimate accuracy not using MSE. instead we use cross entropy:
-(1/n)Sum(i=0,n) of Actual*log(guess)+(1-actual)*log(1-guess)
where
Actual, label value
guess, sigmoid(mx+b)
n, # observations

Converted to matrixes (tensors) it is very similar to MSE calculations.
Features.T*(sigmoid(Features*Weights)-Labels)

TF supports sigmoid function so to apply the sigmoid just chain a .sigmoid() to Features*Weights tensor

            1.4.2.1 Initial Code
[i500695@C02X632CJGH6:2021-04-12 19:08:57:~/work/code/ML/UdemyNodeJS_ML/MLKits/orig/regressions/logistic-regression:]2066$  cat index.js | pbcopy 
require('@tensorflow/tfjs-node');
const tf = require('@tensorflow/tfjs');
const loadCSV = require('../load-csv');
const LogisticRegression = require('./logistic-regression');

let { features, labels, testFeatures, testLabels } = loadCSV('../data/cars.csv', {
  shuffle: true,
  splitTest: 50, // 50 rows for tests
  dataColumns: ['horsepower', 'displacement', 'weight'],
  labelColumns: ['passedemissions'],
  converters: {
      passedemissions: val => { // each value of passedemissions column is passed to the labda. which performs labed encoding
      return val === 'TRUE' ? 1 : 0 // TRUE string to 1, else 0 
    }
  }
});

const regression = new LogisticRegression(features, labels, {
    learningRate: 0.5,
    iterations: 100,
    batchSize: 50
});
regression.train();

console.log('Predict a car we know has failed and a car that passed...');
regression.predict([
    [130, 307, 1.75],
    [88, 97, 1.065],
]).print();
console.log('note 0.23 and 0.94 values. means 23%,94% chance to pass test. If we set descion boundry to 0.5 thats one fail and one pass');

console.log('tested accuracy', regression.test(testFeatures, testLabels));
// console.log(labels); // verify labels are encoded

[i500695@C02X632CJGH6:2021-04-12 19:09:57:~/work/code/ML/UdemyNodeJS_ML/MLKits/orig/regressions/logistic-regression:]2067$  cat logistic-regression.js | pbcopy 
const tf = require('@tensorflow/tfjs');

class LogisticRegression {
  constructor(features, labels, options) {
    this.features = this.processFeatures(features);
    this.labels = tf.tensor(labels);
    this.mseHistory = [];

    this.options = Object.assign(
      { learningRate: 0.1, iterations: 1000 },
      options
    );

    this.weights = tf.zeros([this.features.shape[1], 1]);
  }

  gradientDescent(features, labels) {
    const currentGuesses = features.matMul(this.weights).sigmoid();
    const differences = currentGuesses.sub(labels);

    const slopes = features
      .transpose()
      .matMul(differences)
      .div(features.shape[0]);

    this.weights = this.weights.sub(slopes.mul(this.options.learningRate));
  }

  train() {
    const batchQuantity = Math.floor(
      this.features.shape[0] / this.options.batchSize
    );

    for (let i = 0; i < this.options.iterations; i++) {
      for (let j = 0; j < batchQuantity; j++) {
        const startIndex = j * this.options.batchSize;
        const { batchSize } = this.options;

        const featureSlice = this.features.slice(
          [startIndex, 0],
          [batchSize, -1]
        );
        const labelSlice = this.labels.slice([startIndex, 0], [batchSize, -1]);

        this.gradientDescent(featureSlice, labelSlice);
      }

      this.recordMSE();
      this.updateLearningRate();
    }
  }

  predict(observations) {
    return this.processFeatures(observations).matMul(this.weights).sigmoid();
  }

  test(testFeatures, testLabels) {
      const predictions = this.predict(testFeatures).round();
      testLabels = tf.tensor(testLabels);
      const incorrect = predictions.sub(testLabels).abs().sum().get();
      const numPreds = predictions.shape[0];
      const incorrectRatio = (numPreds-incorrect)/numPreds;
      return incorrectRatio;
  }

  processFeatures(features) {
    features = tf.tensor(features);
    features = tf.ones([features.shape[0], 1]).concat(features, 1);

    if (this.mean && this.variance) {
      features = features.sub(this.mean).div(this.variance.pow(0.5));
    } else {
      features = this.standardize(features);
    }

    return features;
  }

  standardize(features) {
    const { mean, variance } = tf.moments(features, 0);

    this.mean = mean;
    this.variance = variance;

    return features.sub(mean).div(variance.pow(0.5));
  }

  recordMSE() {
    const mse = this.features
      .matMul(this.weights)
      .sub(this.labels)
      .pow(2)
      .sum()
      .div(this.features.shape[0])
      .get();

    this.mseHistory.unshift(mse);
  }

  updateLearningRate() {
    if (this.mseHistory.length < 2) {
      return;
    }

    if (this.mseHistory[0] > this.mseHistory[1]) {
      this.options.learningRate /= 2;
    } else {
      this.options.learningRate *= 1.05;
    }
  }
}

module.exports = LogisticRegression;


Test:
[i500695@C02X632CJGH6:2021-04-12 19:08:24:~/work/code/ML/UdemyNodeJS_ML/MLKits/orig/regressions/logistic-regression:]2066$ node index.js 
2021-04-12 19:08:56.493919: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.2 AVX AVX2 FMA
Predict a car we know has failed and a car that passed...
Tensor
    [[0.2314757],
     [0.9403299]]
note 0.23 and 0.94 values. means 23%,94% chance to pass test. If we set descion boundry to 0.5 thats one fail and one pass
tested accuracy 0.88

            1.4.2.2 improvements
A. round() applies a Decision-Boundary of 0.5. To have a more flexible boundry we can pass the threshold as a parameter and use tf.greater(boundaryVal);
            1.4.2.3
        1.4.3



    1.5 multi value classification


    1.6 Image recognition


    1.7 performance optimization

    1.8 Appendix

    1.9
2.  Python 

    2.1  vectorized Gradient-Descent 

        2.1.1  https://towardsdatascience.com/gradient-descent-in-python-a0d07285742f

        2.1.2 KNN

            2.1.2.1 knn scikit iris demo
https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/


            2.1.2.2


        2.1.3

    2.2

3. NodeJS

    3.1 vectorized Gradient-Descent  
https://www.toptal.com/python/gradient-descent-in-tensorflow#example-1-linear-regression-with-gradient-descent-in-tensorflow-20

    3.2
4. My examples

    4.1 ML intro presentation
    /Users/i500695/work/code/ML/ML_Intro.pptx

- iris knn demo:
/Users/i500695/work/code/ML/python/knn/iris

- ML5js demos
[i500695@C02X632CJGH6:2021-02-11 16:14:56:~/work/code/ML/ml5js/helloml5:]2074$ open -a Google\ Chrome --args --allow-file-access-from-files  ./index.html
[link](~/work/KB/KB_sql)

    4.2

5.
