.........................................Table Of Contents...............................................................
1. postgres <URL:#tn=1. postgres>
    1.1 pagination <URL:#tn=    1.1 pagination>
        1.1.0  Full article <URL:#tn=        1.1.0  Full article>
        1.1.1  Limit offset test <URL:#tn=        1.1.1  Limit offset test>
        1.1.2 cursors demo <URL:#tn=        1.1.2 cursors demo>
        1.1.3 keyset pagination <URL:#tn=        1.1.3 keyset pagination>
            1.1.3.1 keyset pagination demo <URL:#tn=            1.1.3.1 keyset pagination demo>
        1.1.4 keyset pagination <URL:#tn=        1.1.4 keyset pagination>
            1.1.4.1 article <URL:#tn=            1.1.4.1 article>
            1.1.4.2 supporting libraries <URL:#tn=            1.1.4.2 supporting libraries>
                1.1.4.2.1 jOOQ — Java Object Oriented Querying. Docs. <URL:#tn=                1.1.4.2.1 jOOQ — Java Object Oriented Querying. Docs.>
                1.1.4.2.2 Massive.js, a data mapper for Node.js that goes all in on PostgreSQL: Keyset Documentation <URL:#tn=                1.1.4.2.2 Massive.js, a data mapper for Node.js that goes all in on PostgreSQL: Keyset Documentation>
                    1.1.4.2.2.1 https://blog.jooq.org/2013/11/18/faster-sql-pagination-with-keysets-continued/ <URL:#tn=                    1.1.4.2.2.1 https://blog.jooq.org/2013/11/18/faster-sql-pagination-with-keysets-continued/>
                    1.1.4.2.2.2 <URL:#tn=                    1.1.4.2.2.2>
                1.1.4.2.3 <URL:#tn=                1.1.4.2.3>
            1.1.4.3 <URL:#tn=            1.1.4.3>
        1.1.5 <URL:#tn=        1.1.5>
    1.2 CLI <URL:#tn=    1.2 CLI>
        1.2.1 pgcli <URL:#tn=        1.2.1 pgcli>
            1.2.1.1 install, $ brew install pgcli <URL:#tn=            1.2.1.1 install, $ brew install pgcli>
            1.2.1.2 Usage <URL:#tn=            1.2.1.2 Usage>
            1.2.1.3 Example, connect and work with cdm-store DB, tags: Example cdm-store DB <URL:#tn=            1.2.1.3 Example, connect and work with cdm-store DB, tags: Example cdm-store DB>
                1.2.1.3.1 login to space, e.g. LTS <URL:#tn=                1.2.1.3.1 login to space, e.g. LTS>
                1.2.1.3.2 open ssh tunnel <URL:#tn=                1.2.1.3.2 open ssh tunnel>
                    1.2.1.3.2.1 Using Jawad portal CLI <URL:#tn=                    1.2.1.3.2.1 Using Jawad portal CLI>
                        1.2.1.3.2.1.1 raw command <URL:#tn=                        1.2.1.3.2.1.1 raw command>
                        1.2.1.3.2.1.2 my alias <URL:#tn=                        1.2.1.3.2.1.2 my alias>
                        1.2.1.3.2.1.3 possible issues <URL:#tn=                        1.2.1.3.2.1.3 possible issues>
                            1.2.1.3.2.1.3.1 ssh disconnect <URL:#tn=                            1.2.1.3.2.1.3.1 ssh disconnect>
                            1.2.1.3.2.1.3.2 <URL:#tn=                            1.2.1.3.2.1.3.2>
                        1.2.1.3.2.1.4 <URL:#tn=                        1.2.1.3.2.1.4>
                    1.2.1.3.2.2 using SSH <URL:#tn=                    1.2.1.3.2.2 using SSH>
                1.2.1.3.3 connect to DB <URL:#tn=                1.2.1.3.3 connect to DB>
                    1.2.1.3.3.1 via pgcli <URL:#tn=                    1.2.1.3.3.1 via pgcli>
                    1.2.1.3.3.2 via pg admin <URL:#tn=                    1.2.1.3.3.2 via pg admin>
                1.2.1.3.4 some basic operations <URL:#tn=                1.2.1.3.4 some basic operations>
                    1.2.1.3.4.1  describe <URL:#tn=                    1.2.1.3.4.1  describe>
                    1.2.1.3.4.2  describe schemas <URL:#tn=                    1.2.1.3.4.2  describe schemas>
                    1.2.1.3.4.3  describe tables <URL:#tn=                    1.2.1.3.4.3  describe tables>
                    1.2.1.3.4.4 count number of entities <URL:#tn=                    1.2.1.3.4.4 count number of entities>
                        1.2.1.3.4.4.1 set schema, tags: set schema <URL:#tn=                        1.2.1.3.4.4.1 set schema, tags: set schema>
                    1.2.1.3.4.5 <URL:#tn=                    1.2.1.3.4.5>
                    1.2.1.3.4.6 <URL:#tn=                    1.2.1.3.4.6>
                1.2.1.3.5 CDM store interesting statements, tags: CDM store interesting statements <URL:#tn=                1.2.1.3.5 CDM store interesting statements, tags: CDM store interesting statements>
                    1.2.1.3.5.1 get entities <URL:#tn=                    1.2.1.3.5.1 get entities>
                    1.2.1.3.5.2 get site entities <URL:#tn=                    1.2.1.3.5.2 get site entities>
                    1.2.1.3.5.3 get tenants <URL:#tn=                    1.2.1.3.5.3 get tenants>
                    1.2.1.3.5.4 Events log table <URL:#tn=                    1.2.1.3.5.4 Events log table>
                    1.2.1.3.5.5 gather statistics <URL:#tn=                    1.2.1.3.5.5 gather statistics>
                        1.2.1.3.5.5.1 01/02/24 12:40:47  on LTS EU10, tags: 01/02/24 12:40:47  on LTS EU10 <URL:#tn=                        1.2.1.3.5.5.1 01/02/24 12:40:47  on LTS EU10, tags: 01/02/24 12:40:47  on LTS EU10>
                        1.2.1.3.5.5.2 Count number of entities per tenants and also transitive site local entities <URL:#tn=                        1.2.1.3.5.5.2 Count number of entities per tenants and also transitive site local entities>
                        1.2.1.3.5.5.3 <URL:#tn=                        1.2.1.3.5.5.3>
                    1.2.1.3.5.6 get identityZoneIds <URL:#tn=                    1.2.1.3.5.6 get identityZoneIds>
                    1.2.1.3.5.7 get all businessapps of a given tenant <URL:#tn=                    1.2.1.3.5.7 get all businessapps of a given tenant>
                    1.2.1.3.5.8 get all local businessapp that don't contain property in json <URL:#tn=                    1.2.1.3.5.8 get all local businessapp that don't contain property in json>
                    1.2.1.3.5.9 Count apollo providers <URL:#tn=                    1.2.1.3.5.9 Count apollo providers>
                1.2.1.3.6 <URL:#tn=                1.2.1.3.6>
            1.2.1.4 pgcli configuration <URL:#tn=            1.2.1.4 pgcli configuration>
                1.2.1.4.1 Switch between vi and emacs key bindings. F4. <URL:#tn=                1.2.1.4.1 Switch between vi and emacs key bindings. F4.>
                1.2.1.4.2 meta keybindings <URL:#tn=                1.2.1.4.2 meta keybindings>
                1.2.1.4.3 named queries <URL:#tn=                1.2.1.4.3 named queries>
                    1.2.1.4.3.1 https://www.pgcli.com/named_queries.md <URL:#tn=                    1.2.1.4.3.1 https://www.pgcli.com/named_queries.md>
                    1.2.1.4.3.2 my examples <URL:#tn=                    1.2.1.4.3.2 my examples>
                    1.2.1.4.3.3 <URL:#tn=                    1.2.1.4.3.3>
                1.2.1.4.4 <URL:#tn=                1.2.1.4.4>
            1.2.1.5 <URL:#tn=            1.2.1.5>
        1.2.2 psql <URL:#tn=        1.2.2 psql>
            1.2.2.1  common commands <URL:#tn=            1.2.2.1  common commands>
                1.2.2.1.1  list DBs <URL:#tn=                1.2.2.1.1  list DBs>
                1.2.2.1.2 list tables <URL:#tn=                1.2.2.1.2 list tables>
                    1.2.2.1.2.1 \dt <URL:#tn=                    1.2.2.1.2.1 \dt>
                    1.2.2.1.2.2 Chatgpt: <URL:#tn=                    1.2.2.1.2.2 Chatgpt:>
                        1.2.2.1.2.2.1 list all tables in PG db <URL:#tn=                        1.2.2.1.2.2.1 list all tables in PG db>
                        1.2.2.1.2.2.2 pgcli connect to remote pg db <URL:#tn=                        1.2.2.1.2.2.2 pgcli connect to remote pg db>
                        1.2.2.1.2.2.3 pg list schemas <URL:#tn=                        1.2.2.1.2.2.3 pg list schemas>
                        1.2.2.1.2.2.4 pg_repack <URL:#tn=                        1.2.2.1.2.2.4 pg_repack>
                        1.2.2.1.2.2.5 <URL:#tn=                        1.2.2.1.2.2.5>
                1.2.2.1.3 switch DB <URL:#tn=                1.2.2.1.3 switch DB>
                1.2.2.1.4 describe table <URL:#tn=                1.2.2.1.4 describe table>
                1.2.2.1.5 history <URL:#tn=                1.2.2.1.5 history>
                1.2.2.1.6 list commands <URL:#tn=                1.2.2.1.6 list commands>
                1.2.2.1.7 help on commands <URL:#tn=                1.2.2.1.7 help on commands>
                1.2.2.1.8 measure command runtime <URL:#tn=                1.2.2.1.8 measure command runtime>
        1.2.9 edit commands in vim <URL:#tn=        1.2.9 edit commands in vim>
        1.2.10 <URL:#tn=        1.2.10>
            1.2.2.2 <URL:#tn=            1.2.2.2>
        1.2.3 determine size table, db, indexes etc <URL:#tn=        1.2.3 determine size table, db, indexes etc>
            1.2.3.1 https://www.a2hosting.com/kb/developer-corner/postgresql/determining-the-size-of-postgresql-databases-and-tables <URL:#tn=            1.2.3.1 https://www.a2hosting.com/kb/developer-corner/postgresql/determining-the-size-of-postgresql-databases-and-tables>
            1.2.3.2 examples from cdm-store <URL:#tn=            1.2.3.2 examples from cdm-store>
                1.2.3.2.1 get table size <URL:#tn=                1.2.3.2.1 get table size>
                1.2.3.2.2 list top 10 sized tables <URL:#tn=                1.2.3.2.2 list top 10 sized tables>
                1.2.3.2.3 get indexes size <URL:#tn=                1.2.3.2.3 get indexes size>
                1.2.3.2.4 search for provider contention <URL:#tn=                1.2.3.2.4 search for provider contention>
                1.2.3.2.5 <URL:#tn=                1.2.3.2.5>
            1.2.3.3 <URL:#tn=            1.2.3.3>
        1.2.4 <URL:#tn=        1.2.4>
    1.3 text search <URL:#tn=    1.3 text search>
        1.3.1 <URL:#tn=        1.3.1>
        1.3.2 <URL:#tn=        1.3.2>
    1.4 performance <URL:#tn=    1.4 performance>
        1.4.1  SAP cdm-store DB performance <URL:#tn=        1.4.1  SAP cdm-store DB performance>
        1.4.2 <URL:#tn=        1.4.2>
    1.5 local playground, tags: postgres local playground docker pg <URL:#tn=    1.5 local playground, tags: postgres local playground docker pg>
        1.5.1  setup local postgres playground <URL:#tn=        1.5.1  setup local postgres playground>
        1.5.2 common operations <URL:#tn=        1.5.2 common operations>
            1.5.2.1  insert into example <URL:#tn=            1.5.2.1  insert into example>
            1.5.2.2 comparing two tables <URL:#tn=            1.5.2.2 comparing two tables>
            1.5.2.3 <URL:#tn=            1.5.2.3>
        1.5.3 <URL:#tn=        1.5.3>
    1.6 pgadmin <URL:#tn=    1.6 pgadmin>
        1.6.1  install and setup <URL:#tn=        1.6.1  install and setup>
        1.6.2 <URL:#tn=        1.6.2>
    1.7 Mastering SQL using Postgresql <URL:#tn=    1.7 Mastering SQL using Postgresql>
    1.8 PostgreSQL: Advanced SQL Queries and Data Analysis <URL:#tn=    1.8 PostgreSQL: Advanced SQL Queries and Data Analysis>
    1.9 postgres transactions and concurrency, tags: postgres transactions and concurrency <URL:#tn=    1.9 postgres transactions and concurrency, tags: postgres transactions and concurrency>
        1.9.1 https://www.postgresql.org/files/developer/transactions.pdf <URL:#tn=        1.9.1 https://www.postgresql.org/files/developer/transactions.pdf>
            1.9.1.1 What is a transaction, anyway? <URL:#tn=            1.9.1.1 What is a transaction, anyway?>
            1.9.1.2 The ACID test: atomic, consistent, isolated, durable <URL:#tn=            1.9.1.2 The ACID test: atomic, consistent, isolated, durable>
            1.9.1.3 But how can thousands of changes be made "atomically"? <URL:#tn=            1.9.1.3 But how can thousands of changes be made "atomically"?>
            1.9.1.4 But is it really atomic and durable, even if the system crashes? <URL:#tn=            1.9.1.4 But is it really atomic and durable, even if the system crashes?>
            1.9.1.5 Working through the Unix kernel costs us something, too <URL:#tn=            1.9.1.5 Working through the Unix kernel costs us something, too>
            1.9.1.6 User’s view: multi-version concurrency control <URL:#tn=            1.9.1.6 User’s view: multi-version concurrency control>
            1.9.1.7 Concurrent updates are tricky <URL:#tn=            1.9.1.7 Concurrent updates are tricky>
            1.9.1.8 Read committed vs. serializable transaction level <URL:#tn=            1.9.1.8 Read committed vs. serializable transaction level>
            1.9.1.9 How it’s implemented <URL:#tn=            1.9.1.9 How it’s implemented>
            1.9.1.10 Non-overwriting storage management <URL:#tn=            1.9.1.10 Non-overwriting storage management>
            1.9.1.11 Per-tuple status information <URL:#tn=            1.9.1.11 Per-tuple status information>
            1.9.1.12 "Snapshots" filter away active transactions <URL:#tn=            1.9.1.12 "Snapshots" filter away active transactions>
            1.9.1.13 Table-level locks: still gotta have ’em for some things <URL:#tn=            1.9.1.13 Table-level locks: still gotta have ’em for some things>
            1.9.1.14 Types of locks <URL:#tn=            1.9.1.14 Types of locks>
            1.9.1.15 Lock implementation <URL:#tn=            1.9.1.15 Lock implementation>
            1.9.1.16 Deadlock detection <URL:#tn=            1.9.1.16 Deadlock detection>
            1.9.1.17 Short-term locks <URL:#tn=            1.9.1.17 Short-term locks>
            1.9.1.18 Summary <URL:#tn=            1.9.1.18 Summary>
            1.9.1.19 <URL:#tn=            1.9.1.19>
        1.9.2 <URL:#tn=        1.9.2>
    1.10 postgres DB maintenance , tags: postgres DB maintenance <URL:#tn=    1.10 postgres DB maintenance , tags: postgres DB maintenance>
        1.10.1 VACUUM <URL:#tn=        1.10.1 VACUUM>
            1.10.1.1 get list of dead tuples, tags: get list of dead tuples <URL:#tn=            1.10.1.1 get list of dead tuples, tags: get list of dead tuples>
                1.10.1.1.1  simple query <URL:#tn=                1.10.1.1.1  simple query>
                1.10.1.1.2 Jawad's query <URL:#tn=                1.10.1.1.2 Jawad's query>
                1.10.1.1.3 <URL:#tn=                1.10.1.1.3>
            1.10.1.2 Chatgpt q explain autovacuum_vacuum_scale_factor in pg db, tags: Chatgpt q explain autovacuum_vacuum_scale_factor in pg db <URL:#tn=            1.10.1.2 Chatgpt q explain autovacuum_vacuum_scale_factor in pg db, tags: Chatgpt q explain autovacuum_vacuum_scale_factor in pg db>
            1.10.1.3 q. what are the downsides of setting autovacuum_vacuum_scale_factor to 0.02 <URL:#tn=            1.10.1.3 q. what are the downsides of setting autovacuum_vacuum_scale_factor to 0.02>
            1.10.1.4 <URL:#tn=            1.10.1.4>
        1.10.2 <URL:#tn=        1.10.2>
    1.11 async queries, tags: pg, postgresql async queries <URL:#tn=    1.11 async queries, tags: pg, postgresql async queries>
        1.11.1 pg run queries async <URL:#tn=        1.11.1 pg run queries async>
        1.11.2 is there a PG extension that allows for asyn run of queries <URL:#tn=        1.11.2 is there a PG extension that allows for asyn run of queries>
        1.11.3 from java 8 based PG client <URL:#tn=        1.11.3 from java 8 based PG client>
        1.11.4 how to parse PG resuletSet in java <URL:#tn=        1.11.4 how to parse PG resuletSet in java>
        1.11.5 <URL:#tn=        1.11.5>
    1.12 <URL:#tn=    1.12>
2. courses <URL:#tn=2. courses>
    2.1 Udemy SQL, No SQL, Big Data & Hadoop <URL:#tn=    2.1 Udemy SQL, No SQL, Big Data & Hadoop>
        2.1.1  relational DBs <URL:#tn=        2.1.1  relational DBs>
            2.1.1.1   intro <URL:#tn=            2.1.1.1   intro>
            2.1.1.2 mysql <URL:#tn=            2.1.1.2 mysql>
                2.1.1.2.1 mysql docker <URL:#tn=                2.1.1.2.1 mysql docker>
                2.1.1.2.2 client: <URL:#tn=                2.1.1.2.2 client:>
                2.1.1.2.3 Connect to docker db <URL:#tn=                2.1.1.2.3 Connect to docker db>
                2.1.1.2.4 full guide https://phoenixnap.com/kb/mysql-docker-container <URL:#tn=                2.1.1.2.4 full guide https://phoenixnap.com/kb/mysql-docker-container>
                    2.1.1.2.4.1 Step 1: Pull the MySQL Docker Image <URL:#tn=                    2.1.1.2.4.1 Step 1: Pull the MySQL Docker Image>
                    2.1.1.2.4.2 Step 2: Deploy the MySQL Container <URL:#tn=                    2.1.1.2.4.2 Step 2: Deploy the MySQL Container>
                    2.1.1.2.4.3 Step 3: Connect to the MySQL Docker Container <URL:#tn=                    2.1.1.2.4.3 Step 3: Connect to the MySQL Docker Container>
                    2.1.1.2.4.4 Configure MySQL Container <URL:#tn=                    2.1.1.2.4.4 Configure MySQL Container>
                    2.1.1.2.4.5 Manage Data Storage <URL:#tn=                    2.1.1.2.4.5 Manage Data Storage>
                    2.1.1.2.4.6 <URL:#tn=                    2.1.1.2.4.6>
                2.1.1.2.5 load movielens sql dump <URL:#tn=                2.1.1.2.5 load movielens sql dump>
                2.1.1.2.6 OLTP vs OLAP <URL:#tn=                2.1.1.2.6 OLTP vs OLAP>
                2.1.1.2.7 Data Processing <URL:#tn=                2.1.1.2.7 Data Processing>
                2.1.1.2.8 <URL:#tn=                2.1.1.2.8>
            2.1.1.3 indexes <URL:#tn=            2.1.1.3 indexes>
                2.1.1.3.1 indexes sql <URL:#tn=                2.1.1.3.1 indexes sql>
                2.1.1.3.2 <URL:#tn=                2.1.1.3.2>
            2.1.1.4 OLAP <URL:#tn=            2.1.1.4 OLAP>
                2.1.1.4.1 sql to create Data-Werehousing DB <URL:#tn=                2.1.1.4.1 sql to create Data-Werehousing DB>
                2.1.1.4.2 Analytical Processing <URL:#tn=                2.1.1.4.2 Analytical Processing>
                    2.1.1.4.2.1 Aggregation Queries. Reduce. e.g. sum, avg, std dev etc <URL:#tn=                    2.1.1.4.2.1 Aggregation Queries. Reduce. e.g. sum, avg, std dev etc>
                        2.1.1.4.2.1.1 aggregation_queries_dw.sql <URL:#tn=                        2.1.1.4.2.1.1 aggregation_queries_dw.sql>
                    2.1.1.4.2.2 Window Queries <URL:#tn=                    2.1.1.4.2.2 Window Queries>
                    2.1.1.4.2.3 <URL:#tn=                    2.1.1.4.2.3>
                2.1.1.4.3 <URL:#tn=                2.1.1.4.3>
            2.1.1.5 transaction logs <URL:#tn=            2.1.1.5 transaction logs>
            2.1.1.6 Analytical queries <URL:#tn=            2.1.1.6 Analytical queries>
                2.1.1.6.1 RANK <URL:#tn=                2.1.1.6.1 RANK>
                2.1.1.6.2 <URL:#tn=                2.1.1.6.2>
            2.1.1.7 DB classification <URL:#tn=            2.1.1.7 DB classification>
                2.1.1.7.1 distributed DBs <URL:#tn=                2.1.1.7.1 distributed DBs>
                2.1.1.7.2 CAP theorem <URL:#tn=                2.1.1.7.2 CAP theorem>
                2.1.1.7.3 more DB classification <URL:#tn=                2.1.1.7.3 more DB classification>
                    2.1.1.7.3.1 Data model. <URL:#tn=                    2.1.1.7.3.1 Data model.>
                        2.1.1.7.3.1.1 TSDB <URL:#tn=                        2.1.1.7.3.1.1 TSDB>
                        2.1.1.7.3.1.2 <URL:#tn=                        2.1.1.7.3.1.2>
                    2.1.1.7.3.2 Data Variability <URL:#tn=                    2.1.1.7.3.2 Data Variability>
                    2.1.1.7.3.3 Operational capabilities <URL:#tn=                    2.1.1.7.3.3 Operational capabilities>
                    2.1.1.7.3.4 <URL:#tn=                    2.1.1.7.3.4>
                2.1.1.7.4 <URL:#tn=                2.1.1.7.4>
        2.1.1.8 <URL:#tn=        2.1.1.8>
        2.1.2 key-value store <URL:#tn=        2.1.2 key-value store>
            2.1.2.1 redis <URL:#tn=            2.1.2.1 redis>
                2.1.2.1.1 redis setup and installation <URL:#tn=                2.1.2.1.1 redis setup and installation>
                    2.1.2.1.1.1 redis docker <URL:#tn=                    2.1.2.1.1.1 redis docker>
                    2.1.2.1.1.2 <URL:#tn=                    2.1.2.1.1.2>
                2.1.2.1.2 <URL:#tn=                2.1.2.1.2>
            2.1.2.2 redis data structures <URL:#tn=            2.1.2.2 redis data structures>
                2.1.2.2.1 key and string <URL:#tn=                2.1.2.2.1 key and string>
                2.1.2.2.2 hash and list <URL:#tn=                2.1.2.2.2 hash and list>
                2.1.2.2.3 set, sorted set <URL:#tn=                2.1.2.2.3 set, sorted set>
                2.1.2.2.4 GEO , HyperLogLog <URL:#tn=                2.1.2.2.4 GEO , HyperLogLog>
                    2.1.2.2.4.1 GEO, geographical longitude and latitude <URL:#tn=                    2.1.2.2.4.1 GEO, geographical longitude and latitude>
                    2.1.2.2.4.2 hyperloglog <URL:#tn=                    2.1.2.2.4.2 hyperloglog>
                        2.1.2.2.4.2.1 https://thoughtbot.com/blog/hyperloglogs-in-redis <URL:#tn=                        2.1.2.2.4.2.1 https://thoughtbot.com/blog/hyperloglogs-in-redis>
                        2.1.2.2.4.2.2 usage <URL:#tn=                        2.1.2.2.4.2.2 usage>
                2.1.2.2.5 Pubsub <URL:#tn=                2.1.2.2.5 Pubsub>
                2.1.2.2.6 transaction <URL:#tn=                2.1.2.2.6 transaction>
                    2.1.2.2.6.1 Transactions in redis <URL:#tn=                    2.1.2.2.6.1 Transactions in redis>
                    2.1.2.2.6.2 https://redislabs.com/blog/you-dont-need-transaction-rollbacks-in-redis/ <URL:#tn=                    2.1.2.2.6.2 https://redislabs.com/blog/you-dont-need-transaction-rollbacks-in-redis/>
                    2.1.2.2.6.3 <URL:#tn=                    2.1.2.2.6.3>
                2.1.2.2.7 Redis usage <URL:#tn=                2.1.2.2.7 Redis usage>
                2.1.2.2.8 redis example java applications <URL:#tn=                2.1.2.2.8 redis example java applications>
                2.1.2.2.9 <URL:#tn=                2.1.2.2.9>
            2.1.2.3 <URL:#tn=            2.1.2.3>
        2.1.3 Document oriented DB, Document-Store <URL:#tn=        2.1.3 Document oriented DB, Document-Store>
            2.1.3.1 Intro <URL:#tn=            2.1.3.1 Intro>
            2.1.3.2 Mongo <URL:#tn=            2.1.3.2 Mongo>
                2.1.3.2.1 mongodb terminogy <URL:#tn=                2.1.3.2.1 mongodb terminogy>
                2.1.3.2.2 Modeling in mongodb <URL:#tn=                2.1.3.2.2 Modeling in mongodb>
                2.1.3.2.3 movielens mongodb implementation <URL:#tn=                2.1.3.2.3 movielens mongodb implementation>
                2.1.3.2.4 mongo CRUD <URL:#tn=                2.1.3.2.4 mongo CRUD>
                2.1.3.2.5 indexes <URL:#tn=                2.1.3.2.5 indexes>
                2.1.3.2.6 shard keys <URL:#tn=                2.1.3.2.6 shard keys>
                2.1.3.2.7 aggregation via map-reduce <URL:#tn=                2.1.3.2.7 aggregation via map-reduce>
                2.1.3.2.8  aggregation via aggregation framework <URL:#tn=                2.1.3.2.8  aggregation via aggregation framework>
                2.1.3.2.9 <URL:#tn=                2.1.3.2.9>
            2.1.3.3 <URL:#tn=            2.1.3.3>
        2.1.4 Search engine <URL:#tn=        2.1.4 Search engine>
            2.1.4.1 Elastic search <URL:#tn=            2.1.4.1 Elastic search>
            2.1.4.2 installation <URL:#tn=            2.1.4.2 installation>
            2.1.4.3 Elasticsearch terminology <URL:#tn=            2.1.4.3 Elasticsearch terminology>
            2.1.4.4 modeling for ES <URL:#tn=            2.1.4.4 modeling for ES>
            2.1.4.5 ES CRUD <URL:#tn=            2.1.4.5 ES CRUD>
            2.1.4.6 ES search queries <URL:#tn=            2.1.4.6 ES search queries>
                2.1.4.6.1 //term based query <URL:#tn=                2.1.4.6.1 //term based query>
                2.1.4.6.2 ////////////////////////full-text query <URL:#tn=                2.1.4.6.2 ////////////////////////full-text query>
                2.1.4.6.3 //Compound queries <URL:#tn=                2.1.4.6.3 //Compound queries>
                2.1.4.6.3 aggregation queries <URL:#tn=                2.1.4.6.3 aggregation queries>
                2.1.4.6.4 <URL:#tn=                2.1.4.6.4>
            2.1.4.7 ELK stack <URL:#tn=            2.1.4.7 ELK stack>
            2.1.4.8 UFO sightings demo <URL:#tn=            2.1.4.8 UFO sightings demo>
            2.1.4.9 <URL:#tn=            2.1.4.9>
        2.1.5 Wide column store <URL:#tn=        2.1.5 Wide column store>
            2.1.5.1 hbase <URL:#tn=            2.1.5.1 hbase>
            2.1.5.2 zookeeper <URL:#tn=            2.1.5.2 zookeeper>
            2.1.5.3 model movielens in hbase <URL:#tn=            2.1.5.3 model movielens in hbase>
            2.1.5.4 hbase crud <URL:#tn=            2.1.5.4 hbase crud>
            2.1.5.5 SQL on hbase, Apache Phoenix <URL:#tn=            2.1.5.5 SQL on hbase, Apache Phoenix>
            2.1.5.6 <URL:#tn=            2.1.5.6>
        2.1.6 Time Series DB <URL:#tn=        2.1.6 Time Series DB>
            2.1.6.1 Influx DB <URL:#tn=            2.1.6.1 Influx DB>
            2.1.6.2 <URL:#tn=            2.1.6.2>
        2.1.7 Graph DB <URL:#tn=        2.1.7 Graph DB>
        2.1.8 Hadoop <URL:#tn=        2.1.8 Hadoop>
        2.1.9 Big data SQL engines <URL:#tn=        2.1.9 Big data SQL engines>
    2.2 <URL:#tn=    2.2>
3. SQL Syntax tags: SQL Syntax <URL:#tn=3. SQL Syntax tags: SQL Syntax>
    3.1  Basic <URL:#tn=    3.1  Basic>
        3.1.1   DDL <URL:#tn=        3.1.1   DDL>
            3.1.1.1   CREATE Creates a new table, a view of a table, or other object in the database. <URL:#tn=            3.1.1.1   CREATE Creates a new table, a view of a table, or other object in the database.>
            3.1.1.2 ALTER Modifies an existing database object, such as a table. <URL:#tn=            3.1.1.2 ALTER Modifies an existing database object, such as a table.>
            3.1.1.3 DROP Deletes an entire table, a view of a table or other objects in the database. <URL:#tn=            3.1.1.3 DROP Deletes an entire table, a view of a table or other objects in the database.>
            3.1.1.4 <URL:#tn=            3.1.1.4>
        3.1.2 DML <URL:#tn=        3.1.2 DML>
            3.1.2.1 SELECT Retrieves certain records from one or more tables. <URL:#tn=            3.1.2.1 SELECT Retrieves certain records from one or more tables.>
            3.1.2.2 INSERT Creates a record. <URL:#tn=            3.1.2.2 INSERT Creates a record.>
            3.1.2.3 UPDATE Modifies records. <URL:#tn=            3.1.2.3 UPDATE Modifies records.>
            3.1.2.4 DELETE Deletes records. <URL:#tn=            3.1.2.4 DELETE Deletes records.>
            3.1.2.5 <URL:#tn=            3.1.2.5>
        3.1.3 DCL <URL:#tn=        3.1.3 DCL>
            3.1.3.1 GRANT Gives a privilege to user. <URL:#tn=            3.1.3.1 GRANT Gives a privilege to user.>
            3.1.3.2 REVOKE Takes back privileges granted from user. <URL:#tn=            3.1.3.2 REVOKE Takes back privileges granted from user.>
            3.1.3.3 <URL:#tn=            3.1.3.3>
        3.1.4 RDBMS traites <URL:#tn=        3.1.4 RDBMS traites>
            3.1.4.1 constraints <URL:#tn=            3.1.4.1 constraints>
                3.1.4.1.1 NOT NULL Constraint − Ensures that a column cannot have a NULL value. <URL:#tn=                3.1.4.1.1 NOT NULL Constraint − Ensures that a column cannot have a NULL value.>
                3.1.4.1.2 DEFAULT Constraint − Provides a default value for a column when none is specified. <URL:#tn=                3.1.4.1.2 DEFAULT Constraint − Provides a default value for a column when none is specified.>
                3.1.4.1.3 UNIQUE Constraint − Ensures that all the values in a column are different. <URL:#tn=                3.1.4.1.3 UNIQUE Constraint − Ensures that all the values in a column are different.>
                3.1.4.1.4 PRIMARY Key − Uniquely identifies each row/record in a database table. <URL:#tn=                3.1.4.1.4 PRIMARY Key − Uniquely identifies each row/record in a database table.>
                3.1.4.1.5 FOREIGN Key − Uniquely identifies a row/record in any another database table. <URL:#tn=                3.1.4.1.5 FOREIGN Key − Uniquely identifies a row/record in any another database table.>
                3.1.4.1.6 CHECK Constraint − The CHECK constraint ensures that all values in a column satisfy certain conditions. <URL:#tn=                3.1.4.1.6 CHECK Constraint − The CHECK constraint ensures that all values in a column satisfy certain conditions.>
                3.1.4.1.7 INDEX − Used to create and retrieve data from the database very quickly. <URL:#tn=                3.1.4.1.7 INDEX − Used to create and retrieve data from the database very quickly.>
                3.1.4.1.8 <URL:#tn=                3.1.4.1.8>
            3.1.4.2 Data Integrity <URL:#tn=            3.1.4.2 Data Integrity>
            3.1.4.3 Database Normalization <URL:#tn=            3.1.4.3 Database Normalization>
                3.1.4.3.1 First Normal Form (1NF) <URL:#tn=                3.1.4.3.1 First Normal Form (1NF)>
                3.1.4.3.2 Second Normal Form (2NF) <URL:#tn=                3.1.4.3.2 Second Normal Form (2NF)>
                3.1.4.3.3 Third Normal Form (3NF) <URL:#tn=                3.1.4.3.3 Third Normal Form (3NF)>
                3.1.4.3.4 <URL:#tn=                3.1.4.3.4>
            3.1.4.4 popular RDBMS databases <URL:#tn=            3.1.4.4 popular RDBMS databases>
            3.1.4.5 MySQL <URL:#tn=            3.1.4.5 MySQL>
            3.1.4.6 MS SQL Server <URL:#tn=            3.1.4.6 MS SQL Server>
            3.1.4.7 ORACLE <URL:#tn=            3.1.4.7 ORACLE>
            3.1.4.5 <URL:#tn=            3.1.4.5>
        3.1.5 Syntax <URL:#tn=        3.1.5 Syntax>
            3.1.5.1 SQL SELECT Statement <URL:#tn=            3.1.5.1 SQL SELECT Statement>
            3.1.5.2 SQL DISTINCT Clause <URL:#tn=            3.1.5.2 SQL DISTINCT Clause>
            3.1.5.3 SQL WHERE Clause <URL:#tn=            3.1.5.3 SQL WHERE Clause>
            3.1.5.4 SQL AND/OR Clause <URL:#tn=            3.1.5.4 SQL AND/OR Clause>
            3.1.5.5 SQL IN Clause <URL:#tn=            3.1.5.5 SQL IN Clause>
            3.1.5.6 SQL BETWEEN Clause <URL:#tn=            3.1.5.6 SQL BETWEEN Clause>
            3.1.5.7 SQL LIKE Clause <URL:#tn=            3.1.5.7 SQL LIKE Clause>
            3.1.5.8 SQL ORDER BY Clause <URL:#tn=            3.1.5.8 SQL ORDER BY Clause>
            3.1.5.9 SQL GROUP BY Clause <URL:#tn=            3.1.5.9 SQL GROUP BY Clause>
            3.1.5.10 SQL COUNT Clause <URL:#tn=            3.1.5.10 SQL COUNT Clause>
            3.1.5.11 SQL HAVING Clause <URL:#tn=            3.1.5.11 SQL HAVING Clause>
            3.1.5.12 SQL CREATE TABLE Statement <URL:#tn=            3.1.5.12 SQL CREATE TABLE Statement>
            3.1.5.13 SQL DROP TABLE Statement <URL:#tn=            3.1.5.13 SQL DROP TABLE Statement>
            3.1.5.14 SQL CREATE INDEX Statement <URL:#tn=            3.1.5.14 SQL CREATE INDEX Statement>
            3.1.5.15 SQL DROP INDEX Statement <URL:#tn=            3.1.5.15 SQL DROP INDEX Statement>
            3.1.5.16 SQL DESC Statement <URL:#tn=            3.1.5.16 SQL DESC Statement>
            3.1.5.17 SQL TRUNCATE TABLE Statement <URL:#tn=            3.1.5.17 SQL TRUNCATE TABLE Statement>
            3.1.5.18 SQL ALTER TABLE Statement <URL:#tn=            3.1.5.18 SQL ALTER TABLE Statement>
            3.1.5.19 SQL INSERT INTO Statement <URL:#tn=            3.1.5.19 SQL INSERT INTO Statement>
            3.1.5.20 SQL UPDATE Statement <URL:#tn=            3.1.5.20 SQL UPDATE Statement>
            3.1.5.21 SQL DELETE Statement <URL:#tn=            3.1.5.21 SQL DELETE Statement>
            3.1.5.22 SQL CREATE DATABASE Statement <URL:#tn=            3.1.5.22 SQL CREATE DATABASE Statement>
            3.1.5.23 SQL DROP DATABASE Statement <URL:#tn=            3.1.5.23 SQL DROP DATABASE Statement>
            3.1.5.24 SQL USE Statement <URL:#tn=            3.1.5.24 SQL USE Statement>
            3.1.5.25 SQL COMMIT Statement <URL:#tn=            3.1.5.25 SQL COMMIT Statement>
            3.1.5.26 SQL ROLLBACK Statement <URL:#tn=            3.1.5.26 SQL ROLLBACK Statement>
        3.1.6 Data-types <URL:#tn=        3.1.6 Data-types>
        3.1.7 operators <URL:#tn=        3.1.7 operators>
        3.1.8 Expressions <URL:#tn=        3.1.8 Expressions>
        3.1.9 types <URL:#tn=        3.1.9 types>
        3.1.10 Operator in SQL <URL:#tn=        3.1.10 Operator in SQL>
        3.1.11 Expressions <URL:#tn=        3.1.11 Expressions>
        3.1.12 The SQL CREATE, DROP and USE DATABASE statements <URL:#tn=        3.1.12 The SQL CREATE, DROP and USE DATABASE statements>
        3.1.13 create and drop table <URL:#tn=        3.1.13 create and drop table>
        3.1.14 INSERT INTO <URL:#tn=        3.1.14 INSERT INTO>
        3.1.15 The SQL SELECT statement is used to fetch the data from a database table which returns this data in the form of a result table. These result tables are called result-sets. <URL:#tn=        3.1.15 The SQL SELECT statement is used to fetch the data from a database table which returns this data in the form of a result table. These result tables are called result-sets.>
        3.1.16 The SQL WHERE clause is used to specify a condition while fetching the data from a single table or by joining with multiple tables. If the given condition is satisfied, then only it returns a specific value from the table. You should use the WHERE clause to filter the records and fetching only the necessary records. <URL:#tn=        3.1.16 The SQL WHERE clause is used to specify a condition while fetching the data from a single table or by joining with multiple tables. If the given condition is satisfied, then only it returns a specific value from the table. You should use the WHERE clause to filter the records and fetching only the necessary records.>
        3.1.17 The SQL AND & OR operators are used to combine multiple conditions to narrow data in an SQL statement. These two operators are called as the conjunctive operators. <URL:#tn=        3.1.17 The SQL AND & OR operators are used to combine multiple conditions to narrow data in an SQL statement. These two operators are called as the conjunctive operators.>
        3.1.18 The SQL UPDATE Query is used to modify the existing records in a table. <URL:#tn=        3.1.18 The SQL UPDATE Query is used to modify the existing records in a table.>
        3.1.19 The SQL DELETE Query is used to delete the existing records from a table. <URL:#tn=        3.1.19 The SQL DELETE Query is used to delete the existing records from a table.>
        3.1.20 Constraints are the rules enforced on the data columns of a table. <URL:#tn=        3.1.20 Constraints are the rules enforced on the data columns of a table.>
        3.1.21 The SQL Joins clause is used to combine records from two or more tables in a database. <URL:#tn=        3.1.21 The SQL Joins clause is used to combine records from two or more tables in a database.>
            3.1.21.1 INNER JOIN − returns rows when there is a match in both tables. <URL:#tn=            3.1.21.1 INNER JOIN − returns rows when there is a match in both tables.>
                3.1.21.1.1 The most important and frequently used of the joins is the INNER JOIN. They are also referred to as an EQUIJOIN. <URL:#tn=                3.1.21.1.1 The most important and frequently used of the joins is the INNER JOIN. They are also referred to as an EQUIJOIN.>
                3.1.21.1.2 my example <URL:#tn=                3.1.21.1.2 my example>
                3.1.21.1.3 <URL:#tn=                3.1.21.1.3>
            3.1.21.2 LEFT JOIN − returns all rows from the left table, even if there are no matches in the right table. <URL:#tn=            3.1.21.2 LEFT JOIN − returns all rows from the left table, even if there are no matches in the right table.>
            3.1.21.3 RIGHT JOIN − returns all rows from the right table, even if there are no matches in the left table. <URL:#tn=            3.1.21.3 RIGHT JOIN − returns all rows from the right table, even if there are no matches in the left table.>
            3.1.21.4 FULL JOIN − returns rows when there is a match in one of the tables. <URL:#tn=            3.1.21.4 FULL JOIN − returns rows when there is a match in one of the tables.>
            3.1.21.5 SELF JOIN − is used to join a table to itself as if the table were two tables, temporarily renaming at least one table in the SQL statement. <URL:#tn=            3.1.21.5 SELF JOIN − is used to join a table to itself as if the table were two tables, temporarily renaming at least one table in the SQL statement.>
            3.1.21.6 CARTESIAN JOIN − returns the Cartesian product of the sets of records from the two or more joined tables. <URL:#tn=            3.1.21.6 CARTESIAN JOIN − returns the Cartesian product of the sets of records from the two or more joined tables.>
            3.1.21.7 <URL:#tn=            3.1.21.7>
        3.1.22 The SQL UNION clause/operator is used to combine the results of two or more SELECT statements without returning any duplicate rows. <URL:#tn=        3.1.22 The SQL UNION clause/operator is used to combine the results of two or more SELECT statements without returning any duplicate rows.>
        3.1.23 NULL value <URL:#tn=        3.1.23 NULL value>
        3.1.24 Alias, Name as alias_name <URL:#tn=        3.1.24 Alias, Name as alias_name>
        3.1.25 Indexes <URL:#tn=        3.1.25 Indexes>
        3.1.26 ALTER TABLE <URL:#tn=        3.1.26 ALTER TABLE>
        3.1.27 TRUNCATE TABLE <URL:#tn=        3.1.27 TRUNCATE TABLE>
        3.1.28 Table views <URL:#tn=        3.1.28 Table views>
        3.1.29 GROUP BY HAVING Clause <URL:#tn=        3.1.29 GROUP BY HAVING Clause>
        3.1.30 Transactions <URL:#tn=        3.1.30 Transactions>
        3.1.31 wildcard characters <URL:#tn=        3.1.31 wildcard characters>
        3.1.32 SQL Date Functions <URL:#tn=        3.1.32 SQL Date Functions>
        3.1.33 SQL - temporary tables <URL:#tn=        3.1.33 SQL - temporary tables>
        3.1.34 SQL - clone tables <URL:#tn=        3.1.34 SQL - clone tables>
        3.1.35 Inner Queries , sub queries <URL:#tn=        3.1.35 Inner Queries , sub queries>
        3.1.36 sequences <URL:#tn=        3.1.36 sequences>
        3.1.37 DISTINCT, handle duplicates <URL:#tn=        3.1.37 DISTINCT, handle duplicates>
        3.1.38 preventing SQL injection <URL:#tn=        3.1.38 preventing SQL injection>
        3.1.39 Copy from one table to another , backup table <URL:#tn=        3.1.39 Copy from one table to another , backup table>
        3.1.40 <URL:#tn=        3.1.40>
    3.2 advanced <URL:#tn=    3.2 advanced>
        3.2.1 coalesce <URL:#tn=        3.2.1 coalesce>
        3.2.2 <URL:#tn=        3.2.2>
    3.3 <URL:#tn=    3.3>
    3.4 LLMs, Chatgpt, Gemini  etc, tags: LLMs Chatgpt Gemini <URL:#tn=    3.4 LLMs, Chatgpt, Gemini  etc, tags: LLMs Chatgpt Gemini>
        3.4.1 q. difference between union and inner join sql operations <URL:#tn=        3.4.1 q. difference between union and inner join sql operations>
        3.4.2 q. can inner join be used on same table <URL:#tn=        3.4.2 q. can inner join be used on same table>
        3.4.3 q. other use cases? <URL:#tn=        3.4.3 q. other use cases?>
        3.4.4 q. can you provide sql examples for these use cases? <URL:#tn=        3.4.4 q. can you provide sql examples for these use cases?>
        3.4.5 q. give an example involving source PG DB and nodejs client application and target PG DB and nodejs client application <URL:#tn=        3.4.5 q. give an example involving source PG DB and nodejs client application and target PG DB and nodejs client application>
            3.4.5.1  using pg-copy-streams <URL:#tn=            3.4.5.1  using pg-copy-streams>
            3.4.5.2 without using pg-copy-streams <URL:#tn=            3.4.5.2 without using pg-copy-streams>
            3.4.5.3 give an example of a nodejs application that interacts with a PG DB and exposes and endpoint that allows to download a file which is the result of running a select query on the DB <URL:#tn=            3.4.5.3 give an example of a nodejs application that interacts with a PG DB and exposes and endpoint that allows to download a file which is the result of running a select query on the DB>
            3.4.5.4 User create an example HTTP page with a button JS code that allows user to click it in order to download the file <URL:#tn=            3.4.5.4 User create an example HTTP page with a button JS code that allows user to click it in order to download the file>
            3.4.5.5 User give an ETL example involving source PG DB a spring java client application that interacts with this DB, and a target PG DB with a  spring java client application <URL:#tn=            3.4.5.5 User give an ETL example involving source PG DB a spring java client application that interacts with this DB, and a target PG DB with a  spring java client application>
            3.4.5.6 give same example using python and flask <URL:#tn=            3.4.5.6 give same example using python and flask>
            3.4.5.7 give an example of a java spring application that interacts with a PG DB and exposes and endpoint that allows to download a file which is the result of running a select query on the DB <URL:#tn=            3.4.5.7 give an example of a java spring application that interacts with a PG DB and exposes and endpoint that allows to download a file which is the result of running a select query on the DB>
            3.4.5.8 User give an example of a  python application that interacts with a PG DB and exposes and endpoint that allows to download a file which is the result of running a select query on the DB <URL:#tn=            3.4.5.8 User give an example of a  python application that interacts with a PG DB and exposes and endpoint that allows to download a file which is the result of running a select query on the DB>
        3.4.6 sql ON CONFLICT xxx DO UPDATE SET <URL:#tn=        3.4.6 sql ON CONFLICT xxx DO UPDATE SET>
        3.4.7 I have a DB table of document id and the actual document. create an update statement that appends a prefix of document id to the value in json path foo.bar.id <URL:#tn=        3.4.7 I have a DB table of document id and the actual document. create an update statement that appends a prefix of document id to the value in json path foo.bar.id>
        3.4.8 I have two tables, one with item ids and types, some items can contain others. for example: locker can contain keys, phone etc. For containing there's another table for this relation with 2 columns from_id and to_id. Create a statement that counts how many items are contained in each locker and sort result descending <URL:#tn=        3.4.8 I have two tables, one with item ids and types, some items can contain others. for example: locker can contain keys, phone etc. For containing there's another table for this relation with 2 columns from_id and to_id. Create a statement that counts how many items are contained in each locker and sort result descending>
        3.4.9 <URL:#tn=        3.4.9>
    3.5 <URL:#tn=    3.5>
4. Common concepts <URL:#tn=4. Common concepts>
    4.1  Optimistic-locking <URL:#tn=    4.1  Optimistic-locking>
        4.1.1  Chatgpt <URL:#tn=        4.1.1  Chatgpt>
            4.1.1.1  define optimistic locking <URL:#tn=            4.1.1.1  define optimistic locking>
            4.1.1.2 what kind of conflicts may happen when optimistic locking is used <URL:#tn=            4.1.1.2 what kind of conflicts may happen when optimistic locking is used>
1. Update Conflict: <URL:#tn=1. Update Conflict:>
2. Delete-Insert Conflict: <URL:#tn=2. Delete-Insert Conflict:>
            4.1.1.3 <URL:#tn=            4.1.1.3>
        4.1.2 <URL:#tn=        4.1.2>
    4.2 <URL:#tn=    4.2>
5. Common , interesting, frequent queries , tags: Common , interesting, frequent queries , <URL:#tn=5. Common , interesting, frequent queries , tags: Common , interesting, frequent queries ,>
    5.1 I have two similar DB tables with same primary key. say orders table and orders translations table. what sql would provide all the orders that are not in orders translations? <URL:#tn=    5.1 I have two similar DB tables with same primary key. say orders table and orders translations table. what sql would provide all the orders that are not in orders translations?>
    5.2 <URL:#tn=    5.2>
6. <URL:#tn=6.>
.................................................END TOC..............................................




















1. postgres

    1.1 pagination
https://www.citusdata.com/blog/2016/03/30/five-ways-to-paginate/


        1.1.0  Full article
Postgres DB Server side pagination


Five ways to paginate in Postgres, from the basic to the exotic
 Share this post  
It may surprise you that pagination, pervasive as it is in web applications, is easy to implement inefficiently. In this article we’ll examine several methods of server-side pagination and discuss their tradeoffs when implemented in PostgreSQL. This article will help you identify which technique is appropriate for your situation, including some you may not have seen before which rely on physical clustering and the database stats collector.
Before continuing it makes sense to mention client-side pagination. Some applications transfer all (or a large part) of the server information to the client and paginate there. For small amounts of data client-side pagination can be a better choice, reducing HTTP calls. It gets impractical when records begin numbering in the thousands. Server-side has additional benefits such as
* Faster initial page load
* More accuracy when shared data is changing
* Faster operations on large datasets
* Encapsulation of business logic
* Better performance on resource-constrained clients
PostgreSQL gives us a number of server-side pagination techniques that differ in speed, integrity (not missing records), and support for certain page access patterns. Not all methods work in all situations, some require special data or queries. Let’s consider the methods in order of generality, starting with those that work for any query, then those which require ordered data. We’ll conclude with some exotic methods which rely on PostgreSQL internals.
Paginating Arbitrary Queries
Limit-Offset
The easiest method of pagination, limit-offset, is also most perilous. Sadly it’s a staple of web application development tutorials. Object relational mapping (ORM) libraries make it easy and tempting, from SQLAlchemy’s .slice(1, 3) to ActiveRecord’s .limit(1).offset(3) to Sequelize’s .findAll({ offset: 3, limit: 1 }). They all generate SQL ending in LIMIT 1 OFFSET 3. It’s no coincidence that limit-offset use is widespread, you can tack it onto any query without further modification.
ORM methods to limit and offset the data are one thing, but pagination helper libraries can be even more deceptive. For instance the popular Ruby library Kaminari uses limit-offset by default, while hiding it behind a high-level interface.
The technique has two big problems, result inconsistency and offset inefficiency. Consistency refers to the intention that traversing a resultset should retrieve every item exactly once, without omissions or duplication. Offset inefficiency refers to the delay incurred by shifting the results by a large offset.
Here’s how limit-offset pagination can be inconsistent. Suppose a user moves from page n to n+1 while simultaneously a new element is inserted into page n. This will cause both a duplication (the previously-final element of page n is pushed into page n+1) and an omission (the new element). Alternatively consider an element removed from page n just as the user moves to page n+1. The previously initial element of page n+1 will be shifted to page n and be omitted.
Now for the inefficiency. Large offsets are intrinsically expensive. Even in the presence of an index the database must scan through storage, counting rows. To utilize an index we would have to filter a column by a value, but in this case we require a certain number of rows irrespective of their column values. Furthermore the rows needn’t have the same size in storage, and some may be present on disk but marked as deleted so the database cannot use simple arithmetic to find a location on disk to begin reading results. Let’s measure the slowdown.
-- Create table with random strings of various lengths
CREATE TABLE medley AS
  SELECT
    generate_series(1,10000000) AS n,
    substr(concat(md5(random()::text), md5(random()::text)), 1, (random() * 64)::integer + 1) AS description;

-- Notify query planner of drastically changed table size
VACUUM ANALYZE;

-- Low offsets are refreshingly fast
EXPLAIN ANALYZE SELECT * FROM medley LIMIT 100;
The estimated cost is quite low:
                                                     QUERY PLAN
--------------------------------------------------------------------------------------------------------------------
 Limit  (cost=0.00..1.85 rows=100 width=38) (actual time=0.008..0.036 rows=100 loops=1)
   ->  Seq Scan on medley  (cost=0.00..185460.60 rows=9999660 width=38) (actual time=0.007..0.017 rows=100 loops=1)
 Planning time: 0.040 ms
 Execution time: 0.059 ms
(4 rows)
Choosing offset=1000 makes cost about 19 and has a 0.609 ms execution time. Once offset=5,000,000 the cost goes up to 92734 and execution time is 758.484 ms.
These problems don’t necessarily mean that limit-offset is inapplicable for your situation. In some applications users don’t typically advance many pages into a resultset, and you might even choose to enforce a server page limit. If result inconsistency and restricted page numbers aren’t a problem in your application then limit-offset may be convenient for your needs.
When to Use: Limit-offset
Applications with restricted pagination depth and tolerant of result inconsistencies.
Cursors
Despite its disadvantages limit-offset does have the advantage of being stateless on the server. Contrast it with another pagination approach, query cursors. Like offsets, cursors can be used in any query, but they differ by requiring the server to hold a dedicated database connection and transaction per HTTP client.
Here is how cursors can be used:
-- We must be in a transaction
BEGIN;
-- Open a cursor for a query
DECLARE medley_cur CURSOR FOR SELECT * FROM medley;
-- Retrieve ten rows
FETCH 10 FROM medley_cur;
-- ...
-- Retrieve ten more from where we left off
FETCH 10 FROM medley_cur;
-- All done
COMMIT;
Cursors have the desirable property of pagination consistency on arbitrary queries, showing results as they exist at the time the transaction was started. The isolation level (link is external) of the transaction guarantees that our paginated view of the results won’t change.
Every pagination approach has a downside, and the problems with cursors are resource usage and client-server coupling. Each open transaction consumes dedicated database resources, and is not scalable for too many clients. There are also “WITH HOLD” cursors which can exist outside of a transaction, but they must materialize data. Either way this makes cursor pagination appropriate only for small scale situations like intranet use.
Bridging HTTP to cursors introduces complications. Servers must identify clients across requests, either through a token or by keeping an identifier such as the client IP address in a session. Servers must also judge when to release transactions due to inactivity. Finally server load balancing becomes complicated, since each client must connect to a dedicated server each time.
When to Use: Cursors A single-server intranet application which must paginate queries with varied and changeable ordering, especially where result consistency matters.
Pagination of Ordered Queries
Keyset Pagination
The techniques above can paginate any kind of query, including queries without order clauses. If we are willing to forgo this generality we reap optimizations. In particular when ordering by indexed column(s) the client can use values in the current page to choose which items to show in the next page. This is called keyset pagination.
For example let’s return to the medley example:
-- Add an index for keyset pagination (btrees support inequality)
CREATE INDEX n_idx ON medley USING btree (n);
SELECT * FROM medley ORDER BY n ASC LIMIT 5;
With my randomized data it returns
 n |                         description
---+-------------------------------------------------------------
 1 | 74f70e009396
 2 | 8dac5a085eb670a29058d
 3 | fce303a32e89181bf5df1601487
 4 | fddcced2c12e83516b3bd6cc94f23a012dfd
 5 | f51ae548dd27f51147e53e839eeceb6b0c92922145276d668e73d4a6621
(5 rows)
Now the client can look at the maximal n in this result and use it for requesting the next page:
SELECT * 
FROM medley
WHERE n > 5
ORDER BY n ASC
LIMIT 5;
Even filtering by n > 5000000 remains fast, unlike the limit-offset example.
                                                           QUERY PLAN
--------------------------------------------------------------------------------------------------------------------------------
 Limit  (cost=0.43..0.62 rows=5 width=38) (actual time=0.101..0.103 rows=5 loops=1)
   ->  Index Scan using n_idx on medley  (cost=0.43..185579.42 rows=5013485 width=38) (actual time=0.100..0.102 rows=5 loops=1)
         Index Cond: (n > 5000000)
 Planning time: 0.071 ms
 Execution time: 0.119 ms
(5 rows)
Keyset pagination is fast, and it is consistent too. Any insertions/deletions before the current page will leave the results unaffected. The two downsides to this method are lack of random access and possible coupling between client and server.
In general there is no way to jump directly to a given page without visiting prior pages to observe their maximal elements. Under certain conditions we can do better though. If values in the indexed column are uniformly distributed (or even better, contiguous numbers without gaps) the client can do some math to find the desired page because the index makes it cheap to find the largest value:
EXPLAIN ANALYZE SELECT max(n) FROM medley;
                                                 QUERY PLAN
------------------------------------------------------------------------------------------------------------
 Result  (cost=0.46..0.47 rows=1 width=0) (actual time=0.021..0.021 rows=1 loops=1)
   InitPlan 1 (returns $0)
     ->  Limit  (cost=0.43..0.46 rows=1 width=4) (actual time=0.018..0.018 rows=1 loops=1)
           ->  Index Only Scan Backward using n_idx on medley  (cost=0.43..284688.43 rows=10000000 width=4) (actual time=0.017..0.017 rows=1 loops=1)
                 Index Cond: (n IS NOT NULL)
                 Heap Fetches: 0
 Planning time: 0.087 ms
 Execution time: 0.042 ms
(8 rows)
The other issue of keyset pagination, client/server coupling, requires care. First the client doesn’t know which columns are indexed. The server will likely need to provide an endpoint with fixed order rather than allowing the client to customize the ordering. Given the client code may not know which column is being ordered, the server must provide a hint for how to request the next page. RFC5988 defines HTTP link relations previous and next to encode links for the client to follow.
Since users typically access pages of information in a linear fashion, keyset pagination is usually considered the best choice for paginating ordered records in high-traffic web servers.
When to Use: Keyset Scalable applications serving data sequentially from column(s) indexed for comparisons. Supports filtering.
Exotic, Specialized Pagination
Clustered TID Scan
We can devise nonstandard pagination techniques for special situations using low level PostgreSQL features. For instance we can implement truly random-access access on data if we
| 1. Don’t require all pages to have exactly the same length
| 2. Support only one order for paginated rows
The trick is to choose returned pages which correspond directly with database pages on disk or to sections of those disk pages. Every table in a PostgreSQL database contains a secret column called ctid which identifies its row:
SELECT ctid, * FROM medley WHERE n <= 10;
  ctid  | n  |                         description
--------+----+-------------------------------------------------------------
 (0,1)  |  1 | 74f70e009396
 (0,2)  |  2 | 8dac5a085eb670a29058d
 (0,3)  |  3 | fce303a32e89181bf5df1601487
 (0,4)  |  4 | fddcced2c12e83516b3bd6cc94f23a012dfd
 (0,5)  |  5 | f51ae548dd27f51147e53e839eeceb6b0c92922145276d668e73d4a6621
 (0,6)  |  6 | eb9fe1dfe1e421903f96b3b5c5dfe1ee1253582d728c35b4ee7330b
 (0,7)  |  7 | e95202d7f5c612f8523ae705d
 (0,8)  |  8 | 6573b64aff262a2b940326
 (0,9)  |  9 | a0a43
 (0,10) | 10 | 82cdc134bd249a612cfddd3088dd09e32de5f4fa33
(10 rows)
Each ctid is of the form (page,row). PostgreSQL can retrieve rows very quickly by ctid, in fact this is how indices work internally – they map column values to ctids.
Note that although PostgreSQL defines an order relation on the tid type, it cannot efficiently retrieve ctids by inequality
EXPLAIN ANALYZE SELECT count(1) FROM medley WHERE ctid >= '(0,1)'::tid AND ctid < '(1,0)'::tid;
                                                      QUERY PLAN
----------------------------------------------------------------------------------------------------------------------
 Aggregate  (cost=235589.00..235589.01 rows=1 width=0) (actual time=1241.851..1241.852 rows=1 loops=1)
   ->  Seq Scan on medley  (cost=0.00..235464.00 rows=50000 width=0) (actual time=477.933..1241.802 rows=116 loops=1)
         Filter: ((ctid >= '(0,1)'::tid) AND (ctid < '(1,0)'::tid))
         Rows Removed by Filter: 9999884
 Planning time: 0.047 ms
 Execution time: 1241.889 ms
(6 rows)
Requesting ranges doesn’t work but there is still a way to efficiently request all rows in a disk page. Every page contains currentsetting(‘blocksize’) bytes of data (often 8k). Rows are referenced by a 32-bit pointer so there are at most block_size/4 rows per page. (In fact rows are typically wider than the minimum size and a quarter of the block size provides an upper bound of rows per page.) The following sequence will generate all possible ctids in the jth page
SELECT ('(' || j || ',' || s.i || ')')::tid
 FROM generate_series(0,current_setting('block_size')::int/4) AS s(i);
Let’s use it to get all rows in medley on page zero.
SELECT * FROM medley WHERE ctid = ANY (ARRAY
  (SELECT ('(0,' || s.i || ')')::tid
    FROM generate_series(0,current_setting('block_size')::int/4) AS s(i)
  )
);
The planner identified this query as having cost=25.03..65.12 and it runs in 2.765ms. Requesting page 10,000 has similar cost. So we’re getting true random access, what’s not to love?
There are three downsides
| 1. When rows are deleted they leave holes in a page.
| 2. The order of the rows may not be meaningful. The database inserts new rows into holes left from deleted rows, which will cause the rows to be out of order.
| 3. “Where” clauses are not supported.
In certain situations this is not a problem. One case is data whose natural order corresponds to insertion order such as append-only time-series data. Another is data that doesn’t change often. This is because we have control over the placement of rows within pages through the CLUSTER command.
Let’s go back to our medley example. Its rows on disk are ordered by the n column ascending because that is the order in which we inserted them. What if we want to sort by the description column? The answer is to physically reorder the table by index the description column and clustering.
CREATE INDEX description_idx ON medley USING btree (description);
CLUSTER medley USING description_idx;
Now selecting all rows in the first page comes back alphabetized by description. If the table changes then new rows will be appended out of alphabetical order, but as long as the table doesn’t change the returned items will fine. It can also be periodically re-clustered after changes although this operation locks the table and cannot be done when people need to access it.
Finally it’s possible to determine the total number of pages for the table using its total byte size.
SELECT pg_relation_size('medley') / current_setting('block_size')::int;
When to Use: TID Scan
When fast deep random page access is required and filtering is not needed. Works especially well with append-only time-series data having low-variance row width.
Keyset with Estimated Bookmarks
As we saw, plain keyset pagination offers no facility to jump a certain percentage into the results except through client guesswork. However the PostgreSQL statistics collector maintains per-column histograms of value distribution. We can use these estimates in conjunction with limits and small offsets to get fast random-access pagination through a hybrid approach.
First let’s look at the statistics of our medley:
SELECT array_length(histogram_bounds, 1) - 1
  FROM pg_stats
 WHERE tablename = 'medley'
   AND attname = 'n';
In my database the column n has 101 bound-markers, i.e. 100 ranges between bound-markers. The particular values aren’t too surprising because my data is uniformly distributed
{719,103188,193973,288794, … ,9690475,9791775,9905770,9999847}
Notice that the values are approximate. The first number is not exactly zero, and the last is not exactly ten million. The ranges divide our information into a block size B = 10,000,000 / 100 = 100,000 rows.
We can use the histogram ranges from the PostgreSQL stats collector to obtain probabilistically correct pages. If we choose a client-side page width of W how do we request the ith page? It will reside in block iW / B, at offset iW % B.
Choosing W=20 let’s request page 270,000 from the medley table. Note that PostgreSQL arrays are one-based so we have to adjust the values in the array lookups:
WITH bookmark AS (
    SELECT (histogram_bounds::text::int[])[((270000 * 20) / 100000)+1] AS start,
           (histogram_bounds::text::int[])[((270000 * 20) / 100000)+2] AS stop
    FROM pg_stats
    WHERE tablename = 'medley'
    AND attname = 'n'
    LIMIT 1
  )
SELECT *
FROM medley
WHERE n >= (select start from bookmark)
AND n < (select stop from bookmark)
ORDER BY n ASC
LIMIT 20
OFFSET ((270000 * 20) % 100000);
This performs blazingly fast (notice the offset happens to be zero here). It gives back rows with n = 5407259 through 5407278. The true values on page 270000 are n = 5400001 through 5400020. The values is off by 7239, or about 0.1%.
We were lucky in our page choice there. For contrast, page 74999 requires an offset of 99980. We do know that our offset will be at most 100,000. The upper bound is within our control if we care to make a tradeoff. By adjusting the PostgreSQL stats collector we can get a more precise column histogram
ALTER TABLE medley ALTER COLUMN n SET statistics 1000;
VACUUM ANALYZE;
Now there are 1000 rather than 100 histogram buckets. On my database they have values
{10,10230,20863, …, 9980444,9989948,9999995}
With this bucket size our offset will be at most 10,000. The tradeoff is that the query planner now has to look through more values, slowing it down. So it’s a tradeoff of potential offset inefficiency vs query planner overhead.
This hybrid keyset/offset method probably doesn’t correspond to many real pagination use cases. It will not work with where clauses. It’s inaccurate and gets more so when the table changes and the stats collector hasn’t recently run.
When to Use: Keyset with Bookmarks When the client would like deep but approximate random access with no extra filtering allowed.
Conclusion
Like many engineering decisions, choosing pagination techniques involves tradeoffs. It’s safe to say that keyset pagination is most applicable for the average site with ordered linear access. However even limit-offset has its strengths, and more exotic techniques provide special performance characteristics for certain kinds of data. You can see there quite a few possibilities. Pick the right tool for the job and don’t let pagination be a closed book.

        1.1.1  Limit offset test

-- Create table with random strings of various lengths
CREATE TABLE medley AS
  SELECT
    generate_series(1,10000000) AS n,
    substr(concat(md5(random()::text), md5(random()::text)), 1, (random() * 64)::integer + 1) AS description;

-- Notify query planner of drastically changed table size
VACUUM ANALYZE;

-- Low offsets are refreshingly fast
EXPLAIN ANALYZE SELECT * FROM medley LIMIT 100;

my test:
CREATE TABLE limoffdemo AS
  SELECT
    generate_series(1,10000000) AS n,
    substr(concat(md5(random()::text), md5(random()::text)), 1, (random() * 64)::integer + 1) AS description;


VACUUM ANALYZE;

EXPLAIN ANALYZE SELECT * FROM limoffdemo LIMIT 100;
i500695=# EXPLAIN ANALYZE SELECT * FROM limoffdemo LIMIT 100;
                                                       QUERY PLAN                                                        
-------------------------------------------------------------------------------------------------------------------------
 Limit  (cost=0.00..1.85 rows=100 width=38) (actual time=0.021..0.040 rows=100 loops=1)
   ->  Seq Scan on limoffdemo  (cost=0.00..185472.66 rows=10000966 width=38) (actual time=0.016..0.024 rows=100 loops=1)
 Planning Time: 1.188 ms
 Execution Time: 0.061 ms
(4 rows)

SELECT * FROM limoffdemo LIMIT 100;
n  |                           description                            
-----+------------------------------------------------------------------
   1 | 1d
   2 | 4ff90c29bd14eba8e1794878f2fc9240ade337e4d8433e3267d
   ...

EXPLAIN ANALYZE SELECT * FROM limoffdemo LIMIT 100 OFFSET 1000;
i500695=# EXPLAIN ANALYZE SELECT * FROM limoffdemo LIMIT 100 OFFSET 1000;
                                                        QUERY PLAN                                                        
--------------------------------------------------------------------------------------------------------------------------
 Limit  (cost=18.55..20.40 rows=100 width=38) (actual time=0.269..0.295 rows=100 loops=1)
   ->  Seq Scan on limoffdemo  (cost=0.00..185472.66 rows=10000966 width=38) (actual time=0.007..0.212 rows=1100 loops=1)
 Planning Time: 0.035 ms
 Execution Time: 0.311 ms
(4 rows)


EXPLAIN ANALYZE SELECT * FROM limoffdemo LIMIT 100 OFFSET 5000000;
i500695=# EXPLAIN ANALYZE SELECT * FROM limoffdemo LIMIT 100 OFFSET 5000000;
                                                          QUERY PLAN                                                           
-------------------------------------------------------------------------------------------------------------------------------
 Limit  (cost=92727.37..92729.23 rows=100 width=38) (actual time=1055.353..1055.388 rows=100 loops=1)
   ->  Seq Scan on limoffdemo  (cost=0.00..185472.66 rows=10000966 width=38) (actual time=0.007..657.224 rows=5000100 loops=1)
 Planning Time: 0.032 ms
 Execution Time: 1055.408 ms
(4 rows)

performance degradation as function of offset:
offset 0. Execution time: 0.061 ms
offset 1000. Execution time: 0.311 ms
offset 5000000. Execution time: 1055.408 ms

cleanup:
DROP TABLE limoffdemo;

        1.1.2 cursors demo

-- We must be in a transaction
BEGIN;
-- Open a cursor for a query
DECLARE limoffdemo_cur CURSOR FOR SELECT * FROM limoffdemo;
-- Retrieve ten rows
FETCH 10 FROM limoffdemo_cur;
-- ...
-- Retrieve ten more from where we left off
FETCH 10 FROM limoffdemo_cur;
-- All done
COMMIT;

        1.1.3 keyset pagination

            1.1.3.1 keyset pagination demo


-- Add an index for keyset pagination (btrees support inequality)
CREATE INDEX n_idx ON limoffdemo USING btree (n);

-- ask for first 5
SELECT * FROM limoffdemo ORDER BY n ASC LIMIT 5;

-- ask for next 
SELECT * FROM limoffdemo WHERE n > 5 ORDER BY n ASC LIMIT 5;

ex:
i500695=# SELECT * FROM limoffdemo ORDER BY n ASC LIMIT 5;
 n |               description               
---+-----------------------------------------
 1 | 47cea231817dae4e9f4b92ca7c86878e102596d
 2 | e46e19967e918216e71f9
 3 | 4c45cb1d80d2b865934b1
 4 | 39a7597bb0253ebaaf4b64e244a5da14dd1
 5 | afc2f6cf838588e24514663377144
(5 rows)

i500695=# SELECT * FROM limoffdemo WHERE n > 5 ORDER BY n ASC LIMIT 5;
 n  |                      description                       
----+--------------------------------------------------------
  6 | 25d0153a0ed32aa5624d77cc627cb7fe053803bac4c3ebd303de
  7 | 12b106f23962e37c7065437a3d6
  8 | 721f32f0adfcb5457e
  9 | e301059327374
 10 | b3560a35070bb6a096936625e454a5115d52f0400661ec27b67db4
(5 rows)

i500695=# SELECT * FROM limoffdemo WHERE n > 10 ORDER BY n ASC LIMIT 5;
 n  |                   description                    
----+--------------------------------------------------
 11 | 93e076f2285b6edac6a4985172657686b0333
 12 | 620
 13 | f6b197b710f9b8abfd796df7188f22e8d99fe4fd4d8ad7ad
 14 | 8e231ec45bdc3622bdc41315b1927bb13
 15 | 84c292e686eb34478cbad07a12d2de76f
(5 rows)

i500695=# SELECT * FROM limoffdemo ORDER BY n ASC LIMIT 15;
 n  |                      description                       
----+--------------------------------------------------------
  1 | 47cea231817dae4e9f4b92ca7c86878e102596d
  2 | e46e19967e918216e71f9
  3 | 4c45cb1d80d2b865934b1
  4 | 39a7597bb0253ebaaf4b64e244a5da14dd1
  5 | afc2f6cf838588e24514663377144
  6 | 25d0153a0ed32aa5624d77cc627cb7fe053803bac4c3ebd303de
  7 | 12b106f23962e37c7065437a3d6
  8 | 721f32f0adfcb5457e
  9 | e301059327374
 10 | b3560a35070bb6a096936625e454a5115d52f0400661ec27b67db4
 11 | 93e076f2285b6edac6a4985172657686b0333
 12 | 620
 13 | f6b197b710f9b8abfd796df7188f22e8d99fe4fd4d8ad7ad
 14 | 8e231ec45bdc3622bdc41315b1927bb13
 15 | 84c292e686eb34478cbad07a12d2de76f

large offset still efficient:
i500695=# EXPLAIN ANALYZE SELECT * FROM limoffdemo WHERE n > 5000000 ORDER BY n ASC LIMIT 5;
                                                             QUERY PLAN                                                             
------------------------------------------------------------------------------------------------------------------------------------
 Limit  (cost=0.43..0.62 rows=5 width=38) (actual time=0.070..0.072 rows=5 loops=1)
   ->  Index Scan using n_idx on limoffdemo  (cost=0.43..186551.71 rows=5039730 width=38) (actual time=0.070..0.071 rows=5 loops=1)
         Index Cond: (n > 5000000)
 Planning Time: 0.060 ms
 Execution Time: 0.084 ms

find max
EXPLAIN ANALYZE SELECT max(n) FROM limoffdemo;

        1.1.4 keyset pagination
        https://use-the-index-luke.com/no-offset

            1.1.4.1 article
Did you know pagination with offset is very troublesome but easy to avoid?

offset instructs the databases skip the first N results of a query. However, the database must still fetch these rows from the disk and bring them in order before it can send the following ones.

This is not an implementation problem, it's the way offset is designed:

…the rows are first sorted according to the <order by clause> and then limited by dropping the number of rows specified in the <result offset clause> from the beginning…

SQL:2016, Part 2, §4.15.3 Derived tables

There crucial point here is that offset takes only one single parameter: the number of rows to be dropped. No more context. The only thing the database can do with this number is fetching and dropping that many rows. In other words, big offsets impose a lot of work on the database—no matter whether SQL or NoSQL.

But the trouble with offset doesn't stop here: think about what happens if a new row is inserted between fetching two pages?


When using offset➌ to skip the previously fetched entries❶, you'll get duplicates in case there were new rows inserted between fetching two pages➋. There are other anomalies possible too, this is just the most common one.

This is not even a database problem, it is the way frameworks implement pagination: they just say which page number to fetch or how many rows to skip. With this information alone, no database can do any better.

Also note that the offset problem comes in may different syntaxes:

The offset keyword
The 2-parameter limit [offset,] limit (the 1 parameter limit is fine)
Lower-bound filtering based on row numbering (e.g. row_number(), rownum, …).
The root problem all these methods have in common is that they just provide a number of rows to be dropped—no more context. In this article, I'm using offset to refer to any of these methods.

Life Without OFFSET
Now imagine a world without these problems. As it turns out, living without offset is quite simple: just use a where clause that selects only data you haven't seen yet.

For that, we exploit the fact that we work on an ordered set—you do have an order by clause, ain't you? Once there is a definite sort order, we can use a simple filter to only select what follows the entry we have see last:

SELECT ...
  FROM ...
 WHERE ...
   AND id < ?last_seen_id
 ORDER BY id DESC
 FETCH FIRST 10 ROWS ONLY
This is the basic recipe. It gets more interesting when sorting on multiple columns, but the idea is the same. This recipe is also applicable to many NoSQL systems.

This approach—called seek method or keyset pagination—solves the problem of drifting results as illustrated above and is even faster than offset. If you'd like to know what happens inside the database when using offset or keyset pagination, have a look at these slides (benchmarks, benchmarks!):


On slide 43 you can also see that keyset pagination has some limitations: most notably that you cannot directly navigate to arbitrary pages. However, this is not a problem when using infinite scrolling. Showing page number to click on is a poor navigation interface anyway—IMHO.

If you want to read more about how to properly implement keyset pagination in SQL, please read this article. Even if you are not involved with SQL, it's worth reading that article before starting to implement anything.

But the Frameworks…
The main reason to prefer offset over keyset pagination is the lack of tool support. Most tools offer pagination based on offset, but don't offer any convenient way to use keyset pagination.

Please note that keyset pagination affects the whole technology stack up to the JavaScript running in the browser doing AJAX for infinite scrolling: instead of passing a simple page number to the server, you must pass a full keyset (often multiple columns) down to the server.

The hall of fame of frameworks that do support keyset pagination is constantly growing:

jOOQ — Java Object Oriented Querying. Docs.
Ruby order_query, nexter, and Sequel::SeekPagination
Django (Python): chunkator and Django Infinite Scroll Pagination
SQL Alchemy sqlakeyset.
blaze-persistence — a rich Criteria API for JPA providers
Perl DBIx::Class::Wrapper
Node.js: bookshelf-cursor-pagination
Massive.js, a data mapper for Node.js that goes all in on PostgreSQL: Keyset Documentation
This is where I need your help. If you are maintaining a framework that is somehow involved with pagination, I ask you, I urge you, I beg you, to build in native support for keyset pagination too. If you have any questions about the details, I'm happy to help (forum, contact form, Twitter)!

Even if you are just using software that should support keyset pagination such as a content management system or webshop, let the maintainers know about it. You might just file a feature request (link to this page) or, if possible, supply a patch. Again, I'm happy to help out getting the details right.

Take WordPress as an example.

Spread the Word
The problem with key-set pagination is not a technical one. The problem is just that it is hardly known in the field and has no tool support. If you like the idea of offset-less pagination, please help spreading the word. Tweet it, share it, mail it, you can even re-blog this post (CC-BY-NC-ND). Translations are also welcome, just contact me beforehand—I'll include a link to the translation on this page too!

Oh, and if you are blogging, you could also add a banner on your blog to make your readers aware of it. I've prepared a NoOffset banner gallery with some common banner formats. Just pick what suits you best.

            1.1.4.2 supporting libraries

                1.1.4.2.1 jOOQ — Java Object Oriented Querying. Docs.

                1.1.4.2.2 Massive.js, a data mapper for Node.js that goes all in on PostgreSQL: Keyset Documentation

                    1.1.4.2.2.1 https://blog.jooq.org/2013/11/18/faster-sql-pagination-with-keysets-continued/

                    1.1.4.2.2.2
                1.1.4.2.3
Ruby order_query, nexter, and Sequel::SeekPagination
Django (Python): chunkator and Django Infinite Scroll Pagination
SQL Alchemy sqlakeyset.
blaze-persistence — a rich Criteria API for JPA providers
Perl DBIx::Class::Wrapper
Node.js: bookshelf-cursor-pagination
            1.1.4.3


        1.1.5
    1.2 CLI

clients:

        1.2.1 pgcli 

            1.2.1.1 install, $ brew install pgcli

            1.2.1.2 Usage
$ pgcli --help
Usage: pgcli [OPTIONS] [DBNAME] [USERNAME]

Options:
  -h, --host TEXT         Host address of the postgres database.
  -p, --port INTEGER      Port number at which the postgres instance is
                          listening.
  -U, --username TEXT     Username to connect to the postgres database.
  -u, --user TEXT         Username to connect to the postgres database.
  -W, --password          Force password prompt.
  -w, --no-password       Never prompt for password.
  --single-connection     Do not use a separate connection for completions.
  -v, --version           Version of pgcli.
  -d, --dbname TEXT       database name to connect to.
  --pgclirc PATH          Location of pgclirc file.
  -D, --dsn TEXT          Use DSN configured into the [alias_dsn] section of
                          pgclirc file.
  --list-dsn              list of DSN configured into the [alias_dsn] section
                          of pgclirc file.
  --row-limit INTEGER     Set threshold for row limit prompt. Use 0 to disable
                          prompt.
  --less-chatty           Skip intro on startup and goodbye on exit.
  --prompt TEXT           Prompt format (Default: "\u@\h:\d> ").
  --prompt-dsn TEXT       Prompt format for connections using DSN aliases
                          (Default: "\u@\h:\d> ").
  -l, --list              list available databases, then exit.
  --auto-vertical-output  Automatically switch to vertical output mode if the
                          result is wider than the terminal width.
  --warn / --no-warn      Warn before running a destructive query.
  --help                  Show this message and exit.

id=__ExampleconnectandworkwithcdmstoreDB__
            1.2.1.3 Example, connect and work with cdm-store DB, tags: Example cdm-store DB 

a. login, ex: LTS <URL:#r=__login_to_lts__>
b. connect via ssh  <URL:#r=__connect_via_ssh___>
c. connect to DB <URL:#r=__connect_to_cdm_store_db>
d. set schema <URL:#r=__set_cdm_entities_schema__>

                1.2.1.3.1 login to space, e.g. LTS 
id=__login_to_lts__
$ cf login --sso -a  https://api.cf.sap.hana.ondemand.com
or if api endpoint set just login:
[i500695@WYLQRXL9LQ:2022-04-17 17:04:46:~/git/portal-cf-provisioning-service-new:]2103$ cf login --sso
API endpoint: https://api.cf.sap.hana.ondemand.com

Temporary Authentication Code ( Get one at https://login.cf.sap.hana.ondemand.com/passcode ): 
Authenticating...
OK


| Select an org:
| 1. DynamicForms
| 2. flpci
| 3. i500695launchpad
| 4. launchpad
| 5. mainsub
| 6. perfdt
| 7. portal-prov
| 8. rcsub
| 9. SaarTest
| 10. SAP_PORTAL_SERVICES
| 11. swz
| 12. swzmainlts
| 13. swzmainltshtml5apps
| 14. swzperf
| 15. swzrcrcthtml5apps
| 
| Org (enter to skip): 5
| Targeted org mainsub.
| 
| Select a space:
| 1. daily
| 2. flplts
| 3. lts

Space (enter to skip): 3
Targeted space lts.

API endpoint:   https://api.cf.sap.hana.ondemand.com
API version:    3.115.0
user:           yosi.izaq@sap.com
org:            mainsub
space:          lts

                1.2.1.3.2 open ssh tunnel
id=__connect_via_ssh___

                    1.2.1.3.2.1 Using Jawad portal CLI

                        1.2.1.3.2.1.1 raw command
                        portal postgres-ssh -p 6000 -a portal-cf-cdm-store-service -i portal-postgresql-db-dt


                        1.2.1.3.2.1.2 my alias

$ remCDMStorePGTunnel 6687

pgcli -h localhost -u 156bd25e7362 -d xrXhwTxYfDGJ -p 6687 
pgcli -h localhost -u d691b7fe8db3 -d DuLNcQfxgqIo -p 6687 
pgcli -h localhost -u 0c0996ff32f0 -d OHYHVbLjWezY -p 6543 
code:
__remCDMStorePGTunnel() {
    if [ -z $1 ] 
        then 
        echo "please provide local port"
        return  0
    fi
    PORT="$1"
__remPortalPGTunnel portal-cf-cdm-store-service $PORT
}

__remPortalPGTunnel() {
    if [ -z $2 ] 
        then 
        echo "please provide service name and local port"
        return  0
    fi
    SERVICE_NAME="$1"
    PORT="$2"
    portal postgres-ssh -a ${SERVICE_NAME} -p ${PORT}
}


                        1.2.1.3.2.1.3 possible issues
First -  better to connected via portal-cf-technical-support 


                            1.2.1.3.2.1.3.1 ssh disconnect 
ex: 
[i500695@WYLQRXL9LQ:2024-06-30 18:08:02:~/git/portal-cf-transport-service:]2007$ portal postgres-ssh -p 6000 -a portal-cf-technical-support-blue  -i portal-postgresql-db-dt
┌──────────┬──────────────────┐
│ server   │ localhost:6000   │
│ username │ af8d85a03968     │
│ password │ c0df7471efa294d1 │
│ dbname   │ ojliJbEfbhFx     │
└──────────┴──────────────────┘
Hit CTRL+C to terminate.
Error opening SSH connection: You are not authorized to perform the requested action.
FAILED
Failed - exec: Error opening SSH connection: You are not authorized to perform the requested action.

Fix: enable-ssh this app, then restart it using --strategy rolling
[i500695@WYLQRXL9LQ:2024-06-30 18:08:54:~/git/portal-cf-transport-service:]2008$ portal postgres-ssh -p 6000 -a portal-cf-technical-support-blue  -i portal-postgresql-db-dt 
[i500695@WYLQRXL9LQ:2024-06-30 18:16:15:~/git/portal-cf-transport-service:]2008$ cf ssh-enable portal-cf-technical-support-blue
'ssh-enable' is not a registered command. See 'cf help -a'

Did you mean?
    ssh-enabled
[i500695@WYLQRXL9LQ:2024-06-30 18:16:26:~/git/portal-cf-transport-service:]2009$ cf ssh-enabled portal-cf-technical-support-blue
ssh support is disabled for app 'portal-cf-technical-support-blue'.
ssh is disabled for app
[i500695@WYLQRXL9LQ:2024-06-30 18:16:33:~/git/portal-cf-transport-service:]2010$ cf enable-ssh portal-cf-technical-support-blue
Enabling ssh support for app portal-cf-technical-support-blue as yosi.izaq@sap.com...
OK

[i500695@WYLQRXL9LQ:2024-06-30 18:16:52:~/git/portal-cf-transport-service:]2011$ cf restart --strategy rolling portal-cf-technical-support-blue
Restarting app portal-cf-technical-support-blue in org SAP_PORTAL_SERVICES / space Portal as yosi.izaq@sap.com...

Creating deployment for app portal-cf-technical-support-blue...

Waiting for app to deploy...


                            1.2.1.3.2.1.3.2

                        1.2.1.3.2.1.4

                    1.2.1.3.2.2 using SSH

$ remCDMStorePGTunnel  6789
┌──────────┬────────────────┐
│ server   │ localhost:6789 │
│ username │ df6624eb2935   │
│ password │ 16b07ef591af   │
│ dbname   │ DuLNcQfxgqIo   │
└──────────┴────────────────┘

code:
__remServicePGTunnel() {
#Obseleted by Jawad's portal cli. Wrapped by __remPortalPGTunnel

    if [ -z $2 ] 
        then 
        echo "please provide service name and local port"
        return  0
    fi
    SERVICE_NAME="$1"
    PORT="$2"
    echo "exporting $SERVICE_NAME cf env params "
    #echo "Full env printout:"
    #cf ssh ${SERVICE_NAME} -c "echo \$VCAP_SERVICES"
        export ServicedbName=$( cf ssh ${SERVICE_NAME} -c "echo \$VCAP_SERVICES" | jq -r ' .postgresql[0].credentials.dbname' )
        export ServicedbHost=$( cf ssh ${SERVICE_NAME} -c "echo \$VCAP_SERVICES" | jq -r ' .postgresql[0].credentials.hostname' )
        export ServicedbUsr=$( cf ssh ${SERVICE_NAME} -c "echo \$VCAP_SERVICES" | jq -r ' .postgresql[0].credentials.username ' )
        export ServicedbPwd=$( cf ssh ${SERVICE_NAME} -c "echo \$VCAP_SERVICES" | jq -r ' .postgresql[0].credentials.password ' )
        export ServicedbPort=$( cf ssh ${SERVICE_NAME} -c "echo \$VCAP_SERVICES" | jq -r ' .postgresql[0].credentials.port ' )
        echo "ServicedbName=${ServicedbName},ServicedbHost=${ServicedbHost},ServicedbUsr=${ServicedbUsr},ServicedbPwd=${ServicedbPwd},ServicedbPort=${ServicedbPort},"

        export LOCALPORT="$PORT"
        echo "spawning local http server listening on ${LOCALPORT}"
        python3 -m http.server $LOCALPORT &
        echo "create a ssh tunnel by running: cf ssh -L ${LOCALPORT}:${ServicedbHost}:${ServicedbPort} ${SERVICE_NAME} " 
        echo "cf ssh -L ${LOCALPORT}:${ServicedbHost}:${ServicedbPort} $SERVICE_NAME"  | pbcopy
        echo "command is also copied to clipboard"
#cf ssh -L ${LOCALPORT}:${CDMdbName}:${CDMdbPort} portal-cf-cdm-store-service
}

                1.2.1.3.3 connect to DB
id=__connect_to_cdm_store_db

                    1.2.1.3.3.1 via pgcli



 pgcli -h localhost -u edb8650044ea -d DuLNcQfxgqIo -p 6687 -l 

b. pgcli -h localhost -u df6624eb2935   -d DuLNcQfxgqIo   -p 6789 
paste password 
pgcli supports auto complete...

Server: PostgreSQL 9.6.17
Version: 3.0.0
Chat: https://gitter.im/dbcli/pgcli
Home: http://pgcli.com

pgcli -h localhost -u d691b7fe8db3 -d DuLNcQfxgqIo -p 6789 


                    1.2.1.3.3.2 via pg admin
launch pg admin, create new connection and copy paste db and user name
on connect enter pwd
navigate to schemas and run query tool

                1.2.1.3.4 some basic operations

                    1.2.1.3.4.1  describe
46083fb6bcc784ee3c6899cbb4a0ef16> \d                                                                                                                                         
+----------+--------------------+----------+----------------------------------+
| Schema   | Name               | Type     | Owner                            |
|----------+--------------------+----------+----------------------------------|
| public   | migration          | table    | dbo                              |
| public   | migration_id_seq   | sequence | dbo                              |
| public   | pg_stat_statements | view     | 7dd4ab577be25f2d29f485680de69093 |
+----------+--------------------+----------+----------------------------------+
SELECT 3
Time: 0.115s

                    1.2.1.3.4.2  describe schemas
46083fb6bcc784ee3c6899cbb4a0ef16> \dn                                                                                                                                        
+--------------+-----------------+
| Name         | Owner           |
|--------------+-----------------|
| cdm          | dbo             |
| pgboss       | dbo             |
| provisioning | dbo             |
| public       | postgresql_user |
| report       | dbo             |
+--------------+-----------------+
SELECT 5
Time: 0.111s

                    1.2.1.3.4.3  describe tables
- just cdm tables 
  DuLNcQfxgqIo> \dt cdm.*
+--------+-----------------------------+-------+-------+
| Schema | Name                        | Type  | Owner |
|--------+-----------------------------+-------+-------|
| cdm    | CDM_ENTITIES                | table | dbo   |
| cdm    | RELATIONS                   | table | dbo   |
| cdm    | SYNC_CANDIDATE_CDM_ENTITIES | table | dbo   |
| cdm    | entities_searchable_text    | table | dbo   |
| cdm    | flyway_schema_history       | table | dbo   |
| cdm    | imports                     | table | dbo   |
| cdm    | migrations_lock             | table | dbo   |
| cdm    | pre_commit_cdm_entities     | table | dbo   |
| cdm    | pre_commit_relations        | table | dbo   |
| cdm    | snapshot_imports            | table | dbo   |
+--------+-----------------------------+-------+-------+

- all tables
46083fb6bcc784ee3c6899cbb4a0ef16> \dt *.*
+------------------------------+-------------------------------------+--------+----------+
| Schema                       | Name                                | Type   | Owner    |
|------------------------------+-------------------------------------+--------+----------|
| cdm                          | CDM_ENTITIES                        | table  | dbo      |
| cdm                          | RELATIONS                           | table  | dbo      |
| cdm                          | SYNC_CANDIDATE_CDM_ENTITIES         | table  | dbo      |
| cdm                          | entities_searchable_text            | table  | dbo      |
| cdm                          | flyway_schema_history               | table  | dbo      |
...
| information_schema           | sql_features                        | table  | rdsadmin |


                    1.2.1.3.4.4 count number of entities

                        1.2.1.3.4.4.1 set schema, tags: set schema

id=__set_cdm_entities_schema__
SET search_path TO cdm;

46083fb6bcc784ee3c6899cbb4a0ef16> select COUNT(*) from cdm."CDM_ENTITIES";                                                                                                   
+---------+
| count   |
|---------|
| 5       |
+---------+
SELECT 1
Time: 0.114s
46083fb6bcc784ee3c6899cbb4a0ef16> 

select * from cdm."CDM_ENTITIES" where "entityId" = 'mock_ep_exceeding';                                                                   

46083fb6bcc784ee3c6899cbb4a0ef16> select * from cdm."CDM_ENTITIES" where "entityId" = 'mock_ep_exceeding';                                                                   
+---------------+-------------+---------------+-------------+-------------------+--------------+----------------------------+---------------------+--------------------------
| createdOn     | createdBy   | updatedOn     | updatedBy   | entityId          | entityType   | entityTitle                | entityDescription   | tenantId                 
|---------------+-------------+---------------+-------------+-------------------+--------------+----------------------------+---------------------+--------------------------
| 1606990908000 | Yosi Izaq   | 1606990944000 | sysUser     | mock_ep_exceeding | provider     | mock_ep_exceeding_size_lim |                     | 810cd5c3489a9ef95a15cdf40
+---------------+-------------+---------------+-------------+-------------------+--------------+----------------------------+---------------------+--------------------------
SELECT 1

46083fb6bcc784ee3c6899cbb4a0ef16> explain select * from cdm."CDM_ENTITIES" where "entityId" = 'mock_ep_exceeding';                                                           
+-------------------------------------------------------------------+
| QUERY PLAN                                                        |
|-------------------------------------------------------------------|
| Seq Scan on "CDM_ENTITIES"  (cost=0.00..1051.03 rows=1 width=928) |
|   Filter: ("entityId" = 'mock_ep_exceeding'::text)                |
+-------------------------------------------------------------------+
EXPLAIN
Time: 0.104s
46083fb6bcc784ee3c6899cbb4a0ef16> explain analyze select * from cdm."CDM_ENTITIES" where "entityId" = 'mock_ep_exceeding';                                                   
+-------------------------------------------------------------------------------------------------------------+
| QUERY PLAN                                                                                                  |
|-------------------------------------------------------------------------------------------------------------|
| Seq Scan on "CDM_ENTITIES"  (cost=0.00..1051.03 rows=1 width=928) (actual time=0.012..0.616 rows=1 loops=1) |
|   Filter: ("entityId" = 'mock_ep_exceeding'::text)                                                          |
|   Rows Removed by Filter: 4                                                                                 |
| Planning time: 0.092 ms                                                                                     |
| Execution time: 0.645 ms                                                                                    |
+-------------------------------------------------------------------------------------------------------------+
EXPLAIN
Time: 0.102s

                    1.2.1.3.4.5 

                    1.2.1.3.4.6
                1.2.1.3.5 CDM store interesting statements, tags: CDM store interesting statements

                    1.2.1.3.5.1 get entities
explain analyze

SELECT json_strip_nulls(json_build_object( 'entity', json_build_object('metadata', json_build_object( 'updatedOn', "sub_account.updatedOn", 'updatedBy', "sub_account.updatedBy", 'createdBy', "sub_account.createdBy", 'createdOn', "sub_account.createdOn", 'context', json_build_object( 'id', "sub_account.id", 'contextType', "sub_account.contextType", 'tenantId', "sub_account.tenantId", 'identityZoneId', "sub_account.identityZoneId", 'instanceId', "sub_account.instanceId"))), 'base_entity', json_build_object( 'metadata', json_build_object( 'updatedOn', "snapshot.updatedOn", 'updatedBy', "snapshot.updatedBy", 'createdBy', "snapshot.createdBy", 'createdOn', "snapshot.createdOn", 'context', json_build_object( 'id', "snapshot.id", 'contextType', "snapshot.contextType", 'tenantId', "snapshot.tenantId", 'identityZoneId', "snapshot.identityZoneId", 'instanceId', "snapshot.instanceId"))) )) as final_result, "sub_account.entityMD5" as md5_value, coalesce("sub_account.updatedOnToSortBy", "snapshot.updatedOnToSortBy") as sorted_by_value, total_entities FROM ( SELECT *, count(*) OVER() as total_entities FROM ( ( SELECT sub_account."entityMD5" AS "sub_account.entityMD5", snapshot."entityMD5" AS "snapshot.entityMD5", sub_account."updatedOn" AS "sub_account.updatedOnToSortBy",snapshot."updatedOn" AS "snapshot.updatedOnToSortBy" , sub_account."updatedOn" AS "sub_account.updatedOn",sub_account."updatedBy" AS "sub_account.updatedBy",sub_account."createdBy" AS "sub_account.createdBy",sub_account."createdOn" AS "sub_account.createdOn",sub_account."id" AS "sub_account.id",sub_account."contextType" AS "sub_account.contextType",sub_account."tenantId" AS "sub_account.tenantId",sub_account."identityZoneId" AS "sub_account.identityZoneId",sub_account."instanceId" AS "sub_account.instanceId",snapshot."updatedOn" AS "snapshot.updatedOn",snapshot."updatedBy" AS "snapshot.updatedBy",snapshot."createdBy" AS "snapshot.createdBy",snapshot."createdOn" AS "snapshot.createdOn",snapshot."id" AS "snapshot.id",snapshot."contextType" AS "snapshot.contextType",snapshot."tenantId" AS "snapshot.tenantId",snapshot."identityZoneId" AS "snapshot.identityZoneId",snapshot."instanceId" AS "snapshot.instanceId" FROM ( SELECT * FROM "CDM_ENTITIES" INNER JOIN "RELATIONS" ON ("entityMD5" = "fromMD5" AND "fromTenantId" IN ('cf78569a871bd1a68b04b82e453cf303') AND "fromContextId" = 'SUB_ACCOUNT' AND lower("fromContextType") = lower('sub_account') AND "RELATIONS"."relationType" = 'extends') WHERE "tenantId" IN ('cf78569a871bd1a68b04b82e453cf303') AND "id" = 'SUB_ACCOUNT' AND lower("contextType") = lower('sub_account')) sub_account INNER JOIN "CDM_ENTITIES" AS snapshot ON sub_account."toMD5" = snapshot."entityMD5"

) UNION ALL SELECT sub_account."entityMD5" AS "sub_account.entityMD5", null AS "snapshot.entityMD5", sub_account."updatedOn" AS "sub_account.updatedOnToSortBy", null AS "snapshot.updatedOnToSortBy" , sub_account."updatedOn" AS "sub_account.updatedOn",sub_account."updatedBy" AS "sub_account.updatedBy",sub_account."createdBy" AS "sub_account.createdBy",sub_account."createdOn" AS "sub_account.createdOn",sub_account."id" AS "sub_account.id",sub_account."contextType" AS "sub_account.contextType",sub_account."tenantId" AS "sub_account.tenantId",sub_account."identityZoneId" AS "sub_account.identityZoneId",sub_account."instanceId" AS "sub_account.instanceId",null AS "snapshot.updatedOn",null AS "snapshot.updatedBy",null AS "snapshot.createdBy",null AS "snapshot.createdOn",null AS "snapshot.id",null AS "snapshot.contextType",null AS "snapshot.tenantId",null AS "snapshot.identityZoneId",null AS "snapshot.instanceId" FROM "CDM_ENTITIES" as sub_account WHERE ( "tenantId" IN ('cf78569a871bd1a68b04b82e453cf303') AND "id" = 'SUB_ACCOUNT' AND lower("contextType") = lower('sub_account') AND cdm -> 'relations' -> 'base' is null AND cdm -> 'payload' -> 'targetAppConfig' -> 'sap.integration' ->> 'urlTemplateId' <> 'urltemplate.nativeIOS' ) ) as inner_result ORDER BY coalesce("sub_account.updatedOnToSortBy", "snapshot.updatedOnToSortBy") DESC, "sub_account.entityMD5" LIMIT 5000 ) final_result;

                    1.2.1.3.5.2 get site entities
explain analyze SELECT "CDM_ENTITIES"."entityMD5",
"CDM_ENTITIES".cdm,
baseEntities.cdm as base_cdm,
coalesce("CDM_ENTITIES"."updatedOn", baseEntities."updatedOn") as "updatedOn",
coalesce("CDM_ENTITIES"."entityTitle", baseEntities."entityTitle") as "entityTitle",
coalesce("CDM_ENTITIES"."entityDescription", baseEntities."entityDescription") as "entityDescription",
coalesce("CDM_ENTITIES"."entityId", baseEntities."entityId") as "entityId"
, count(*) OVER() as total_entities
FROM "CDM_ENTITIES" INNER JOIN (
WITH RECURSIVE recursive_relations AS (
SELECT "entityMD5", "tenantId", 'ref' as "relationType"
FROM "CDM_ENTITIES"
WHERE "CDM_ENTITIES"."tenantId" = '1d59c4d7-5985-4f21-ad3c-1ac66bf14d2b'
And "CDM_ENTITIES"."id" = '2c8719b3-fec3-4e4e-83e0-90dde79e5b1c'
And "CDM_ENTITIES"."contextType" = 'site'
UNION
SELECT "RELATIONS"."toMD5", "RELATIONS"."toTenantId", "RELATIONS"."relationType"
FROM recursive_relations INNER JOIN "RELATIONS"
ON recursive_relations."entityMD5" = "RELATIONS"."fromMD5"
Where NOT ("RELATIONS"."fromEntityType" = 'businessapp' And "RELATIONS"."toEntityType" = 'section') And NOT ("RELATIONS"."fromEntityType" = 'section' And "RELATIONS"."toEntityType" = 'businessapp')
)
Select "entityMD5", "tenantId", "relationType"
From recursive_relations
) relations On relations."tenantId" = "CDM_ENTITIES"."tenantId" AND relations."entityMD5" = "CDM_ENTITIES"."entityMD5"
Left join "RELATIONS" On
relations."entityMD5" = "RELATIONS"."fromMD5" And "RELATIONS"."relationType" = 'extends'
Left join "CDM_ENTITIES" baseEntities On baseEntities."entityMD5" = "RELATIONS"."toMD5"
Where relations."relationType" != 'extends'
-- AND "CDM_ENTITIES"."tenantId" = '1d59c4d7-5985-4f21-ad3c-1ac66bf14d2b'
ORDER BY coalesce("CDM_ENTITIES"."entityMD5", baseEntities."entityMD5") DESC, "CDM_ENTITIES"."entityMD5" LIMIT 10000


                    1.2.1.3.5.3 get tenants 
select distinct "CDM_ENTITIES"."tenantId" from cdm."CDM_ENTITIES";                                                                                                   

                    1.2.1.3.5.4 Events log table
- get all events for events for subaccount (identity_zone_id) and specific content provider (context_id)
select * from cdm.entities_event_log where context_id = 'uyz200' and identity_zone_id = '75507021-30c8-49ea-bbf8-a579841a47c3'

- get all unprocessed events for subaccount (identity_zone_id) and specific content provider (context_id)
 select * from cdm.entities_event_log where context_id = 'uyz200' and identity_zone_id = '75507021-30c8-49ea-bbf8-a579841a47c3' and status <> 'PROCESSED';   


                    1.2.1.3.5.5 gather statistics

                        1.2.1.3.5.5.1 01/02/24 12:40:47  on LTS EU10, tags: 01/02/24 12:40:47  on LTS EU10

db size in megabytes:
set schema 'cdm'
SELECT pg_size_pretty(pg_total_relation_size('"CDM_ENTITIES"')) AS table_size;
5560 MB

select count(*) from cdm."CDM_ENTITIES";
603509

select count(*) from cdm."CDM_ENTITIES" where "entityType" = 'space'; 
11426

select count(*) from cdm."CDM_ENTITIES" where "entityType" = 'space' and "contextType"='sub_account' ; 
1331

select distinct "entityType" from cdm."CDM_ENTITIES" ;
"businessapp"
"catalog"
"contentunit"
"group"
"layout"
"menu"
"page"
"pageapp"
"provider"
"role"
"section"
"site"
"sitereferences"
"space"
"urltemplate"
"viztype"
"workpage"

SELECT "entityType", COUNT(*) AS count FROM cdm."CDM_ENTITIES" GROUP BY "entityType" ORDER BY count DESC;
"entityType" count
"businessapp" 343010
"catalog" 94931
"section" 46727
"group" 42144
"role" 32886
"page" 17901
"space" 11426
"workpage" 3174
"pageapp" 1796
"site" 1718
"sitereferences" 1699
"menu" 1492
"provider" 1392
"urltemplate" 1151
"viztype" 1139
"contentunit" 639
"layout" 284

select distinct "contextType" from cdm."CDM_ENTITIES" ;
"site"
"snapshot"
"bluebox"
"sub_account"

SELECT "contextType", COUNT(*) AS count FROM cdm."CDM_ENTITIES" GROUP BY "contextType" ORDER BY count DESC;
"snapshot"	570120
"sub_account"	23736
"site"	9116
"bluebox"	538



                        1.2.1.3.5.5.2 Count number of entities per tenants and also transitive site local entities 

- get tenants: 
  select distinct "CDM_ENTITIES"."tenantId" from cdm."CDM_ENTITIES";   
- get entityTypes 
select distinct "entityType" from cdm."CDM_ENTITIES" ;

- count entity types per tenant
SELECT "CDM_ENTITIES"."tenantId", "CDM_ENTITIES"."entityType", COUNT(*) AS typePerTenantCount FROM cdm."CDM_ENTITIES" GROUP BY "CDM_ENTITIES"."tenantId", "CDM_ENTITIES"."entityType" ORDER BY typePerTenantCount DESC;

- count local content entity types per tenant
SELECT "CDM_ENTITIES"."tenantId", "CDM_ENTITIES"."entityType", COUNT(*) AS typePerTenantCount FROM cdm."CDM_ENTITIES" WHERE  "CDM_ENTITIES"."contextType"='sub_account' GROUP BY "CDM_ENTITIES"."tenantId", "CDM_ENTITIES"."entityType" ORDER BY typePerTenantCount DESC;

- get local site entities per tenant
SELECT t."tenantId", t."entityId", t."entityMD5", COUNT(*) AS siteEntitiesPerTenant 
FROM  "CDM_ENTITIES" AS t
INNER JOIN  "RELATIONS" as r
ON t."entityMD5" = r."fromMD5"
WHERE t."entityType" = 'sitereferences'
GROUP BY t."tenantId", t."entityId",t."entityMD5"
ORDER BY siteEntitiesPerTenant DESC;

my trial and error session scratchpad:
SET search_path TO cdm;

select COUNT(*) from "CDM_ENTITIES" where "entityType" = 'site';                                                                                                   

select * from "CDM_ENTITIES"  LIMIT 10;

select * from "RELATIONS"  LIMIT 10;

SELECT t."tenantId", t."entityId", t."entityMD5", COUNT(*) AS siteEntitiesPerTenant 
FROM  "CDM_ENTITIES" AS t
INNER JOIN  "RELATIONS" as r
ON t."entityMD5" = r."fromMD5"
WHERE t."entityType" = 'sitereferences'
GROUP BY t."tenantId", t."entityId",t."entityMD5"
ORDER BY siteEntitiesPerTenant DESC;

select * from "CDM_ENTITIES" where "entityMD5"='b99c50fa1810d4b077ff8c458ee30713';
select * from "RELATIONS" where "fromMD5"='b99c50fa1810d4b077ff8c458ee30713';
select count(*) from "RELATIONS" where "fromMD5"='b99c50fa1810d4b077ff8c458ee30713';

select distinct "entityType" from cdm."CDM_ENTITIES" ;

SELECT "CDM_ENTITIES"."tenantId", "CDM_ENTITIES"."entityType", COUNT(*) AS typePerTenantCount FROM cdm."CDM_ENTITIES" WHERE  "CDM_ENTITIES"."contextType"='sub_account' GROUP BY "CDM_ENTITIES"."tenantId", "CDM_ENTITIES"."entityType" ORDER BY typePerTenantCount DESC;


                        1.2.1.3.5.5.3
                    1.2.1.3.5.6 get identityZoneIds 
select distinct "CDM_ENTITIES"."identityZoneId" from cdm."CDM_ENTITIES";                                                                                                   

                    1.2.1.3.5.7 get all businessapps of a given tenant
select * from "CDM_ENTITIES" where "entityType"='businessapp' and "tenantId"='foo';
                   
                    1.2.1.3.5.8 get all local businessapp that don't contain property in json

select * from "CDM_ENTITIES" where "entityType"='businessapp' and "contextType"='subaccount' and cdm->payload->targetAppConfig->sap->app-->providerId is NULL  ;

                    1.2.1.3.5.9 Count apollo providers
SELECT COUNT(distinct p.instance_id) as apollo_instances_count FROM provisioning."providers" p INNER JOIN provisioning."subscribers" s ON p.instance_id = s.instance_id WHERE p.is_product_provider = false;

basically search for all providers whose instance_id is same as subscriber's instance_id (meaning they are the same provider - only apollo solution is such that subscriber and provider are same)
                1.2.1.3.6
            1.2.1.4 pgcli configuration

                1.2.1.4.1 Switch between vi and emacs key bindings. F4.
Or edit configuration file, ~/.config/pgcli/config
change: vi = False 
to: vi = True 

                1.2.1.4.2 meta keybindings
Meta-commands (backslash commands)
Pgcli has implemented most of the meta-commands that are supported by psql. These meta-commands start with the backslash character. To see these, type \? at the prompt:

+--------------------------------------+------------------------------------------------+      
| Command                              | Description                                    |      
|--------------------------------------+------------------------------------------------|      
| \#                                   | Refresh auto-completions.                      |      
| \?                                   | Show Commands.                                 |      
| \T [format]                          | Change the table format used to output results |      
| \c[onnect] database_name             | Change to a new database.                      |      
| \conninfo                            | Get connection details                         |      
| \copy [tablename] to/from [filename] | Copy data between a file and a table.          |      
| \d[+] [pattern]                      | List or describe tables, views and sequences.  |      
| \dD[+] [pattern]                     | List or describe domains.                      |      
| \dT[S+] [pattern]                    | List data types                                |      
| \db[+] [pattern]                     | List tablespaces.                              |      
| \df[+] [pattern]                     | List functions.                                |      
| \di[+] [pattern]                     | List indexes.                                  |      
| \dm[+] [pattern]                     | List materialized views.                       |      
| \dn[+] [pattern]                     | List schemas.                                  |      
| \ds[+] [pattern]                     | List sequences.                                |      
| \dt[+] [pattern]                     | List tables.                                   |      
| \du[+] [pattern]                     | List roles.                                    |      
| \dv[+] [pattern]                     | List views.                                    |      
| \dx[+] [pattern]                     | List extensions.                               |      
| \e [file]                            | Edit the query with external editor.           |      
| \h                                   | Show SQL syntax and help.                      |      
| \i filename                          | Execute commands from file.                    |      
| \l[+] [pattern]                      | List databases.                                |      
| \n[+] [name] [param1 param2 ...]     | List or execute named queries.                 |      
| \nd [name]                           | Delete a named query.                          |      
| \ns name query                       | Save a named query.                            |      
| \o [filename]                        | Send all query results to file.                |      
| \pager [command]                     | Set PAGER. Print the query results via PAGER.  |      
| \pset [key] [value]                  | A limited version of traditional \pset         |      
| \q                                   | Quit pgcli.                                    |      
| \refresh                             | Refresh auto-completions.                      |      
| \sf[+] FUNCNAME                      | Show a function's definition.                  |      
| \timing                              | Toggle timing of commands.                     |      
| \x                                   | Toggle expanded output.                        |      
| quit                                 | Quit pgcli.                                    |      
+--------------------------------------+------------------------------------------------+   

                1.2.1.4.3 named queries

                    1.2.1.4.3.1 https://www.pgcli.com/named_queries.md
Named Queries
Named Queries are a way to save frequently used queries with a short name.

\n - list all named queries.

\n <name> - Invoke a named query by its name.

\ns <name> <query> - Save a new named query called 'name'.

\nd <name> - Delete an existing named query by its name.

Examples:

    # Save a new named query.
    > \ns simple select * from abc where a is not Null;

    # List all named queries.
    > \n
    +--------+---------------------------------------+
    | Name   | Query                                 |
    |--------+---------------------------------------|
    | simple | SELECT * FROM abc where a is not NULL |
    +--------+---------------------------------------+

    # Run a named query.
    > \n simple
    +--------+--------+
    | a      | b      |
    |--------+--------|
    | 日本語 | 日本語   |
    +--------+--------+

    # Delete a named query.
    > \nd simple
    simple: Deleted
Positional Parameters
Named queries support shell-style parameter substitution. Save your named query with parameters as placeholders (e.g. $1, $2, $3, etc.):

\ns user_by_name select * from users where name = '$1'
When you call a named query with parameters, just add the parameters after the query's name. You can put quotes around arguments that include spaces.

\n user_by_name "Skelly McDermott"
Parameters Aggregation
Named queries also support parameters aggregation via two placeholders. $* for raw aggregation and $@ for string aggregation. The former will use raw values of aggregated parameters, the later will quote each aggregated value.

Raw Aggregation
\ns users_by_age select * from users where id in ($*)
When you call a named query with parameters, just add any (at least one) parameters after the query's name.

\n users_by_age 42 1337
String Aggregation
\ns users_by_categories select * from users where category in ($@)
When you call a named query with parameters, just add any (at least one) parameters after the query's name. You can put quotes around arguments that include spaces.

\n users_by_categories "home user" "mobile user" superuser
Combining Positional Parameters and Parameters Aggregation
It is possible to combine both positional parameters and parameters aggregation. The positional parameters substitution takes place before the aggregation. Which means positional parameters can be placed after parameters aggregation in the query ; see example bellow. Please note that the positional parameters will not be part of the aggregation taking place afterwards!

\ns users_by_categories_and_age select * from users where name in ($@) and age = $1
\n users_by_categories_and_age 42 "Skelly McDermott" "François Pignon"
The query after substitution would be:

select * from users where name in ('Skelly McDermott', 'François Pignon') and age = 42

                    1.2.1.4.3.2 my examples
DuLNcQfxgqIo> \ns cdm_entities_of_tenant select * from cdm."CDM_ENTITIES" where "tenantId" = '$1'
Saved.
Time: 0.004s
DuLNcQfxgqIo> \n cdm_entities_of_tenant 02f588210b38c2cca281cabcd2adeb19
DuLNcQfxgqIo> \ns count_cdm_entities_of_tenant select count (*)  from cdm."CDM_ENTITIES" where "tenantId" = '$1'
Saved.
Time: 0.004s
DuLNcQfxgqIo> \n count_cdm_entities_of_tenant 02f588210b38c2cca281cabcd2adeb19
> select count (*)  from cdm."CDM_ENTITIES" where "tenantId" = '$1'
+---------+
| count   |
|---------|
| 129970  |
+---------+
SELECT 1
Time: 0.092s

                    1.2.1.4.3.3
                1.2.1.4.4
            1.2.1.5
        1.2.2 psql

            1.2.2.1  common commands

                1.2.2.1.1  list DBs
        \l

                1.2.2.1.2 list tables

                    1.2.2.1.2.1 \dt

                    1.2.2.1.2.2 Chatgpt:

                        1.2.2.1.2.2.1 list all tables in PG db

To list all tables in a PostgreSQL database, you can use the following SQL query:

SELECT table_name FROM information_schema.tables WHERE table_schema = 'public' AND table_type = 'BASE TABLE';
This query will return a list of all table names in the "public" schema of the database. If your tables are in a different schema, replace "public" with the appropriate schema name.




                        1.2.2.1.2.2.2 pgcli connect to remote pg db
To connect to a remote PostgreSQL database using pgcli, you can use the following command:

pgcli postgres://username:password@hostname:port/databasename
Replace "username" with your PostgreSQL username, "password" with your PostgreSQL password, "hostname" with the IP address or hostname of the remote server, "port" with the PostgreSQL port number (default is 5432), and "databasename" with the name of the PostgreSQL database you want to connect to.

For example, if your PostgreSQL username is "myuser", your password is "mypassword", the IP address of the remote server is "123.45.67.89", the port number is 5432, and the database name is "mydb", the command would be:

pgcli postgres://myuser:mypassword@123.45.67.89:5432/mydb
After running this command, you will be prompted to enter your password. Once you have entered your password, you should be connected to the remote PostgreSQL database using pgcli.



                        1.2.2.1.2.2.3 pg list schemas
To list all the schemas in a PostgreSQL database, you can use the following SQL query:

SELECT schema_name FROM information_schema.schemata;
This query will return a list of all the schema names in the current database.

You can execute this query using any PostgreSQL client or tool, such as pgAdmin or psql, by connecting to the database and running the query. For example, if you're using psql, you can connect to the database and run the query like this:

psql -h hostname -d dbname -U username -c "SELECT schema_name FROM information_schema.schemata;"
Replace "hostname", "dbname", and "username" with the appropriate values for your PostgreSQL setup. You'll be prompted to enter your PostgreSQL password, and then you should see a list of all the schema names in the database.



                        1.2.2.1.2.2.4 pg_repack

pg_repack is a PostgreSQL extension that provides a way to perform online table reorganizations and other maintenance operations with minimal disruption to the availability of the table. It is designed to work with large databases and can be used to vacuum tables in addition to other maintenance operations.

To use pg_repack to vacuum a table, you can follow these steps:

Install the pg_repack extension if it is not already installed. You can do this by running the following command:

CREATE EXTENSION pg_repack;
Connect to the database using a PostgreSQL client or tool, such as psql.

Run the following command to vacuum the table:

SELECT pg_repack('public.mytable', 'verbose');
Replace "public.mytable" with the name of the schema and table that you want to vacuum.

The second argument, "verbose", is optional and will provide more detailed output during the vacuum operation.

Wait for the pg_repack operation to complete. This may take some time depending on the size of the table.

Check the status of the vacuum operation by running the following command:

SELECT * FROM pg_stat_activity WHERE query LIKE 'SELECT pg_repack%';
This will show you the progress of the pg_repack operation, as well as any other information about the process.

Note that pg_repack is a powerful tool that can be used for other maintenance operations as well, such as table compression and index optimization. Be sure to read the documentation carefully and test any changes in a non-production environment before applying them to a production database.

                        1.2.2.1.2.2.5
                1.2.2.1.3 switch DB
\c dbname

                1.2.2.1.4 describe table
\d tablename
\dt tablename
ex:
AjjAxgyOruVg> \d cdm."CDM_ENTITIES"
+-------------------+--------+-----------+
| Column            | Type   | Modifiers |
|-------------------+--------+-----------|
| createdOn         | bigint |  not null |
| createdBy         | text   |  not null |
| updatedOn         | bigint |  not null |
| updatedBy         | text   |  not null |
| entityId          | text   |  not null |
| entityType        | text   |  not null |
| entityTitle       | text   |           |
| entityDescription | text   |           |
| tenantId          | text   |  not null |
| instanceId        | text   |  not null |
| identityZoneId    | text   |  not null |
| contextType       | text   |  not null |
| id                | text   |  not null |
| cdm               | jsonb  |  not null |
| entityMD5         | text   |           |
+-------------------+--------+-----------+
Indexes:
    "CDM_ENTITIES_pkey" PRIMARY KEY, btree ("entityType", "entityId", id, "contextType", "tenantId")
    "md5_unique" UNIQUE CONSTRAINT, btree ("entityMD5")
    "cdm_entities_entity_md5_index" btree ("entityMD5")
    "cdm_entities_entity_type_index" btree ("entityType")
    "cdm_entities_table_tenant_context_id_type_index" btree ("tenantId", id, "contextType")
    "cdm_entities_tenant_id_index" btree ("tenantId")
Referenced by:
    TABLE "cdm.entities_searchable_text" CONSTRAINT "entities_searchable_text_entity_md5_fkey" FOREIGN KEY (entity_md5) REFERENCES cdm."CDM_ENTITIES"("entityMD5") ON UPDATE CASCADE ON DELETE CASCADE

Time: 1.123s (1 second), executed in: 1.118s (1 second)
AjjAxgyOruVg> \dt cdm."CDM_ENTITIES"
+--------+--------------+-------+-------+
| Schema | Name         | Type  | Owner |
|--------+--------------+-------+-------|
| cdm    | CDM_ENTITIES | table | dbo   |
+--------+--------------+-------+-------+


                1.2.2.1.5 history
\g

                1.2.2.1.6 list commands
    \?

                1.2.2.1.7 help on commands 
            i500695-# \h command
            ex:
            i500695-# \h DROP


                1.2.2.1.8 measure command runtime
\timing
(run command)

        1.2.9 edit commands in vim
\e

        1.2.10
            1.2.2.2
        1.2.3 determine size table, db, indexes etc

            1.2.3.1 https://www.a2hosting.com/kb/developer-corner/postgresql/determining-the-size-of-postgresql-databases-and-tables
You can use the psql command-line program to determine the sizes of PostgreSQL databases and tables. To do this, follow these steps:

Log in to your account using SSH.
At the command line, type the following command. Replace dbname with the name of the database, and username with the database username:
sql dbname username
At the Password prompt, type the database user's password. When you type the correct password, the psql prompt appears.
To determine the size of a database, type the following command. Replace dbname with the name of the database that you want to check:

SELECT pg_size_pretty( pg_database_size('dbname') );
Psql displays the size of the database.

To determine the size of a table in the current database, type the following command. Replace tablename with the name of the table that you want to check:

SELECT pg_size_pretty( pg_total_relation_size('tablename') );
SELECT pg_size_pretty( pg_total_relation_size('CARDS') );

        - From blogs 
        https://www.postgresql.org/docs/9.5/textsearch-intro.html
https://www.postgresqltutorial.com/postgresql-administration/postgresql-database-indexes-table-size/

- get table size 
 SELECT pg_size_pretty( pg_total_relation_size(cdm."CDM_ENTITIES") ) FROM cdm."CDM_ENTITIES";

 or with pg_admin:
set schema 'cdm'
SELECT pg_size_pretty(pg_total_relation_size('"CDM_ENTITIES"')) AS table_size;

- get DB size 
#list databases
DuLNcQfxgqIo> \l
+--------------+--------------+----------+-------------+-------------+-------------------------------+
| Name         | Owner        | Encoding | Collate     | Ctype       | Access privileges             |
|--------------+--------------+----------+-------------+-------------+-------------------------------|
| DuLNcQfxgqIo | lbsSHTeBEQeo | UTF8     | en_US.UTF-8 | en_US.UTF-8 | =Tc/lbsSHTeBEQeo              |
|              |              |          |             |             | lbsSHTeBEQeo=CTc/lbsSHTeBEQeo |
|              |              |          |             |             | dbo=C*T*c*/lbsSHTeBEQeo       |
| postgres     | lbsSHTeBEQeo | UTF8     | en_US.UTF-8 | en_US.UTF-8 | <null>                        |
| rdsadmin     | rdsadmin     | UTF8     | en_US.UTF-8 | en_US.UTF-8 | rdsadmin=CTc/rdsadmin         |
| template0    | rdsadmin     | UTF8     | en_US.UTF-8 | en_US.UTF-8 | =c/rdsadmin                   |
|              |              |          |             |             | rdsadmin=CTc/rdsadmin         |
| template1    | lbsSHTeBEQeo | UTF8     | en_US.UTF-8 | en_US.UTF-8 | lbsSHTeBEQeo=CTc/lbsSHTeBEQeo |
|              |              |          |             |             | =c/lbsSHTeBEQeo               |
+--------------+--------------+----------+-------------+-------------+-------------------------------+
DuLNcQfxgqIo> SELECT pg_size_pretty( pg_database_size('postgres') );
+----------------+
| pg_size_pretty |
|----------------|
| 9569 kB        |
+----------------+

#real DB:
DuLNcQfxgqIo> SELECT pg_size_pretty( pg_database_size('DuLNcQfxgqIo') );
+----------------+
| pg_size_pretty |
|----------------|
| 28 GB          |
+----------------+

- get indexes size 
DuLNcQfxgqIo> SELECT pg_size_pretty( pg_indexes_size(cdm."CDM_ENTITIES") );

            1.2.3.2 examples from cdm-store

                1.2.3.2.1 get table size


DuLNcQfxgqIo> SELECT pg_size_pretty( pg_total_relation_size('"CDM_ENTITIES"') );
+----------------+
| pg_size_pretty |
|----------------|
| 5789 MB        |
+----------------+
SELECT 1
Time: 0.076s
DuLNcQfxgqIo> SELECT pg_size_pretty( pg_total_relation_size('cdm."CDM_ENTITIES"') );
+----------------+
| pg_size_pretty |
|----------------|
| 5789 MB        |
+----------------+
SELECT 1
Time: 0.083s

                1.2.3.2.2 list top 10 sized tables 
DuLNcQfxgqIo> select schemaname as table_schema,
     relname as table_name,
     pg_size_pretty(pg_total_relation_size(relid)) as total_size,
     pg_size_pretty(pg_relation_size(relid)) as data_size,
     pg_size_pretty(pg_total_relation_size(relid) - pg_relation_size(relid))
       as external_size
 from pg_catalog.pg_statio_user_tables
 order by pg_total_relation_size(relid) desc,
          pg_relation_size(relid) desc
 limit 10;
+------------------------------+--------------------------+------------+-----------+---------------+
| table_schema                 | table_name               | total_size | data_size | external_size |
|------------------------------+--------------------------+------------+-----------+---------------|
| cdm                          | RELATIONS                | 21 GB      | 5534 MB   | 16 GB         |
| cdm                          | CDM_ENTITIES             | 5789 MB    | 836 MB    | 4953 MB       |
| cdm                          | entities_searchable_text | 763 MB     | 240 MB    | 524 MB        |
| cdm                          | pre_commit_relations     | 620 MB     | 0 bytes   | 620 MB        |
| report                       | reports                  | 120 MB     | 90 MB     | 30 MB         |
| cdm                          | pre_commit_cdm_entities  | 118 MB     | 0 bytes   | 118 MB        |
| pgboss                       | job                      | 94 MB      | 624 kB    | 93 MB         |
| public                       | CARD_TRANSLATIONS        | 32 MB      | 5064 kB   | 27 MB         |
| portal.cf::ActivationService | SITES_STATES             | 28 MB      | 15 MB     | 13 MB         |
| pgboss                       | archive                  | 10 MB      | 1144 kB   | 9360 kB       |
+------------------------------+--------------------------+------------+-----------+---------------+
SELECT 10
Time: 0.092s

                1.2.3.2.3 get indexes size 
DuLNcQfxgqIo> SELECT pg_size_pretty( pg_indexes_size('cdm."CDM_ENTITIES"') );
+----------------+
| pg_size_pretty |
|----------------|
| 676 MB         |
+----------------+
SELECT 1
Time: 0.077s
DuLNcQfxgqIo> SELECT pg_size_pretty( pg_indexes_size('cdm."RELATIONS') );
invalid name syntax
LINE 1: SELECT pg_size_pretty( pg_indexes_size('cdm."RELATIONS') )
                                               ^

Time: 0.075s
DuLNcQfxgqIo> SELECT pg_size_pretty( pg_indexes_size('cdm."RELATIONS"') );
+----------------+
| pg_size_pretty |
|----------------|
| 16 GB          |
+----------------+
SELECT 1
Time: 0.077s

                1.2.3.2.4 search for provider contention
select * from cdm."CDM_ENTITIES" where "identityZoneId" = 'c4e3c145-69b5-4b1f-a534-8c17c0d702b8' and "entityId" like '%sample_pkg1%';

note that contextId is named id, and entity id is entityId
                1.2.3.2.5
            1.2.3.3
        1.2.4
    1.3 text search
        1.3.1  

        1.3.2

    1.4 performance

        1.4.1  SAP cdm-store DB performance
https://wiki.wdf.sap.corp/wiki/display/CPPortals/Design-time+PG+DB+Performance+improvements

        1.4.2
    1.5 local playground, tags: postgres local playground docker pg 

        1.5.1  setup local postgres playground
a. install docker, https://docs.docker.com/desktop/mac/apple-silicon/ 
b. pull PG image and run it 
$ docker pull postgres:9.6
$  docker run --name local-postgres -e POSTGRES_USER=i500695 -e POSTGRES_PASSWORD=Abcd1234 -d -p 5432:5432 postgres:9.6
optional,  attach to container
[i500695@WYLQRXL9LQ:2022-03-08 18:40:38:~/work/code/interviewQs/leetcode:]2048$ docker ps
CONTAINER ID   IMAGE          COMMAND                  CREATED              STATUS              PORTS                    NAMES
9d4c98470620   postgres:9.6   "docker-entrypoint.s…"   About a minute ago   Up About a minute   0.0.0.0:5432->5432/tcp   local-postgres
[i500695@WYLQRXL9LQ:2022-03-08 18:42:37:~/work/code/interviewQs/leetcode:]2052$ docker exec -it 9d4c98470620 /bin/bash
root@9d4c98470620:/# pgcli
c. run pgcli 
[i500695@WYLQRXL9LQ:2022-03-08 18:44:52:~/work/code/interviewQs/leetcode:]2058$ pgcli -h localhost -p 5432 -u i500695
Password for i500695: 
Server: PostgreSQL 9.6.24
Version: 3.4.0
Home: http://pgcli.com
i500695@localhost:i500695>

d. create db 
CREATE DATABASE playground;

check its listed 
i500695@localhost:i500695> \l
+------------+---------+----------+------------+------------+---------------------+
| Name       | Owner   | Encoding | Collate    | Ctype      | Access privileges   |
|------------+---------+----------+------------+------------+---------------------|
| i500695    | i500695 | UTF8     | en_US.utf8 | en_US.utf8 | <null>              |
| playground | i500695 | UTF8     | en_US.utf8 | en_US.utf8 | <null>              |
| postgres   | i500695 | UTF8     | en_US.utf8 | en_US.utf8 | <null>              |
| template0  | i500695 | UTF8     | en_US.utf8 | en_US.utf8 | =c/i500695          |
|            |         |          |            |            | i500695=CTc/i500695 |
| template1  | i500695 | UTF8     | en_US.utf8 | en_US.utf8 | =c/i500695          |
|            |         |          |            |            | i500695=CTc/i500695 |
+------------+---------+----------+------------+------------+---------------------+

- create src table
i500695@localhost:i500695> create table src( ID INT NOT NULL, Name varchar (30) NOT NULL) 
i500695@localhost:i500695> \dt
+--------+------+-------+---------+
| Schema | Name | Type  | Owner   |
|--------+------+-------+---------|
| public | src  | table | i500695 |
+--------+------+-------+---------+
-
        1.5.2 common operations 

            1.5.2.1  insert into example


i500695@localhost:i500695> insert into src values(1, 'yosi')
INSERT 0 1
Time: 0.015s
i500695@localhost:i500695> select * from src
+----+------+
| id | name |
|----+------|
| 1  | yosi |
+----+------+
SELECT 1
Time: 0.019s
i500695@localhost:i500695> insert into src values(2, 'Deby')
INSERT 0 1
Time: 0.004s
i500695@localhost:i500695> select * from src
+----+------+
| id | name |
|----+------|
| 1  | yosi |
| 2  | Deby |
+----+------+

            1.5.2.2 comparing two tables
(select * from src except select * from trg) union all (select * from trg except select * from src)
example:
i500695@localhost:i500695> select * from src
+----+------+
| id | name |
|----+------|
| 1  | yosi |
| 2  | Deby |
+----+------+
SELECT 2
Time: 0.010s
i500695@localhost:i500695> select * from trg
+----+------+
| id | name |
|----+------|
| 2  | Deby |
+----+------+
SELECT 1
Time: 0.016s
i500695@localhost:i500695> (select * from src except select * from trg) union all (select * from trg except select * from src)
+----+------+
| id | name |
|----+------|
| 1  | yosi |
+----+------+
SELECT 1
Time: 0.016s
            1.5.2.3
        1.5.3
    1.6 pgadmin

        1.6.1  install and setup
google pgadmin mac install 
install and launch 
create new server 
run 
$ remCDMStorePGTunnel  6789
┌──────────┬────────────────┐
│ server   │ localhost:6789 │
│ username │ 20397e970aac   │
│ password │ feeb65f51f5c   │
│ dbname   │ DuLNcQfxgqIo   │
└──────────┴────────────────┘
copy paste to server connection settings 
connect 
viola 
        1.6.2

    1.7 Mastering SQL using Postgresql
https://postgresql.itversity.com/mastering-sql-using-postgresql.html#desired-audience

    1.8 PostgreSQL: Advanced SQL Queries and Data Analysis 
https://www.udemy.com/course/postgresql-advanced-sql-queries-and-data-analysis/?utm_source=adwords&utm_medium=udemyads&utm_campaign=LongTail_la.EN_cc.ROW&utm_content=deal4584&utm_term=_._ag_77879424134_._ad_535397245863_._kw__._de_c_._dm__._pl__._ti_dsa-1007766171312_._li_1007990_._pd__._&matchtype=&gclid=CjwKCAjwvNaYBhA3EiwACgndgtlgaGYUELdDwIkS1BKKPEQwm0Qzrkr_q2I_NkLIdRF7B8kq_iJPTRoCOhcQAvD_BwE

    1.9 postgres transactions and concurrency, tags: postgres transactions and concurrency

        1.9.1 https://www.postgresql.org/files/developer/transactions.pdf

            1.9.1.1 What is a transaction, anyway?
Definition: a transaction is a group of SQL
commands whose results will be made
visible to the rest of the system as a
unit when the transaction commits --- or not
at all, if the transaction aborts.
Transactions are expected to be atomic, consistent,
isolated, and durable.
• Postgres does not support distributed transactions, so all commands
of a transaction
are executed by one backend.
• We don’t currently handle nested transactions, either

            1.9.1.2 The ACID test: atomic, consistent, isolated, durable
Atomic: results of a transaction are seen entirely or not at all within other transactions.
(A transaction need not appear atomic to itself.)

Consistent: system-defined consistency constraints are enforced on the results of
transactions. (Not going to discuss constraint checking today.)

Isolated: transactions are not affected by the behavior of concurrently-running transactions.
Stronger variant: serializable. If the final results of a set of concurrent transactions are the same as if we’d run the
transactions serially in some order (not necessarily any predetermined order), then we say the behavior is serializable.

Durable: once a transaction commits, its results will not be lost regardless of subsequent failures.

            1.9.1.3 But how can thousands of changes be made "atomically"?
• The actual tuple insertions/deletions/updates are all marked
as done by transaction N
as they are being made. Concurrently running backends ignore the changes
because they know transaction N is not committed yet. When the transaction
commits,
all those changes become logically visible at once.
• The control file pg_log contains 2 status bits per
transaction ID, with possible states
in progress, committed,
aborted. Setting those two bits to the
value committed is
the atomic action that marks a transaction committed.
• An aborting transaction will normally set its pg_log status
to aborted. But even if
the process crashes without having
done so, everything is safe. The next time some
backend checks the state of
that transaction, it will observe that the transaction is
marked in
progress but is not running on any backend, deduce that it crashed,
and
update the pg_log entry to aborted on its behalf.
No changes are needed
in any table data file during abort.


            1.9.1.4 But is it really atomic and durable, even if the system crashes?
Well ... that depends on how much you trust your kernel and hard disk.
• Postgres transactions are only guaranteed atomic if a disk page write is
an atomic
action. On most modern hard drives that’s true if a page is a
physical sector, but most
people run with disk pages configured as 8K or so,
which makes it a little more dubious
whether a page write is all-or-nothing.
• pg_log is safe anyway since we’re only flipping bits in
it, and both bits of a
transaction’s status must be in the same sector.
• But when moving tuples around in a data page, there’s a potential for
data corruption
if a power failure should manage to abort the page write
partway through (perhaps only
some of the component sectors get written).
This is one reason to keep page sizes
small ... and to buy a UPS for your server!



            1.9.1.5 Working through the Unix kernel costs us something, too
It’s critical that we force a transaction’s data page changes down to disk
before we write
pg_log. If the disk writes occur in the wrong
order, a power failure could leave us with a
transaction that’s
marked committed in pg_log but not all of
whose data changes are
reflected on disk --- thus failing the atomicity test.
• Unix kernels allow us to force the correct write order via fsync(2), but
the performance
penalty of fsync’ing many files is pretty high.
• We’re looking at ways to avoid needing so many fsync()s, but that’s a
different talk.

            1.9.1.6 User’s view: multi-version concurrency control
A PostgreSQL application sees the following behavior of concurrent
transactions:
• Each transaction sees a snapshot (database version) as of its start
time,
no matter what other transactions are doing while it runs
• Readers do not block writers, writers do not block readers
• Writers only block each other when updating the same row


            1.9.1.7 Concurrent updates are tricky
Consider this example: transaction A does
UPDATE foo SET x = x + 1 WHERE rowid = 42
and before it commits,
transaction B comes along and wants to do the same thing
on the same row.
• B clearly
must wait to see if A commits or not.
• If A aborts then B can go ahead,
using the pre-existing value of x.
• But if A commits, what then?
• Using
the old value of x will yield a clearly unacceptable
result: x ends up
incremented by 1 not 2 after both transactions commit.
• But if B is allowed to increment the new value
of x, then B is reading data committed
since it began execution. This violates the basic principle of
transaction isolation.
        

            1.9.1.8 Read committed vs. serializable transaction level
PostgreSQL offers two answers to the concurrent-update problem
(out of four
transaction isolation levels defined in the ISO SQL standard):
Read committed level: allow B to use new tuple as input
values (after checking
to ensure new tuple still satisfies query’s WHERE clause). Thus, B is
allowed to
see just this tuple of A’s results.
Serializable level: abort B with "not serializable" error.
Client application must redo
the whole transaction B, which will then be allowed
to see the new value of x under
strict serializable-behavior rules.
• Serializable level is logically cleaner but requires more code in
application, so by
default we run in read-committed level which usually produces the desired
behavior.
• In either case a pure SELECT transaction only sees data committed before
it started.
It’s just updates and deletes that are interesting.


            1.9.1.9 How it’s implemented
"O say, can you see that tuple?"
The most fundamental implementation concept is tuple
visibility: which versions
of which table rows are seen by which transactions.
Ignoring tuples you’re not supposed to be able to see is the key to
making
transactions appear atomic.
Definition: a tuple is a specific stored object in
a table,
representing one version
of some logical table row. A row may exist in
multiple versions simultaneously.

            1.9.1.10 Non-overwriting storage management
We must store multiple versions of every row. A tuple can be removed only
after
it’s been committed as deleted for long enough that no active
transaction
can see it anymore.
Fortunately, PostgreSQL has always practiced "non overwriting" storage
management: updated tuples are appended to the table, and older versions
are
removed sometime later.
Currently, removal of long-dead tuples is handled by
a VACUUM maintenance
command that must be issued
periodically. We are looking at ways to reduce
need for VACUUM by recycling dead tuples on-the-fly.


            1.9.1.11 Per-tuple status information
Tuple headers contain:
• xmin: transaction ID of inserting transaction
• xmax: transaction ID of replacing/deleting transaction (initially NULL)
• forward link: link to newer version of same logical row, if any
Basic idea: tuple is visible if xmin is valid and xmax is not. "Valid"
means
"either committed or the current transaction".
If we plan to update rather than delete, we first add new version of row
to table,
then set xmax and forward link in old tuple. Forward link will
be needed by
concurrent updaters (but not by readers).
To avoid repeated consultation of pg_log, there are also
some status
bits that indicate
"known committed" or "known aborted" for xmin and xmax.
These are set by the first
backend that inspects xmin or xmax after the
referenced transaction commits/aborts.


            1.9.1.12 "Snapshots" filter away active transactions
If Transaction A commits while Transaction B is running, we don’t want B
to
suddenly start seeing A’s updates partway through.
• Hence, we make a list
at transaction start of which transactions are currently being
run by other backends.
(Cheap shared-memory communication is essential here: we
just look in a
shared-memory table, in which each backend records its current
transaction
number.)
• These transaction IDs will never be considered valid
by the current transaction,
even if they are shown to be committed in pg_log or on-row status bits.
• Nor will a transaction with ID higher than the current transaction’s
ever be
considered valid.
• These rules ensure that no transaction committing after the current
transaction’s
start will be considered committed.
• Validity is in the eye of the beholder.


            1.9.1.13 Table-level locks: still gotta have ’em for some things
Even though readers and writers don’t block each other under MVCC, we still
need
table-level locking.
This exists mainly to prevent the entire table from
being altered or deleted
out from under readers or writers.
We also offer various lock levels for application use (mainly for
porting applications
that take a traditional lock-based approach to
concurrency).


            1.9.1.14 Types of locks
Lock type Acquired by system for Conflicts with
| 1 AccessShareLock SELECT 7
| 2 RowShareLock SELECT FOR UPDATE 6,7
| 3 RowExclusiveLock UPDATE, INSERT, DELETE 4,5,6,7
| 4 ShareLock CREATE INDEX 3,5,6,7
| 5 ShareRowExclusiveLock 3,4,5,6,7
| 6 ExclusiveLock 2,3,4,5,6,7
| 7 AccessExclusiveLock DROP TABLE, ALTER TABLE, VACUUM all
All lock types can be obtained by user LOCK TABLE commands.
Locks are held till end of transaction: you can grab a lock, but you can’t
release it
except by ending your transaction.

            1.9.1.15 Lock implementation
Locks are recorded in a shared-memory hash table keyed by kind and ID of
object
being locked. Each item shows the types and numbers of locks held or
pending on
its object. Would-be lockers who have a conflict with an existing
lock must wait.
Waiting is handled by waiting on a per-process IPC semaphore, which will
be
signaled when another process releases the wanted lock. Note we need
only one
semaphore per concurrent backend, not one per potentially lockable
object.

            1.9.1.16 Deadlock detection
Deadlock is possible if two transactions try to grab conflicting locks
in different orders.
If a would-be locker sleeps for more than a second without getting the
desired lock,
it runs a deadlock-check algorithm that searches the
lock hash table for circular
lock dependencies. If it finds any, then
obtaining the lock will be impossible, so it
gives up and reports an
error. Else it goes back to sleep and waits till granted the
lock (or
till client application gives up and requests transaction cancel).
• The delay before running the deadlock check algorithm can be
tuned to match the
typical transaction time in a particular server’s
workload. In this way, unnecessary
deadlock checks are seldom
performed, but real deadlocks are detected reasonably
quickly.


            1.9.1.17 Short-term locks
Short-term locks protect datastructures in shared memory, such as the lock
hashtable described above.
These locks should only be held for long enough to
examine and/or update a
shared item --- in particular a backend should never
block while holding one.
Implementation: spin locks based on platform-specific
atomic test-and-set
instructions. This allows the lock code to fall through extremely quickly
in the
common case where there is no contention for the lock. If the test-and-set
fails,
we sleep for a short period (using select(2)) and
try again. No deadlock detection
as such, but we give up and report error
if fail too many times


            1.9.1.18 Summary
PostgreSQL offers true ACID semantics for transactions, given some
reasonable
assumptions about the behavior of the underlying Unix kernel
and hardware.
Multi-version concurrency control allows concurrent reading
and writing of tables,
blocking only for concurrent updates of same row.
MVCC is practical because of non-overwriting storage manager that we
inherited
from the Berkeley POSTQUEL project. Traditional
row-overwriting storage
management would have a much harder time.

            1.9.1.19
        1.9.2
    1.10 postgres DB maintenance , tags: postgres DB maintenance 

        1.10.1 VACUUM

            1.10.1.1 get list of dead tuples, tags: get list of dead tuples

                1.10.1.1.1  simple query


Query to get the dead rows 
SELECT relname, n_dead_tup
FROM pg_stat_user_tables
WHERE schemaname = 'cdm'
ORDER BY relname;

                1.10.1.1.2 Jawad's query
with table_bloat_query as (SELECT schemaname, tblname as tablename,
('"' || schemaname || '"."' || tblname || '"')::regclass::oid as table_oid,
bs*tblpages AS real_size,
  (tblpages-est_tblpages)*bs AS extra_size,
  CASE WHEN tblpages - est_tblpages > 0
    THEN 100 * (tblpages - est_tblpages)/tblpages::float
    ELSE 0
  END AS extra_ratio, fillfactor,
  CASE WHEN tblpages - est_tblpages_ff > 0
    THEN (tblpages-est_tblpages_ff)*bs
    ELSE 0
  END AS bloat_size,
  CASE WHEN tblpages - est_tblpages_ff > 0
    THEN 100 * (tblpages - est_tblpages_ff)/tblpages::float
    ELSE 0
  END AS bloat_ratio, is_na,
  current_timestamp::timestamp(0) as stats_time
FROM (
  SELECT ceil( reltuples / ( (bs-page_hdr)/tpl_size ) ) + ceil( toasttuples / 4 ) AS est_tblpages,
    ceil( reltuples / ( (bs-page_hdr)*fillfactor/(tpl_size*100) ) ) + ceil( toasttuples / 4 ) AS est_tblpages_ff,
    tblpages, fillfactor, bs, tblid, schemaname, tblname, heappages, toastpages, is_na
  FROM (
    SELECT
      ( 4 + tpl_hdr_size + tpl_data_size + (2*ma)
        - CASE WHEN tpl_hdr_size%ma = 0 THEN ma ELSE tpl_hdr_size%ma END
        - CASE WHEN ceil(tpl_data_size)::int%ma = 0 THEN ma ELSE ceil(tpl_data_size)::int%ma END
      ) AS tpl_size, bs - page_hdr AS size_per_block, (heappages + toastpages) AS tblpages, heappages,
      toastpages, reltuples, toasttuples, bs, page_hdr, tblid, schemaname, tblname, fillfactor, is_na
    FROM (
      SELECT
        tbl.oid AS tblid, ns.nspname AS schemaname, tbl.relname AS tblname, tbl.reltuples,
        tbl.relpages AS heappages, coalesce(toast.relpages, 0) AS toastpages,
        coalesce(toast.reltuples, 0) AS toasttuples,
        coalesce(substring(
          array_to_string(tbl.reloptions, ' ')
          FROM 'fillfactor=([0-9]+)')::smallint, 100) AS fillfactor,
        current_setting('block_size')::numeric AS bs,
        CASE WHEN version()~'mingw32' OR version()~'64-bit|x86_64|ppc64|ia64|amd64' THEN 8 ELSE 4 END AS ma,
        24 AS page_hdr,
        23 + CASE WHEN MAX(coalesce(s.null_frac,0)) > 0 THEN ( 7 + count(s.attname) ) / 8 ELSE 0::int END
           + CASE WHEN bool_or(att.attname = 'oid' and att.attnum < 0) THEN 4 ELSE 0 END AS tpl_hdr_size,
        sum( (1-coalesce(s.null_frac, 0)) * coalesce(s.avg_width, 0) ) AS tpl_data_size,
        bool_or(att.atttypid = 'pg_catalog.name'::regtype)
          OR sum(CASE WHEN att.attnum > 0 THEN 1 ELSE 0 END) <> count(s.attname) AS is_na
      FROM pg_attribute AS att
        JOIN pg_class AS tbl ON att.attrelid = tbl.oid
        JOIN pg_namespace AS ns ON ns.oid = tbl.relnamespace
        LEFT JOIN pg_stats AS s ON s.schemaname=ns.nspname
          AND s.tablename = tbl.relname AND s.inherited=false AND s.attname=att.attname
        LEFT JOIN pg_class AS toast ON tbl.reltoastrelid = toast.oid
      WHERE NOT att.attisdropped
        AND tbl.relkind in ('r','m')
      GROUP BY 1,2,3,4,5,6,7,8,9,10
    ) AS s
  ) AS s2
) AS s3
)
select * from table_bloat_query  where extra_size>100*1024*1024 or bloat_size>100*1024*1024
UNION
select * from table_bloat_query where extra_ratio>10 or bloat_ratio>10
UNION
select * from table_bloat_query where table_oid in
(select oid from pg_class where relkind='r' order by relpages DESC limit 50)
UNION
select * from table_bloat_query where table_oid in
(select relid from pg_stat_user_tables order by (idx_scan+seq_scan) DESC limit 50)
UNION
select * from table_bloat_query where table_oid in
(select relid from pg_stat_user_tables order by n_live_tup DESC limit 50)
UNION
select * from table_bloat_query where table_oid in
(select relid from pg_stat_user_tables order by n_dead_tup DESC limit 50)
UNION
select * from table_bloat_query where table_oid in
(select relid from pg_stat_user_tables order by (n_tup_upd+n_tup_del+n_tup_ins) DESC limit 50)
;
 
                1.10.1.1.3
            1.10.1.2 Chatgpt q explain autovacuum_vacuum_scale_factor in pg db, tags: Chatgpt q explain autovacuum_vacuum_scale_factor in pg db

a. In PostgreSQL, the autovacuum_vacuum_scale_factor is a configuration parameter that determines when the autovacuum process should start a VACUUM operation on a table.

When a table has a high rate of update, insert, or delete operations, the table's data and index pages may become fragmented, which can lead to performance degradation. To prevent this, PostgreSQL's autovacuum process periodically cleans up the fragmented pages in the table to reclaim space and improve performance.

The autovacuum_vacuum_scale_factor parameter specifies the fraction of the table's total pages that must become "dead" (unused) before the autovacuum process will start a VACUUM operation on the table. The default value of autovacuum_vacuum_scale_factor is 0.2, which means that autovacuum will start a VACUUM operation on a table when at least 20% of the table's pages are dead.

You can adjust the value of autovacuum_vacuum_scale_factor to better match the needs of your application. A higher value means that autovacuum will start VACUUM operations less frequently, which can reduce the overhead on the database server, but it can also increase the risk of fragmentation and performance degradation. Conversely, a lower value means that autovacuum will start VACUUM operations more frequently, which can help keep the table's data and indexes organized and improve performance, but it can also increase the overhead on the database server.

            1.10.1.3 q. what are the downsides of setting autovacuum_vacuum_scale_factor to 0.02


2 / 2

Setting the autovacuum_vacuum_scale_factor to a lower value such as 0.02 can trigger VACUUM operations more frequently, which can be beneficial for very active or large tables with high rates of updates, inserts, and deletes. However, there are also potential downsides to consider when setting this parameter to such a low value:

Increased overhead: Frequent VACUUM operations can add a significant overhead to the database server, especially for very large tables. Each VACUUM operation requires exclusive access to the table being vacuumed, which can cause delays and contention for other queries trying to access the table.

Increased disk usage: When a VACUUM operation is executed, it may create new pages on disk to store data that was previously fragmented across multiple pages. If VACUUM is executed too frequently, it can lead to an increase in disk usage and disk I/O operations.

Reduced performance: While VACUUM is running, it can consume a significant amount of system resources, such as CPU, memory, and disk I/O. If the server is already under heavy load, frequent VACUUM operations can further reduce the performance of the database.

Potential for table bloating: If the autovacuum_vacuum_scale_factor is set too low, VACUUM operations may run unnecessarily on tables that don't require it, leading to table bloating and an increase in disk usage.

In summary, while setting the autovacuum_vacuum_scale_factor to a low value can be beneficial for some tables, it's important to balance the benefits of frequent VACUUM operations with the potential drawbacks such as increased overhead, disk usage, and reduced performance. The optimal value for this parameter will depend on the specific workload and characteristics of the database.

            1.10.1.4 
        1.10.2

    1.11 async queries, tags: pg, postgresql async queries

        1.11.1 pg run queries async  
To run queries asynchronously in PostgreSQL using Java, you can use the PostgreSQL JDBC driver version 4.2 or higher, which includes support for the Java 8 CompletableFuture API.

Here's an example of how you can use CompletableFuture to run a query asynchronously in PostgreSQL:

java
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.Executor;
import java.util.concurrent.Executors;
import java.sql.*;

public class AsyncQueryRunner {
    private final String url = "jdbc:postgresql://localhost:5432/mydatabase";
    private final String user = "myuser";
    private final String password = "mypassword";
    private final Executor executor = Executors.newFixedThreadPool(10);

    public CompletableFuture<ResultSet> executeQueryAsync(String sql) {
        return CompletableFuture.supplyAsync(() -> {
            try (Connection conn = DriverManager.getConnection(url, user, password)) {
                Statement stmt = conn.createStatement();
                return stmt.executeQuery(sql);
            } catch (SQLException e) {
                throw new RuntimeException(e);
            }
        }, executor);
    }
}
In this example, we create a new instance of the AsyncQueryRunner class, which contains a thread pool executor that we can use to execute our queries asynchronously. The executeQueryAsync method takes a SQL query string as input and returns a CompletableFuture that will eventually contain the ResultSet of the query.

To execute the query asynchronously, we use the CompletableFuture.supplyAsync method to create a new CompletableFuture that will execute the query in a separate thread. Inside the lambda expression, we establish a new connection to the database, create a Statement object, and execute the query using the executeQuery method. Finally, we return the ResultSet as the result of the CompletableFuture.

To use this class, you can create an instance of AsyncQueryRunner and call the executeQueryAsync method to run your queries asynchronously:

java
AsyncQueryRunner runner = new AsyncQueryRunner();
CompletableFuture<ResultSet> future = runner.executeQueryAsync("SELECT * FROM mytable");
future.thenAccept(resultSet -> {
    // Process the ResultSet here
});
In this example, we execute a query to select all rows from the mytable table and retrieve the result as a CompletableFuture. We then use the thenAccept method to specify a callback that will be called when the query completes. Inside the callback, we can process the ResultSet as needed.

        1.11.2 is there a PG extension that allows for asyn run of queries  
Yes, PostgreSQL has a built-in extension called pgxn_async that provides support for running queries asynchronously.

The pgxn_async extension provides a set of functions that allow you to execute queries asynchronously and retrieve their results as soon as they become available. It uses PostgreSQL's asynchronous API to send queries to the server and receive their results, allowing you to continue processing other tasks while waiting for the query results.

To use the pgxn_async extension, you need to install it first. You can do this by running the following command:

$ sudo pgxn install pgxn_async
Once the extension is installed, you can use the async function to execute a query asynchronously. Here's an example:

SELECT async('SELECT * FROM mytable');
This will return a unique ID for the asynchronous query that you can use to retrieve its results later.

To retrieve the results of an asynchronous query, you can use the wait_async function, which blocks until the query results become available. Here's an example:

SELECT wait_async('unique_id');
This will return the result of the asynchronous query, or an error message if the query failed.

You can also use the is_async_done function to check if an asynchronous query has completed yet. This function returns true if the query is complete, and false otherwise. Here's an example:

SELECT is_async_done('unique_id');
This will return true if the query has completed, or false if it is still running.

Note that the pgxn_async extension may not be suitable for all use cases, as it can add additional overhead and complexity to your queries. You should evaluate your specific needs and performance requirements before using this extension.

        1.11.3 from java 8 based PG client
Yes, you can use the Java 8-based PG client called PgAsync, which provides an asynchronous API for interacting with a PostgreSQL database.

PgAsync uses the CompletableFutures API introduced in Java 8 to provide an easy-to-use asynchronous API for running queries and retrieving results. It also supports connection pooling and SSL/TLS encryption for secure communication with the database server.

Here's an example of how you can use PgAsync to connect to a PostgreSQL database and execute a query:

java
import com.github.pgasync.ConnectionPoolBuilder;
import com.github.pgasync.Db;
import com.github.pgasync.ResultSet;

import java.util.concurrent.CompletableFuture;
import java.util.concurrent.ExecutionException;

public class PgAsyncExample {
    public static void main(String[] args) throws ExecutionException, InterruptedException {
        Db db = ConnectionPoolBuilder.builder()
                .hostname("localhost")
                .port(5432)
                .database("mydatabase")
                .username("myuser")
                .password("mypassword")
                .build();

        CompletableFuture<ResultSet> future = db.querySet("SELECT * FROM mytable");

        ResultSet resultSet = future.get();

        // Process the ResultSet here
    }
}
In this example, we create a new Db object using the ConnectionPoolBuilder class, which provides a fluent interface for configuring the connection pool. We specify the hostname, port, database name, username, and password for the connection, and then build the connection pool.

To execute a query asynchronously, we use the querySet method of the Db object, which returns a CompletableFuture<ResultSet> that will eventually contain the result of the query. We can then use the get method of the CompletableFuture to retrieve the ResultSet when it becomes available.

Inside the callback, we can process the ResultSet as needed.

Note that you should make sure to properly handle exceptions that may be thrown when using PgAsync, such as SQLExceptions and InterruptedExceptions. You should also close the Db object and the associated connection pool when you're done using it.

        1.11.4 how to parse PG resuletSet in java
To parse a PostgreSQL ResultSet in Java using the java.sql.ResultSet class, you can follow these steps:

Execute the query and get the ResultSet object.
Use the next() method of the ResultSet object to move the cursor to the next row and check if there are any more rows.
Use the various get methods of the ResultSet object to retrieve the values of the columns for the current row. These methods take the index of the column or the name of the column as a parameter, and return the value of the column as the appropriate Java data type.
Process the retrieved data as needed.
Here's an example of how to parse a PostgreSQL ResultSet in Java:

java
import java.sql.*;

public class PgResultSetExample {
    public static void main(String[] args) throws SQLException {
        String url = "jdbc:postgresql://localhost/mydatabase";
        String user = "myuser";
        String password = "mypassword";
        Connection connection = DriverManager.getConnection(url, user, password);

        String query = "SELECT name, age, email FROM users";
        Statement statement = connection.createStatement();
        ResultSet resultSet = statement.executeQuery(query);

        while (resultSet.next()) {
            String name = resultSet.getString("name");
            int age = resultSet.getInt("age");
            String email = resultSet.getString("email");

            // Process the retrieved data here
        }

        statement.close();
        connection.close();
    }
}
In this example, we first establish a connection to the PostgreSQL database using the DriverManager.getConnection method. We then create a Statement object using the createStatement method of the Connection object, and execute the query using the executeQuery method of the Statement object.

Inside the while loop, we use the various get methods of the ResultSet object to retrieve the values of the columns for the current row. In this example, we retrieve the values of the name, age, and email columns, which are of type String, int, and String, respectively.

Note that you should always close the Statement, ResultSet, and Connection objects when you're done using them to avoid resource leaks.

        1.11.5
    1.12
2. courses

    2.1 Udemy SQL, No SQL, Big Data & Hadoop
course materials: https://github.com/okmich/rdbms_2_nosql
/Users/i500695/work/courses/sql_nosql_bigdata
dataset: movielens dataset


        2.1.1  relational DBs

            2.1.1.1   intro
relations, aka tables
records, aka rows
Primary key - identify row
Fields/Columnds - propeties
Data types - CHAR, INT etc
constraints - NULL, NOT NULL, UNIQUE
Transactions aka ACID
Index, usually tree data structure for fater access to records
ERD - Entity Relations Diagram 
RDBMS - set of tools and proceses to manage & monitor DB

SQL - declarative language.
made of:
- DDL, Data Defenition language, create alter drop
- DML, Data manipulation language, insert update delete select
- Access control - Grant Revoke
- Procedural language extensions 

Data modeling steps:
a. identify main actors
b. Create tables for actors
c. Actions for actors
d. Create tables for actions
e. Actors - identify non trivial properties. like customer payment method. They require foreign key to a dedicated table
many
f. Actions - identify non trivial properties. like customer payment method. They require foreign key to a dedicated table
g. optimizations. read/write
normalization / denormalization

Data modeling steps:
a. identify main actors. 
Users: id, age, gender, zip code
Movies: id, title, release

b. Create tables for actors

c. Actions for actors
user rate
user tag
d. Create tables for actions
ratings: user, rate, movie, id is composite (userid+movieid)
user tag: user, tag, movie , id is composite (userid+movieid)


e. Actors - identify non trivial properties. like customer payment method. They require foreign key to a dedicated table
movie generes. we could use a list but perormance is not good
instead add table:
genre_movies: genre, movie. uniqueness constraint 
To get movie genres join the movie and genre_movies tables

f. Actions - identify non trivial properties. like customer payment method. They require foreign key to a dedicated table
g. optimizations. read/write
normalization / denormalization

practice. model movie lens DB.

When modeling DB consider how it will be used. For transactions (OLTP) or for data analysis. 

normalization. 
a. genre_movies. movie id (foreign-key), genre
consider genre is free text. there's room for errors. like "action", "Action", "ation" etc.
we could use another helper table genre. id, genre.
like:
1 Action
2 SciFi

pros. a. eliminate errors. b. reduce redundancy c. table optimization
cons. a. less readable, b. require joins
So, normalization works better for transactional DBs and less for data analytics
When modeling for data analytics denormalization is prefered.

b. users 
age can use age group table
1, <18
2, 18-24
3, 24-35

same for occupation etc.

            2.1.1.2 mysql
open source c-c++ DB maintained by Oracle.
Defailt storage engine InnoDB

                2.1.1.2.1 mysql docker
 $ docker pull mysql
 $ docker images
 $ docker run --name yosi-mysql -e MYSQL_ROOT_PASSWORD=abcd1234 -d mysql:latest
 $ docker ps

                2.1.1.2.2 client:
$ brew install mysql

                2.1.1.2.3 Connect to docker db
[i500695@C02X632CJGH6:2020-11-02 17:56:58:~/work/courses/sql_nosql_bigdata:]2035$ docker exec -it yosi-mysql mysql -uroot -p
Enter password: 
Welcome to the MySQL monitor.  Commands end with ; or \g.
Your MySQL connection id is 8
Server version: 8.0.22 MySQL Community Server - GPL

Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

mysql> 

- to change the root password.
mysql> ALTER USER 'root'@'localhost' IDENTIFIED BY '[newpassword]'; 

                2.1.1.2.4 full guide https://phoenixnap.com/kb/mysql-docker-container
           Running a MySQL Docker Container
If you need to set up a database quickly and without using up too many resources, deploying MySQL in a container is a fast and efficient solution. This is only appropriate for small and medium-sized applications. Enterprise-level applications would not find a MySQL Docker container sufficient for their workload.

Using the Docker software for setting up your database is becoming increasingly popular for small-scale apps. Instead of having a separate server for database hosting, you can deploy a MySQL database container.

Multiple containers can run on your computer. The containers share the same kernel and libraries of the host while packaging the deployed application or software into single units. This makes the database extraordinarily lightweight and fast to spin up.

Installing a MySQL Docker Container
Setting up a database in Docker is simply building a container based on a MySQL image. Follow the steps outlined below to get your MySQL container up and running.

Note: This tutorial assumes you already have Docker on your system. If you don't have the software, take a look at one of our articles on how to install Docker on CentOS, installing Docker on Ubuntu, or Docker guides for other operating systems.

                    2.1.1.2.4.1 Step 1: Pull the MySQL Docker Image
a. Start by pulling the appropriate Docker image for MySQL. You can download a specific version or opt for the latest release as seen in the following command:

docker pull mysql/mysql-server:latest
If you want a particular version of MySQL, replace latest with the version number.

b. Verify the image is now stored locally by listing the downloaded Docker images:

docker images
The output should include mysql/mysql-server among the listed images.

example Docker image on your system for deploying a MySQL Docker container

                    2.1.1.2.4.2 Step 2: Deploy the MySQL Container
a. Once you have the image, move on to deploying a new MySQL container with:

docker run --name=[container_name] -d mysql/mysql-server:latest
Replace [container_name] with the name of your choice. If you do not provide a name, Docker generates a random one.
The -d option instructs Docker to run the container as a service in the background.
In the command above, we used the latest version tag. This may differ according to the image you downloaded.
b. Then, check to see if the MySQL container is running:

docker ps
You should see the newly created container listed in the output. It includes container details, one being the status of this virtual environment. The status changes from health: starting to healthy, once the setup is complete.

Command to deploy a MySQL Docker container and verify MySQL container is running

                    2.1.1.2.4.3 Step 3: Connect to the MySQL Docker Container
a. Before you can connect the MySQL server container with the host, you need to make sure the MySQL client package is installed:

apt-get install mysql-client
b. Then, start a MySQL client inside the container by typing:

docker exec -it [container_name] mysql -uroot -p
c. Provide the root password, when prompted. With that, you have connected the MySQL client to the server.

d. Finally, change the server root password to protect your information:

mysql> ALTER USER 'root'@'localhost' IDENTIFIED BY '[newpassword]';
Replace [newpassword] with a strong password of your choice.

                    2.1.1.2.4.4 Configure MySQL Container
When you install a MySQL container, you will find its configuration options in the /etc/mysql/my.cnf directory.

If you need to modify the configuration, create an alternative config file on the host machine and mount them inside the container.

a. First, create a new directory on the host machine:

mkdir -p /root/docker/[container_name]/conf.d
b. Create a custom MySQL config file inside that directory:

nano /root/docker/[container_name]/conf.d/my-custom.cnf
c. Once in the file, you can add lines with the desired configuration.

For example, if you want to increase the maximum number of connections to 250 (instead of the default 151), add the following lines to the configuration file:

[mysqld]
max_connections=250
File changing the configuration of a MySQL container by expanding the maximum number of connections.
d. Save and exit the file.

e. For the changes to take place, you need to remove and rerun the MySQL container. This time, the container uses a combination of configuration settings from the newly created file and the default config files.

To do this, run the container and map the volume path with the command:

docker run \
--detach \
--name=[container_name]\
--env="MYSQL_ROOT_PASSWORD=[my_password]" \
--publish 6603:3306 \
--volume=/root/docker/[container_name]/conf.d:/etc/mysql/conf.d \
mysql
f. To check whether the container loaded the configuration from the host, run the following command:

mysql -uroot -pmypassword -h127.0.0.1 -P6603 -e 'show global variables like "max_connections"';
You should see that the maximum number of connections is now 250.

                    2.1.1.2.4.5 Manage Data Storage
By default, Docker stores data in its internal volume.

To check the location of the volumes, use the command:

docker inspect [container_name]
You will see the /var/lib/mysql mounted in the internal volume.

List details of MySQL Docker container and data storage location
You can also change the location of the data directory and create one on the host. Having a volume outside the container allows other applications and tools to access the volumes when needed.

a. First, find an appropriate volume on the host and create a data directory on it:

mkdir -p /storage/docker/mysql-data
b. Now start the container again, mounting the previously made directory:

docker run \
--detach \
--name=[container_name] \
--env="MYSQL_ROOT_PASSWORD=my_password" \
--publish 6603:3306 \
--volume=/root/docker/[container_name]/conf.d:/etc/mysql/conf.d \
--volume=/storage/docker/mysql-data:/var/lib/mysql \
mysql
If you inspect the container, you should see that the MySQL container now stores its data on the host system. Run the command:

docker inspect [container_name]
Start, Stop, and Restart MySQL Container
The container automatically stops when the process running in it stops.

To start the MySQL container run:

docker start [container_name]
Stop the MySQL container, use the command:

docker stop [container_name]
To restart the MySQL container run:

docker restart [container_name]
Delete MySQL Container
Before deleting a MySQL container, make sure you stop it first.

Then, remove the docker container with:

docker rm [container_name] 

                    2.1.1.2.4.6
                2.1.1.2.5 load movielens sql dump
a. mount host volume to container
delete container.
[i500695@C02X632CJGH6:2020-11-02 18:17:30:~/work/courses/sql_nosql_bigdata:]2040$ docker stop ebb6e87d3382
ebb6e87d3382
[i500695@C02X632CJGH6:2020-11-02 18:17:41:~/work/courses/sql_nosql_bigdata:]2041$ docker rm ebb6e87d3382
ebb6e87d3382

rerun w/ mount to sql dump of DB
[i500695@C02X632CJGH6:2020-11-02 18:12:22:~/work/courses/sql_nosql_bigdata/rdbms_2_nosql/datasets/movielens:]2012$ unzip movielens-mysql-dump.zip -d /Users/i500695/Downloads/
Archive:  movielens-mysql-dump.zip
  inflating: /Users/i500695/Downloads/movielens-mysql-dump.sql  

[i500695@C02X632CJGH6:2020-11-02 18:21:53:~/work/courses/sql_nosql_bigdata:]2051$ docker run --name yosi-mysql -v/Users/i500695/Downloads/:/downloads -e MYSQL_ROOT_PASSWORD=abcd1234 -d mysql:latest 
docker: Error response from daemon: Conflict. The container name "/yosi-mysql" is already in use by container "323b207526047a39774d6f8a2c444798f2b6a75322f38b3dd4d2c6dc7cdc9299". You have to remove (or rename) that container to be able to reuse that name.
See 'docker run --help'.
[i500695@C02X632CJGH6:2020-11-02 18:22:09:~/work/courses/sql_nosql_bigdata:]2052$ docker rm 323b207526047a39774d6f8a2c444798f2b6a75322f38b3dd4d2c6dc7cdc9299
323b207526047a39774d6f8a2c444798f2b6a75322f38b3dd4d2c6dc7cdc9299
[i500695@C02X632CJGH6:2020-11-02 18:22:25:~/work/courses/sql_nosql_bigdata:]2053$ docker run --name yosi-mysql -v/Users/i500695/Downloads/:/downloads -e MYSQL_ROOT_PASSWORD=abcd1234 -d mysql:latest 
1af403d0ae3e6c76cda85b0ff49bd797b68a99e1254718b377cc190dad172b5b

b. load dump
$ docker exec -it yosi-mysql mysql -uroot -p
> source movielens-mysql-dump.sql 

                2.1.1.2.6 OLTP vs OLAP
OLTP - Online Transactions Processing
CRUD, transactions, performant R/W, 
transactions.
start: START TRANSACTION;
run multiple write operations on multiple tables as one atomic operation
end by COMMIT; 
or to unde ROLLBACK;

paginated query
select * from table limit n, n+100;


ERPs, Mgmt, CRM
return small/paginated amount of data

OLAP - Onlin Analytical Processing

OLTP sql example:
[i500695@C02X632CJGH6:2020-11-03 16:03:47:~/work/courses/sql_nosql_bigdata/rdbms_2_nosql:]2020$ cat !$
cat mysql/transactional_query.sql
use movielens;

-- Signing up a new user
select * from age_group;
select * from occupations where id = 3;
insert into users(age, gender, occupation_id, zip_code) values(35, 'F', 3, '00000'); -- 6041

-- Adding a new movie
START TRANSACTION;
insert into movies(title, release_year) values('Toy story 4', 2020); 

-- find the just inserted movies
select LAST_INSERT_ID(); -- 3953

select * from genres;

-- choose Animation, Adventure, Comedy 
insert into genres_movies(movie_id, genre_id) values(3953, 4);
insert into genres_movies(movie_id, genre_id) values(3953, 2);
insert into genres_movies values(null, 3953, 8);

COMMIT;

-- Updating movie information
update movies set title='Toy Story 4' where id=3953;

-- Delete a movie
delete from movies where id=3953;
-- error: ERROR 1451 (23000): Cannot delete or update a parent row: a foreign key constraint fails (`movielens`.`genres_movies`, CONSTRAINT `genre_movies` FOREIGN KEY (`movie_id`) REFERENCES `movies` (`id`))
-- can't delete this movie row b/c it has a foreign-key relation to genre_movies table. see:


-- get the cause of the foreign constraint violation message
select * from genres_movies where movie_id = 3953;
-- +------+----------+----------+
-- | id   | movie_id | genre_id |
-- +------+----------+----------+
-- | 6409 |     3953 |        4 |
-- | 6410 |     3953 |        2 |
-- | 6411 |     3953 |        8 |
-- +------+----------+----------+
-- 3 rows in set (0.00 sec)

START TRANSACTION;

delete from genres_movies where movie_id=3953;
delete from movies where id=3953;

ROLLBACK;

-- Paginated query of movie
SELECT * from movies order by title desc limit 1, 100;
SELECT * from movies limit 101, 200;

-- sort
SELECT * from movies order by title limit 0, 50;

-- descending
SELECT * from movies order by title desc limit 0, 50;

                2.1.1.2.7 Data Processing
Either in application space (Django, spring etc). Good for small amounts of data
or in DB space.  Good for big amounts of data
                2.1.1.2.8
            2.1.1.3 indexes
Avoid full table scans. faster reads.
some queries can be answered by the index alone. like how many time a certain record appears.
have impact on write operations. since update data -> update index.

                2.1.1.3.1 indexes sql


[i500695@C02X632CJGH6:2020-11-04 16:05:41:~/work/courses/sql_nosql_bigdata/rdbms_2_nosql:]2025$ cat mysql/indexes.sql
use movielens;

-- explain statement
explain select * from ratings where user_id=20;
-- +----+-------------+---------+------------+------+----------------+----------------+---------+-------+------+----------+-------+
-- | id | select_type | table   | partitions | type | possible_keys  | key            | key_len | ref   | rows | filtered | Extra |
-- +----+-------------+---------+------------+------+----------------+----------------+---------+-------+------+----------+-------+
-- |  1 | SIMPLE      | ratings | NULL       | ref  | rating_user_fk | rating_user_fk | 5       | const |   24 |   100.00 | NULL  |
-- +----+-------------+---------+------------+------+----------------+----------------+---------+-------+------+----------+-------+
--                                                   means there's an index: rating_user_fk of length 5
-- 1 row in set, 1 warning (0.00 sec)

explain select * from movies where release_year = 1990;
-- +----+-------------+--------+------------+------+---------------+------+---------+------+------+----------+-------------+
-- | id | select_type | table  | partitions | type | possible_keys | key  | key_len | ref  | rows | filtered | Extra       |
-- +----+-------------+--------+------------+------+---------------+------+---------+------+------+----------+-------------+
-- |  1 | SIMPLE      | movies | NULL       | ALL  | NULL          | NULL | NULL    | NULL | 3884 |    10.00 | Using where |
-- +----+-------------+--------+------------+------+---------------+------+---------+------+------+----------+-------------+
-- note:                                     all, no ref, no key, all rows were scanned -- 1 row in set, 1 warning (0.00 sec)

-- show indexes 
show indexes from ratings; -- show index from ratings will also work
-- +---------+------------+------------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+---------+------------+
-- | Table   | Non_unique | Key_name         | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment | Visible | Expression |
-- +---------+------------+------------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+---------+------------+
-- | ratings |          0 | PRIMARY          |            1 | id          | A         |      997875 |     NULL |   NULL |      | BTREE      |         |               | YES     | NULL       |
-- | ratings |          1 | rating_user_fk   |            1 | user_id     | A         |        6808 |     NULL |   NULL | YES  | BTREE      |         |               | YES     | NULL       |
-- | ratings |          1 | ratings_movie_fk |            1 | movie_id    | A         |        3039 |     NULL |   NULL | YES  | BTREE      |         |               | YES     | NULL       |
-- +---------+------------+------------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+---------+------------+
-- 3 rows in set (0.12 sec)
-- Cardinality, # of values
show keys from ratings;

-- create a table without index
create table ratings_without_index as select * from ratings;
-- Query OK, 1000209 rows affected (8.97 sec)
-- Records: 1000209  Duplicates: 0  Warnings: 0

show keys from ratings_without_index;
-- Empty set (0.00 sec)

--explain on the new table
explain select * from ratings_without_index where user_id = 20;
-- +----+-------------+-----------------------+------------+------+---------------+------+---------+------+--------+----------+-------------+
-- | id | select_type | table                 | partitions | type | possible_keys | key  | key_len | ref  | rows   | filtered | Extra       |
-- +----+-------------+-----------------------+------------+------+---------------+------+---------+------+--------+----------+-------------+
-- |  1 | SIMPLE      | ratings_without_index | NULL       | ALL  | NULL          | NULL | NULL    | NULL | 997560 |    10.00 | Using where |
-- +----+-------------+-----------------------+------------+------+---------------+------+---------+------+--------+----------+-------------+
-- 1 row in set, 1 warning (0.00 sec)

-- let do more complex queries
SELECT g.name genre, m.release_year, count(1) rating_count, sum(r.rating) total_rating FROM ratings r JOIN movies m ON m.id = r.movie_id JOIN genres_movies gm ON gm.movie_id = m.id JOIN genres g ON g.id = gm.genre_id GROUP BY g.name, m.release_year;
SELECT g.name genre, m.release_year, count(1) rating_count, sum(r.rating) total_rating FROM ratings r JOIN movies m ON m.id = r.movie_id JOIN genres_movies gm ON gm.movie_id = m.id JOIN genres g ON g.id = gm.genre_id GROUP BY g.name, m.release_year order by rating_count desc limit 0,50;
-- +----------+--------------+--------------+--------------+
-- | genre    | release_year | rating_count | total_rating |
-- +----------+--------------+--------------+--------------+
-- | Comedy   | 1999         |        36869 |       129465 |
-- | Drama    | 1999         |        29602 |       107481 |
-- | Drama    | 1995         |        26390 |        95976 |
-- | Drama    | 1997         |        25287 |        91922 |
-- | Comedy   | 1998         |        24911 |        86780 |


explain SELECT g.name genre,
       m.release_year,
       count(1) rating_count,
       sum(r.rating) total_rating
FROM ratings r
JOIN movies m ON m.id = r.movie_id
JOIN genres_movies gm ON gm.movie_id = m.id
JOIN genres g ON g.id = gm.genre_id
GROUP BY g.name,
         m.release_year;

-- primary key has not null + unique constraing
-- unique index, like primary key but can be null
-- indexes can also be non unique
-- there geo indexes for map DBs 
-- full text search indexes
CREATE unique index genre_name_indx on genre(name);

-- full text index
explain select m.title, t.tags from tags t join movies m on m.id = t.movie_id 
-- where match (t.tags) against('super heroes');
where t.tags = 'super heroes';

-- create a full text index
create fulltext index tag_tag_ft_indx on tags(tags);

-- now use the same full text search function. It should work now.
select * from tags t join movies m on m.id = t.movie_id 
where match (t.tags) against('super hero');

--lets explain the full text query
explain select * from tags t join movies m on m.id = t.movie_id 
where match (t.tags) against('super hero');

-- show indexs on the tag table
show keys from tags;

-- drop indexes 
drop index movie_release_year_indx on movies;

                2.1.1.3.2
            2.1.1.4 OLAP 
used for storing huge amounts of data. For research, analytics BI, etc. 
infrequent writes.
huge reads.
not transactional
prefer de-normalization

data source comes from OLTP DBs (CRM, ERP, Billing etc) --ETL--> Data Werehousing --> OLAP DBs
ETL - extraction, transformation, loading

Why not just use OLTP DBs??
OLTP vs OLAP has some contention points.
a. query complexity. OLTP, simple, short. OLAP, complex
b. history. OLTP, none to basic. OLAP. keep history for analytics
c. big data. OLAP
d. multi data source integration. OLAP requirement. ETL

                2.1.1.4.1 sql to create Data-Werehousing DB 


[i500695@C02X632CJGH6:2020-11-04 18:13:47:~/work/courses/sql_nosql_bigdata/rdbms_2_nosql:]2028$ cat mysql/movielen_dw.sql
create database movielens_dw;
use movielens_dw;

-- create the procedures to load the tables from the movielens schema
DROP PROCEDURE IF EXISTS load_date_dim;
DELIMITER $
CREATE PROCEDURE load_date_dim (start_date DATE, stop_date DATE)
BEGIN
    DECLARE last_date date;
    DECLARE i_date date;
    
    DELETE FROM dim_date;

    SET i_date = start_date;
    WHILE i_date <= stop_date DO
        INSERT INTO dim_date (`date`, day_of_week, dayname, is_weekend, month, monthname, day_of_month, quarter, `year`, year_quarter) 
            VALUES (
                i_date,
                dayofweek(i_date),
                dayname(i_date),
                weekday(i_date) = 5 or weekday(i_date) = 6,
                month(i_date),
                monthname(i_date),
                dayofmonth(i_date),
                concat('Q', quarter(i_date)),
                year(i_date),
                concat(year(i_date), '_Q', quarter(i_date))
            );

        SET i_date = DATE_ADD(i_date, INTERVAL 1 DAY);
    END WHILE;
END $
DELIMITER ;


--- create a function to get the time of day
DROP FUNCTION IF EXISTS get_time_of_day;
DELIMITER $$
CREATE FUNCTION get_time_of_day(t_day TIME) RETURNS char(10) DETERMINISTIC
BEGIN
  DECLARE time_of_day CHAR(10);
  DECLARE vtime INT;
    
  SET vtime = hour(t_day);
    
  IF vtime = 0 THEN
    SET time_of_day = 'Mid-Night';
  ELSEIF (vtime > 0 AND vtime < 6) THEN 
    SET time_of_day ='Night';
  ELSEIF (vtime > 6 AND vtime < 12) THEN 
    SET time_of_day ='Morning';
  ELSEIF (vtime = 12) THEN 
    SET time_of_day ='Noon';
  ELSEIF (vtime > 12 AND vtime < 16) THEN 
    SET time_of_day ='Afternoon';
  ELSEIF (vtime > 16 AND vtime < 20) THEN 
    SET time_of_day ='Evening';
  ELSE
    SET time_of_day ='Late Night';
  END IF;
    
  RETURN time_of_day;
END $$
DELIMITER ;

--- create the procedures to load the time dimension 
DROP PROCEDURE IF EXISTS load_time_dim;
DELIMITER $
CREATE PROCEDURE load_time_dim ()
BEGIN
    DECLARE last_time time;
    DECLARE start_time time;
    DECLARE i_t time;
    
    DELETE FROM dim_time;

    SET last_time = '23:59:59';
    SET i_t = '00:00:00';
    WHILE i_t <= last_time DO
        INSERT INTO dim_time (full_time, hour, hour24, minute, ampm, timeofdayname) 
            VALUES (
                i_t,
                TIME_FORMAT(i_t, '%h'),
                TIME_FORMAT(i_t, '%H'),
                minute(i_t),
                TIME_FORMAT(i_t, '%p'),
                get_time_of_day(i_t)
            );

        SET i_t = DATE_ADD(i_t, INTERVAL 1 MINUTE);
    END WHILE;
END $
DELIMITER ;




DROP TABLE IF EXISTS dim_date;
CREATE TABLE dim_date (
  date_pk         bigint primary key auto_increment, 
  date            date not null,
  day_of_week     char(10) not null,
  dayname         char(12) not null,
  is_weekend      boolean not null default false,
  month           tinyint not null,
  monthname       char(10) not null,
  day_of_month    tinyint not null, 
  quarter         char(2) not null,
  year            int not null,
  year_quarter    char(7) not null,
  unique key dim_date_date_idx (date)
);
-- call procedure
call load_date_dim ('2000-01-01', '2010-12-31');

DROP TABLE IF EXISTS dim_time;
CREATE TABLE dim_time (
  time_pk         bigint primary key auto_increment, 
  full_time       time not null,
  hour            char(2) not null,
  hour24          char(2) not null,
  minute          char(2) not null,
  ampm            char(2) not null, 
  timeofdayname   varchar(10) not null comment 'morning, noon, afternoon, evening, midnight, night',
  unique key dim_time_time_unique_idx (full_time)
);

-- load data 
call load_time_dim();

drop table if exists dim_movie;
create table dim_movie (
  movie_pk        bigint primary key,  -- map to movie_id
  title           varchar(100) not null,
  release_year    char(4),
  insert_date     datetime
);

-- load dim_movie
INSERT INTO dim_movie (movie_pk, title,release_year, insert_date)
SELECT id,
       title,
       release_year,
       curdate()
FROM movielens.movies;



drop table if exists dim_movie_genre;
create table dim_movie_genre (
  movie_genre_pk  bigint primary key auto_increment, 
  movie_pk        bigint not null,
  movie_genre_id  bigint not null,
  genre_id        bigint not null,
  genre_name      varchar(20) not null,
  insert_date     datetime,
  unique key dim_movie_genre_unique_idx (movie_pk, genre_id),
  foreign key dim_movie_genre_dim_movie_fk (movie_pk) REFERENCES dim_movie (movie_pk)
);

-- load dim_movie
INSERT INTO dim_movie_genre (movie_pk, movie_genre_id, genre_id, genre_name, insert_date)
SELECT gm.movie_id,
       gm.id AS movie_genre_id,
       g.id AS genre_id,
       g.name,
       curdate()
FROM movielens.genres_movies gm
JOIN movielens.genres g ON g.id = gm.genre_id;


drop table if exists dim_user;
create table dim_user (
  user_pk         bigint primary key auto_increment, 
  user_id         bigint not null,
  age_group_id    bigint not null,
  age_group       char(15),
  occupation_id   bigint not null,
  occupation      char(20),
  gender          char(6),
  zip_code        char(12),
  insert_date     datetime
);

-- load dim_movie
INSERT INTO dim_user (user_id, age_group_id, age_group, occupation_id, occupation, gender, zip_code, insert_date)
SELECT u.id,
       u.age,
       ag.name AS age_group,
       u.occupation_id,
       oc.name AS occuption,
       CASE u.gender
           WHEN 'F' THEN 'Female'
           WHEN 'M' THEN 'Male'
           ELSE 'na'
       END,
       u.zip_code,
       curdate()
FROM movielens.users u
LEFT JOIN movielens.age_group ag ON ag.id = u.age
LEFT JOIN movielens.occupations oc ON oc.id = u.occupation_id;


-- fact_rating
drop table if exists fact_rating;
create table fact_rating (
  rating_pk       bigint primary key auto_increment, 
  movie_pk        bigint not null,
  user_pk         bigint not null,
  date_pk         bigint not null,
  time_pk         bigint not null,
  rating          float,
  insert_date     datetime,
  foreign key fact_rating_dim_date_fk (date_pk) REFERENCES dim_date (date_pk),
  foreign key fact_rating_dim_time_fk (time_pk) REFERENCES dim_time (time_pk),
  foreign key fact_rating_dim_movie_fk (movie_pk) REFERENCES dim_movie (movie_pk),
  foreign key fact_rating_dim_user_fk (user_pk) REFERENCES dim_user (user_pk)
);


INSERT INTO fact_rating (rating_pk, movie_pk, user_pk, date_pk, time_pk, rating, insert_date)
SELECT r.id,
       m.movie_pk,
       u.user_pk,
       d.date_pk,
       t.time_pk,
       r.rating,
       curdate()
FROM movielens.ratings r
JOIN movielens_dw.dim_movie m ON m.movie_pk = r.movie_id
JOIN movielens_dw.dim_user u ON u.user_pk = r.user_id
JOIN movielens_dw.dim_date d ON DATE(d.date) = DATE(r.rated_at)
JOIN movielens_dw.dim_time t ON t.full_time = time(DATE_FORMAT(rated_at, '%H:%i:00'));

                2.1.1.4.2 Analytical Processing
                
                    2.1.1.4.2.1 Aggregation Queries. Reduce. e.g. sum, avg, std dev etc

                        2.1.1.4.2.1.1 aggregation_queries_dw.sql 
[i500695@C02X632CJGH6:2020-11-05 10:58:19:~/work/courses/sql_nosql_bigdata/rdbms_2_nosql:]2031$ cat mysql/aggregation_queries_dw.sql | pbcopy 
use movielens_dw;

-- genre movie counting for the year 2000
SELECT gm.genre_name,
       count(1)
FROM dim_movie_genre gm
JOIN dim_movie m ON m.movie_pk = gm.movie_pk
WHERE release_year = '2000'
GROUP BY gm.genre_name;


-- total rating summary for each movie
SELECT m.title,
       m.release_year,
       count(f.rating) rating_count,
       sum(f.rating) total_rating,
       avg(f.rating) avg_rating,
       variance(f.rating)
FROM fact_rating f
JOIN dim_movie m ON f.movie_pk = m.movie_pk
GROUP BY m.title,
         m.release_year;


-- total rating summary for genres for years
SELECT gm.genre_name,
       count(1) rating_count,
       sum(r.rating) total_rating,
       avg(r.rating) avg_rating,
       variance(r.rating)
FROM fact_rating r
JOIN dim_movie_genre gm ON gm.movie_pk = r.movie_pk
GROUP BY gm.genre_name;


-- number of count for each rating class (1-5) for a certain genre on a monthly basis
SELECT gm.genre_name,
       d.year event_year,
       d.monthname event_month,
       r.rating stars,
       count(1) event_count
FROM fact_rating r
JOIN dim_movie_genre gm ON gm.movie_pk = r.movie_pk
JOIN dim_date d ON d.date_pk = r.date_pk
GROUP BY gm.genre_name,
         d.year,
         d.monthname,
         r.rating;

                    2.1.1.4.2.2 Window Queries
Aggregate values not by reducing multiple rows to one value 
common keyword OVER (default, whole data set) or add PARTITION BY and/or ORDER by

[i500695@C02X632CJGH6:2020-11-05 12:24:39:~/work/courses/sql_nosql_bigdata/rdbms_2_nosql:]2039$ cat mysql/analytical_queries_dw.sql | pbcopy
use movielens_dw;
-- find the percentage of ratings for genres made for each month of a specific year
select v.*, 
    v.rating_counts / SUM(v.rating_counts) OVER (PARTITION BY v.year,v.monthname) AS factor, 
    (v.rating_counts / SUM(v.rating_counts) OVER (PARTITION BY v.year,v.monthname)) * 100 AS percentage
    ,RANK() OVER (PARTITION BY v.year,v.monthname ORDER BY v.rating_counts DESC) AS irank
FROM ( 
    SELECT d.year,
        d.monthname,
        gm.genre_name,
        COUNT(1) rating_counts
    FROM fact_rating f
    JOIN dim_movie_genre gm ON gm.movie_pk = f.movie_pk
    JOIN dim_date d ON d.date_pk = f.date_pk
    WHERE d.year = 2001 and d. month = 1 -- change the year in focus
    GROUP BY d.year,
        d.monthname,
        gm.genre_name
) AS v;
  

-- fetch the top 3 genres for each demographics (gender, occupation) of viewers
WITH my_query as (
    SELECT genre_name, gender, occupation, 
        ROW_NUMBER() OVER(PARTITION BY gender, occupation ORDER BY avg_rating DESC) as position
    FROM
        (SELECT u.gender,
            u.occupation,
            mg.genre_name,
            avg(f.rating) avg_rating
        FROM fact_rating f
        JOIN dim_movie m ON m.movie_pk = f.movie_pk
        JOIN dim_user u ON u.user_pk = f.user_pk
        JOIN dim_movie_genre mg ON mg.movie_pk = f.movie_pk
        GROUP BY u.gender,
            u.occupation,
            mg.genre_name
        HAVING COUNT(1) > 200) v
) 
select * from my_query q
where q.position  <= 3;

                    2.1.1.4.2.3

                2.1.1.4.3
            2.1.1.5 transaction logs
- WAL. write ahead logs. logs of write operations before commit/rollback.
it is a source of truth in case of crash/restart etc.

- replication

-

            2.1.1.6 Analytical queries

                2.1.1.6.1 RANK
https://www.sqlservertutorial.net/sql-server-window-functions/sql-server-rank-function/
Introduction to SQL Server RANK() function
The RANK() function is a window function that assigns a rank to each row within a partition of a result set.

The rows within a partition that have the same values will receive the same rank. The rank of the first row within a partition is one. The RANK() function adds the number of tied rows to the tied rank to calculate the rank of the next row, therefore, the ranks may not be consecutive.

The following shows the syntax of the RANK() function:

RANK() OVER (
    [PARTITION BY partition_expression, ... ]
    ORDER BY sort_expression [ASC | DESC], ...
)
In this syntax:

First, the PARTITION BY clause divides the rows of the result set partitions to which the function is applied.
Second, the ORDER BY clause specifies the logical sort order of the rows in each a partition to which the function is applied.
The RANK() function is useful for top-N and bottom-N reports.

SQL Server RANK() illustration
First, create a new table named sales.rank_demo that has one column:

CREATE TABLE sales.rank_demo (
	v VARCHAR(10)
);
Second, insert some rows into the sales.rank_demo table:

INSERT INTO sales.rank_demo(v)
VALUES('A'),('B'),('B'),('C'),('C'),('D'),('E');
Third, query data from the sales.rank_demo table:

SELECT v FROM sales.rank_demo;
Fourth, use the ROW_NUMBER() to assign ranks to the rows in the result set of sales.rank_demo table:

SELECT
	v,
	RANK () OVER ( 
		ORDER BY v 
	) rank_no 
FROM
	sales.rank_demo;
Here is the output:
v rank_no
A 1
B 2
...

SQL Server RANK Function Example
As shown clearly from the output, the second and third rows receive the same rank because they have the same value B. The fourth and fifth rows get the rank 4 because the RANK() function skips the rank 3 and both of them also have the same values.

SQL Server RANK() function examples
We’ll use the production.products table to demonstrate the RANK() function:

products
Using SQL Server RANK() function over a result set example
The following example uses the RANK() function to assign ranks to the products by their list prices:

SELECT
	product_id,
	product_name,
	list_price,
	RANK () OVER ( 
		ORDER BY list_price DESC
	) price_rank 
FROM
	production.products;
Here is the result set:
product_id, product_name, list_price, price_rank
155, The Domane SLR, 11999, 1
...

SQL Server RANK Function Over Result Set Example
In this example, because we skipped the PARTITION BY clause, the RANK() function treated the whole result set as a single partition.

The RANK() function assigns a rank to each row within the result set sorted by list price from high to low.

Using SQL Server RANK() function over partitions example
This example uses the RANK() function to assign a rank to each product by list price in each brand and returns products with rank less than or equal to three:

SELECT * FROM (
	SELECT
		product_id,
		product_name,
		brand_id,
		list_price,
		RANK () OVER ( 
			PARTITION BY brand_id
			ORDER BY list_price DESC
		) price_rank 
	FROM
		production.products
) t
WHERE price_rank <= 3;
The following picture shows the output:
product_id, product_name, list_price, price_rank


SQL Server RANK Function Over Partition Example
In this example:

First, the PARTITION BY clause divides the products into partitions by brand Id.
Second, the ORDER BY clause sorts products in each partition by list prices.
Third, the outer query returns the products whose rank values are less than or equal to three.
The RANK() function is applied to each row in each partition and reinitialized when crossing the partition’s boundary.


                2.1.1.6.2
            2.1.1.7 DB classification

                2.1.1.7.1 distributed DBs
horizontal scaling.
replication factor. how many copies are made.
usually replicas handle read requests

                2.1.1.7.2 CAP theorem
three guarantees:
a. network partition tolerance, 
b. Consistency. reads always get the most upto date value. ACID (Atomicity, Consistency, Isolation, Durability). Banks etc
c. availability. BASE (Basical availability, Soft State, Eventual consistency) YouTube etc.

only 2 can be guaranteed. in a distributed system either Consistency or availability must be compromised.
Example.
For consistency. writes are not complete until all replicas are up to date. In a replica is down fail the write.
For availability. writed replicate but don't fail write if there's a network partition so its possible for reads to get outdated data

Consistency + partition tolerance:
examples:
MongoDB, REDIS
distributed locking (Apache zookeeper)

traites:
pessimistic locking

availability + partition tolerance:
examples:
Cassandra, Riak, CouchDB
DNS

traites:
NSPF (No Single Point of Failure)
Conflict resolution
Optimistic

configuration (in Cassandra/MongoDB) read/write Consistency


                2.1.1.7.3 more DB classification

                    2.1.1.7.3.1 Data model.
                    graphs and key-value are not well handled by relational DB
also document stores, wide columns

                        2.1.1.7.3.1.1 TSDB
https://blog.timescale.com/blog/what-the-heck-is-time-series-data-and-why-do-i-need-a-time-series-database-dcf3b1b18563/

A primer on time-series data, and why you may not want to use a “normal” database to store it.

Here’s a riddle: what do self-driving Teslas, autonomous Wall Street trading algorithms, smart homes, transportation networks that fulfill lightning-fast same-day deliveries, and an open-data-publishing NYPD have in common?

For one, they are signs that our world is changing at warp speed, thanks to our ability to capture and analyze more and more data in faster and faster ways than before.

However, if you look closely, you’ll notice that each of these applications requires a special kind of data:

Self-driving cars continuously collect data about how their local environment is changing around them.
Autonomous trading algorithms continuously collect data on how the markets are changing.
Our smart homes monitor what’s going on inside of them to regulate temperature, identify intruders, and respond to our beck-and-call (“Alexa, play some relaxing music”).
Our retail industry monitors how their assets are moving with such precision and efficiency that cheap same-day delivery is a luxury that many of us take for granted.
The NYPD tracks its vehicles to allow us to hold them more accountable (e.g., for analyzing 911 response times).
These applications rely on a form of data that measures how things change over time. Where time isn’t just a metric, but a primary axis. This is time-series data and it’s starting to play a larger role in our world.

Software developer usage patterns already reflect this. In fact, over the past 24 months time-series databases (TSDBs) have steadily remained the fastest growing category of databases:


Source: DB-Engines, November 2018. 
As the developers of an open source time-series database, my team and I are often asked about this trend. So I’ll start with a more in-depth description of time-series data and then jump into when would you would need a time-series database.

What is time-series data?
Some think of “time-series data” as a sequence of data points, measuring the same thing over time, stored in time order. That’s true, but it just scratches the surface.

Others may think of a series of numeric values, each paired with a timestamp, defined by a name and a set of labeled dimensions (or “tags”). This is perhaps one way to model time-series data, but not a definition of the data itself.

Here’s a basic illustration. Imagine sensors collecting data from three settings: a city, farm, and factory. In this example, each of these sources periodically sends new readings, creating a series of measurements collected over time.


Here’s another example, with real data from the City of New York, showing taxicab rides for the first few seconds of 2018. As you can see, each row is a “measurement” collected at a specific time:


NYC taxi rides for the first few seconds of 2018. Source: NYC.gov
There are many other kinds of time-series data. To name a few: DevOps monitoring data, mobile/web application event streams, industrial machine data, scientific measurements.

These datasets primarily have 3 things in common:

The data that arrives is almost always recorded as a new entry
The data typically arrives in time order
Time is a primary axis (time-intervals can be either regular or irregular)
In other words, time-series data workloads are generally “append-only.” While they may need to correct erroneous data after the fact, or handle delayed or out-of-order data, these are exceptions, not the norm.

You may ask: How is this different than just having a time-field in a dataset? Well, it depends: how does your dataset track changes? By updating the current entry, or by inserting a new one?

When you collect a new reading for sensor_x, do you overwrite your previous reading, or do you create a brand new reading in a separate row? While both methods will provide you the current state of the system, only by writing the new reading in a separate row will you be able to track all states of the system over time.

Simply put: time-series datasets track changes to the overall system as INSERTs, not UPDATEs.

This practice of recording each and every change to the system as a new, different row is what makes time-series data so powerful. It allows us to measure change: analyze how something changed in the past, monitor how something is changing in the present, predict how it may change in the future.

Put simply, here’s how I like to define time-series data: data that collectively represents how a system/process/behavior changes over time.

This is more than just an academic distinction. By centering our definition around “change”, we can start to identify time-series datasets that we aren’t collecting today, but that we should be collecting down the line. In fact, often people have time-series data but don’t realize it.

Imagine you maintain a web application. Every time a user logs in, you may just update a “last_login” timestamp for that user in a single row in your “users” table. But what if you treated each login as a separate event, and collected them over time? Then you could: track historical login activity, see how usage is (in-/de-)creasing over time, bucket users by how often they access the app, and more.

This example illustrates a key point: by preserving the inherent time-series nature of our data, we are able to preserve valuable information on how that data changes over time. Another point: event data is also time-series data.

Of course, storing data at this resolution comes with an obvious problem: you end up with a lot of data, rather fast. So that’s the catch: time-series data piles up very quickly.

Having a lot of data creates problems when both recording it and querying it in a performant way, which is why people are now turning to time-series databases.

Why do I need a time-series database?
You might ask: Why can’t I just use a “normal” (i.e., non-time-series) database?

The truth is that you can, and some people do. Yet why are TSDBs the fastest growing category of databases today? Two reasons: (1) scale and (2) usability.

Scale: Time-series data accumulates very quickly. (For example, a single connected car will collect 4,000 GB of data per day.) And normal databases are not designed to handle that scale. Relational databases fare poorly with very large datasets; NoSQL databases fare better at scale, but can still be outperformed by a database fine-tuned for time-series data. In contrast, time-series databases (which can be based on relational or NoSQL databases) handle scale by introducing efficiencies that are only possible when you treat time as a first class citizen. These efficiencies result in performance improvements, including higher ingest rates, faster queries at scale (although some support more queries than others), and better data compression.

Usability: TSDBs also typically include functions and operations common to time-series data analysis such as data retention policies, continuous queries, flexible time aggregations, etc. Even if scale it not a concern at the moment (e.g., if you are just starting to collect data), these features can still provide a better user experience and make your life easier.

This is why developers are increasingly adopting time-series databases and using them for a variety of use cases:

Monitoring software systems: Virtual machines, containers, services, applications
Monitoring physical systems: Equipment, machinery, connected devices, the environment, our homes, our bodies
Asset tracking applications: Vehicles, trucks, physical containers, pallets
Financial trading systems: Classic securities, newer cryptocurrencies
Eventing applications: Tracking user/customer interaction data
Business intelligence tools: Tracking key metrics and the overall health of the business
(and more)
Even then, you’ll need to pick a time-series database that best fits your data model and write/read patterns.

A parting thought: Is all data time-series data?
For the past decade or so, we have lived in the era of “Big Data”, collecting massive amounts of information about our world and applying computational resources to make sense of it.

Even though this era started with modest computing technology, our ability to capture, store, and analyze data has improved at an exponential pace, thanks to major macro-trends: Moore’s law, Kryder’s law, cloud computing, an entire industry of “big data” technologies.


Under Moore’s Law, computational power (transistor density) doubles every 18 months, while Kryder’s Law postulates that storage capacity doubles every 12 months.
Now we need more. We are no longer content to just observe the state of the world, but we now want to measure how our world changes over time, down to sub-second intervals. Our “big data” datasets are now being dwarfed by another type of data, one that relies heavily on time to preserve information about the change that is happening.

Does all data start off as time-series data? Recall the earlier web application example: we had time-series data but didn’t realize it. Or think of any “normal” dataset. Say, the current accounts and balances at a major retail bank. Or the source code for a software project. Or the text for this article.

Typically we choose to store the latest state of the system, but instead, what if we stored every change and computed the latest state at query time? Isn’t a “normal” dataset just a view on top of an inherently time-series dataset (cached for performance reasons)? Don’t banks have transaction ledgers? (And aren’t blockchains just distributed, immutable time-series logs?) Wouldn’t a software project have version control (e.g., git commits)? Doesn’t this article have revision history? (Undo. Redo.)

Put differently: Don’t all databases have logs?

We recognize that many applications may never require time-series data (and would be better served by a “current-state view”). But as we continue along the exponential curve of technological progress, it would seem that these “current-state views” become less necessary. And that by storing more and more data in its time-series form, we may be able to understand it better.

So is all data time-series data? I’ve yet to find a good counter example. If you’ve got one, I’m open to hearing it. Regardless, one thing is clear: time-series data already surrounds us. It’s time we put it to use.

                        2.1.1.7.3.1.2
                    2.1.1.7.3.2 Data Variability
Data is not well structured

                    2.1.1.7.3.3 Operational capabilities
Each different model of DB is expected different capabilities.
e.x.
graph DBS: excell at graph operations
search engines: excell at text searches
TSDB: monitoring, anomality detection, trend visualization

                    2.1.1.7.3.4
                2.1.1.7.4

        2.1.1.8


        2.1.2 key-value store

like hash map. only distributed. no indexes, joins, sql etc.
redis, memcached...

            2.1.2.1 redis
in memory nosql DB
written in C
keeps keys in memory
persistent/non-persistent modes
master slave topology
extensible - lua, C
Users: Twitter, Weibo, Pinterest, Github

                2.1.2.1.1 redis setup and installation

                    2.1.2.1.1.1 redis docker
a. retreive and start redis container.
$ docker run --name my-redis -d redis

b. check that its running
$ docker ps -a | grep Up
488793583e4e        redis                         "docker-entrypoint.s…"   57 seconds ago      Up 56 seconds               6379/tcp                 my-redis
1af403d0ae3e        mysql:latest                  "docker-entrypoint.s…"   5 days ago          Up 5 days                   3306/tcp, 33060/tcp      yosi-mysql
d623d12848f8        redis:4.0                     "docker-entrypoint.s…"   6 weeks ago         Up 4 days                   0.0.0.0:6379->6379/tcp   local-redis

if not just start it
docker start my-redis

c. shell access to container
docker exec -it my-redis sh, then redis-cli. or in 1 cmd:
docker exec -it my-redis redis-cli

For benchmark run
docker exec -it my-redis redis-benchmark

d. check redis server up
| 127.0.0.1:6379> ping
PONG

e. set/get
| 127.0.0.1:6379> set name yosi
OK
| 127.0.0.1:6379> get name
"yosi"


                    2.1.2.1.1.2

                2.1.2.1.2

            2.1.2.2 redis data structures

                2.1.2.2.1 key and string
use cases: mem cache, sessions management etc

key can be anything. max size 512 MB. max # keys, 2^32-1 (~ 4*10^9)

keys commands
[i500695@C02X632CJGH6:2020-11-08 18:00:26:~/work/courses/sql_nosql_bigdata/rdbms_2_nosql:]2051$ cat redis/data-structure-commands/keys.txt | pbcopy 
ECHO "list keys in your redis server"

KEYS *
KEYS demo.*

ECHO "there is no command to create a key. Keys are created when key-value pairs are set."
ECHO "I will create some key-value pairs"

SET foos bar
SET demo.redis.cli.hello world
SET demo.redis.cli.keys 12999030039

SADD demo.redis.cli.set0001 apple
HSET demo.redis.cli.hash001 movieId 3 title "Shawshank Redemption" genres "Comedy|Adventure" year 1998
ZADD demo.redis.cli.10202-scoreboard 6 James 2 Shane 9 Ezekiel 1 Darwin
PFADD demo.redis.cli.hll001 

KEYS *
KEYS demo.*

ECHO "confirm the existence of a key"

EXISTS demo.redis.cli.hll001
EXISTS demo.redis.cli.hll002s


ECHO "confirm the TTL of a key"
TTL demo.redis.cli.set0001
PTTL demo.redis.cli.10202-scoreboard

ECHO "set the TTL of a key"

EXPIRE foos 5					# in seconds
PEXPIRE mylist 3600000				# in milliseconds


ECHO "delete a key" 

DEL demo.redis.cli.keys


ECHO "check the type of the value held by a key"

TYPE demo.redis.cli.set0001
TYPE demo.redis.cli.hash001
TYPE demo.redis.cli.10202-scoreboard
TYPE demo.redis.cli.hll001 

ECHO "checkout out other commands for key management from the official command referent https://redis.io/commands#generic"

#scan command.
SCAN 0
#1) "13" #next cursor
#2)  1) "foo4"
#    2) "key:__rand_int__"
#    3) "demo.redis.cli.set0001"
#    4) "name"
#    5) "foo3"
#    6) "foo7"
#    7) "mylist"
#    8) "foo1"
#    9) "demo.redis.cli.hello"
#   10) "foo6"
#   11) "foo5"
#   12) "demo.redis.cli.keys"
SCAN 13 #scan next cursor
#1) "0" #0 return cursor means last page
#2) 1) "demo.redis.cli.hll001"
#   2) "foo2"
#   3) "myhash"
#   4) "counter:__rand_int__"


strings. all simple values (non set/hash etc). Can be also files etc.
[i500695@C02X632CJGH6:2020-11-08 18:19:36:~/work/courses/sql_nosql_bigdata/rdbms_2_nosql:]2051$ cat redis/data-structure-commands/strings.txt | pbcopy 
ECHO "Lets create some string data"

SET demo.webapp.visitor_count 2
SET demo.ml.movie.2  "2::Jumanji (1995)::Adventure|Children's|Fantasy"
SET demo.app.usertoken:abc@yahoo.co.za RBPouKfAKzSUJdPihrJJAMkRCGcn5P5MICuzLZ2rPAldqeQa4U EX 21600 NX

ECHO "lets now get our results back"

GET demo.webapp.visitor_count
GET demo.ml.movie.2
GET demo.app.usertoken:abc@yahoo.co.za
GET demo.app.usertoken:doesnotexist@yahoo.co.za

## MSET MGET
ECHO "they type of our key is "

TYPE demo.webapp.visitor_count
string
TYPE demo.ml.movie.2
string
TYPE demo.app.usertoken:abc@yahoo.co.za
string

ECHO "but the encoding will differ"

OBJECT ENCODING demo.webapp.visitor_count
int
OBJECT ENCODING demo.ml.movie.2
raw
OBJECT ENCODING demo.app.usertoken:abc@yahoo.co.za
raw

ECHO "they object encoding determines the permissible operation that can be performed."

INCR demo.webapp.visitor_count   # we cannot do this to demo.app.usertoken:abc@yahoo.co.za for instance 
DECR demo.webapp.visitor_count

INCR demo.app.usertoken:abc@yahoo.co.za

INCRBY demo.webapp.visitor_count 10
INCRBY demo.ml.movie.2 2.1
DECRBY demo.webapp.visitor_count 2

ECHO "get the length of our user token"
STRLEN demo.app.usertoken:abc@yahoo.co.za
50

ECHO "checkout out other commands for string from the official command referent https://redis.io/commands#string"

                2.1.2.2.2 hash and list 
O(1) access time

[i500695@C02X632CJGH6:2020-11-09 11:58:33:~/work/courses/sql_nosql_bigdata/rdbms_2_nosql:]2053$ cat redis/data-structure-commands/hashes.txt | pbcopy
ECHO "Lets create some string data"
HSET demo.ml.movie#1 title "Toy Story (1995)"
HSET demo.ml.movie#1 year 1995
HSET demo.ml.movie#1 genres "Animation|Children's|Comedy"
HSET demo.ml.movie#1 id 1

ECHO "Return all pairs in the hash"
HGETALL demo.ml.movie#1

ECHO "Return the value for a field in the hash"
HGET demo.ml.movie#1 id

ECHO "Load all key value pair in one command. See string MSET and MGET"
HMSET demo.ml.movie#2 title "Jumanji (1995)" year 1995  genres "Adventure|Children's|Fantasy" id 2 id 1
HMSET demo.ml.movie#3952 title "Contender, The (2000)" year 2000  genres "Thriller" id 3952
HMSET demo.ml.movie#3947 title "Get Carter (1971)" year 1971  genres "Thriller" id 3947
HMSET demo.ml.movie#3945 title "The Movie (2000)" year 2000  genres "Adventure|Animation|Children's" id 3945
HMSET demo.ml.movie#3941 title "Sorority House Massacre (1986)" year 1986  genres "Horror" id 3941
HMSET demo.ml.movie#3942 title "Sorority House Massacre II (1990)" year 1990  genres "Horror" id 3942
HMSET demo.ml.movie#3927 title "Fantastic Voyage (1966)" year 1966  genres "Adventure|Sci-Fi" id 3927

ECHO "list all the keys in the hash for this key"
HKEYS demo.ml.movie#3927

ECHO "list all the values in the hash for this key"
HVALS demo.ml.movie#2 

ECHO "returns the size of the hash for this key. how many sub keys..."
HLEN demo.ml.movie#3941

ECHO "confirm the existence of this key in the hash for this key"
HEXISTS demo.ml.movie#3927 id 
HEXISTS demo.ml.movie#3927 ids

ECHO "inspecting the type of this key"
TYPE demo.ml.movie#3952
OBJECT ENCODING demo.ml.movie#3952

ECHO "return the listed fields from the hash identified by this key. Non-existing field return nill"
HMGET demo.ml.movie#1 id title year genres field5

ECHO "increase and decrease the value of this field in the hash for this key by the value 20"
HINCRBY demo.ml.movie#1 year -1

ECHO "checkout out other commands for hashes from the official command referent https://redis.io/commands#hash"


Lists, key value pair. values are strings only
add/remove from both head and tail. all O(1).
inner list changes O(n)
use case: stack/queue, keep order  
[i500695@C02X632CJGH6:2020-11-09 12:46:41:~/work/courses/sql_nosql_bigdata/rdbms_2_nosql:]2055$ cat redis/data-structure-commands/list.txt | pbcopy
ECHO "Lets create some string data. push left side. then right side"
LPUSH demo.ml.ratings "1::1193::5::978300760" "1::661::3::978302109"
RPUSH demo.ml.ratings "1::914::3::978301968" "1::3408::4::978300275"

ECHO "See all the content of the list"
LRANGE demo.ml.ratings 0 -1

ECHO "Add more data from the tail end of the list"
RPUSH demo.ml.ratings "1::2355::5::978824291" "1::1197::3::978302268"

ECHO "return the element at the index of the list. This happens from the head"
LINDEX demo.ml.ratings 0
LINDEX demo.ml.ratings 1

ECHO "return and remove an item from the head and tail respectively."
LPOP demo.ml.ratings
RPOP demo.ml.ratings 

ECHO "Add more data from the head end of the list"
LPUSH demo.ml.ratings "1::1287::5::978302039" "1::1287::5::978302039" "1::2804::5::978300719" "1::594::4::978302268"

ECHO "Add a number of duplications from the head end of the list"
LPUSH demo.ml.ratings "1::919::4::978301368" "1::919::4::978301368" "1::919::4::978301368" "1::919::4::978301368"

ECHO "Use LTRIM to cap the list. This time, the purpose is to remove the duplication I entered above"
LTRIM demo.ml.ratings 3 -1

ECHO "Use LTRIM to cap the list to just one item" 
LTRIM demo.ml.ratings 0 0

ECHO "use blocking pop to remove an item from the list with 0 timeout"
ECHO "the first command should work while the second will block indefinitely"
BLPOP demo.ml.ratings 0
BLPOP demo.ml.ratings 0


ECHO "checkout out other commands for lists from the official command referent https://redis.io/commands#list"

                2.1.2.2.3 set, sorted set
set. no duplicates. order not gauranteed. O(1) add/rem/lookup. 
supported group operations. union, intersection etc.
[i500695@C02X632CJGH6:2020-11-09 12:53:51:~/work/courses/sql_nosql_bigdata/rdbms_2_nosql:]2057$ cat redis/data-structure-commands/set.txt | pbcopy
ECHO "Lets create some string data"
SADD demo.ml.genres Action Adventure Animation Children Comedy Crime Drama Fantasy 
SADD demo.ml.genres War Western Horror Sci-Fi  Mystery Romance  Thriller Fantasy Documentary Film-Noir

ECHO "Return the cardinality (size) of the set"
SCARD demo.ml.genres

ECHO "Inspect the type and object encoding"
TYPE demo.ml.genres
OBJECT ENCODING demo.ml.genres

ECHO "Return all members of the set. Be careful as this could contain a lot of records"
SMEMBERS demo.ml.genres

ECHO "Check if an item is a member of the set"
SISMEMBER demo.ml.genres Action
SISMEMBER demo.ml.genres Actions

ECHO "randomly return x members of the set"
SRANDMEMBER demo.ml.genres 2

ECHO "Create a new set"
SADD demo.ml.genres.viewed Documentary Adventure Thriller Comedy Drama Fantasy 


ECHO "Find all items common to both sets. use SINTERSTORE to do the same but store the value in a new set"
SINTER demo.ml.genres demo.ml.genres.viewed 

ECHO "Find the difference between the first set and the rest. use SDIFFSTORE to do the same but store the value in a new set"
SDIFF demo.ml.genres demo.ml.genres.viewed

ECHO "Remove and return a value from this set."
SPOP demo.ml.genres.viewed

ECHO "Remove the item named from this set."
SREM demo.ml.genres.viewed Drama

ECHO "checkout out other commands for sets from the official command reference https://redis.io/commands#set"

sorted sets. Like sets but each entry has a score (for sorting)
$ cat ~/work/courses/sql_nosql_bigdata/rdbms_2_nosql/redis/data-structure-commands/sortedset.txt | pbcopy
ECHO "Lets create some string data"
ZADD demo.ml.genre_hist 20 Action 34 Adventure 10 Animation 16 Children 55 Comedy 19 Crime 39 Drama 11 Fantasy
ZADD demo.ml.genre_hist NX 22 War 8 Western 
ZADD demo.ml.genre_hist CH 8.9 Horror 8 Sci-Fi 3 Mystery 24 Romance 10 Thriller 3.11 Documentary 1 Film-Noir

ECHO "get cardinality"
ZCARD demo.ml.genre_hist

ECHO "list out the items of this sorted set. Then do it in a reverse order"
ZRANGE demo.ml.genre_hist 0 -1
ZRANGE demo.ml.genre_hist 0 -1 WITHSCORES

ZREVRANGE demo.ml.genre_hist 0 2
ZREVRANGE demo.ml.genre_hist 0 2 WITHSCORES

ZRANGE demo.ml.genre_hist 0 2000


ECHO "inspecting our data structure"
TYPE demo.ml.genre_hist
OBJECT ENCODING demo.ml.genre_hist

ECHO "Getting the ranks of member or non-members of this sorted set"
ZRANK demo.ml.genre_hist Action
ZREVRANK demo.ml.genre_hist Action

ZRANK demo.ml.genre_hist Actions

ECHO "Return the score for this member"
ZSCORE demo.ml.genre_hist Action

ECHO "Increasing and decreasing an item score and seeing the effect on ranking "
ZRANK demo.ml.genre_hist Action
ZINCRBY demo.ml.genre_hist 3 Action
ZRANK demo.ml.genre_hist Action
ZINCRBY demo.ml.genre_hist -3 Action
ZRANK demo.ml.genre_hist Action


ECHO "Removing items from a sorted set"
ZPOPMIN demo.ml.genre_hist 
ZRANGE demo.ml.genre_hist 0 -1

ZPOPMAX demo.ml.genre_hist 
ZRANGE demo.ml.genre_hist 0 -1

ZREM demo.ml.genre_hist XXXXXX
ZRANGE demo.ml.genre_hist 0 -1

ECHO "checkout out other commands for sorted set from the official command referent https://redis.io/commands#sorted_set"



                2.1.2.2.4 GEO , HyperLogLog

                    2.1.2.2.4.1 GEO, geographical longitude and latitude 
used similar to sorted set
supports distance and radius
$ cat ~/work/courses/sql_nosql_bigdata/rdbms_2_nosql/redis/data-structure-commands/geo.txt | pbcopy
ECHO "Lets create some string data"
GEOADD demo.cities -118.243685  34.052234 "Los Angeles"
GEOADD demo.cities 28.047305 -26.204103 "Johannesburg"
GEOADD demo.cities 18.424055  -33.924869 "Cape Town"

ECHO "We can add more than one at a time"
GEOADD demo.cities -0.127758 51.507351 London 139.650311 35.676192 Tokyo 172.636225 -43.532054 Christchurch 
GEOADD demo.cities -74.005941 40.712784 "New York" -46.633309 -23.550520 "Sao paolo" 3.379206 6.524379 Lagos 
GEOADD demo.cities 13.404954 52.520007 Berlin 55.270783 25.204849 Dubai 3.379206 6.524379 Lagos 
               
ECHO "Lets use a sorted set command to see the list of cities in our geo structure"
ZRANGE demo.cities 0 -1 WITHSCORES

ECHO "Lets using GEODIST to see the distance between any two members in kilometers."
GEODIST demo.cities Dubai Berlin km

ECHO "Return the position of a member"
GEOPOS demo.cities Christchurch

ECHO "Introspecting our geo data structure. It is just a sorted set"
TYPE demo.cities 
OBJECT ENCODING demo.cities 

GEOHASH demo.cities "Los Angeles"

ECHO "checkout out other commands for geo from the official command referent https://redis.io/commands#geo"

                    2.1.2.2.4.2 hyperloglog
solves the problem of counting unique items (like visitor IDs for facebook) across distributed systems.
brute force query select distinct is not efficient. on a single node its o(n) time/space complexity.
on distributed its gets worse as the data needs to be aggregated and merged 

hyperloglog saves this problem by tracking the max number of ending zeros of items in the set
where each item is 64bit represented. the approximation is that if we have n max zeros we have 2^n numbers
to normalize and avoid high std deviation the numbers are divided to buckets according to k leading bits.
Then the avg of max zeros per bucket is used.
so possible values 2^64
value is 64 bit (log2(2^64))
max zeros 6 (log(64))
hence the name hyperloglog. 
very small amount of space is required

                        2.1.2.2.4.2.1 https://thoughtbot.com/blog/hyperloglogs-in-redis
HyperLogLogs in Redis
Calle Erlandsson  August 1, 2016 UPDATED ON March 23, 2019
REDIS  WEB  RUBY
A hyper-what-now?

A HyperLogLog is a probabilistic data structure used to count unique values — or as it’s referred to in mathematics: calculating the cardinality of a set.

These values can be anything: for example, IP addresses for the visitors of a website, search terms, or email addresses.

Counting unique values with exact precision requires an amount of memory proportional to the number of unique values. The reason for this is that there is no way of determining if a value has already been seen other than by comparing it to the previously seen values.

Since memory is a limited resource, doing this becomes problematic when working with large sets of values.

A HyperLogLog solves this problem by allowing to trade memory consumption for precision making it possible to estimate cardinalities larger than 109 with a standard error of 2% using only 1.5 kilobytes of memory[1].

How to use HyperLogLogs in Redis
HyperLogLogs have been available in Redis since version 2.8.9 and are accessed using the PFADD, PFCOUNT, and PFMERGE commands.

A Redis HyperLogLog consumes at most 12 kilobytes of memory and produces approximations with a standard error of 0.81%. The 12 kilobytes do not include the bytes required to store the actual key.

Ignoring PFMERGE, using a HyperLogLog is very similar to using a Set for the same purpose: instead of adding members with SADD, add them with PFADD and instead of retrieving the cardinality with SCARD, retrieve it with PFCOUNT.

The example below shows the output of an interactive Redis session. You can follow along by running redis-cli and entering the commands shown on the lines beginning with >. Make sure an instance of redis-server is also running.

> PFADD visitors alice bob carol
(integer) 1
> PFCOUNT visitors
(integer) 3
PFADD returns 1 if the approximated cardinality of the HyperLogLog was changed when adding the element. If not, it returns 0.

After calculating the cardinality, PFCOUNT will store the calculated value in the last 8 bytes of the HyperLogLog to serve as a cache. This cache is invalidated on the next PFADD.

The PFMERGE command is used to produce a HyperLogLog that approximates the cardinality of the union of two or more existing HyperLogLogs:

> PFADD customers alice dan
(integer) 1
> PFMERGE everyone visitors customers
OK
> PFCOUNT everyone
(integer) 4
The same result can be achieved by supplying multiple keys to PFCOUNT. In this case an on-the-fly merge is performed:

> PFCOUNT visitors customers
(integer) 4
Under the hood, HyperLogLogs are actually stored as strings and can be GET and SET as such:

> GET visitors
"HYLL\x01\x00\x00\x00\x03\x00\x00\x00\x00\x00\x00\x00E<\x94X\x10\x84Qi\x8cQD"
> SET visitors "HYLL\x01\x00\x00\x00\x03\x00\x00\x00\x00\x00\x00\x00E<\x94X\x10\x84Qi\x8cQD"
OK
> PFCOUNT visitors
(integer) 3
This is useful if the HyperLogLog needs to be persisted elsewhere and later restored.

The PFADD, PFCOUNT, and PFMERGE commands are prefixed with PF in memory of Philippe Flajolet, the inventor of the HyperLogLog algorithm.

The theory behind HyperLogLogs
To get started, let’s look at something more familiar than HyperLogLogs.

Imagine someone flipping a coin multiple times, while recording the maximum number of heads they get in a row. If they told you this number — would you be able to estimate how many times they’d flipped the coin?

Not with great accuracy, but since a longer run of heads is more probable when flipping the coin many times, the number of consecutive heads or tails in a series of coin flips can indicate how many times the coin was flipped: if the number of heads in a row is low, the number of coin flips is most probably also low. Conversely, if the number of heads in a row is high the number of coin flips is most probably high.

If the same number of coin flips were done and recorded separately, a much more precise estimate could be made by combining these numbers. This can be observed using the following methods:

def coin_flips(n)
  Array.new(n) { [:heads, :tails].sample }
end

def heads_in_a_row(flips)
  run = max = 0

  flips.each do |flip|
    if flip == :heads
      run += 1
    else
      max = [run, max].max
      run = 0
    end
  end

  max
end

class Array
  def average
    reduce(:+).to_f / size
  end
end
The bigger the number of coin flips, the bigger the number of heads in a row.

heads_in_a_row coin_flips(10)    #=> 3
heads_in_a_row coin_flips(100)   #=> 5
heads_in_a_row coin_flips(1000)  #=> 9
heads_in_a_row coin_flips(10000) #=> 15
The bigger the number of series, the better the stability of the output. With a more stable output, a better estimate can be made:

heads_in_a_row coin_flips(10) #=> 3
heads_in_a_row coin_flips(10) #=> 7
heads_in_a_row coin_flips(10) #=> 4

Array.new(1000) { heads_in_a_row coin_flips(10) }.average #=> 2.449
Array.new(1000) { heads_in_a_row coin_flips(10) }.average #=> 2.442
Array.new(1000) { heads_in_a_row coin_flips(10) }.average #=> 2.469
Similarly to how the number of coin flips can be estimated by observing the number of heads in a row, the calculations of a HyperLogLog are based on the observation that the cardinality of a set of uniformly distributed random numbers can be estimated by counting the maximum number of leading zeros in the binary representation of the numbers.

For there to be leading zeros, the numbers must be represented as integers with the same number of bits, for instance as 64-bit unsigned integers.

If the maximum number of leading zeros is n, the estimated number of unique values in the set is 2n.

To make sure values are uniformly distributed and represented as same-size integers, the HyperLogLog algorithm applies a hash function to all values. This transforms the values into a set of uniformly distributed random numbers with the same cardinality as the original set.

The first few bits of the hashed values are used to divide them into different subsets, much like the separate series of coin flips in the example above. For each subset the maximum number of leading zeros within its values are stored in a register.

To calculate the approximate cardinality of the whole set, the estimates for all the subsets are combined using a harmonic mean.

An example use-case
Here’s an example of how to use a Redis HyperLogLog to count unique visitors of a site built with Sinatra using a custom Rack middleware:

require 'redis'
require 'sinatra'

$redis = Redis.new

class VisitorCounter
  def initialize(app)
    @app = app
  end

  def call(env)
    $redis.pfadd 'visitors', Rack::Request.new(env).ip
    @app.call(env)
  end
end

use VisitorCounter

get '/' do
  visitors = $redis.pfcount 'visitors'
  "This website has been visited by #{visitors} unique visitors."
end
                        2.1.2.2.4.2.2 usage
A special data structure optimized for the "distinct count" query on a collection that may contain duplicates of same ID/key.
$ cat ~/work/courses/sql_nosql_bigdata/rdbms_2_nosql/redis/data-structure-commands/hyperloglog.txt | pbcopy
ECHO "Lets create some string data"
PFADD demo.ml.rating.users 1 1 3 4 12 3 4 6 3 5 7 65 5 33 2 213 3 7 4 

ECHO "Introspecting our structure"
TYPE demo.ml.rating.users 
OBJECT ENCODING demo.ml.rating.users 

ECHO "Count the number of items in our hyperloglog"
PFCOUNT demo.ml.rating.users 

ECHO "checkout out other commands for hyperloglog from the official command referent https://redis.io/commands#hyperloglog"



 

                2.1.2.2.5 Pubsub 

                                                                         +----------------+
                                                                         |                |
                                                              +----------> subscriber1    |
                                                              |          |                |
                                                              |          +----------------+
+-------+                                                     |
|       |                                                     |
|       |            +----------------------------------------+
|       +------------>                                        |           +---------------+
| Publisher          |                                        |           |               |
|       |            |       Channel                          +-----------> subscriber1   |
+-------+            |                                        |           |               |
                     +----------------------------------------+           +---------------+
                                                              |
                                                              |
                                                              |
                                                              |           +---------------+
                                                              |           |               |
                                                              +----------->  subscriber3  |
                                                                          +---------------+

 cat ~/work/courses/sql_nosql_bigdata/rdbms_2_nosql/redis/data-structure-commands/pubsub.txt | pbcopy

 #
# open two terms and have redis-cli open in both.
#
ECHO "THIS IS THE CLIENT SUBSCRIBER"
SUBSCRIBE channel_test_1





ECHO "in the second terminal with redis-cli"
PUBLISH channel_test_1 "this is my hello world"

ECHO "checkout out the commands for pubsub from the official command referent https://redis.io/commands#pubsub"



                2.1.2.2.6 transaction

                    2.1.2.2.6.1 Transactions in redis
 cat ~/work/courses/sql_nosql_bigdata/rdbms_2_nosql/redis/data-structure-commands/transaction.txt | pbcopy
ECHO "Lets create some string data"
SET foos 230

ECHO "Start a transaction, enter numerous command and after call EXEC to effect all were effected"
MULTI
INCRBY foos 11
DECRBY foos 6
EXEC

GET foos


ECHO "Start a transaction, enter numerous command and after call DISCARD to effect none of the commands"
MULTI
INCRBY foos 11
DECRBY foos 6
DISCARD

GET foos

ECHO "checkout out other commands for transactions from the official command referent https://redis.io/commands#transactions"

                    2.1.2.2.6.2 https://redislabs.com/blog/you-dont-need-transaction-rollbacks-in-redis/
Redis features two main mechanisms for executing multiple operations atomically: MULTI/EXEC transactions and Lua scripts. One peculiarity of transactions in Redis that often trips up newcomers is the absence of a rollback mechanism. In my tenure as a Developer Advocate at Redis Labs, I’ve talked to a few engineers with traditional SQL backgrounds who found this troubling, so with this blog I want to share my opinion on the subject, and argue that you don’t need rollbacks in Redis.

MULTI/EXEC transactions
Transactions in Redis start with the MULTI command. Once it’s sent, the connection switches mode and all subsequent commands sent through the connection will be queued by Redis instead of being immediately executed, with the exception of DISCARD and EXEC (which will instead cause the transaction to abort or commit, respectively). Committing the transaction means executing the previously queued commands.

MULTI
SET mykey hello
INCRBY counter 10
EXEC

Transactions (and Lua scripts) ensure two important things:

Other clients won’t see a partial state—meaning that all clients will see the state either before the transaction was applied, or after. 
In case of a node failure, once Redis restarts, it will reload either the whole transaction or none of it from the AOF file. 
One last basic thing to keep in mind about transactions: Redis will continue to serve other clients even when a MULTI transaction has been initiated. Redis will stop applying other clients’ commands only briefly when the transaction gets committed by calling EXEC. This is very different from SQL databases, where transactions engage various mechanisms within the DBMS to provide varying degrees of isolation assurances, and where clients can read values from the database while performing the transaction. In Redis, transactions are “one shot”—in other words, just a sequence of commands that get executed all at once. So, how do you create a transaction that depends on the data present in Redis? For this purpose, Redis implements WATCH, a command for performing optimistic locking.

Optimistic locking with WATCH

Let me show you on a practical level why you can’t read values from Redis while in a transaction:

MULTI
SET counter 42
GET counter
EXEC

If you run this series of commands in redis-cli, the reply from “GET counter” will be “QUEUED”, and the value “42” will be returned only as a result of calling EXEC, alongside the “OK” returned from executing the SET command.

To write a transaction that depends on data read from Redis, you must use WATCH. Once run, the command will ensure that the subsequent transaction will be executed only if the keys being WATCHed have not changed before EXEC gets called.

For example, this is how you would implement an atomic increment operation if INCRBY did not exist:

WATCH counter
GET counter
MULTI
SET counter <the value obtained from GET + any increment>
EXEC

In this example, we first create a WATCH trigger over the “counter” key, then we GET its value. Notice how GET happens before we start the transaction’s body, meaning it will be executed immediately and it will return the key’s current value. At this point, we start the transaction using MULTI and apply the change by computing on the clientside what the new value of “counter” should be.

If multiple clients were trying to concurrently apply this same transaction to the “counter” key, some transactions would be automatically discarded by Redis. At this point it would usually be the client’s job to retry the transaction. This is similar to SQL transactions, where higher isolation levels will occasionally cause the transaction to abort, leaving the client the task of retrying it.

Lua scripts
While WATCH can be very useful for performing articulated transactions, it’s usually easier and more efficient to use a Lua script when you need to perform multiple operations that depend on data in Redis. With a Lua script, you send the logic to Redis (in the form of the script itself) and have Redis execute the code locally, instead of pushing data to the client as we were doing in the example above. This is faster for several reasons, but here’s the big one: Lua scripts can read data from Redis without needing optimistic locking.

This how the previous transaction would be implemented as a Lua one-liner:

EVAL "redis.call('SET', KEYS[1], tonumber(redis.call('GET', KEYS[1]) or 0) + tonumber(ARGV[1]))" 1 counter 42

There are, in my opinion, a couple of reasonable situations where you might legitimately prefer transactions with optimistic locking over Lua:

The keys your transaction depends on are not modified frequently, meaning that you are confident optimistic locking will almost never abort transactions.
You depend on a lot of logic written on the client side—or maybe a third-party service—so there is no easy way to move that logic to a Lua script.
Unless both these points are true for your application, I recommend you choose Lua over WATCH.

Errors in transactions
To recap: MULTI/EXEC transactions (without WATCH) and Lua scripts never get discarded by Redis, while MULTI/EXEC + WATCH will cause Redis to abort transactions that depend on values changed after the corresponding keys were WATCHed. Lua scripts are more powerful than simple (i.e. WATCH-less) transactions because they can also read values from Redis, and are more efficient than “WATCHed” transactions because they don’t require optimistic locking to read values.

The key point about optimistic locking is that when a WATCHed key is changed, the whole transaction is discarded immediately when the client commits it using EXEC. Redis has a main, single-threaded command execution loop, so when the transaction queue is being executed no other command will run. This means that Redis transactions have a true serializable isolation level, and also means that no rollback mechanism is required to implement WATCH.

But what happens when there’s an error in a transaction? The answer is that Redis will continue to execute all commands and report all errors that happened.

To be more precise, there are some types of errors that Redis can catch before the client calls EXEC. One basic example are blatant syntax errors:

MULTI
GOT key? (NOTE: Redis has no GOT command and, after season 8, it never will)
EXEC


But not all errors can be discovered by inspecting the command syntax, and those could cause the transaction to misbehave. As an example:

MULTI
SET counter banana
INCRBY counter 10
EXEC

The example above will be executed but the INCRBY command will fail because the “counter” key doesn’t contain a number. This type of error can be discovered only when running the transaction (nevermind that in this simplified example we are the ones setting the wrong initial value).

This is the moment where one might say that rollbacks would be nice to have. I might agree if not for two considerations:

The snapshotting mechanism required to implement rollbacks would have a considerable computational cost. That extra complexity wouldn’t sit well with Redis’ philosophy and ecosystem.
Rollbacks can’t catch all errors. In the example above, we set “counter” to “banana” in order to show a blatant error, but in the real world the process that used the “counter” key in the wrong way might instead have deleted it, or put in a credit-card number, for example. Rollbacks would add a considerable amount of complexity and would still not fully solve the problem.
The second point is particularly important because it also applies to SQL: SQL DBMSs offer many mechanisms to help protect data integrity, but even they can’t completely protect you from programming errors. On both platforms, the burden of writing correct transactions remains on you. 

Rollbacks in SQL DBMSs
If that seems to conflict with your experience using SQL databases, let’s look at the difference between relying on errors to enforce constraints vs. relying on errors to protect the data from bugs in your code.

It’s common practice in SQL to use indexes to implement constraints on the data and rely on those indexes on the client side for correctness. A common example would be to add a “UNIQUE” constraint to a “username” column to ensure that each user has a different username. At that point clients would try to insert new users and expect the insertion to fail when another user with the same name already exists.

This is a perfectly legitimate use of a SQL database, but relying on the constraint to implement application logic is very different than expecting rollbacks to protect you from mistakes in the transaction logic itself.

At AWS re:Invent 2019, when an attendee asked me “Why doesn’t Redis have rollbacks?” my answer was based on enumerating why people use rollbacks in SQL. In my opinion, there are only two main reasons to do so:

First reason to use rollbacks: concurrency

Most common SQL databases are multithreaded applications, and when a client requests a high isolation level, the DBMS prefers to trigger an exception rather than stop serving all other clients. This makes sense for the SQL ecosystem because SQL transactions are “chatty”: a client locks a few rows, reads a few values, computes what changes to apply, and finally commits the transaction.

In Redis, transactions are not meant to be as interactive. The single-threaded nature of the main event loop in Redis ensures that while the transaction is running, no other command gets executed. This ensures that all transactions are truly serializable without violating the isolation level. When a transaction uses optimistic locking, Redis will be able to abort it before executing any command in the transaction queue—which doesn’t require a rollback.

Second reason to use rollbacks: leveraging index constraints

In SQL, it’s common to use index constraints to implement logic in the application. I mentioned UNIQUE, but the same applies to foreign key constraints and more. The premise is that the application relies on the database to have been properly configured and leverages index constraints to implement the logic in an efficient way. But I’m sure everyone has seen applications misbehave when somebody forgets to put in a UNIQUE constraint, for example.

While SQL DBMSs do a great job of protecting data integrity, you can’t expect to be protected from all errors in your transaction code. There is a significant class of errors that don’t violate type checking or index constraints.

Redis has no built-in index system (Redis modules are a different story and don’t apply here). To force uniqueness, for example, you would use a Set (or equivalent) data type. This means the correct way to express an operation in Redis looks different from the equivalent in SQL. Redis’ data model and execution model are different enough from SQL that the same logical operation would be expressed in different ways depending on the platform, but the application must always be in sync with the state of the database.

An application that tries to INCRBY a key that contains a non-numeric value is the same as an application that expects a SQL schema inconsistent with what’s on the database. If you have gremlins in your Redis database making unexpected changes, lock them out using access control lists (ACLs).

Redis vs. SQL
If you come from a SQL background, you might understandably be surprised by how transactions work in Redis. Given that NoSQL has demonstrated that relational databases are not the only valuable model for storing data, don’t make the mistake of assuming that any diversion from what SQL offers is inherently inferior. SQL transactions are chatty, based on a multi-threaded model and interoperate with other subsystems to leverage rollbacks in case of failures. In contrast, Redis transactions are more focused on performance and there is no indexing subsystem to leverage for enforcing constraints. Because of these differences, the transaction-writing “style” that you use in Redis is fundamentally different from the SQL one.

This means that the lack of rollbacks in Redis doesn’t limit expressiveness. All reasonable SQL transactions can be rewritten to a functionally equivalent Redis transaction, but it’s not always trivial to do in practice. Reasoning about a problem originally articulated in SQL in Redis requires you to think about your data in a different way, and you also need to account for the different execution model. 

Finally, it’s true that rollbacks can be useful to protect your data from programming errors, but they are not meant to be a solution to that problem. As a multi-model database based on a key-value structure, Redis doesn’t offer the same level of “type checking” ease that SQL does, but there are techniques to help with that, as Kyle Davis, Head of Developer Advocacy at Redis Labs, explained in this recent blog post: Bullet-Proofing Lua Scripts in RedisPy.

That said, your applications need to be in sync with what’s in the database, both when using a relational database and when using Redis. For Redis, the utility of rollbacks would not outweigh the costs in terms of performance and additional complexity. If you ever wondered how Redis can be so much faster than other databases, here’s yet another reason why.
                    2.1.2.2.6.3

                2.1.2.2.7 Redis usage
Redis can be used as read queries cache. This is simple enough. store finished query results in cache. 
Fetch from cache rather than DB.

Redis can be used as the primary DB. But this requires 'Strict Access Pattern'. This is Because SQL is not available with it's wealth of operations. Just key, value is supported.
This technique means that first we need the model the requirements from the system.
Like in movielens use case breakdown to 
user:
find by id
insert/update
find by age/occupation

movies:
find by id
insert/update
find by genre/release-year

ratings:
etc. 

genre:
etc.

Also model read/write load according to use case.
Ex:
movie genre doesn't change but can be read a lot.
ratings can be uploaded in peak (for new movies) so favor efficient write
etc.

Now to support user related queries we need different data structures
a hash/set of user ids
a set of users per age group
a set of users per occupation 

                2.1.2.2.8 redis example java applications
source code
cd  /Users/i500695/work/courses/sql_nosql_bigdata/rdbms_2_nosql/redis/movielens-redis-ui
mvn package

                2.1.2.2.9
            2.1.2.3


id=__Document_oriented_DB_Document-Store__
        2.1.3 Document oriented DB, Document-Store

Store semi structured data, usually json/xml format

            2.1.3.1 Intro

            2.1.3.2 Mongo
written in c++
serves json but internally stored as binary (bson)
uses sharding for partioning
Also supports goespatial (coordinates, points, etc)

install:
$ docker pull mongo
[i500695@C02X632CJGH6:2020-12-03 18:04:08:~/temp:]2022$ mkdir mongodb
[i500695@C02X632CJGH6:2020-12-03 18:04:57:~/temp:]2023$ docker run -it -v mongodb:/data/db --name mongodb -d mongo
513ef862e7e8445c3d178ae4913243f872905df247cc8e9a19171aaad0149296

logs:
$ docker logs mongodb

bash access:
 docker exec -it mongodb bash
 
 start mongo
 root@513ef862e7e8:/# mongo
MongoDB shell version v4.4.2
connecting to: mongodb://127.0.0.1:27017/?compressors=disabled&gssapiServiceName=mongodb
Implicit session: session { "id" : UUID("507ae19e-4ff4-4251-aaa5-1f3f0d1a30ec") }
MongoDB server version: 4.4.2
Welcome to the MongoDB shell.
For interactive help, type "help

LCM:
docker stop/start mongodb 

                2.1.3.2.1 mongodb terminogy
database - namespace, similar to sql schema
Collection - 
                2.1.3.2.2 Modeling in mongodb
- Unlike sql, there are no FK constraints so data integrity can't rely on DB level. it needs to be in app level.
- where normalization breaks "logical" entities in sql, in mongo such relations are not required. 
  For example a user can have occupation as property and not as a key to a separate table
- movielens example. basically denormalization...
so user will be a json like:
{
    id: aid,
    gender: agender,
    occupation: {
        occupation_sub_property: ...,
    }
    age: age
}

-  nXn relations in sql are required b/c on row value can not be an array so join is required
   In mongo it's possible to have an array sub property
   so for example movie <-> movie-generes many2many relation can be represented as an array.
   e.g.
   movie json:
   {
       id: id,
       title: title,
       release-year: year,
       genres: [
        action,
        thriller
       ],
    }
- what about ratings table?
it related both to users and movies. how to model?
one option is to refer to full movie object in rating entry of user (duplication). This is refered to as embeding (denormalization).

another option, that avoids duplcation is to use ObjRef (similar to FK or pointer). This is refered to as referencing (normalization) 
ex. rating json:
{
    id: ratingid,
    user: ObjRef ("desc", userid),
    movie: ObjRef ("desc", movieid),
}

- some rule of thumbs. in 1:1 relations, If the refered entity is seldom changed (think movie) it can be normalized. 
  If we need history we should embed (think age histor for user)
  big objects should referenced 

                2.1.3.2.3 movielens mongodb implementation
/Users/i500695/work/courses/sql_nosql_bigdata/rdbms_2_nosql/mongodb

                2.1.3.2.4 mongo CRUD
CLI supports completion. 
db.<table name>.insert/find 

 $  cat /Users/i500695/work/courses/sql_nosql_bigdata/rdbms_2_nosql/mongodb/mongdb_crud.js | pbcopy
 //insert a new movie
db.movies.insertOne({
    "title" : "Avenger Endgame (2019)",
    "release_year" : 2019,
    "genres" : [
            "Fantasy",
            "Sci-Fi"
    ]
});

// insert many movies at the same time
db.movies.insertMany([
{"title" : "Glass", "release_year" : 2019, "genres" : [ "Drama", "Sci-Fi", "Thriller" ] },
{"title" : "Fighting with My Family", "release_year" : 2019, "genres" : [ "Biography", "Comedy", "Drama" ] },
{"title" : "Black Panther", "release_year" : 2018, "genres" : [ "Action", "Adventure", "Sci-Fi" ] }])

//search
db.movies.findOne()
//find functions generally take two documents
//the first part is the query predicate definition, 
//the second part controls the projection
db.movies.find({'title' : "Black Panther"}, {}) //.pretty()
//now project only the id and title
db.movies.find({'title' : "Black Panther"}, {'title' : 1, '_id' : 0})

//examples of operations
//search for the 10 Crime and Action movies
db.movies.find({$and : [{'genres' : 'Crime'}, {'genres' : 'Action'}]}).limit(10)
//and some pagnation
db.movies.find({$and : [{'genres' : 'Crime'}, {'genres' : 'Action'}]}).skip(2).limit(10)

//update 
//has to parts: query part and the updating document part
db.movies.update({'title' : "Black Panther"}, {$set : {'release_year' : 2019}})

//delete
db.movies.delete({})


//get the max id
db.movie.find().sort({"_id": -1}).limit(1)


                2.1.3.2.5 indexes
 $  cat /Users/i500695/work/courses/sql_nosql_bigdata/rdbms_2_nosql/mongodb/indexes.js | pbcopy

//explain
db.users.find().explain()

//console:
// query planner report (query resolver selects optimal plan e.g. COLLSCAN)
//

db.users.find({'age_id' : 1}).explain()
// filter: { age_id: {$eq: "1"}}

db.users.find({$and : [{'age_id' : 1}, {'gender' : 'F'}]}).limit(10).explain()

//add executionStats to explain output
db.users.find({'age_id' : 1}).explain("executionStats")

//get indexes on the users collection
db.users.getIndexes()
// only on id primary-key

//to understand the impact of index on a query, it is important to use the explain function 
db.ratings.find({'genres' : 'Action'}).explain()

//create indexes on the rating collection
db.users.createIndex({'age_id' : 1})

// unique compound index, no user can rate the same movie twice
db.ratings.createIndexes([{'movie_id' : 1}, {'user_id': 1}], {unique: true})  //with options
//sort the ratings in descending order or time
db.ratings.createIndex({'rated_at' : -1})
//assumming a unique index on user c
db.users.createIndex({'ssn' : 1}, {unique: true}) //many other options. Check the documentaion


//create full text indexes on the tags collection
db.tags.createIndex({
		'tag' : 'text',
		'movie.title' : 'text'
	}, 
	{
		'weights' : {
			'tag' : 5,
			'movie.title' : 10
		},
		'name' : 'tag_text_idx'
})

//search using text index
db.tags.find( { $text: { $search: "bitter" } }, {'tag': 1, 'movie.title' : 1} )

//add the score of the relevance to the search
db.tags.find( 
	{ $text: { $search: "awesome romance" }},
   	{ score: { $meta: "textScore" } } 
)

//sort by the score of the relevance to the search
db.tags.find( 
	{ $text: { $search: "awesome romance" }},
   	{ score: { $meta: "textScore" } } 
).sort({score: { $meta: "textScore"}})

//drop movie_id and user_id indexes on the rating collection
db.ratings.dropIndex('idx44')
 

                2.1.3.2.6 shard keys
all shared collection must have an index on the shard key

                2.1.3.2.7 aggregation via map-reduce

note, mongo shell supports javascript syntax

dbname.count() -- return count
let c = dbname.count() -- c is a cursor
c.count()

map reduce, same as in functional programming.
pass a map function, map produces (k,v) pairs
then a reduce function

+------------------+              +------------------------+         +-----------------------+
|                  |              |                        |         |                       |
|                  +-------------->                        +--------->                       |
|   Documents      |              | Map                    |         |  Reduce               |
|                  |              |                        |         |                       |
|                  |              |                        +--------->                       |
|                  |              |                        |         |                       |
|                  +-------------->                        |         |                       |
|                  |              |                        |         |                       |
|                  |              |                        +--------->                       |
|                  |              |                        |         |                       |
|                  +-------------->                        |         +-----------------------+
+------------------+              +------------------------+
                     k1,v1                                  k1, [v1,v2,v5]
                     k2,v2                                  k2, [v2,v5]
                     k1,v3
                     k1,v4
                     k2,v5

example. find the distribution of genres of movies in a given year for all years.
reminder, movie doc looks like this:
db.movies.findOne()
{"id":1, "title" : "Black Panther", "release_year" : 2018, "genres" : [ "Action", "Adventure", "Sci-Fi" ] }])


code:
 cat $(pwd)/analytical_queries/movie_genre_count_per_year_using_mapreduce.js | pbcopy
//fetch a frequency of genre of movies per year for all years

var mapFn = function(){
	var genres = this.genres;
	var year = this.release_year;
	//emit each genre as count 1
	for (var i = 0; i < genres.length; i++)
		emit({'year': year, 'genre': genres[i]}, 1); //we want to count 1 for each genre in a given year, note key is {year, genre}
}

var reduceFn = function(key, values) { //since key is {year, genre} and value is 1 per movie we need to sum all the 1s to get result
	return Array.sum(values);
}

db.movies.mapReduce(mapFn, reduceFn, {out: {inline: 1}, query: {}}) //out instructs output to screen, query, what find to run for the map ({} -> return all)

db.movies.mapReduce(mapFn, reduceFn, {out: 'genre_yearly_hist', query: {}}) //outputs to a collection

//we will now query the genre_yearly_hist table
//get the frequency distribution of genres in a particular year. Order by frequency in descending order
db.genre_yearly_hist.find({'_id.year' : 1999}, {}).sort({'value': -1})
// we get descending list, drama 130, comedy 103 etc

//get the number of movies for a particular genre through the years
db.genre_yearly_hist.find({'_id.genre' : 'Action'}, {}).sort({'_id.year': -1})
// 

// sql equivalent
//select genre, release_year as year, count(1) from v_movie_genre group by genre, release_year;
   




                2.1.3.2.8  aggregation via aggregation framework
works like a pipeline of tasks...

[i500695@C02X632CJGH6:2020-12-10 16:04:31:~/work/courses/sql_nosql_bigdata/rdbms_2_nosql/mongodb:]2052$ cat $(pwd)/analytical_queries/movie_genre_count_per_year.js | pbcopy
//fetch a histogram of genre of movies per year for all years
db.movies.aggregate([
	{$project : {'release_year' : 1, 'genres' : 1, '_id' : 0}},
	{$unwind : '$genres'},
	{$group : {
		_id : { 'year' : '$release_year', 'genre' : '$genres'},
		value : {$sum : 1}
	}},
	// {$match : {'_id.year' : 2000}}
])

// sql equivalent
//select genre, release_year as year, count(1) from v_movie_genre group by genre, release_year;

keywords are functions that do operation. project - :1 add filed to result, 0 - omit it
unwind, break up lists of values to multiple results. Example {"release_year": 1995, "genres": ["Horror", "comedy", "Action"]}
will be broken to:
{"release_year": 1995, "genres": ["Horror"]}
{"release_year": 1995, "genres": ["comedy"]}
{"release_year": 1995, "genres": ["Action"]}
group, group keys to single key value, key {release_year, genre} value - 1 for each
value: $sum, sum add one for each {release_year, genre}
match only matches according to param key : match


[i500695@C02X632CJGH6:2020-12-10 16:17:27:~/work/courses/sql_nosql_bigdata/rdbms_2_nosql/mongodb:]2054$ cat $(pwd)/analytical_queries/users_who_gave_5_star_reviews.js | pbcopy
//get all users who gave a 5 star review. query on a normalized collection.
// explanation. match users who gave rating 5. since id is in users table perform lookup for user id. its like a join...
// only works on non sharded collection
// unwind, unpack array of users to separate lines
// project, take key $user value and put in result as user: value
db.ratings.aggregate([
	{$match : {"rating" : 5}},
	{$lookup : {
	        from : 'users',
	        localField: 'user_id',
	        foreignField: '_id',
	        as: 'user'
	    }
	},
	{$unwind : '$user'},
	{$project : {'user' : '$user'}}
])


//get all ratings where the user gave a 5 start review. query on a denormalized collection.
db.ratings_v2.aggregate([
    {$match : {"rating" : 5}},
    //{$project: {'user' : '$user', '_id' : 0}}
    {$project: {'user_id' : '$user.user_id', 'gender' : '$user.gender', 'occupation' : '$user.occupation', 'age_group' : '$user.age_group'}}
])


//or return all users who have given five star ratings and the movies they gave them on
db.ratings.aggregate([
	{$match : {"rating" : 5}},
	{$lookup : {
	        from : 'users',
	        localField: 'user_id',
	        foreignField: '_id',
	        as: 'user'
	    }
	},
	{$unwind : '$user'},
	{$project : {'movie_id' : 1, 'user' : 1, '_id': 0}},
	{$lookup : {
	        from : 'movies',
	        localField: 'movie_id',
	        foreignField: '_id',
	        as: 'movie'
	    }
	},
    {$group : {
            _id : '$user_id',
            'movies' : { $addToSet: { movie: "$movie.title" } }
        }
    }
])


//or return all users who have given five star ratings and the movies they gave them on
db.ratings_v2.aggregate([
	{$match : {"rating" : 5}},
    {$project: {'user' : '$user', 'movie' : 1, '_id': 0}},
    {$group : {
            _id : '$user.user_id',
            'movies' : { $addToSet: { movie: "$movie.title" } }
        }
    }
])

~/work/courses/sql_nosql_bigdata/rdbms_2_nosql/mongodb:]2057$ cat $(pwd)/analytical_queries//movie_rating_summary.js  | pbcopy
// the denormalized version
db.ratings_v2.aggregate([
    {$project : {'rating' : 1, 'movie.title' : 1, 'movie_id' : 1, '_id' : 0}},
	{$group : {
		'_id' : {'title' : '$movie.title', 'movie_id' : '$movie_id'},
		'no_rating' : {$sum : 1},
		'average_rating' : {$avg : '$rating'},
		'std_dev' :  {$stdDevPop: "$rating" }
		}
	},
	{$project : {'_id.title' : 1, 'no_rating' : 1, 'average_rating' : 1, 'std_dev' : 1, 'var_rating' : { $pow: ['$std_dev', 2 ] }}}
])


//for a movie, get the rating summary - count, sum, mean and variance for all movies (normalized)
db.ratings.aggregate([
	{$lookup : {
	        from : 'movies',
	        localField: 'movie_id',
	        foreignField: '_id',
	        as: 'movie'
	    }
	},
	{$unwind : '$movie'},
    {$project : {'rating' : 1, 'movie.title' : 1, 'movie_id' : 1, '_id' : 0}},
	{$group : {
		'_id' : {'title' : '$movie.title', 'movie_id' : '$movie_id'},
		'no_rating' : {$sum : 1},
		'average_rating' : {$avg : '$rating'},
		'std_dev' :  { $stdDevPop: "$rating" }
		}
	},
	{$project : {'_id.title' : 1, 'no_rating' : 1, 'average_rating' : 1, 'std_dev' : 1, 'var_rating' : { $pow: ['$std_dev', 2 ] }}}
])


// select title, count(1) no_movies, sum(rating) total_ratings, avg(rating) average_rating,
//variance(rating) from v_rating group by title;


                2.1.3.2.9

            2.1.3.3


        2.1.4 Search engine
document DB that support full text search, order by relevance 
The big difference vs normal DB as that full text search indexed are treated as first class citizens

so query will be tokenized, stemmed etc. pass to inverse index lookup table that will match the records with a relevance score.

Apache Lucene is a popular java based full text search library. Used by Elastic search and apache Solar. 

            2.1.4.1 Elastic search
most popular search engine db.
open source. horizontal scale.
used by Github, stack-overflow, Foursquare

            2.1.4.2 installation
install on host: https://www.elastic.co/start
docker install:
https://www.elastic.co/guide/en/elastic-stack-get-started/master/get-started-docker.html
cat docker-compose.yml
version: '3'
services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.9.3-amd64
    env_file:
      - elasticsearch.env
    volumes:
      - ./data/elasticsearch:/usr/share/elasticsearch/data

  kibana:
    image: docker.elastic.co/kibana/kibana:7.9.3
    env_file:
      - kibana.env
    ports:
      - 5601:5601

cat kibana.env
SERVER_HOST="0"
ELASTICSEARCH_URL=http://elasticsearch:9200
XPACK_SECURITY_ENABLED=false
[i500695@C02X632CJGH6:2020-12-10 17:57:13:~/work/tools/elasticsearch-docker:]2071$ cat elasticsearch.env 
cluster.name=my-awesome-elasticsearch-cluster
network.host=0.0.0.0
bootstrap.memory_lock=true
discovery.type=single-node

Navigate to the directory where you have created your docker-compose.yml file and create a subdirectory data. Then inside the data directory create another directory elasticsearch.
mkdir data
cd data
mkdir elasticsearch


run the compose:
docker-compose up -d

[i500695@C02X632CJGH6:2020-12-10 18:05:43:~/work/tools/elasticsearch-docker/data:]2076$ docker ps -a
CONTAINER ID        IMAGE                                                       COMMAND                  CREATED             STATUS                      PORTS                    NAMES
f9dbcdcd4e42        docker.elastic.co/elasticsearch/elasticsearch:7.9.3-amd64   "/tini -- /usr/local…"   41 seconds ago      Up 39 seconds               9200/tcp, 9300/tcp       elasticsearch-docker_elasticsearch_1
bc0c632c6289        docker.elastic.co/kibana/kibana:7.9.3                       "/usr/local/bin/dumb…"   42 seconds ago      Up 39 seconds               0.0.0.0:5601->5601/tcp   elasticsearch-docker_kibana_1
[i500695@C02X632CJGH6:2020-12-10 18:06:23:~/work/tools/elasticsearch-docker/data:]2077$ docker-compose ps
                Name                              Command               State           Ports         
------------------------------------------------------------------------------------------------------
elasticsearch-docker_elasticsearch_1   /tini -- /usr/local/bin/do ...   Up      9200/tcp, 9300/tcp    
elasticsearch-docker_kibana_1          /usr/local/bin/dumb-init - ...   Up      0.0.0.0:5601->5601/tcp

 docker-compose logs -f <optional - container name>

 when up kibana client: http://localhost:5601

 docker-compose logs -f kibana 

 throrough guide: https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html
 Install Elasticsearch with Dockeredit
Elasticsearch is also available as Docker images. The images use centos:8 as the base image.

A list of all published Docker images and tags is available at www.docker.elastic.co. The source files are in Github.

These images are free to use under the Elastic license. They contain open source and free commercial features and access to paid commercial features. Start a 30-day trial to try out all of the paid commercial features. See the Subscriptions page for information about Elastic license levels.

Pulling the imageedit
Obtaining Elasticsearch for Docker is as simple as issuing a docker pull command against the Elastic Docker registry.

docker pull docker.elastic.co/elasticsearch/elasticsearch:7.10.1
Alternatively, you can download other Docker images that contain only features available under the Apache 2.0 license. To download the images, go to www.docker.elastic.co.

Starting a single node cluster with Dockeredit
To start a single-node Elasticsearch cluster for development or testing, specify single-node discovery to bypass the bootstrap checks:

docker run -p 9200:9200 -p 9300:9300 -e "discovery.type=single-node" docker.elastic.co/elasticsearch/elasticsearch:7.10.1
Starting a multi-node cluster with Docker Composeedit
To get a three-node Elasticsearch cluster up and running in Docker, you can use Docker Compose:

Create a docker-compose.yml file:
version: '2.2'
services:
  es01:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.10.1
    container_name: es01
    environment:
      - node.name=es01
      - cluster.name=es-docker-cluster
      - discovery.seed_hosts=es02,es03
      - cluster.initial_master_nodes=es01,es02,es03
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - data01:/usr/share/elasticsearch/data
    ports:
      - 9200:9200
    networks:
      - elastic
  es02:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.10.1
    container_name: es02
    environment:
      - node.name=es02
      - cluster.name=es-docker-cluster
      - discovery.seed_hosts=es01,es03
      - cluster.initial_master_nodes=es01,es02,es03
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - data02:/usr/share/elasticsearch/data
    networks:
      - elastic
  es03:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.10.1
    container_name: es03
    environment:
      - node.name=es03
      - cluster.name=es-docker-cluster
      - discovery.seed_hosts=es01,es02
      - cluster.initial_master_nodes=es01,es02,es03
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - data03:/usr/share/elasticsearch/data
    networks:
      - elastic

volumes:
  data01:
    driver: local
  data02:
    driver: local
  data03:
    driver: local

networks:
  elastic:
    driver: bridge
This sample Docker Compose file brings up a three-node Elasticsearch cluster. Node es01 listens on localhost:9200 and es02 and es03 talk to es01 over a Docker network.

Please note that this configuration exposes port 9200 on all network interfaces, and given how Docker manipulates iptables on Linux, this means that your Elasticsearch cluster is publically accessible, potentially ignoring any firewall settings. If you don’t want to expose port 9200 and instead use a reverse proxy, replace 9200:9200 with 127.0.0.1:9200:9200 in the docker-compose.yml file. Elasticsearch will then only be accessible from the host machine itself.

The Docker named volumes data01, data02, and data03 store the node data directories so the data persists across restarts. If they don’t already exist, docker-compose creates them when you bring up the cluster.

Make sure Docker Engine is allotted at least 4GiB of memory. In Docker Desktop, you configure resource usage on the Advanced tab in Preference (macOS) or Settings (Windows).

Docker Compose is not pre-installed with Docker on Linux. See docs.docker.com for installation instructions: Install Compose on Linux

Run docker-compose to bring up the cluster:

docker-compose up
Submit a _cat/nodes request to see that the nodes are up and running:

curl -X GET "localhost:9200/_cat/nodes?v&pretty"
Log messages go to the console and are handled by the configured Docker logging driver. By default you can access logs with docker logs.

To stop the cluster, run docker-compose down. The data in the Docker volumes is preserved and loaded when you restart the cluster with docker-compose up. To delete the data volumes when you bring down the cluster, specify the -v option: docker-compose down -v.

Start a multi-node cluster with TLS enablededit
See Encrypting communications in an Elasticsearch Docker Container and Run the Elastic Stack in Docker with TLS enabled.

Using the Docker images in productionedit
The following requirements and recommendations apply when running Elasticsearch in Docker in production.

Set vm.max_map_count to at least 262144edit
The vm.max_map_count kernel setting must be set to at least 262144 for production use.

How you set vm.max_map_count depends on your platform:

Linux

The vm.max_map_count setting should be set permanently in /etc/sysctl.conf:

grep vm.max_map_count /etc/sysctl.conf
vm.max_map_count=262144
To apply the setting on a live system, run:

sysctl -w vm.max_map_count=262144
macOS with Docker for Mac

The vm.max_map_count setting must be set within the xhyve virtual machine:

From the command line, run:

screen ~/Library/Containers/com.docker.docker/Data/vms/0/tty
Press enter and use`sysctl` to configure vm.max_map_count:

sysctl -w vm.max_map_count=262144
To exit the screen session, type Ctrl a d.
Windows and macOS with Docker Desktop

The vm.max_map_count setting must be set via docker-machine:

docker-machine ssh
sudo sysctl -w vm.max_map_count=262144
Windows with Docker Desktop WSL 2 backend

The vm.max_map_count setting must be set in the docker-desktop container:

wsl -d docker-desktop
sysctl -w vm.max_map_count=262144
Configuration files must be readable by the elasticsearch useredit
By default, Elasticsearch runs inside the container as user elasticsearch using uid:gid 1000:0.

One exception is Openshift, which runs containers using an arbitrarily assigned user ID. Openshift presents persistent volumes with the gid set to 0, which works without any adjustments.

If you are bind-mounting a local directory or file, it must be readable by the elasticsearch user. In addition, this user must have write access to the data and log dirs. A good strategy is to grant group access to gid 0 for the local directory.

For example, to prepare a local directory for storing data through a bind-mount:

mkdir esdatadir
chmod g+rwx esdatadir
chgrp 0 esdatadir
As a last resort, you can force the container to mutate the ownership of any bind-mounts used for the data and log dirs through the environment variable TAKE_FILE_OWNERSHIP. When you do this, they will be owned by uid:gid 1000:0, which provides the required read/write access to the Elasticsearch process.

Increase ulimits for nofile and nprocedit
Increased ulimits for nofile and nproc must be available for the Elasticsearch containers. Verify the init system for the Docker daemon sets them to acceptable values.

To check the Docker daemon defaults for ulimits, run:

docker run --rm centos:8 /bin/bash -c 'ulimit -Hn && ulimit -Sn && ulimit -Hu && ulimit -Su'
If needed, adjust them in the Daemon or override them per container. For example, when using docker run, set:

--ulimit nofile=65535:65535
Disable swappingedit
Swapping needs to be disabled for performance and node stability. For information about ways to do this, see Disable swapping.

If you opt for the bootstrap.memory_lock: true approach, you also need to define the memlock: true ulimit in the Docker Daemon, or explicitly set for the container as shown in the sample compose file. When using docker run, you can specify:

-e "bootstrap.memory_lock=true" --ulimit memlock=-1:-1
Randomize published portsedit
The image exposes TCP ports 9200 and 9300. For production clusters, randomizing the published ports with --publish-all is recommended, unless you are pinning one container per host.

Set the heap sizeedit
To configure the heap size, you can bind mount a JVM options file under /usr/share/elasticsearch/config/jvm.options.d that includes your desired heap size settings. Note that while the default root jvm.options file sets a default heap of 1 GB, any value you set in a bind-mounted JVM options file will override it.

While setting the heap size via bind-mounted JVM options is the recommended method, you can also configure this by using the ES_JAVA_OPTS environment variable to set the heap size. For example, to use 16 GB, specify -e ES_JAVA_OPTS="-Xms16g -Xmx16g" with docker run. Note that while the default root jvm.options file sets a default heap of 1 GB, any value you set in ES_JAVA_OPTS will override it. The docker-compose.yml file above sets the heap size to 512 MB.

You must configure the heap size even if you are limiting memory access to the container.

Pin deployments to a specific image versionedit
Pin your deployments to a specific version of the Elasticsearch Docker image. For example docker.elastic.co/elasticsearch/elasticsearch:7.10.1.

Always bind data volumesedit
You should use a volume bound on /usr/share/elasticsearch/data for the following reasons:

The data of your Elasticsearch node won’t be lost if the container is killed
Elasticsearch is I/O sensitive and the Docker storage driver is not ideal for fast I/O
It allows the use of advanced Docker volume plugins
Avoid using loop-lvm modeedit
If you are using the devicemapper storage driver, do not use the default loop-lvm mode. Configure docker-engine to use direct-lvm.

Centralize your logsedit
Consider centralizing your logs by using a different logging driver. Also note that the default json-file logging driver is not ideally suited for production use.

Configuring Elasticsearch with Dockeredit
When you run in Docker, the Elasticsearch configuration files are loaded from /usr/share/elasticsearch/config/.

To use custom configuration files, you bind-mount the files over the configuration files in the image.

You can set individual Elasticsearch configuration parameters using Docker environment variables. The sample compose file and the single-node example use this method.

To use the contents of a file to set an environment variable, suffix the environment variable name with _FILE. This is useful for passing secrets such as passwords to Elasticsearch without specifying them directly.

For example, to set the Elasticsearch bootstrap password from a file, you can bind mount the file and set the ELASTIC_PASSWORD_FILE environment variable to the mount location. If you mount the password file to /run/secrets/bootstrapPassword.txt, specify:

-e ELASTIC_PASSWORD_FILE=/run/secrets/bootstrapPassword.txt
You can also override the default command for the image to pass Elasticsearch configuration parameters as command line options. For example:

docker run <various parameters> bin/elasticsearch -Ecluster.name=mynewclustername
While bind-mounting your configuration files is usually the preferred method in production, you can also create a custom Docker image that contains your configuration.

Mounting Elasticsearch configuration filesedit
Create custom config files and bind-mount them over the corresponding files in the Docker image. For example, to bind-mount custom_elasticsearch.yml with docker run, specify:

-v full_path_to/custom_elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml
The container runs Elasticsearch as user elasticsearch using uid:gid 1000:0. Bind mounted host directories and files must be accessible by this user, and the data and log directories must be writable by this user.

Mounting an Elasticsearch keystoreedit
By default, Elasticsearch will auto-generate a keystore file for secure settings. This file is obfuscated but not encrypted. If you want to encrypt your secure settings with a password, you must use the elasticsearch-keystore utility to create a password-protected keystore and bind-mount it to the container as /usr/share/elasticsearch/config/elasticsearch.keystore. In order to provide the Docker container with the password at startup, set the Docker environment value KEYSTORE_PASSWORD to the value of your password. For example, a docker
run command might have the following options:

-v full_path_to/elasticsearch.keystore:/usr/share/elasticsearch/config/elasticsearch.keystore
-E KEYSTORE_PASSWORD=mypassword
Using custom Docker imagesedit
In some environments, it might make more sense to prepare a custom image that contains your configuration. A Dockerfile to achieve this might be as simple as:

FROM docker.elastic.co/elasticsearch/elasticsearch:7.10.1
COPY --chown=elasticsearch:elasticsearch elasticsearch.yml /usr/share/elasticsearch/config/
You could then build and run the image with:

docker build --tag=elasticsearch-custom .
docker run -ti -v /usr/share/elasticsearch/data elasticsearch-custom
Some plugins require additional security permissions. You must explicitly accept them either by:

Attaching a tty when you run the Docker image and allowing the permissions when prompted.
Inspecting the security permissions and accepting them (if appropriate) by adding the --batch flag to the plugin install command.
See Plugin management for more information.

Next stepsedit
You now have a test Elasticsearch environment set up. Before you start serious development or go into production with Elasticsearch, you must do some additional setup:

Learn how to configure Elasticsearch.
Configure important Elasticsearch settings.
Configure important system settings.



            2.1.4.3 Elasticsearch terminology

index - noun. like DB table. unique name identifier. Elasticsearch will auto create an index for new documents
indexes are Lucene indexes saved on disk.
verb. proces of writing a document to an index

type - a deprecated concept. since ver. 6.0.

mapping - Defenition how to write the index for a document. like a schema. will be auto generated when not specified.

documents - json objects. encoureged to have _id which will be used as primary-key

field - key-value pair

node - single instance

cluster - 1 or more nodes working as one

shard - horizontal partition of DB. they would be Lucene indexes spread over different nodes

replica - for HA and fault tolerance shards can be replicated 

analyzer -  the document processing steps for index creation are.
a. character filter. preprocess char stream. can add/change/remove special chars. 
example. remove <> in html docs.

b. tokenizer.
input - char stream
output - tokens (words)
simple example. line.split() in python.

c. token filter
process a stream of tokens. 
for example remove stop words. change case.  

simple type ex:
{ "male": {"type": "boolean"}

multi field datatype. has more than one type. ex:
{ "country": {
    "type": "text",
    "fields": {
        "type":"keyword",
        "ignore_above": 256
    }
}
}
meaning country can be used as a text field (full search) or as keyword (not indexed)


            2.1.4.4 modeling for ES
since joins are not supported. Nor refs like in mongo. the way to model relations and hierarchy is by denormalization (e.g. big contain all doc)

Use rest for creating mappings (PUT w/ json payload of mappings)

cd ~/work/courses/sql_nosql_bigdata/rdbms_2_nosql/

mappings:
[i500695@C02X632CJGH6:2020-12-13 13:51:48:~/work/courses/sql_nosql_bigdata/rdbms_2_nosql/elasticsearch:]2006$ lt movielens/schemas/
total 32
drwxr-xr-x  6 i500695  staff   192B Nov  2 12:18 ../
-rw-r--r--  1 i500695  staff   537B Nov  2 12:18 movie_schema.json
-rw-r--r--  1 i500695  staff   561B Nov  2 12:18 rating_schema.json
-rw-r--r--  1 i500695  staff   763B Nov  2 12:18 tag_schema.json
drwxr-xr-x  6 i500695  staff   192B Nov  2 12:18 ./
-rw-r--r--  1 i500695  staff   314B Nov  2 12:18 user_schema.json

use http put to elasticsearch to use these mappings.

ex:
cat movielens/schemas/rating_schema.json | pbcopy

PUT /ml-ratings

{
	"mappings" : {
		"dynamic" : false,
		"properties" : {
			"userId" : {"type": "long", "store": true},
			"movieId" : {"type": "long", "store": true},
			"title" : {"type" : "text", "store" : true, "index" : true, "analyzer" : "snowball"},
			"year" : {"type" : "short", "store" : true},
			"genres" : {"type" : "keyword", "store": true},
			"rating" : {"type" : "float", "store" : true},
			"ts" : {"type" : "date", "store": true, "format":"dd/MM/yyyy HH:mm"}
		}
	},
	"settings" : {
		"number_of_shards" : 5,
		"number_of_replicas" : 1
	}
}

- list indexes
  GET, /_cat/indices 

            2.1.4.5 ES CRUD

- first example
 cat creat-index-without-schema.json | pbcopy 
-- posts for creation:
http://localhost:9200/review/1

  {
    "id": 0,
    "country": "Italy",
    "description": "Aromas include tropical fruit, broom, brimstone and dried herb. The palate isn't overly expressive, offering unripened apple, citrus and dried sage alongside brisk acidity.",
    "designation": "Vulkà Bianco",
    "points": 87,
    "price": 0.0,
    "province": "Sicily & Sardinia",
    "region_1": "Etna",
    "region_2": "",
    "taster_name": "Kerin O’Keefe",
    "taster_twitter_handle": "@kerinokeefe",
    "title": "Nicosia 2013 Vulkà Bianco  (Etna)",
    "variety": "White Blend",
    "winery": "Nicosia",
    "active" : true
  }

http://localhost:9200/review/2
  {
    "id": 1,
    "country": "Portugal",
    "description": "This is ripe and fruity, a wine that is smooth while still structured. Firm tannins are filled out with juicy red berry fruits and freshened with acidity. It's  already drinkable, although it will certainly be better from 2016.",
    "designation": "Avidagos",
    "points": 87,
    "price": 15.12,
    "province": "Douro",
    "region_1": "",
    "region_2": "",
    "taster_name": "Roger Voss",
    "taster_twitter_handle": "@vossroger",
    "title": "Quinta dos Avidagos 2011 Avidagos Red (Douro)",
    "variety": "Portuguese Red",
    "winery": "Quinta dos Avidagos",
    "active" : true
  }

http://localhost:9200/review/3
  {
    "id": 2,
    "country": "US",
    "description": "Tart and snappy, the flavors of lime flesh and rind dominate. Some green pineapple pokes through, with crisp acidity underscoring the flavors. The wine was all stainless-steel fermented.",
    "designation": "",
    "points": 87,
    "price": 14.08,
    "province": "Oregon",
    "region_1": "Willamette Valley",
    "region_2": "Willamette Valley",
    "taster_name": "Paul Gregutt",
    "taster_twitter_handle": "@paulgwine",
    "title": "Rainstorm 2013 Pinot Gris (Willamette Valley)",
    "variety": "Pinot Gris",
    "winery": "Rainstorm",
    "active" : true
  }

http://localhost:9200/review/4
  {
    "id": 3,
    "country": "US",
    "description": "Pineapple rind, lemon pith and orange blossom start off the aromas. The palate is a bit more opulent, with notes of honey-drizzled guava and mango giving way to a slightly astringent, semidry finish.",
    "designation": "Reserve Late Harvest",
    "points": 87,
    "price": 13.90,
    "province": "Michigan",
    "region_1": "Lake Michigan Shore",
    "region_2": "",
    "taster_name": "Alexander Peartree",
    "taster_twitter_handle": "",
    "title": "St. Julian 2013 Reserve Late Harvest Riesling (Lake Michigan Shore)",
    "variety": "Riesling",
    "winery": "St. Julian",
    "active" : true
  }

http://localhost:9200/review/5
  {
    "id": 4,
    "country": "US",
    "description": "Much like the regular bottling from 2012, this comes across as rather rough and tannic, with rustic, earthy, herbal characteristics. Nonetheless, if you think of it as a pleasantly unfussy country wine, it's a good companion to a hearty winter stew.",
    "designation": "Vintner's Reserve Wild Child Block",
    "points": 87,
    "price": 65.50,
    "province": "Oregon",
    "region_1": "Willamette Valley",
    "region_2": "Willamette Valley",
    "taster_name": "Paul Gregutt",
    "taster_twitter_handle": "@paulgwine",
    "title": "Sweet Cheeks 2012 Vintner's Reserve Wild Child Block Pinot Noir (Willamette Valley)",
    "variety": "Pinot Noir",
    "winery": "Sweet Cheeks",
    "active" : true
  }

http://localhost:9200/review/6
  {
    "id": 5,
    "country": "Spain",
    "description": "Blackberry and raspberry aromas show a typical Navarran whiff of green herbs and, in this case, horseradish. In the mouth, this is fairly full bodied, with tomatoey acidity. Spicy, herbal flavors complement dark plum fruit, while the finish is fresh but grabby.",
    "designation": "Ars In Vitro",
    "points": 87,
    "price": 15.99,
    "province": "Northern Spain",
    "region_1": "Navarra",
    "region_2": "",
    "taster_name": "Michael Schachner",
    "taster_twitter_handle": "@wineschach",
    "title": "Tandem 2011 Ars In Vitro Tempranillo-Merlot (Navarra)",
    "variety": "Tempranillo-Merlot",
    "winery": "Tandem",
    "active" : true
  }

http://localhost:9200/review/7
  {
    "id": 6,
    "country": "Italy",
    "description": "Here's a bright, informal red that opens with aromas of candied berry, white pepper and savory herb that carry over to the palate. It's balanced with fresh acidity and soft tannins.",
    "designation": "Belsito",
    "points": 87,
    "price": 16.39,
    "province": "Sicily & Sardinia",
    "region_1": "Vittoria",
    "region_2": "",
    "taster_name": "Kerin O’Keefe",
    "taster_twitter_handle": "@kerinokeefe",
    "title": "Terre di Giurfo 2013 Belsito Frappato (Vittoria)",
    "variety": "Frappato",
    "winery": "Terre di Giurfo",
    "active" : true
  }

http://localhost:9200/review/8
  {
    "id": 7,
    "country": "France",
    "description": "This dry and restrained wine offers spice in profusion. Balanced with acidity and a firm texture, it's very much for food.",
    "designation": "",
    "points": 87,
    "price": 24.85,
    "province": "Alsace",
    "region_1": "Alsace",
    "region_2": "",
    "taster_name": "Roger Voss",
    "taster_twitter_handle": "@vossroger",
    "title": "Trimbach 2012 Gewurztraminer (Alsace)",
    "variety": "Gewürztraminer",
    "winery": "Trimbach",
    "active" : true
  }

http://localhost:9200/review/9
  {
    "id": 8,
    "country": "Germany",
    "description": "Savory dried thyme notes accent sunnier flavors of preserved peach in this brisk, off-dry wine. It's fruity and fresh, with an elegant, sprightly footprint.",
    "designation": "Shine",
    "points": 87,
    "price": 12.99,
    "province": "Rheinhessen",
    "region_1": "",
    "region_2": "",
    "taster_name": "Anna Lee C. Iijima",
    "taster_twitter_handle": "",
    "title": "Heinz Eifel 2013 Shine Gewürztraminer (Rheinhessen)",
    "variety": "Gewürztraminer",
    "winery": "Heinz Eifel",
    "active" : true
  }

http://localhost:9200/review/10
  {
    "id": 9,
    "country": "France",
    "description": "This has great depth of flavor with its fresh apple and pear fruits and touch of spice. It's off dry while balanced with acidity and a crisp texture. Drink now.",
    "designation": "Les Natures",
    "points": 87,
    "price": 27.50,
    "province": "Alsace",
    "region_1": "Alsace",
    "region_2": "",
    "taster_name": "Roger Voss",
    "taster_twitter_handle": "@vossroger",
    "title": "Jean-Baptiste Adam 2012 Les Natures Pinot Gris (Alsace)",
    "variety": "Pinot Gris",
    "winery": "Jean-Baptiste Adam",
    "active" : true
  }

http://localhost:9200/review/11
  {
    "id": 10,
    "country": "US",
    "description": "Soft, supple plum envelopes an oaky structure in this Cabernet, supported by 15% Merlot. Coffee and chocolate complete the picture, finishing strong at the end, resulting in a value-priced wine of attractive flavor and immediate accessibility.",
    "designation": "Mountain Cuvée",
    "points": 87,
    "price": 19.99,
    "province": "California",
    "region_1": "Napa Valley",
    "region_2": "Napa",
    "taster_name": "Virginie Boone",
    "taster_twitter_handle": "@vboone",
    "title": "Kirkland Signature 2011 Mountain Cuvée Cabernet Sauvignon (Napa Valley)",
    "variety": "Cabernet Sauvignon",
    "winery": "Kirkland Signature",
    "active" : true
  }

http://localhost:9200/review/12
  {
    "id": 11,
    "country": "France",
    "description": "This is a dry wine, very spicy, with a tight, taut texture and strongly mineral character layered with citrus as well as pepper. It's a food wine with its almost crisp aftertaste.",
    "designation": "",
    "points": 87,
    "price": 30.00,
    "province": "Alsace",
    "region_1": "Alsace",
    "region_2": "",
    "taster_name": "Roger Voss",
    "taster_twitter_handle": "@vossroger",
    "title": "Leon Beyer 2012 Gewurztraminer (Alsace)",
    "variety": "Gewürztraminer",
    "winery": "Leon Beyer",
    "active" : true
  }

http://localhost:9200/review/13
  {
    "id": 12,
    "country": "US",
    "description": "Slightly reduced, this wine offers a chalky, tannic backbone to an otherwise juicy explosion of rich black cherry, the whole accented throughout by firm oak and cigar box.",
    "designation": "",
    "points": 87,
    "price": 34.00,
    "province": "California",
    "region_1": "Alexander Valley",
    "region_2": "Sonoma",
    "taster_name": "Virginie Boone",
    "taster_twitter_handle": "@vboone",
    "title": "Louis M. Martini 2012 Cabernet Sauvignon (Alexander Valley)",
    "variety": "Cabernet Sauvignon",
    "winery": "Louis M. Martini",
    "active" : true
  }

- Get documents
http://localhost:9200/<index>/<type>/<id>
ex: http://localhost:9200/review/1/12
get all: http://localhost:9200/review/_search
count: http://localhost:9200/review/_count

- update. use PUT w/ the updated body
ex: http://localhost:9200/review/1/12

- delete, use DELETE method

- get/delete index. GET/DELETE on index.
  ex: http://localhost:9200/review


            2.1.4.6 ES search queries
entry point _search: http://localhost:9200/review/_search
for count, use _count.
$ cat movielens/movielens_query.json | pbcopy
//general format for query DSL 
{
	"query": {},
	"from": 0,
	"size": 20, 
	"sort" : [...],
	"_source": [...],
	"min_score" : ...,
	...
}



main query types:
//////////////////////////
///// TERM QUERIES
///// FULL TEXT QUERIES
///// COMPOUND QUERIES
///// AGGREGATE QUERIES
//////////////////////////

                2.1.4.6.1 //term based query
//for numbers, dates, and enums as the query strings are not analyzed
//matching is done by exactness comparison
{
  "query": {
    "term" : {
      "genres" : "Action"
    }
  }
}

//matching more than one term at a time
{
  "query": {
  	"terms" : {
  		"genres" : ["Action", "Adventure"]
  	}
  }
}

//matching a range of values
{
  "query": {
  	"range" : {
  		"year" : {
            "gte" : 1998,
            "lte" : 2001
        }
  	}
  }
}

//we also have exist, prefix, wildcard, regexp, etc.

                2.1.4.6.2 ////////////////////////full-text query
//match all. all documents for which the pass predicate is true
{
	"query": {
		"match_all": {}
	}
}

//match none. all documents for which the pass predicate is false
{
	"query": {
		"match_none": {}
	}
}

//match. unless otherwise specified will use same analyzer as in mapping. multiple words treated as or
{
	"query": {
		"match": {
			"title" : "awesome light"
		}
	}
}

//match_phrase. multiple words are search in exact order
{
	"query": {
		"match_phrase": {
			"tag" : "super hero"
		}
	}
}

//multi match. Match against more than one property 
{
	"query": {
		"multi_match": {
			"query" : "awesome light",
			"fields" : ["genres", "tag", "title"]
		}
	}
}

//query_string
//this is where you play by Lucene's rules and syntaxt
{
	"query": {
		"query_string": {
			"default_field" : "title",
			"query" : "title:romance^2 +tag:lovely"
		}
	}
}

                2.1.4.6.3 //Compound queries
//these are queries that can contain other simple or compound queries
//use them when you want near-precision search results with 
//very specific search using complex rules

//we will look at only the bool query which is quite popular
{
  "query": {
    "bool": {
      "must": [...],
      "should": [...],
      "must_not": [...],
      "filter": [...]
    }
  }
}
// example
{
  "query": {
    "bool": {
      "must": [
      	{
      		"match_all": {}	
      	}
      ],
      "should": [
      	{
      		"match_phrase": {
      			"tag" : "super heroes"
      		}	
      	}
      ],
      "must_not": [
      	{
      		"match_none": {}	
      	}
      ],
      "filter": [
      	{
      		"terms" : {
      			"genres" : ["Action", "Adventure", "Sci-Fi", "Comedy"]
      		}
      	},
      	{
      		"range" : {
      			"year" : {
	      			"gte" : 1990,
	      			"lte" : 2000
	      		}
      		}
      	}
      ]
    }
  }
}

// other compound query opertion include constant_score, function_score, and much more.

                2.1.4.6.3 aggregation queries
cat movielens/movielens_analytical_query.json | pbcopy

                //the general structure of aggregation query is 
{
	"query": { … },
	"aggregations" : { … }
}

//we already seen query in action, so lets focus on aggregations
//the value of aggregations should follow this format
{
	"<aggregation_name>" : {
        "<aggregation_type>" : {
            <aggregation_body>
        }
        [,"aggregations" : { [<sub_aggregation>]+ } ]?
    },
	"<aggregation_name_2>" : {
        "<aggregation_type>" : {
            <aggregation_body>
        }
        [,"aggregations" : { [<sub_aggregation_2>]+ } ]?
    }
}

//////////////////////////
///// BUCKETING
///// METRICS
///// MATRIX
///// PIPELINE
//////////////////////////

//BUCKETING EXAMPLE
{
	"query" : {
		"match_all" : {}
    },
    "size" : 0,
    "aggregations" : {
        "genre_hist_counting" : {
            "terms" : { 
                "field" : "genres",
                "size"  : 15
            } 
        }
    }
}


//METRIC EXAMPLE
{
	"query" : {
		"match_all" : {}
    },
    "size" : 0,
    "aggregations" : {
        "genre_hist_counting" : {
            "terms" : { 
                "field" : "genres",
                "size"  : 20
            }, 
            "aggregations" : {
                "statistics" : {
                    "stats" : { 
                        "field" : "rating"
                    } 
                }
            }
        }
    }
}


                2.1.4.6.4

            2.1.4.7 ELK stack
ELK - Elasticsearch, logstash, Kibana

docker pull elasticsearch:7.10.1
docker network create elknetwork
docker run -d --name elasticsearch --net elknetwork -p 9200:9200 -p 9300:9300 -e "discovery.type=single-node" elasticsearch:7.10.1

docker pull logstash:7.10.1
docker pull kibana:7.10.1
docker run -d --name kibana --link elasticsearch:elasticsearch --net elknetwork -p 5601:5601 kibana:7.10.1

[i500695@C02X632CJGH6:2020-12-13 18:04:15:~/work/tools/elasticsearch-docker:]2029$  docker logs --follow kibana
...
{"type":"log","@timestamp":"2020-12-13T16:01:34Z","tags":["listening","info"],"pid":7,"message":"Server running at http://0:5601"}
{"type":"log","@timestamp":"2020-12-13T16:01:35Z","tags":["info","http","server","Kibana"],"pid":7,"message":"http server running at http://0:5601"}
{"type":"log","@timestamp":"2020-12-13T16:01:37Z","tags":["warning","plugins","reporting"],"pid":7,"message":"Enabling the Chromium sandbox provides an additional layer of protection."}

http://localhost:5601/ - access kibana

            2.1.4.8 UFO sightings demo
files:
/Users/i500695/work/courses/sql_nosql_bigdata/rdbms_2_nosql/elasticsearch/ufo

mapping:
$ cat ufo_mappings.json | pbcopy
//create the ufo mapping
PUT http://localhost:9200/ufo, use postman or $ curl --location --request PUT 'http://localhost:9200/ufo
' \
--header 'Content-Type: application/json' \
--data-raw '<the json mapping>'
mapping:
{
	"mappings" : {
		"properties" : {
			"datetime" : {"type": "date", "store": true, "format": "M/d/yyyy HH:mm"},
			"city" : {"type":"keyword", "store": true},
			"state" : {"type":"keyword", "store": true},
			"country" : {"type":"keyword", "store": true},
			"shape" : {"type":"keyword", "store": true},
			"duration_in_sec" : {"type" : "integer","store": true},
			"duration_in_hrs_mins" : {"type" : "text", "store" : false, "index":"false"},
			"comments" : {"type":"text", "store": true, "index":"true", "analyzer" : "snowball"},
			"date_posted" : {"type" : "date", "store" : true, "format":"M/d/yyyy", "ignore_malformed": "true"},
			"location" : {"type" : "geo_point", "ignore_malformed" : true}
		}
	},
	"settings" : {
		"number_of_shards" : 2,
		"number_of_replicas" : 1,
		"index" : {
			"analysis" : {
				"analyzer" : {
					"like_keyword_analyzer" : {
						"type" : "custom",
						"tokenizer" : "keyword",
						"filters" :  ["lowercase"]
					}
				}
			}
		}
	}
}

index creation response:
{
    "acknowledged": true,
    "shards_acknowledged": true,
    "index": "ufo\n"
}

//index a document into the ufo index and sighting type
{
	"datetime" : "10/10/1996 22:30",
	"city" : "monroe county",
	"state" : "oh",
	"country" : "us",
	"shape" : "cylinder",
	"duration_in_sec" : 60,
	"duration_in_hrs_mins" : "1 minute",
	"comments" : "Looked like it went through the hillside",
	"date_posted" : "7/8/2004",
	"latitude" : 39.4402778,
	"longitude" : -84.3622222
}

//write a scala application that will convert the complete.csv file to a json file for bulk import

the json file for bulk upload has 2 lines per entry. one for the index and one for the actuall data.
file:
$ ls complete.es.zip


//unzip and upload
curl -XPOST 'localhost:9200/_bulk?pretty' -H "Content-Type: application/json" --data-binary @complete.es.json

response:
...
    {
      "index" : {
        "_index" : "ufo",
        "_type" : "_doc",
        "_id" : "88875",
        "_version" : 1,
        "result" : "created",
        "_shards" : {
          "total" : 2,
          "successful" : 1,
          "failed" : 0
        },
        "_seq_no" : 88843,
        "_primary_term" : 1,
        "status" : 201
      }
    }
  ]
}

in kibana, create space, http://localhost:5601/app/management/kibana/spaces/create.
call it ufo
create index pattern
in index pattern name set ufo

now, you can discover and visualize.
example visualization:
choose new visualization -> horizontal bar -> source: ufo -> 
metrics. y-axis no change
buckets -> x-axis -> aggregations: term -> histogram ->



            2.1.4.9

id=__Wide_column_store__
        2.1.5 Wide column store

has rows like regular DBs but their columns are not fixed.
each column is a name plus k,v pair
no need for null values. no value, no need for column.
good for sparse dataset
schemaless
good for horizontal scaling
examples. google big table, amazon dynamodb, cassandra, hbase etc

            2.1.5.1 hbase
modeled after googles big table
very fast lookups, even on bilions of entries
runs on HDFS (Hadoop distributed FS)
Primary, masters topology.
regions are DB distribution (akin to sharding in normal DB). see hbase architecture note
each region has log and stores. the stores have store files that use HFile and use DFS clients to access Hadoop to access DataNode
no data types. just binary bytes

HLog is append only read ahead log
memstore is a buffer for writes 
hfile, the data on Hadoop

namespace - akin to db. grouping of tables.
tables are described using DDL or java
specify column-families aka CF
structure

row-key-> CF1, CF2, ..., CFN
each CF can have columns C1,...,Ck

row-key are sorted
cell, pointed by 4 key set {row-key, cf, cell name, timestamp}
read will return latest timestamp value

uses bloom filter (probablistic data structure that uses hashes to give close to 100% accuracy answer to question whether an element is in a set)
no answer is 100% sure
yes has a certain std err
used in hbase to answer whether a column is in a row

regions can be split. either automatically or by configuration.

            2.1.5.2 zookeeper
ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. All of these kinds of services are used in some form or another by distributed applications. Each time they are implemented there is a lot of work that goes into fixing the bugs and race conditions that are inevitable. Because of the difficulty of implementing these kinds of services, applications initially usually skimp on them, which make them brittle in the presence of change and difficult to manage. Even when done correctly, different implementations of these services lead to management complexity when the applications are deployed.

used by hbase, hdfs and kafka

hbase uses ZooKeeper for node health monitoring, shared vars etc

            2.1.5.3 model movielens in hbase
movie_id - move id + movie properties
genre - we need genre|movieId as key
ratings - movieId|userId

so, tables
movie -> CF(m) : title, year, generes cells, 

altMovie (genere|movieId) -> CF(a) movie ids (maybe copy of properties as well)

            2.1.5.4 hbase crud
thrift protocol.
java
hbase shell
list_namespaces
list
create_namespace 'mvl'
#create a table movie in namespace mvl
create 'mvl:movie', 'a'
create 'mvl:user', 'u'
create 'mvl:altmovie', 'gm'

cd /Users/i500695/work/courses/sql_nosql_bigdata/rdbms_2_nosql/hbase/
[i500695@C02X632CJGH6:2020-12-20 18:02:31:~/work/courses/sql_nosql_bigdata/rdbms_2_nosql/hbase:]2005$ ls movielens-hbase/run_code.sh 
movielens-hbase/run_code.sh

migration code:
movielens-hbase/src/main/java/com/okmich/movielens/hbase/Main.java

            2.1.5.5 SQL on hbase, Apache Phoenix
created by salesforce and donated to Apache

            2.1.5.6


id=__Time_Series_DB__
        2.1.6 Time Series DB
matrix/regular - measurements every x seconds/minutes etc
events/iregular - measurements when something happens (stock sells etc)

univariant - one var (temperature)
multivariant - more than one variant 

why do we need it?
a. auto detect regular vs. iregular
b. more granularity is required for recent data. So need to manage data
lifecycle
c. exploration / aggregations on TSDBs are so common that the queries are
highly optimized
d.  update not needed

examples:
InfluxDB, OpenTSDB (over HBASE), kdb+, Timescale (postgres), Graphite, Prometheos

            2.1.6.1 Influx DB

            2.1.6.2

        2.1.7 Graph DB

        2.1.8 Hadoop

        2.1.9 Big data SQL engines
    2.2


3. SQL Syntax tags: SQL Syntax

Sources: https://www.tutorialspoint.com/sql/

    3.1  Basic

        3.1.1   DDL

            3.1.1.1   CREATE Creates a new table, a view of a table, or other object in the database.

            3.1.1.2 ALTER Modifies an existing database object, such as a table.

            3.1.1.3 DROP Deletes an entire table, a view of a table or other objects in the database.

            3.1.1.4
        3.1.2 DML

            3.1.2.1 SELECT Retrieves certain records from one or more tables.

            3.1.2.2 INSERT Creates a record.

            3.1.2.3 UPDATE Modifies records.

            3.1.2.4 DELETE Deletes records.

            3.1.2.5
        3.1.3 DCL

            3.1.3.1 GRANT Gives a privilege to user.

            3.1.3.2 REVOKE Takes back privileges granted from user.

            3.1.3.3

        3.1.4 RDBMS traites

            3.1.4.1 constraints
Constraints are the rules enforced on data columns on a table. These are used to limit the type of data that can go into a table. This ensures the accuracy and reliability of the data in the database.

Constraints can either be column level or table level. Column level constraints are applied only to one column whereas, table level constraints are applied to the entire table.

Following are some of the most commonly used constraints available in SQL −

                3.1.4.1.1 NOT NULL Constraint − Ensures that a column cannot have a NULL value.
By default, a column can hold NULL values. If you do not want a column to have a NULL value, then you need to define such a constraint on this column specifying that NULL is now not allowed for that column.

A NULL is not the same as no data, rather, it represents unknown data.

Example
For example, the following SQL query creates a new table called CUSTOMERS and adds five columns, three of which, are ID NAME and AGE, In this we specify not to accept NULLs −

CREATE TABLE CUSTOMERS(
   ID   INT              NOT NULL,
   NAME VARCHAR (20)     NOT NULL,
   AGE  INT              NOT NULL,
   ADDRESS  CHAR (25) ,
   SALARY   DECIMAL (18, 2),       
   PRIMARY KEY (ID)
);
If CUSTOMERS table has already been created, then to add a NOT NULL constraint to the SALARY column in Oracle and MySQL, you would write a query like the one that is shown in the following code block.

ALTER TABLE CUSTOMERS
   MODIFY SALARY  DECIMAL (18, 2) NOT NULL;


                3.1.4.1.2 DEFAULT Constraint − Provides a default value for a column when none is specified.
The DEFAULT constraint provides a default value to a column when the INSERT INTO statement does not provide a specific value.

Example
For example, the following SQL creates a new table called CUSTOMERS and adds five columns. Here, the SALARY column is set to 5000.00 by default, so in case the INSERT INTO statement does not provide a value for this column, then by default this column would be set to 5000.00.

CREATE TABLE CUSTOMERS(
   ID   INT              NOT NULL,
   NAME VARCHAR (20)     NOT NULL,
   AGE  INT              NOT NULL,
   ADDRESS  CHAR (25) ,
   SALARY   DECIMAL (18, 2) DEFAULT 5000.00,       
   PRIMARY KEY (ID)
);
If the CUSTOMERS table has already been created, then to add a DEFAULT constraint to the SALARY column, you would write a query like the one which is shown in the code block below.

ALTER TABLE CUSTOMERS
MODIFY SALARY  DECIMAL (18, 2) DEFAULT 5000.00; 
Drop Default Constraint
To drop a DEFAULT constraint, use the following SQL query.

ALTER TABLE CUSTOMERS
   ALTER COLUMN SALARY DROP DEFAULT;

                3.1.4.1.3 UNIQUE Constraint − Ensures that all the values in a column are different.
The UNIQUE Constraint prevents two records from having identical values in a column. In the CUSTOMERS table, for example, you might want to prevent two or more people from having an identical age.

Example
For example, the following SQL query creates a new table called CUSTOMERS and adds five columns. Here, the AGE column is set to UNIQUE, so that you cannot have two records with the same age.

CREATE TABLE CUSTOMERS(
   ID   INT              NOT NULL,
   NAME VARCHAR (20)     NOT NULL,
   AGE  INT              NOT NULL UNIQUE,
   ADDRESS  CHAR (25) ,
   SALARY   DECIMAL (18, 2),       
   PRIMARY KEY (ID)
);
If the CUSTOMERS table has already been created, then to add a UNIQUE constraint to the AGE column. You would write a statement like the query that is given in the code block below.

ALTER TABLE CUSTOMERS
   MODIFY AGE INT NOT NULL UNIQUE;
You can also use the following syntax, which supports naming the constraint in multiple columns as well.

ALTER TABLE CUSTOMERS
   ADD CONSTRAINT myUniqueConstraint UNIQUE(AGE, SALARY);
DROP a UNIQUE Constraint
To drop a UNIQUE constraint, use the following SQL query.

ALTER TABLE CUSTOMERS
   DROP CONSTRAINT myUniqueConstraint;
If you are using MySQL, then you can use the following syntax −

ALTER TABLE CUSTOMERS
   DROP INDEX myUniqueConstraint;

                3.1.4.1.4 PRIMARY Key − Uniquely identifies each row/record in a database table.
A primary key is a field in a table which uniquely identifies each row/record in a database table. Primary keys must contain unique values. A primary key column cannot have NULL values.

A table can have only one primary key, which may consist of single or multiple fields. When multiple fields are used as a primary key, they are called a composite key.

If a table has a primary key defined on any field(s), then you cannot have two records having the same value of that field(s).

Note − You would use these concepts while creating database tables.

Create Primary Key
Here is the syntax to define the ID attribute as a primary key in a CUSTOMERS table.

CREATE TABLE CUSTOMERS(
   ID   INT              NOT NULL,
   NAME VARCHAR (20)     NOT NULL,
   AGE  INT              NOT NULL,
   ADDRESS  CHAR (25) ,
   SALARY   DECIMAL (18, 2),       
   PRIMARY KEY (ID)
);
To create a PRIMARY KEY constraint on the "ID" column when the CUSTOMERS table already exists, use the following SQL syntax −

ALTER TABLE CUSTOMER ADD PRIMARY KEY (ID);
NOTE − If you use the ALTER TABLE statement to add a primary key, the primary key column(s) should have already been declared to not contain NULL values (when the table was first created).

For defining a PRIMARY KEY constraint on multiple columns, use the SQL syntax given below.

CREATE TABLE CUSTOMERS(
   ID   INT              NOT NULL,
   NAME VARCHAR (20)     NOT NULL,
   AGE  INT              NOT NULL,
   ADDRESS  CHAR (25) ,
   SALARY   DECIMAL (18, 2),        
   PRIMARY KEY (ID, NAME)
);
To create a PRIMARY KEY constraint on the "ID" and "NAMES" columns when CUSTOMERS table already exists, use the following SQL syntax.

ALTER TABLE CUSTOMERS 
   ADD CONSTRAINT PK_CUSTID PRIMARY KEY (ID, NAME);
Delete Primary Key
You can clear the primary key constraints from the table with the syntax given below.

ALTER TABLE CUSTOMERS DROP PRIMARY KEY ;
                3.1.4.1.5 FOREIGN Key − Uniquely identifies a row/record in any another database table.
A foreign key is a key used to link two tables together. This is sometimes also called as a referencing key.

A Foreign Key is a column or a combination of columns whose values match a Primary Key in a different table.

The relationship between 2 tables matches the Primary Key in one of the tables with a Foreign Key in the second table.

If a table has a primary key defined on any field(s), then you cannot have two records having the same value of that field(s).

Example
Consider the structure of the following two tables.

CUSTOMERS table

CREATE TABLE CUSTOMERS(
   ID   INT              NOT NULL,
   NAME VARCHAR (20)     NOT NULL,
   AGE  INT              NOT NULL,
   ADDRESS  CHAR (25) ,
   SALARY   DECIMAL (18, 2),       
   PRIMARY KEY (ID)
);
ORDERS table

CREATE TABLE ORDERS (
   ID          INT        NOT NULL,
   DATE        DATETIME, 
   CUSTOMER_ID INT references CUSTOMERS(ID),
   AMOUNT     double,
   PRIMARY KEY (ID)
);
If the ORDERS table has already been created and the foreign key has not yet been set, the use the syntax for specifying a foreign key by altering a table.

ALTER TABLE ORDERS 
   ADD FOREIGN KEY (Customer_ID) REFERENCES CUSTOMERS (ID);
DROP a FOREIGN KEY Constraint
To drop a FOREIGN KEY constraint, use the following SQL syntax.

ALTER TABLE ORDERS
   DROP FOREIGN KEY;
                3.1.4.1.6 CHECK Constraint − The CHECK constraint ensures that all values in a column satisfy certain conditions.
The CHECK Constraint enables a condition to check the value being entered into a record. If the condition evaluates to false, the record violates the constraint and isn't entered the table.

Example
For example, the following program creates a new table called CUSTOMERS and adds five columns. Here, we add a CHECK with AGE column, so that you cannot have any CUSTOMER who is below 18 years.

CREATE TABLE CUSTOMERS(
   ID   INT              NOT NULL,
   NAME VARCHAR (20)     NOT NULL,
   AGE  INT              NOT NULL CHECK (AGE >= 18),
   ADDRESS  CHAR (25) ,
   SALARY   DECIMAL (18, 2),       
   PRIMARY KEY (ID)
);
If the CUSTOMERS table has already been created, then to add a CHECK constraint to AGE column, you would write a statement like the one given below.

ALTER TABLE CUSTOMERS
   MODIFY AGE INT NOT NULL CHECK (AGE >= 18 );
You can also use the following syntax, which supports naming the constraint in multiple columns as well −

ALTER TABLE CUSTOMERS
   ADD CONSTRAINT myCheckConstraint CHECK(AGE >= 18);
DROP a CHECK Constraint
To drop a CHECK constraint, use the following SQL syntax. This syntax does not work with MySQL.

ALTER TABLE CUSTOMERS
   DROP CONSTRAINT myCheckConstraint;
                3.1.4.1.7 INDEX − Used to create and retrieve data from the database very quickly.
The INDEX is used to create and retrieve data from the database very quickly. An Index can be created by using a single or group of columns in a table. When the index is created, it is assigned a ROWID for each row before it sorts out the data.

Proper indexes are good for performance in large databases, but you need to be careful while creating an index. A Selection of fields depends on what you are using in your SQL queries.

Example
For example, the following SQL syntax creates a new table called CUSTOMERS and adds five columns in it.

CREATE TABLE CUSTOMERS(
   ID   INT              NOT NULL,
   NAME VARCHAR (20)     NOT NULL,
   AGE  INT              NOT NULL,
   ADDRESS  CHAR (25) ,
   SALARY   DECIMAL (18, 2),       
   PRIMARY KEY (ID)
);
Now, you can create an index on a single or multiple columns using the syntax given below.

CREATE INDEX index_name
   ON table_name ( column1, column2.....);
To create an INDEX on the AGE column, to optimize the search on customers for a specific age, you can use the follow SQL syntax which is given below −

CREATE INDEX idx_age
   ON CUSTOMERS ( AGE );
DROP an INDEX Constraint
To drop an INDEX constraint, use the following SQL syntax.

ALTER TABLE CUSTOMERS
   DROP INDEX idx_age;
                3.1.4.1.8
            3.1.4.2 Data Integrity
The following categories of data integrity exist with each RDBMS −

Entity Integrity − There are no duplicate rows in a table.

Domain Integrity − Enforces valid entries for a given column by restricting the type, the format, or the range of values.

Referential integrity − Rows cannot be deleted, which are used by other records.

User-Defined Integrity − Enforces some specific business rules that do not fall into entity, domain or referential integrity.

            3.1.4.3 Database Normalization
Database normalization is the process of efficiently organizing data in a database. There are two reasons of this normalization process −

Eliminating redundant data, for example, storing the same data in more than one table.

Ensuring data dependencies make sense.

Both these reasons are worthy goals as they reduce the amount of space a database consumes and ensures that data is logically stored. Normalization consists of a series of guidelines that help guide you in creating a good database structure.

Normalization guidelines are divided into normal forms; think of a form as the format or the way a database structure is laid out. The aim of normal forms is to organize the database structure, so that it complies with the rules of first normal form, then second normal form and finally the third normal form.

It is your choice to take it further and go to the fourth normal form, fifth normal form and so on, but in general, the third normal form is more than enough.

                3.1.4.3.1 First Normal Form (1NF)
The First normal form (1NF) sets basic rules for an organized database −

Define the data items required, because they become the columns in a table.

Place the related data items in a table.

Ensure that there are no repeating groups of data.

Ensure that there is a primary key.

First Rule of 1NF
You must define the data items. This means looking at the data to be stored, organizing the data into columns, defining what type of data each column contains and then finally putting the related columns into their own table.

For example, you put all the columns relating to locations of meetings in the Location table, those relating to members in the MemberDetails table and so on.

Second Rule of 1NF
The next step is ensuring that there are no repeating groups of data. Consider we have the following table −

CREATE TABLE CUSTOMERS(
   ID   INT              NOT NULL,
   NAME VARCHAR (20)     NOT NULL,
   AGE  INT              NOT NULL,
   ADDRESS  CHAR (25),
   ORDERS   VARCHAR(155)
);
So, if we populate this table for a single customer having multiple orders, then it would be something as shown below −

ID	NAME	AGE	ADDRESS	ORDERS
100	Sachin	36	Lower West Side	Cannon XL-200
100	Sachin	36	Lower West Side	Battery XL-200
100	Sachin	36	Lower West Side	Tripod Large
But as per the 1NF, we need to ensure that there are no repeating groups of data. So, let us break the above table into two parts and then join them using a key as shown in the following program −

CUSTOMERS table −

CREATE TABLE CUSTOMERS(
   ID   INT              NOT NULL,
   NAME VARCHAR (20)     NOT NULL,
   AGE  INT              NOT NULL,
   ADDRESS  CHAR (25),
   PRIMARY KEY (ID)
);
This table would have the following record −

ID	NAME	AGE	ADDRESS
100	Sachin	36	Lower West Side
ORDERS table −

CREATE TABLE ORDERS(
   ID   INT              NOT NULL,
   CUSTOMER_ID INT       NOT NULL,
   ORDERS   VARCHAR(155),
   PRIMARY KEY (ID)
);
This table would have the following records −

ID	CUSTOMER_ID	ORDERS
10	100	Cannon XL-200
11	100	Battery XL-200
12	100	Tripod Large
Third Rule of 1NF
The final rule of the first normal form, create a primary key for each table which we have already created.

                3.1.4.3.2 Second Normal Form (2NF)
The Second Normal Form states that it should meet all the rules for 1NF and there must be no partial dependences of any of the columns on the primary key −

Consider a customer-order relation and you want to store customer ID, customer name, order ID and order detail and the date of purchase −

CREATE TABLE CUSTOMERS(
   CUST_ID    INT              NOT NULL,
   CUST_NAME VARCHAR (20)      NOT NULL,
   ORDER_ID   INT              NOT NULL,
   ORDER_DETAIL VARCHAR (20)  NOT NULL,
   SALE_DATE  DATETIME,
   PRIMARY KEY (CUST_ID, ORDER_ID)
);
This table is in the first normal form; in that it obeys all the rules of the first normal form. In this table, the primary key consists of the CUST_ID and the ORDER_ID. Combined, they are unique assuming the same customer would hardly order the same thing.

However, the table is not in the second normal form because there are partial dependencies of primary keys and columns. CUST_NAME is dependent on CUST_ID and there's no real link between a customer's name and what he purchased. The order detail and purchase date are also dependent on the ORDER_ID, but they are not dependent on the CUST_ID, because there is no link between a CUST_ID and an ORDER_DETAIL or their SALE_DATE.

To make this table comply with the second normal form, you need to separate the columns into three tables.

First, create a table to store the customer details as shown in the code block below −

CREATE TABLE CUSTOMERS(
   CUST_ID    INT              NOT NULL,
   CUST_NAME VARCHAR (20)      NOT NULL,
   PRIMARY KEY (CUST_ID)
);
The next step is to create a table to store the details of each order −

CREATE TABLE ORDERS(
   ORDER_ID   INT              NOT NULL,
   ORDER_DETAIL VARCHAR (20)  NOT NULL,
   PRIMARY KEY (ORDER_ID)
);
Finally, create a third table storing just the CUST_ID and the ORDER_ID to keep a track of all the orders for a customer −

CREATE TABLE CUSTMERORDERS(
   CUST_ID    INT              NOT NULL,
   ORDER_ID   INT              NOT NULL,
   SALE_DATE  DATETIME,
   PRIMARY KEY (CUST_ID, ORDER_ID)
);
                3.1.4.3.3 Third Normal Form (3NF)
A table is in a third normal form when the following conditions are met −

It is in second normal form.
All nonprimary fields are dependent on the primary key.
The dependency of these non-primary fields is between the data. For example, in the following table – the street name, city and the state are unbreakably bound to their zip code.

CREATE TABLE CUSTOMERS(
   CUST_ID       INT              NOT NULL,
   CUST_NAME     VARCHAR (20)      NOT NULL,
   DOB           DATE,
   STREET        VARCHAR(200),
   CITY          VARCHAR(100),
   STATE         VARCHAR(100),
   ZIP           VARCHAR(12),
   EMAIL_ID      VARCHAR(256),
   PRIMARY KEY (CUST_ID)
);
The dependency between the zip code and the address is called as a transitive dependency. To comply with the third normal form, all you need to do is to move the Street, City and the State fields into their own table, which you can call as the Zip Code table. −

CREATE TABLE ADDRESS(
   ZIP           VARCHAR(12),
   STREET        VARCHAR(200),
   CITY          VARCHAR(100),
   STATE         VARCHAR(100),
   PRIMARY KEY (ZIP)
);
The next step is to alter the CUSTOMERS table as shown below −

CREATE TABLE CUSTOMERS(
   CUST_ID       INT              NOT NULL,
   CUST_NAME     VARCHAR (20)      NOT NULL,
   DOB           DATE,
   ZIP           VARCHAR(12),
   EMAIL_ID      VARCHAR(256),
   PRIMARY KEY (CUST_ID)
);
The advantages of removing transitive dependencies are mainly two-fold. First, the amount of data duplication is reduced and therefore your database becomes smaller.

The second advantage is data integrity. When duplicated data changes, there is a big risk of updating only some of the data, especially if it is spread out in many different places in the database.

For example, if the address and the zip code data were stored in three or four different tables, then any changes in the zip codes would need to ripple out to every record in those three or four tables.

                3.1.4.3.4 
            3.1.4.4 popular RDBMS databases

            3.1.4.5 MySQL
MySQL is an open source SQL database, which is developed by a Swedish company – MySQL AB. MySQL is pronounced as "my ess-que-ell," in contrast with SQL, pronounced "sequel."

MySQL is supporting many different platforms including Microsoft Windows, the major Linux distributions, UNIX, and Mac OS X.

MySQL has free and paid versions, depending on its usage (non-commercial/commercial) and features. MySQL comes with a very fast, multi-threaded, multi-user and robust SQL database server.

History
Development of MySQL by Michael Widenius & David Axmark beginning in 1994.

First internal release on 23rd May 1995.

Windows Version was released on the 8th January 1998 for Windows 95 and NT.

Version 3.23: beta from June 2000, production release January 2001.

Version 4.0: beta from August 2002, production release March 2003 (unions).

Version 4.1: beta from June 2004, production release October 2004.

Version 5.0: beta from March 2005, production release October 2005.

Sun Microsystems acquired MySQL AB on the 26th February 2008.

Version 5.1: production release 27th November 2008.

Features
High Performance.
High Availability.
Scalability and Flexibility Run anything.
Robust Transactional Support.
Web and Data Warehouse Strengths.
Strong Data Protection.
Comprehensive Application Development.
Management Ease.
Open Source Freedom and 24 x 7 Support.
Lowest Total Cost of Ownership.


            3.1.4.6 MS SQL Server
MS SQL Server is a Relational Database Management System developed by Microsoft Inc. Its primary query languages are −

T-SQL
ANSI SQL
History
1987 - Sybase releases SQL Server for UNIX.

1988 - Microsoft, Sybase, and Aston-Tate port SQL Server to OS/2.

1989 - Microsoft, Sybase, and Aston-Tate release SQL Server 1.0 for OS/2.

1990 - SQL Server 1.1 is released with support for Windows 3.0 clients.

Aston - Tate drops out of SQL Server development.

2000 - Microsoft releases SQL Server 2000.

2001 - Microsoft releases XML for SQL Server Web Release 1 (download).

2002 - Microsoft releases SQLXML 2.0 (renamed from XML for SQL Server).

2002 - Microsoft releases SQLXML 3.0.

2005 - Microsoft releases SQL Server 2005 on November 7th, 2005.

Features
High Performance
High Availability
Database mirroring
Database snapshots
CLR integration
Service Broker
DDL triggers
Ranking functions
Row version-based isolation levels
XML integration
TRY...CATCH
Database Mail

            3.1.4.7 ORACLE
It is a very large multi-user based database management system. Oracle is a relational database management system developed by 'Oracle Corporation'.

Oracle works to efficiently manage its resources, a database of information among the multiple clients requesting and sending data in the network.

It is an excellent database server choice for client/server computing. Oracle supports all major operating systems for both clients and servers, including MSDOS, NetWare, UnixWare, OS/2 and most UNIX flavors.

History
Oracle began in 1977 and celebrating its 32 wonderful years in the industry (from 1977 to 2009).

1977 - Larry Ellison, Bob Miner and Ed Oates founded Software Development Laboratories to undertake development work.

1979 - Version 2.0 of Oracle was released and it became first commercial relational database and first SQL database. The company changed its name to Relational Software Inc. (RSI).

1981 - RSI started developing tools for Oracle.

1982 - RSI was renamed to Oracle Corporation.

1983 - Oracle released version 3.0, rewritten in C language and ran on multiple platforms.

1984 - Oracle version 4.0 was released. It contained features like concurrency control - multi-version read consistency, etc.

1985 - Oracle version 4.0 was released. It contained features like concurrency control - multi-version read consistency, etc.

2007 - Oracle released Oracle11g. The new version focused on better partitioning, easy migration, etc.

Features
Concurrency
Read Consistency
Locking Mechanisms
Quiesce Database
Portability
Self-managing database
SQL*Plus
ASM
Scheduler
Resource Manager
Data Warehousing
Materialized views
Bitmap indexes
Table compression
Parallel Execution
Analytic SQL
Data mining
Partitioning
MS ACCESS
This is one of the most popular Microsoft products. Microsoft Access is an entry-level database management software. MS Access database is not only inexpensive but also a powerful database for small-scale projects.

MS Access uses the Jet database engine, which utilizes a specific SQL language dialect (sometimes referred to as Jet SQL).

MS Access comes with the professional edition of MS Office package. MS Access has easyto-use intuitive graphical interface.

1992 - Access version 1.0 was released.

1993 - Access 1.1 released to improve compatibility with inclusion the Access Basic programming language.

The most significant transition was from Access 97 to Access 2000.

2007 - Access 2007, a new database format was introduced ACCDB which supports complex data types such as multi valued and attachment fields.

Features
Users can create tables, queries, forms and reports and connect them together with macros.

Option of importing and exporting the data to many formats including Excel, Outlook, ASCII, dBase, Paradox, FoxPro, SQL Server, Oracle, ODBC, etc.

There is also the Jet Database format (MDB or ACCDB in Access 2007), which can contain the application and data in one file. This makes it very convenient to distribute the entire application to another user, who can run it in disconnected environments.

Microsoft Access offers parameterized queries. These queries and Access tables can be referenced from other programs like VB6 and .NET through DAO or ADO.

The desktop editions of Microsoft SQL Server can be used with Access as an alternative to the Jet Database Engine.

Microsoft Access is a file server-based database. Unlike the client-server relational database management systems (RDBMS), Microsoft Access does not implement database triggers, stored procedures or transaction logging.

            3.1.4.5
        3.1.5 Syntax

            3.1.5.1 SQL SELECT Statement
SELECT column1, column2....columnN
FROM   table_name;

practice: https://www.w3schools.com/sql/trysql.asp?filename=trysql_asc
SELECT  CustomerName,ContactName FROM Customers
ORDER BY CustomerName ASC;
Number of Records: 91
CustomerName	ContactName
Alfreds Futterkiste	Maria Anders
Ana Trujillo Emparedados y helados	Ana Trujillo
Antonio Moreno Taquería	Antonio Moreno


            3.1.5.2 SQL DISTINCT Clause
SELECT DISTINCT column1, column2....columnN
FROM   table_name;

            3.1.5.3 SQL WHERE Clause
SELECT column1, column2....columnN
FROM   table_name
WHERE  CONDITION;
SELECT  * FROM Categories WHERE CategoryName like '%rages';
Result:
Number of Records: 1
CategoryID	CategoryName	Description
1	Beverages	Soft drinks, coffees, teas, beers, and ales

            3.1.5.4 SQL AND/OR Clause
SELECT column1, column2....columnN
FROM   table_name
WHERE  CONDITION-1 {AND|OR} CONDITION-2;
SELECT  * FROM Categories WHERE  CategoryName='Beverages' or CategoryName like '%meat%';
    
Number of Records: 2
CategoryID	CategoryName	Description
1	Beverages	Soft drinks, coffees, teas, beers, and ales
6	Meat/Poultry	Prepared meats

            3.1.5.5 SQL IN Clause
SELECT column1, column2....columnN
FROM   table_name
WHERE  column_name IN (val-1, val-2,...val-N);

            3.1.5.6 SQL BETWEEN Clause
SELECT column1, column2....columnN
FROM   table_name
WHERE  column_name BETWEEN val-1 AND val-2;

            3.1.5.7 SQL LIKE Clause
SELECT column1, column2....columnN
FROM   table_name
WHERE  column_name LIKE { PATTERN };

            3.1.5.8 SQL ORDER BY Clause
SELECT column1, column2....columnN
FROM   table_name
WHERE  CONDITION
ORDER BY column_name {ASC|DESC};

            3.1.5.9 SQL GROUP BY Clause
SELECT SUM(column_name)
FROM   table_name
WHERE  CONDITION
GROUP BY column_name;

            3.1.5.10 SQL COUNT Clause
SELECT COUNT(column_name)
FROM   table_name
WHERE  CONDITION;

            3.1.5.11 SQL HAVING Clause
SELECT SUM(column_name)
FROM   table_name
WHERE  CONDITION
GROUP BY column_name
HAVING (arithematic function condition);

            3.1.5.12 SQL CREATE TABLE Statement
CREATE TABLE table_name(
column1 datatype,
column2 datatype,
column3 datatype,
.....
columnN datatype,
PRIMARY KEY( one or more columns )
);

            3.1.5.13 SQL DROP TABLE Statement
DROP TABLE table_name;

            3.1.5.14 SQL CREATE INDEX Statement
CREATE UNIQUE INDEX index_name
ON table_name ( column1, column2,...columnN);

            3.1.5.15 SQL DROP INDEX Statement
ALTER TABLE table_name
DROP INDEX index_name;

            3.1.5.16 SQL DESC Statement
DESC table_name;

            3.1.5.17 SQL TRUNCATE TABLE Statement
TRUNCATE TABLE table_name;

            3.1.5.18 SQL ALTER TABLE Statement
ALTER TABLE table_name {ADD|DROP|MODIFY} column_name {data_ype};
SQL ALTER TABLE Statement (Rename)
ALTER TABLE table_name RENAME TO new_table_name;

            3.1.5.19 SQL INSERT INTO Statement
INSERT INTO table_name( column1, column2....columnN)
VALUES ( value1, value2....valueN);

            3.1.5.20 SQL UPDATE Statement
UPDATE table_name
SET column1 = value1, column2 = value2....columnN=valueN
[ WHERE  CONDITION ];

            3.1.5.21 SQL DELETE Statement
DELETE FROM table_name
WHERE  {CONDITION};

            3.1.5.22 SQL CREATE DATABASE Statement
CREATE DATABASE database_name;

            3.1.5.23 SQL DROP DATABASE Statement
DROP DATABASE database_name;

            3.1.5.24 SQL USE Statement
USE database_name;

            3.1.5.25 SQL COMMIT Statement
COMMIT;

            3.1.5.26 SQL ROLLBACK Statement
ROLLBACK;

        3.1.6 Data-types

        3.1.7 operators

        3.1.8 Expressions

        3.1.9 types 
Exact Numeric Data Types
DATA TYPE	FROM	TO
bigint	-9,223,372,036,854,775,808	9,223,372,036,854,775,807
int	-2,147,483,648	2,147,483,647
smallint	-32,768	32,767
tinyint	0	255
bit	0	1
decimal	-10^38 +1	10^38 -1
numeric	-10^38 +1	10^38 -1
money	-922,337,203,685,477.5808	+922,337,203,685,477.5807
smallmoney	-214,748.3648	+214,748.3647
Approximate Numeric Data Types

DATA TYPE	FROM	TO
float	-1.79E + 308	1.79E + 308
real	-3.40E + 38	3.40E + 38

Date and Time Data Types
DATA TYPE	FROM	TO
datetime	Jan 1, 1753	Dec 31, 9999
smalldatetime	Jan 1, 1900	Jun 6, 2079
date	Stores a date like June 30, 1991
time	Stores a time of day like 12:30 P.M.
Note − Here, datetime has 3.33 milliseconds accuracy where as smalldatetime has 1 minute accuracy.

Character Strings Data Types
Sr.No.	DATA TYPE & Description
1	char Maximum length of 8,000 characters.( Fixed length non-Unicode characters)

2	varchar Maximum of 8,000 characters.(Variable-length non-Unicode data).

3	varchar(max) Maximum length of 2E + 31 characters, Variable-length non-Unicode data (SQL Server 2005 only).

4	text Variable-length non-Unicode data with a maximum length of 2,147,483,647 characters.

Unicode Character Strings Data Types
Sr.No.	DATA TYPE & Description
1	nchar Maximum length of 4,000 characters.( Fixed length Unicode)

2	nvarchar Maximum length of 4,000 characters.(Variable length Unicode)

3	nvarchar(max) Maximum length of 2E + 31 characters (SQL Server 2005 only).( Variable length Unicode)

4	ntext Maximum length of 1,073,741,823 characters. ( Variable length Unicode )

        3.1.10 Operator in SQL
An operator is a reserved word or a character used primarily in an SQL statement's WHERE clause to perform operation(s), such as comparisons and arithmetic operations. These Operators are used to specify conditions in an SQL statement and to serve as conjunctions for multiple conditions in a statement.

Arithmetic operators
Comparison operators
Logical operators
Operators used to negate conditions
SQL Arithmetic Operators
Assume 'variable a' holds 10 and 'variable b' holds 20, then −

Show Examples

Operator	Description	Example
+ (Addition)	Adds values on either side of the operator.	a + b will give 30
- (Subtraction)	Subtracts right hand operand from left hand operand.	a - b will give -10
* (Multiplication)	Multiplies values on either side of the operator.	a * b will give 200
/ (Division)	Divides left hand operand by right hand operand.	b / a will give 2
% (Modulus)	Divides left hand operand by right hand operand and returns remainder.	b % a will give 0
SQL Comparison Operators
Assume 'variable a' holds 10 and 'variable b' holds 20, then −

Show Examples

Operator	Description	Example
=	Checks if the values of two operands are equal or not, if yes then condition becomes true.	(a = b) is not true.
!=	Checks if the values of two operands are equal or not, if values are not equal then condition becomes true.	(a != b) is true.
<>	Checks if the values of two operands are equal or not, if values are not equal then condition becomes true.	(a <> b) is true.
>	Checks if the value of left operand is greater than the value of right operand, if yes then condition becomes true.	(a > b) is not true.
<	Checks if the value of left operand is less than the value of right operand, if yes then condition becomes true.	(a < b) is true.
>=	Checks if the value of left operand is greater than or equal to the value of right operand, if yes then condition becomes true.	(a >= b) is not true.
<=	Checks if the value of left operand is less than or equal to the value of right operand, if yes then condition becomes true.	(a <= b) is true.
!<	Checks if the value of left operand is not less than the value of right operand, if yes then condition becomes true.	(a !< b) is false.
!>	Checks if the value of left operand is not greater than the value of right operand, if yes then condition becomes true.	(a !> b) is true.
SQL Logical Operators
Here is a list of all the logical operators available in SQL.

Show Examples

Sr.No.	Operator & Description
1	ALL The ALL operator is used to compare a value to all values in another value set.

2	AND The AND operator allows the existence of multiple conditions in an SQL statement's WHERE clause.

3	ANY The ANY operator is used to compare a value to any applicable value in the list as per the condition.

4	BETWEEN The BETWEEN operator is used to search for values that are within a set of values, given the minimum value and the maximum value.

5	EXISTS The EXISTS operator is used to search for the presence of a row in a specified table that meets a certain criterion.

6	IN The IN operator is used to compare a value to a list of literal values that have been specified.

7	LIKE The LIKE operator is used to compare a value to similar values using wildcard operators.

8	NOT The NOT operator reverses the meaning of the logical operator with which it is used. Eg: NOT EXISTS, NOT BETWEEN, NOT IN, etc. This is a negate operator.

9	OR The OR operator is used to combine multiple conditions in an SQL statement's WHERE clause.

10	IS NULL The NULL operator is used to compare a value with a NULL value.

11	UNIQUE The UNIQUE operator searches every row of a specified table for uniqueness (no duplicates).

        3.1.11 Expressions
Syntax
Consider the basic syntax of the SELECT statement as follows −

SELECT column1, column2, columnN 
FROM table_name 
WHERE [CONDITION|EXPRESSION];
There are different types of SQL expressions, which are mentioned below −

Boolean
Numeric
Date
Let us now discuss each of these in detail.

Boolean Expressions
SQL Boolean Expressions fetch the data based on matching a single value. Following is the syntax −

SELECT column1, column2, columnN 
FROM table_name 
WHERE SINGLE VALUE MATCHING EXPRESSION;
Consider the CUSTOMERS table having the following records −

SQL> SELECT * FROM CUSTOMERS;
+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
7 rows in set (0.00 sec)
The following table is a simple example showing the usage of various SQL Boolean Expressions −

SQL> SELECT * FROM CUSTOMERS WHERE SALARY = 10000;
+----+-------+-----+---------+----------+
| ID | NAME  | AGE | ADDRESS | SALARY   |
+----+-------+-----+---------+----------+
|  7 | Muffy |  24 | Indore  | 10000.00 |
+----+-------+-----+---------+----------+
1 row in set (0.00 sec)
Numeric Expression
These expressions are used to perform any mathematical operation in any query. Following is the syntax −

SELECT numerical_expression as  OPERATION_NAME
[FROM table_name
WHERE CONDITION] ;
Here, the numerical_expression is used for a mathematical expression or any formula. Following is a simple example showing the usage of SQL Numeric Expressions −

SQL> SELECT (15 + 6) AS ADDITION
+----------+
| ADDITION |
+----------+
|       21 |
+----------+
1 row in set (0.00 sec)
There are several built-in functions like avg(), sum(), count(), etc., to perform what is known as the aggregate data calculations against a table or a specific table column.

SQL> SELECT COUNT(*) AS "RECORDS" FROM CUSTOMERS; 
+---------+
| RECORDS |
+---------+
|       7 |
+---------+
1 row in set (0.00 sec)
Date Expressions
Date Expressions return current system date and time values −

SQL>  SELECT CURRENT_TIMESTAMP;
+---------------------+
| Current_Timestamp   |
+---------------------+
| 2009-11-12 06:40:23 |
+---------------------+
1 row in set (0.00 sec)
Another date expression is as shown below −

SQL>  SELECT  GETDATE();;
+-------------------------+
| GETDATE                 |
+-------------------------+
| 2009-10-22 12:07:18.140 |
+-------------------------+
1 row in set (0.00 sec)

        3.1.12 The SQL CREATE, DROP and USE DATABASE statements

Syntax
The basic syntax of this CREATE DATABASE statement is as follows −

CREATE DATABASE DatabaseName;
Always the database name should be unique within the RDBMS.

Example
If you want to create a new database <testDB>, then the CREATE DATABASE statement would be as shown below −

SQL> CREATE DATABASE testDB;
Make sure you have the admin privilege before creating any database. Once a database is created, you can check it in the list of databases as follows −

SQL> SHOW DATABASES;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| AMROOD             |
| TUTORIALSPOINT     |
| mysql              |
| orig               |
| test               |
| testDB             |
+--------------------+
7 rows in set (0.00 sec)

The SQL DROP DATABASE statement is used to drop an existing database in SQL schema.

Syntax
The basic syntax of DROP DATABASE statement is as follows −

DROP DATABASE DatabaseName;
Always the database name should be unique within the RDBMS.

Example
If you want to delete an existing database <testDB>, then the DROP DATABASE statement would be as shown below −

SQL> DROP DATABASE testDB;
When you have multiple databases in your SQL Schema, then before starting your operation, you would need to select a database where all the operations would be performed.

The SQL USE statement is used to select any existing database in the SQL schema.

Syntax
The basic syntax of the USE statement is as shown below −

USE DatabaseName;

        3.1.13 create and drop table
Creating a basic table involves naming the table and defining its columns and each column's data type.

The SQL CREATE TABLE statement is used to create a new table.

Syntax
The basic syntax of the CREATE TABLE statement is as follows −

CREATE TABLE table_name(
   column1 datatype,
   column2 datatype,
   column3 datatype,
   .....
   columnN datatype,
   PRIMARY KEY( one or more columns )
);
CREATE TABLE is the keyword telling the database system what you want to do. In this case, you want to create a new table. The unique name or identifier for the table follows the CREATE TABLE statement.

Then in brackets comes the list defining each column in the table and what sort of data type it is. The syntax becomes clearer with the following example.

A copy of an existing table can be created using a combination of the CREATE TABLE statement and the SELECT statement. You can check the complete details at Create Table Using another Table.

Example
The following code block is an example, which creates a CUSTOMERS table with an ID as a primary key and NOT NULL are the constraints showing that these fields cannot be NULL while creating records in this table −

SQL> CREATE TABLE CUSTOMERS(
   ID   INT              NOT NULL,
   NAME VARCHAR (20)     NOT NULL,
   AGE  INT              NOT NULL,
   ADDRESS  CHAR (25) ,
   SALARY   DECIMAL (18, 2),       
   PRIMARY KEY (ID)
);
You can verify if your table has been created successfully by looking at the message displayed by the SQL server, otherwise you can use the DESC command as follows −

SQL> DESC CUSTOMERS;
+---------+---------------+------+-----+---------+-------+
| Field   | Type          | Null | Key | Default | Extra |
+---------+---------------+------+-----+---------+-------+
| ID      | int(11)       | NO   | PRI |         |       |
| NAME    | varchar(20)   | NO   |     |         |       |
| AGE     | int(11)       | NO   |     |         |       |
| ADDRESS | char(25)      | YES  |     | NULL    |       |
| SALARY  | decimal(18,2) | YES  |     | NULL    |       |
+---------+---------------+------+-----+---------+-------+
5 rows in set (0.00 sec)
Now, you have CUSTOMERS table available in your database which you can use to store the required information related to customers.

note re. numeric datatypes
Data type	Description
BIGINT	Integer numerical (no decimal). Precision 19
DECIMAL(p,s)	Exact numerical, precision p, scale s. Example: decimal(5,2) is a number that has 3 digits before the decimal and 2 digits after the decimal
NUMERIC(p,s)	Exact numerical, precision p, scale s. (Same as DECIMAL)

The SQL DROP TABLE statement is used to remove a table definition and all the data, indexes, triggers, constraints and permission specifications for that table.

NOTE − You should be very careful while using this command because once a table is deleted then all the information available in that table will also be lost forever.

Syntax
The basic syntax of this DROP TABLE statement is as follows −

DROP TABLE table_name;
Example
Let us first verify the CUSTOMERS table and then we will delete it from the database as shown below −

SQL> DESC CUSTOMERS;
+---------+---------------+------+-----+---------+-------+
| Field   | Type          | Null | Key | Default | Extra |
+---------+---------------+------+-----+---------+-------+
| ID      | int(11)       | NO   | PRI |         |       |
| NAME    | varchar(20)   | NO   |     |         |       |
| AGE     | int(11)       | NO   |     |         |       |
| ADDRESS | char(25)      | YES  |     | NULL    |       |
| SALARY  | decimal(18,2) | YES  |     | NULL    |       |
+---------+---------------+------+-----+---------+-------+
5 rows in set (0.00 sec)
This means that the CUSTOMERS table is available in the database, so let us now drop it as shown below.

SQL> DROP TABLE CUSTOMERS;
Query OK, 0 rows affected (0.01 sec)
Now, if you would try the DESC command, then you will get the following error −

SQL> DESC CUSTOMERS;
ERROR 1146 (42S02): Table 'TEST.CUSTOMERS' doesn't exist
Here, TEST is the database name which we are using for our examples.

        3.1.14 INSERT INTO
The SQL INSERT INTO Statement is used to add new rows of data to a table in the database.

Syntax
There are two basic syntaxes of the INSERT INTO statement which are shown below.

INSERT INTO TABLE_NAME (column1, column2, column3,...columnN)  
VALUES (value1, value2, value3,...valueN);
Here, column1, column2, column3,...columnN are the names of the columns in the table into which you want to insert the data.

You may not need to specify the column(s) name in the SQL query if you are adding values for all the columns of the table. But make sure the order of the values is in the same order as the columns in the table.

The SQL INSERT INTO syntax will be as follows −

INSERT INTO TABLE_NAME VALUES (value1,value2,value3,...valueN);
Example
The following statements would create six records in the CUSTOMERS table.

INSERT INTO CUSTOMERS (ID,NAME,AGE,ADDRESS,SALARY)
VALUES (1, 'Ramesh', 32, 'Ahmedabad', 2000.00 );

INSERT INTO CUSTOMERS (ID,NAME,AGE,ADDRESS,SALARY)
VALUES (2, 'Khilan', 25, 'Delhi', 1500.00 );

INSERT INTO CUSTOMERS (ID,NAME,AGE,ADDRESS,SALARY)
VALUES (3, 'kaushik', 23, 'Kota', 2000.00 );

INSERT INTO CUSTOMERS (ID,NAME,AGE,ADDRESS,SALARY)
VALUES (4, 'Chaitali', 25, 'Mumbai', 6500.00 );

INSERT INTO CUSTOMERS (ID,NAME,AGE,ADDRESS,SALARY)
VALUES (5, 'Hardik', 27, 'Bhopal', 8500.00 );

INSERT INTO CUSTOMERS (ID,NAME,AGE,ADDRESS,SALARY)
VALUES (6, 'Komal', 22, 'MP', 4500.00 );
You can create a record in the CUSTOMERS table by using the second syntax as shown below.

INSERT INTO CUSTOMERS 
VALUES (7, 'Muffy', 24, 'Indore', 10000.00 );
All the above statements would produce the following records in the CUSTOMERS table as shown below.

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
Populate one table using another table
You can populate the data into a table through the select statement over another table; provided the other table has a set of fields, which are required to populate the first table.

Here is the syntax −

INSERT INTO first_table_name [(column1, column2, ... columnN)] 
   SELECT column1, column2, ...columnN 
   FROM second_table_name
   [WHERE condition];

        3.1.15 The SQL SELECT statement is used to fetch the data from a database table which returns this data in the form of a result table. These result tables are called result-sets.

Syntax
The basic syntax of the SELECT statement is as follows −

SELECT column1, column2, columnN FROM table_name;
Here, column1, column2... are the fields of a table whose values you want to fetch. If you want to fetch all the fields available in the field, then you can use the following syntax.

SELECT * FROM table_name;
Example
Consider the CUSTOMERS table having the following records −

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
The following code is an example, which would fetch the ID, Name and Salary fields of the customers available in CUSTOMERS table.

SQL> SELECT ID, NAME, SALARY FROM CUSTOMERS;
This would produce the following result −

+----+----------+----------+
| ID | NAME     | SALARY   |
+----+----------+----------+
|  1 | Ramesh   |  2000.00 |
|  2 | Khilan   |  1500.00 |
|  3 | kaushik  |  2000.00 |
|  4 | Chaitali |  6500.00 |
|  5 | Hardik   |  8500.00 |
|  6 | Komal    |  4500.00 |
|  7 | Muffy    | 10000.00 |
+----+----------+----------+
If you want to fetch all the fields of the CUSTOMERS table, then you should use the following query.

SQL> SELECT * FROM CUSTOMERS;
This would produce the result as shown below.

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+

        3.1.16 The SQL WHERE clause is used to specify a condition while fetching the data from a single table or by joining with multiple tables. If the given condition is satisfied, then only it returns a specific value from the table. You should use the WHERE clause to filter the records and fetching only the necessary records.

The WHERE clause is not only used in the SELECT statement, but it is also used in the UPDATE, DELETE statement, etc., which we would examine in the subsequent chapters.

Syntax
The basic syntax of the SELECT statement with the WHERE clause is as shown below.

SELECT column1, column2, columnN 
FROM table_name
WHERE [condition]
You can specify a condition using the comparison or logical operators like >, <, =, LIKE, NOT, etc. The following examples would make this concept clear.

Example
Consider the CUSTOMERS table having the following records −

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
The following code is an example which would fetch the ID, Name and Salary fields from the CUSTOMERS table, where the salary is greater than 2000 −

SQL> SELECT ID, NAME, SALARY 
FROM CUSTOMERS
WHERE SALARY > 2000;
This would produce the following result −

+----+----------+----------+
| ID | NAME     | SALARY   |
+----+----------+----------+
|  4 | Chaitali |  6500.00 |
|  5 | Hardik   |  8500.00 |
|  6 | Komal    |  4500.00 |
|  7 | Muffy    | 10000.00 |
+----+----------+----------+
The following query is an example, which would fetch the ID, Name and Salary fields from the CUSTOMERS table for a customer with the name Hardik.

Here, it is important to note that all the strings should be given inside single quotes (''). Whereas, numeric values should be given without any quote as in the above example.
SQL> SELECT ID, NAME, SALARY 
FROM CUSTOMERS
WHERE NAME = 'Hardik';
This would produce the following result −

+----+----------+----------+
| ID | NAME     | SALARY   |
+----+----------+----------+
|  5 | Hardik   |  8500.00 |
+----+----------+----------+

        3.1.17 The SQL AND & OR operators are used to combine multiple conditions to narrow data in an SQL statement. These two operators are called as the conjunctive operators.

These operators provide a means to make multiple comparisons with different operators in the same SQL statement.

The AND Operator
The AND operator allows the existence of multiple conditions in an SQL statement's WHERE clause.

Syntax
The basic syntax of the AND operator with a WHERE clause is as follows −

SELECT column1, column2, columnN 
FROM table_name
WHERE [condition1] AND [condition2]...AND [conditionN];
You can combine N number of conditions using the AND operator. For an action to be taken by the SQL statement, whether it be a transaction or a query, all conditions separated by the AND must be TRUE.

Example
Consider the CUSTOMERS table having the following records −

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
Following is an example, which would fetch the ID, Name and Salary fields from the CUSTOMERS table, where the salary is greater than 2000 and the age is less than 25 years −

SQL> SELECT ID, NAME, SALARY 
FROM CUSTOMERS
WHERE SALARY > 2000 AND age < 25;

        3.1.18 The SQL UPDATE Query is used to modify the existing records in a table. 
You can use the WHERE clause with the UPDATE query to update the selected rows, otherwise all the rows would be affected.

Syntax
The basic syntax of the UPDATE query with a WHERE clause is as follows −

UPDATE table_name
SET column1 = value1, column2 = value2...., columnN = valueN
WHERE [condition];
You can combine N number of conditions using the AND or the OR operators.

Example
Consider the CUSTOMERS table having the following records −

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
The following query will update the ADDRESS for a customer whose ID number is 6 in the table.

SQL> UPDATE CUSTOMERS
SET ADDRESS = 'Pune'
WHERE ID = 6;
Now, the CUSTOMERS table would have the following records −

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | Pune      |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
If you want to modify all the ADDRESS and the SALARY column values in the CUSTOMERS table, you do not need to use the WHERE clause as the UPDATE query would be enough as shown in the following code block.

SQL> UPDATE CUSTOMERS
SET ADDRESS = 'Pune', SALARY = 1000.00;
Now, CUSTOMERS table would have the following records −

+----+----------+-----+---------+---------+
| ID | NAME     | AGE | ADDRESS | SALARY  |
+----+----------+-----+---------+---------+
|  1 | Ramesh   |  32 | Pune    | 1000.00 |
|  2 | Khilan   |  25 | Pune    | 1000.00 |
|  3 | kaushik  |  23 | Pune    | 1000.00 |
|  4 | Chaitali |  25 | Pune    | 1000.00 |
|  5 | Hardik   |  27 | Pune    | 1000.00 |
|  6 | Komal    |  22 | Pune    | 1000.00 |
|  7 | Muffy    |  24 | Pune    | 1000.00 |
+----+----------+-----+---------+---------+

        3.1.19 The SQL DELETE Query is used to delete the existing records from a table.

You can use the WHERE clause with a DELETE query to delete the selected rows, otherwise all the records would be deleted.

Syntax
The basic syntax of the DELETE query with the WHERE clause is as follows −

DELETE FROM table_name
WHERE [condition];
You can combine N number of conditions using AND or OR operators.

Example
Consider the CUSTOMERS table having the following records −

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
The following code has a query, which will DELETE a customer, whose ID is 6.

SQL> DELETE FROM CUSTOMERS
WHERE ID = 6;
Now, the CUSTOMERS table would have the following records.

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
If you want to DELETE all the records from the CUSTOMERS table, you do not need to use the WHERE clause and the DELETE query would be as follows −

SQL> DELETE FROM CUSTOMERS;
Now, the CUSTOMERS table would not have any record.

        3.1.20 Constraints are the rules enforced on the data columns of a table. 
        These are used to limit the type of data that can go into a table. This ensures the accuracy and reliability of the data in the database.

Constraints could be either on a column level or a table level. The column level constraints are applied only to one column, whereas the table level constraints are applied to the whole table.

Following are some of the most commonly used constraints available in SQL. These constraints have already been discussed in SQL - RDBMS Concepts chapter, but it’s worth to revise them at this point.

NOT NULL Constraint − Ensures that a column cannot have NULL value.

DEFAULT Constraint − Provides a default value for a column when none is specified.

UNIQUE Constraint − Ensures that all values in a column are different.

PRIMARY Key − Uniquely identifies each row/record in a database table.

FOREIGN Key − Uniquely identifies a row/record in any of the given database table.

CHECK Constraint − The CHECK constraint ensures that all the values in a column satisfies certain conditions.

INDEX − Used to create and retrieve data from the database very quickly.

Constraints can be specified when a table is created with the CREATE TABLE statement or you can use the ALTER TABLE statement to create constraints even after the table is created.

Dropping Constraints
Any constraint that you have defined can be dropped using the ALTER TABLE command with the DROP CONSTRAINT option.

For example, to drop the primary key constraint in the EMPLOYEES table, you can use the following command.

ALTER TABLE EMPLOYEES DROP CONSTRAINT EMPLOYEES_PK;
Some implementations may provide shortcuts for dropping certain constraints. For example, to drop the primary key constraint for a table in Oracle, you can use the following command.

ALTER TABLE EMPLOYEES DROP PRIMARY KEY;
Some implementations allow you to disable constraints. Instead of permanently dropping a constraint from the database, you may want to temporarily disable the constraint and then enable it later.

Integrity Constraints
Integrity constraints are used to ensure accuracy and consistency of the data in a relational database. Data integrity is handled in a relational database through the concept of referential integrity.

There are many types of integrity constraints that play a role in Referential Integrity (RI). These constraints include Primary Key, Foreign Key, Unique Constraints and other constraints which are mentioned above.

        3.1.21 The SQL Joins clause is used to combine records from two or more tables in a database. 
        A JOIN is a means for combining fields from two tables by using values common to each.

Consider the following two tables −

Table 1 − CUSTOMERS Table

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
Table 2 − ORDERS Table

+-----+---------------------+-------------+--------+
|OID  | DATE                | CUSTOMER_ID | AMOUNT |
+-----+---------------------+-------------+--------+
| 102 | 2009-10-08 00:00:00 |           3 |   3000 |
| 100 | 2009-10-08 00:00:00 |           3 |   1500 |
| 101 | 2009-11-20 00:00:00 |           2 |   1560 |
| 103 | 2008-05-20 00:00:00 |           4 |   2060 |
+-----+---------------------+-------------+--------+
Now, let us join these two tables in our SELECT statement as shown below.

SQL> SELECT ID, NAME, AGE, AMOUNT
   FROM CUSTOMERS, ORDERS
   WHERE  CUSTOMERS.ID = ORDERS.CUSTOMER_ID;
This would produce the following result.

+----+----------+-----+--------+
| ID | NAME     | AGE | AMOUNT |
+----+----------+-----+--------+
|  3 | kaushik  |  23 |   3000 |
|  3 | kaushik  |  23 |   1500 |
|  2 | Khilan   |  25 |   1560 |
|  4 | Chaitali |  25 |   2060 |
+----+----------+-----+--------+
Here, it is noticeable that the join is performed in the WHERE clause. Several operators can be used to join tables, such as =, <, >, <>, <=, >=, !=, BETWEEN, LIKE, and NOT; they can all be used to join tables. However, the most common operator is the equal to symbol.

There are different types of joins available in SQL −


            3.1.21.1 INNER JOIN − returns rows when there is a match in both tables.

                3.1.21.1.1 The most important and frequently used of the joins is the INNER JOIN. They are also referred to as an EQUIJOIN.

The INNER JOIN creates a new result table by combining column values of two tables (table1 and table2) based upon the join-predicate. The query compares each row of table1 with each row of table2 to find all pairs of rows which satisfy the join-predicate. When the join-predicate is satisfied, column values for each matched pair of rows of A and B are combined into a result row.

Syntax
The basic syntax of the INNER JOIN is as follows.

SELECT table1.column1, table2.column2...
FROM table1
INNER JOIN table2
ON table1.common_field = table2.common_field;
Example
Consider the following two tables.

Table 1 − CUSTOMERS Table is as follows.

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
Table 2 − ORDERS Table is as follows.

+-----+---------------------+-------------+--------+
| OID | DATE                | CUSTOMER_ID | AMOUNT |
+-----+---------------------+-------------+--------+
| 102 | 2009-10-08 00:00:00 |           3 |   3000 |
| 100 | 2009-10-08 00:00:00 |           3 |   1500 |
| 101 | 2009-11-20 00:00:00 |           2 |   1560 |
| 103 | 2008-05-20 00:00:00 |           4 |   2060 |
+-----+---------------------+-------------+--------+
Now, let us join these two tables using the INNER JOIN as follows −

SQL> SELECT  ID, NAME, AMOUNT, DATE
   FROM CUSTOMERS
   INNER JOIN ORDERS
   ON CUSTOMERS.ID = ORDERS.CUSTOMER_ID;
This would produce the following result.

+----+----------+--------+---------------------+
| ID | NAME     | AMOUNT | DATE                |
+----+----------+--------+---------------------+
|  3 | kaushik  |   3000 | 2009-10-08 00:00:00 |
|  3 | kaushik  |   1500 | 2009-10-08 00:00:00 |
|  2 | Khilan   |   1560 | 2009-11-20 00:00:00 |
|  4 | Chaitali |   2060 | 2008-05-20 00:00:00 |
+----+----------+--------+---------------------+

                3.1.21.1.2 my example 
https://www.mycompiler.io/new/sql
-- create a table
CREATE TABLE students (
  id INTEGER PRIMARY KEY,
  name TEXT NOT NULL,
  gender TEXT NOT NULL
);
CREATE TABLE exams (
  name TEXT NOT NULL,
  student_id TEXT NOT NULL,
  grade INTEGER NOT NULL
);

-- insert some values
INSERT INTO students VALUES (1, 'Ryan', 'M');
INSERT INTO students VALUES (2, 'Joanna', 'F');
-- fetch some values
SELECT * FROM students WHERE gender = 'F';
INSERT INTO exams values ('math', 1, 100);
INSERT INTO exams values ('math', 2, 90);
INSERT INTO exams values ('chemistry', 1, 65);
INSERT INTO exams values ('chemistry', 2, 93);
SELECT * FROM exams;

SELECT students.name, students.gender, exams.grade FROM students inner join exams on students.id = exams.student_id;

run:
2|Joanna|F
math|1|100
math|2|90
chemistry|1|65
chemistry|2|93
Ryan|M|100
Joanna|F|90
Ryan|M|65
Joanna|F|93 

                3.1.21.1.3
            3.1.21.2 LEFT JOIN − returns all rows from the left table, even if there are no matches in the right table.
The SQL LEFT JOIN returns all rows from the left table, even if there are no matches in the right table. This means that if the ON clause matches 0 (zero) records in the right table; the join will still return a row in the result, but with NULL in each column from the right table.

This means that a left join returns all the values from the left table, plus matched values from the right table or NULL in case of no matching join predicate.

Syntax
The basic syntax of a LEFT JOIN is as follows.

SELECT table1.column1, table2.column2...
FROM table1
LEFT JOIN table2
ON table1.common_field = table2.common_field;
Here, the given condition could be any given expression based on your requirement.

Example
Consider the following two tables,

Table 1 − CUSTOMERS Table is as follows.

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
Table 2 − Orders Table is as follows.

+-----+---------------------+-------------+--------+
| OID | DATE                | CUSTOMER_ID | AMOUNT |
+-----+---------------------+-------------+--------+
| 102 | 2009-10-08 00:00:00 |           3 |   3000 |
| 100 | 2009-10-08 00:00:00 |           3 |   1500 |
| 101 | 2009-11-20 00:00:00 |           2 |   1560 |
| 103 | 2008-05-20 00:00:00 |           4 |   2060 |
+-----+---------------------+-------------+--------+
Now, let us join these two tables using the LEFT JOIN as follows.

SQL> SELECT  ID, NAME, AMOUNT, DATE
   FROM CUSTOMERS
   LEFT JOIN ORDERS
   ON CUSTOMERS.ID = ORDERS.CUSTOMER_ID;
This would produce the following result −

+----+----------+--------+---------------------+
| ID | NAME     | AMOUNT | DATE                |
+----+----------+--------+---------------------+
|  1 | Ramesh   |   NULL | NULL                |
|  2 | Khilan   |   1560 | 2009-11-20 00:00:00 |
|  3 | kaushik  |   3000 | 2009-10-08 00:00:00 |
|  3 | kaushik  |   1500 | 2009-10-08 00:00:00 |
|  4 | Chaitali |   2060 | 2008-05-20 00:00:00 |
|  5 | Hardik   |   NULL | NULL                |
|  6 | Komal    |   NULL | NULL                |
|  7 | Muffy    |   NULL | NULL                |
+----+----------+--------+---------------------+

            3.1.21.3 RIGHT JOIN − returns all rows from the right table, even if there are no matches in the left table.
The SQL RIGHT JOIN returns all rows from the right table, even if there are no matches in the left table. This means that if the ON clause matches 0 (zero) records in the left table; the join will still return a row in the result, but with NULL in each column from the left table.

This means that a right join returns all the values from the right table, plus matched values from the left table or NULL in case of no matching join predicate.

Syntax
The basic syntax of a RIGHT JOIN is as follow.

SELECT table1.column1, table2.column2...
FROM table1
RIGHT JOIN table2
ON table1.common_field = table2.common_field;
Example
Consider the following two tables,

Table 1 − CUSTOMERS Table is as follows.

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
Table 2 − ORDERS Table is as follows.

+-----+---------------------+-------------+--------+
|OID  | DATE                | CUSTOMER_ID | AMOUNT |
+-----+---------------------+-------------+--------+
| 102 | 2009-10-08 00:00:00 |           3 |   3000 |
| 100 | 2009-10-08 00:00:00 |           3 |   1500 |
| 101 | 2009-11-20 00:00:00 |           2 |   1560 |
| 103 | 2008-05-20 00:00:00 |           4 |   2060 |
+-----+---------------------+-------------+--------+
Now, let us join these two tables using the RIGHT JOIN as follows.

SQL> SELECT  ID, NAME, AMOUNT, DATE
   FROM CUSTOMERS
   RIGHT JOIN ORDERS
   ON CUSTOMERS.ID = ORDERS.CUSTOMER_ID;
This would produce the following result −

+------+----------+--------+---------------------+
| ID   | NAME     | AMOUNT | DATE                |
+------+----------+--------+---------------------+
|    3 | kaushik  |   3000 | 2009-10-08 00:00:00 |
|    3 | kaushik  |   1500 | 2009-10-08 00:00:00 |
|    2 | Khilan   |   1560 | 2009-11-20 00:00:00 |
|    4 | Chaitali |   2060 | 2008-05-20 00:00:00 |
+------+----------+--------+---------------------+

            3.1.21.4 FULL JOIN − returns rows when there is a match in one of the tables.
The SQL FULL JOIN combines the results of both left and right outer joins.

The joined table will contain all records from both the tables and fill in NULLs for missing matches on either side.

Syntax
The basic syntax of a FULL JOIN is as follows −

SELECT table1.column1, table2.column2...
FROM table1
FULL JOIN table2
ON table1.common_field = table2.common_field;
Here, the given condition could be any given expression based on your requirement.

Example
Consider the following two tables.

Table 1 − CUSTOMERS Table is as follows.

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
Table 2 − ORDERS Table is as follows.

+-----+---------------------+-------------+--------+
|OID  | DATE                | CUSTOMER_ID | AMOUNT |
+-----+---------------------+-------------+--------+
| 102 | 2009-10-08 00:00:00 |           3 |   3000 |
| 100 | 2009-10-08 00:00:00 |           3 |   1500 |
| 101 | 2009-11-20 00:00:00 |           2 |   1560 |
| 103 | 2008-05-20 00:00:00 |           4 |   2060 |
+-----+---------------------+-------------+--------+
Now, let us join these two tables using FULL JOIN as follows.

SQL> SELECT  ID, NAME, AMOUNT, DATE
   FROM CUSTOMERS
   FULL JOIN ORDERS
   ON CUSTOMERS.ID = ORDERS.CUSTOMER_ID;
This would produce the following result −

+------+----------+--------+---------------------+
| ID   | NAME     | AMOUNT | DATE                |
+------+----------+--------+---------------------+
|    1 | Ramesh   |   NULL | NULL                |
|    2 | Khilan   |   1560 | 2009-11-20 00:00:00 |
|    3 | kaushik  |   3000 | 2009-10-08 00:00:00 |
|    3 | kaushik  |   1500 | 2009-10-08 00:00:00 |
|    4 | Chaitali |   2060 | 2008-05-20 00:00:00 |
|    5 | Hardik   |   NULL | NULL                |
|    6 | Komal    |   NULL | NULL                |
|    7 | Muffy    |   NULL | NULL                |
|    3 | kaushik  |   3000 | 2009-10-08 00:00:00 |
|    3 | kaushik  |   1500 | 2009-10-08 00:00:00 |
|    2 | Khilan   |   1560 | 2009-11-20 00:00:00 |
|    4 | Chaitali |   2060 | 2008-05-20 00:00:00 |
+------+----------+--------+---------------------+
If your Database does not support FULL JOIN (MySQL does not support FULL JOIN), then you can use UNION ALL clause to combine these two JOINS as shown below.

SQL> SELECT  ID, NAME, AMOUNT, DATE
   FROM CUSTOMERS
   LEFT JOIN ORDERS
   ON CUSTOMERS.ID = ORDERS.CUSTOMER_ID
UNION ALL
   SELECT  ID, NAME, AMOUNT, DATE
   FROM CUSTOMERS
   RIGHT JOIN ORDERS
   ON CUSTOMERS.ID = ORDERS.CUSTOMER_ID


            3.1.21.5 SELF JOIN − is used to join a table to itself as if the table were two tables, temporarily renaming at least one table in the SQL statement.
            The SQL SELF JOIN is used to join a table to itself as if the table were two tables; temporarily renaming at least one table in the SQL statement.

Syntax
The basic syntax of SELF JOIN is as follows −

SELECT a.column_name, b.column_name...
FROM table1 a, table1 b
WHERE a.common_field = b.common_field;
Here, the WHERE clause could be any given expression based on your requirement.

Example
Consider the following table.

CUSTOMERS Table is as follows.

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
Now, let us join this table using SELF JOIN as follows −

SQL> SELECT  a.ID, b.NAME, a.SALARY
   FROM CUSTOMERS a, CUSTOMERS b
   WHERE a.SALARY < b.SALARY;
This would produce the following result −

+----+----------+---------+
| ID | NAME     | SALARY  |
+----+----------+---------+
|  2 | Ramesh   | 1500.00 |
|  2 | kaushik  | 1500.00 |
|  1 | Chaitali | 2000.00 |
|  2 | Chaitali | 1500.00 |
|  3 | Chaitali | 2000.00 |
|  6 | Chaitali | 4500.00 |
|  1 | Hardik   | 2000.00 |
|  2 | Hardik   | 1500.00 |
|  3 | Hardik   | 2000.00 |
|  4 | Hardik   | 6500.00 |
|  6 | Hardik   | 4500.00 |
|  1 | Komal    | 2000.00 |
|  2 | Komal    | 1500.00 |
|  3 | Komal    | 2000.00 |
|  1 | Muffy    | 2000.00 |
|  2 | Muffy    | 1500.00 |
|  3 | Muffy    | 2000.00 |
|  4 | Muffy    | 6500.00 |
|  5 | Muffy    | 8500.00 |
|  6 | Muffy    | 4500.00 |
+----+----------+---------+

            3.1.21.6 CARTESIAN JOIN − returns the Cartesian product of the sets of records from the two or more joined tables.
The CARTESIAN JOIN or CROSS JOIN returns the Cartesian product of the sets of records from two or more joined tables. Thus, it equates to an inner join where the join-condition always evaluates to either True or where the join-condition is absent from the statement.

Syntax
The basic syntax of the CARTESIAN JOIN or the CROSS JOIN is as follows −

SELECT table1.column1, table2.column2...
FROM  table1, table2 [, table3 ]
Example
Consider the following two tables.

Table 1 − CUSTOMERS table is as follows.

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
Table 2: ORDERS Table is as follows −

+-----+---------------------+-------------+--------+
|OID  | DATE                | CUSTOMER_ID | AMOUNT |
+-----+---------------------+-------------+--------+
| 102 | 2009-10-08 00:00:00 |           3 |   3000 |
| 100 | 2009-10-08 00:00:00 |           3 |   1500 |
| 101 | 2009-11-20 00:00:00 |           2 |   1560 |
| 103 | 2008-05-20 00:00:00 |           4 |   2060 |
+-----+---------------------+-------------+--------+
Now, let us join these two tables using CARTESIAN JOIN as follows −

SQL> SELECT  ID, NAME, AMOUNT, DATE
   FROM CUSTOMERS, ORDERS;
This would produce the following result −

+----+----------+--------+---------------------+
| ID | NAME     | AMOUNT | DATE                |
+----+----------+--------+---------------------+
|  1 | Ramesh   |   3000 | 2009-10-08 00:00:00 |
|  1 | Ramesh   |   1500 | 2009-10-08 00:00:00 |
|  1 | Ramesh   |   1560 | 2009-11-20 00:00:00 |
|  1 | Ramesh   |   2060 | 2008-05-20 00:00:00 |
|  2 | Khilan   |   3000 | 2009-10-08 00:00:00 |
|  2 | Khilan   |   1500 | 2009-10-08 00:00:00 |
|  2 | Khilan   |   1560 | 2009-11-20 00:00:00 |
|  2 | Khilan   |   2060 | 2008-05-20 00:00:00 |
|  3 | kaushik  |   3000 | 2009-10-08 00:00:00 |
|  3 | kaushik  |   1500 | 2009-10-08 00:00:00 |
|  3 | kaushik  |   1560 | 2009-11-20 00:00:00 |
|  3 | kaushik  |   2060 | 2008-05-20 00:00:00 |
|  4 | Chaitali |   3000 | 2009-10-08 00:00:00 |
|  4 | Chaitali |   1500 | 2009-10-08 00:00:00 |
|  4 | Chaitali |   1560 | 2009-11-20 00:00:00 |
|  4 | Chaitali |   2060 | 2008-05-20 00:00:00 |
|  5 | Hardik   |   3000 | 2009-10-08 00:00:00 |
|  5 | Hardik   |   1500 | 2009-10-08 00:00:00 |
|  5 | Hardik   |   1560 | 2009-11-20 00:00:00 |
|  5 | Hardik   |   2060 | 2008-05-20 00:00:00 |
|  6 | Komal    |   3000 | 2009-10-08 00:00:00 |
|  6 | Komal    |   1500 | 2009-10-08 00:00:00 |
|  6 | Komal    |   1560 | 2009-11-20 00:00:00 |
|  6 | Komal    |   2060 | 2008-05-20 00:00:00 |
|  7 | Muffy    |   3000 | 2009-10-08 00:00:00 |
|  7 | Muffy    |   1500 | 2009-10-08 00:00:00 |
|  7 | Muffy    |   1560 | 2009-11-20 00:00:00 |
|  7 | Muffy    |   2060 | 2008-05-20 00:00:00 |
+----+----------+--------+---------------------+

Let us now discuss each of these joins in detail.

            3.1.21.7

        3.1.22 The SQL UNION clause/operator is used to combine the results of two or more SELECT statements without returning any duplicate rows.

To use this UNION clause, each SELECT statement must have

The same number of columns selected
The same number of column expressions
The same data type and
Have them in the same order
But they need not have to be in the same length.

Syntax
The basic syntax of a UNION clause is as follows −

SELECT column1 [, column2 ]
FROM table1 [, table2 ]
[WHERE condition]

UNION

SELECT column1 [, column2 ]
FROM table1 [, table2 ]
[WHERE condition]
Here, the given condition could be any given expression based on your requirement.

Example
Consider the following two tables.

Table 1 − CUSTOMERS Table is as follows.

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
Table 2 − ORDERS Table is as follows.

+-----+---------------------+-------------+--------+
|OID  | DATE                | CUSTOMER_ID | AMOUNT |
+-----+---------------------+-------------+--------+
| 102 | 2009-10-08 00:00:00 |           3 |   3000 |
| 100 | 2009-10-08 00:00:00 |           3 |   1500 |
| 101 | 2009-11-20 00:00:00 |           2 |   1560 |
| 103 | 2008-05-20 00:00:00 |           4 |   2060 |
+-----+---------------------+-------------+--------+
Now, let us join these two tables in our SELECT statement as follows −

SQL> SELECT  ID, NAME, AMOUNT, DATE
   FROM CUSTOMERS
   LEFT JOIN ORDERS
   ON CUSTOMERS.ID = ORDERS.CUSTOMER_ID
UNION
   SELECT  ID, NAME, AMOUNT, DATE
   FROM CUSTOMERS
   RIGHT JOIN ORDERS
   ON CUSTOMERS.ID = ORDERS.CUSTOMER_ID;
This would produce the following result −

+------+----------+--------+---------------------+
| ID   | NAME     | AMOUNT | DATE                |
+------+----------+--------+---------------------+
|    1 | Ramesh   |   NULL | NULL                |
|    2 | Khilan   |   1560 | 2009-11-20 00:00:00 |
|    3 | kaushik  |   3000 | 2009-10-08 00:00:00 |
|    3 | kaushik  |   1500 | 2009-10-08 00:00:00 |
|    4 | Chaitali |   2060 | 2008-05-20 00:00:00 |
|    5 | Hardik   |   NULL | NULL                |
|    6 | Komal    |   NULL | NULL                |
|    7 | Muffy    |   NULL | NULL                |
+------+----------+--------+---------------------+
The UNION ALL Clause
The UNION ALL operator is used to combine the results of two SELECT statements including duplicate rows.

The same rules that apply to the UNION clause will apply to the UNION ALL operator.

Syntax
The basic syntax of the UNION ALL is as follows.

SELECT column1 [, column2 ]
FROM table1 [, table2 ]
[WHERE condition]

UNION ALL

SELECT column1 [, column2 ]
FROM table1 [, table2 ]
[WHERE condition]
Here, the given condition could be any given expression based on your requirement.

Example
Consider the following two tables,

Table 1 − CUSTOMERS Table is as follows.

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
Table 2 − ORDERS table is as follows.

+-----+---------------------+-------------+--------+
|OID  | DATE                | CUSTOMER_ID | AMOUNT |
+-----+---------------------+-------------+--------+
| 102 | 2009-10-08 00:00:00 |           3 |   3000 |
| 100 | 2009-10-08 00:00:00 |           3 |   1500 |
| 101 | 2009-11-20 00:00:00 |           2 |   1560 |
| 103 | 2008-05-20 00:00:00 |           4 |   2060 |
+-----+---------------------+-------------+--------+
Now, let us join these two tables in our SELECT statement as follows −

SQL> SELECT  ID, NAME, AMOUNT, DATE
   FROM CUSTOMERS
   LEFT JOIN ORDERS
   ON CUSTOMERS.ID = ORDERS.CUSTOMER_ID
UNION ALL
   SELECT  ID, NAME, AMOUNT, DATE
   FROM CUSTOMERS
   RIGHT JOIN ORDERS
   ON CUSTOMERS.ID = ORDERS.CUSTOMER_ID;
This would produce the following result −

+------+----------+--------+---------------------+
| ID   | NAME     | AMOUNT | DATE                |
+------+----------+--------+---------------------+
|    1 | Ramesh   |   NULL | NULL                |
|    2 | Khilan   |   1560 | 2009-11-20 00:00:00 |
|    3 | kaushik  |   3000 | 2009-10-08 00:00:00 |
|    3 | kaushik  |   1500 | 2009-10-08 00:00:00 |
|    4 | Chaitali |   2060 | 2008-05-20 00:00:00 |
|    5 | Hardik   |   NULL | NULL                |
|    6 | Komal    |   NULL | NULL                |
|    7 | Muffy    |   NULL | NULL                |
|    3 | kaushik  |   3000 | 2009-10-08 00:00:00 |
|    3 | kaushik  |   1500 | 2009-10-08 00:00:00 |
|    2 | Khilan   |   1560 | 2009-11-20 00:00:00 |
|    4 | Chaitali |   2060 | 2008-05-20 00:00:00 |
+------+----------+--------+---------------------+
There are two other clauses (i.e., operators), which are like the UNION clause.

SQL INTERSECT Clause − This is used to combine two SELECT statements, but returns rows only from the first SELECT statement that are identical to a row in the second SELECT statement.

SQL EXCEPT Clause − This combines two SELECT statements and returns rows from the first SELECT statement that are not returned by the second SELECT statement.

        3.1.23 NULL value
The SQL NULL is the term used to represent a missing value. A NULL value in a table is a value in a field that appears to be blank.

A field with a NULL value is a field with no value. It is very important to understand that a NULL value is different than a zero value or a field that contains spaces.

Syntax
The basic syntax of NULL while creating a table.

SQL> CREATE TABLE CUSTOMERS(
   ID   INT              NOT NULL,
   NAME VARCHAR (20)     NOT NULL,
   AGE  INT              NOT NULL,
   ADDRESS  CHAR (25) ,
   SALARY   DECIMAL (18, 2),       
   PRIMARY KEY (ID)
);
Here, NOT NULL signifies that column should always accept an explicit value of the given data type. There are two columns where we did not use NOT NULL, which means these columns could be NULL.

A field with a NULL value is the one that has been left blank during the record creation.

Example
The NULL value can cause problems when selecting data. However, because when comparing an unknown value to any other value, the result is always unknown and not included in the results. You must use the IS NULL or IS NOT NULL operators to check for a NULL value.

Consider the following CUSTOMERS table having the records as shown below.

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |          |
|  7 | Muffy    |  24 | Indore    |          |
+----+----------+-----+-----------+----------+
Now, following is the usage of the IS NOT NULLoperator.

SQL> SELECT  ID, NAME, AGE, ADDRESS, SALARY
   FROM CUSTOMERS
   WHERE SALARY IS NOT NULL;
This would produce the following result −

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
+----+----------+-----+-----------+----------+
Now, following is the usage of the IS NULL operator.

SQL> SELECT  ID, NAME, AGE, ADDRESS, SALARY
   FROM CUSTOMERS
   WHERE SALARY IS NULL;
This would produce the following result −

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  6 | Komal    |  22 | MP        |          |
|  7 | Muffy    |  24 | Indore    |          |
+----+----------+-----+-----------+----------+

        3.1.24 Alias, Name as alias_name
You can rename a table or a column temporarily by giving another name known as Alias. The use of table aliases is to rename a table in a specific SQL statement. The renaming is a temporary change and the actual table name does not change in the database. The column aliases are used to rename a table's columns for the purpose of a particular SQL query.

Syntax
The basic syntax of a table alias is as follows.

SELECT column1, column2....
FROM table_name AS alias_name
WHERE [condition];
The basic syntax of a column alias is as follows.

SELECT column_name AS alias_name
FROM table_name
WHERE [condition];
Example
Consider the following two tables.

Table 1 − CUSTOMERS Table is as follows.

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
Table 2 − ORDERS Table is as follows.

+-----+---------------------+-------------+--------+
|OID  | DATE                | CUSTOMER_ID | AMOUNT |
+-----+---------------------+-------------+--------+
| 102 | 2009-10-08 00:00:00 |           3 |   3000 |
| 100 | 2009-10-08 00:00:00 |           3 |   1500 |
| 101 | 2009-11-20 00:00:00 |           2 |   1560 |
| 103 | 2008-05-20 00:00:00 |           4 |   2060 |
+-----+---------------------+-------------+--------+
Now, the following code block shows the usage of a table alias.

SQL> SELECT C.ID, C.NAME, C.AGE, O.AMOUNT 
   FROM CUSTOMERS AS C, ORDERS AS O
   WHERE  C.ID = O.CUSTOMER_ID;
This would produce the following result.

+----+----------+-----+--------+
| ID | NAME     | AGE | AMOUNT |
+----+----------+-----+--------+
|  3 | kaushik  |  23 |   3000 |
|  3 | kaushik  |  23 |   1500 |
|  2 | Khilan   |  25 |   1560 |
|  4 | Chaitali |  25 |   2060 |
+----+----------+-----+--------+
Following is the usage of a column alias.

SQL> SELECT  ID AS CUSTOMER_ID, NAME AS CUSTOMER_NAME
   FROM CUSTOMERS
   WHERE SALARY IS NOT NULL;
This would produce the following result.

+-------------+---------------+
| CUSTOMER_ID | CUSTOMER_NAME |
+-------------+---------------+
|           1 | Ramesh        |
|           2 | Khilan        |
|           3 | kaushik       |
|           4 | Chaitali      |
|           5 | Hardik        |
|           6 | Komal         |
|           7 | Muffy         |
+-------------+---------------+

        3.1.25 Indexes
Indexes are special lookup tables that the database search engine can use to speed up data retrieval. Simply put, an index is a pointer to data in a table. An index in a database is very similar to an index in the back of a book.

For example, if you want to reference all pages in a book that discusses a certain topic, you first refer to the index, which lists all the topics alphabetically and are then referred to one or more specific page numbers.

An index helps to speed up SELECT queries and WHERE clauses, but it slows down data input, with the UPDATE and the INSERT statements. Indexes can be created or dropped with no effect on the data.

Creating an index involves the CREATE INDEX statement, which allows you to name the index, to specify the table and which column or columns to index, and to indicate whether the index is in an ascending or descending order.

Indexes can also be unique, like the UNIQUE constraint, in that the index prevents duplicate entries in the column or combination of columns on which there is an index.

The CREATE INDEX Command
The basic syntax of a CREATE INDEX is as follows.

CREATE INDEX index_name ON table_name;
Single-Column Indexes
A single-column index is created based on only one table column. The basic syntax is as follows.

CREATE INDEX index_name
ON table_name (column_name);
Unique Indexes
Unique indexes are used not only for performance, but also for data integrity. A unique index does not allow any duplicate values to be inserted into the table. The basic syntax is as follows.

CREATE UNIQUE INDEX index_name
on table_name (column_name);
Composite Indexes
A composite index is an index on two or more columns of a table. Its basic syntax is as follows.

CREATE INDEX index_name
on table_name (column1, column2);
Whether to create a single-column index or a composite index, take into consideration the column(s) that you may use very frequently in a query's WHERE clause as filter conditions.

Should there be only one column used, a single-column index should be the choice. Should there be two or more columns that are frequently used in the WHERE clause as filters, the composite index would be the best choice.

Implicit Indexes
Implicit indexes are indexes that are automatically created by the database server when an object is created. Indexes are automatically created for primary key constraints and unique constraints.

The DROP INDEX Command
An index can be dropped using SQL DROP command. Care should be taken when dropping an index because the performance may either slow down or improve.

The basic syntax is as follows −

DROP INDEX index_name;
You can check the INDEX Constraint chapter to see some actual examples on Indexes.

When should indexes be avoided?
Although indexes are intended to enhance a database's performance, there are times when they should be avoided.

The following guidelines indicate when the use of an index should be reconsidered.

Indexes should not be used on small tables.

Tables that have frequent, large batch updates or insert operations.

Indexes should not be used on columns that contain a high number of NULL values.

Columns that are frequently manipulated should not be indexed.

        3.1.26 ALTER TABLE 
The SQL ALTER TABLE command is used to add, delete or modify columns in an existing table. You should also use the ALTER TABLE command to add and drop various constraints on an existing table.

Syntax
The basic syntax of an ALTER TABLE command to add a New Column in an existing table is as follows.

ALTER TABLE table_name ADD column_name datatype;
The basic syntax of an ALTER TABLE command to DROP COLUMN in an existing table is as follows.

ALTER TABLE table_name DROP COLUMN column_name;
The basic syntax of an ALTER TABLE command to change the DATA TYPE of a column in a table is as follows.

ALTER TABLE table_name MODIFY COLUMN column_name datatype;
The basic syntax of an ALTER TABLE command to add a NOT NULL constraint to a column in a table is as follows.

ALTER TABLE table_name MODIFY column_name datatype NOT NULL;
The basic syntax of ALTER TABLE to ADD UNIQUE CONSTRAINT to a table is as follows.

ALTER TABLE table_name 
ADD CONSTRAINT MyUniqueConstraint UNIQUE(column1, column2...);
The basic syntax of an ALTER TABLE command to ADD CHECK CONSTRAINT to a table is as follows.

ALTER TABLE table_name 
ADD CONSTRAINT MyUniqueConstraint CHECK (CONDITION);
The basic syntax of an ALTER TABLE command to ADD PRIMARY KEY constraint to a table is as follows.

ALTER TABLE table_name 
ADD CONSTRAINT MyPrimaryKey PRIMARY KEY (column1, column2...);
The basic syntax of an ALTER TABLE command to DROP CONSTRAINT from a table is as follows.

ALTER TABLE table_name 
DROP CONSTRAINT MyUniqueConstraint;
If you're using MySQL, the code is as follows −

ALTER TABLE table_name 
DROP INDEX MyUniqueConstraint;
The basic syntax of an ALTER TABLE command to DROP PRIMARY KEY constraint from a table is as follows.

ALTER TABLE table_name 
DROP CONSTRAINT MyPrimaryKey;
If you're using MySQL, the code is as follows −

ALTER TABLE table_name 
DROP PRIMARY KEY;
Example
Consider the CUSTOMERS table having the following records −

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
Following is the example to ADD a New Column to an existing table −

ALTER TABLE CUSTOMERS ADD SEX char(1);
Now, the CUSTOMERS table is changed and following would be output from the SELECT statement.

+----+---------+-----+-----------+----------+------+
| ID | NAME    | AGE | ADDRESS   | SALARY   | SEX  |
+----+---------+-----+-----------+----------+------+
|  1 | Ramesh  |  32 | Ahmedabad |  2000.00 | NULL |
|  2 | Ramesh  |  25 | Delhi     |  1500.00 | NULL |
|  3 | kaushik |  23 | Kota      |  2000.00 | NULL |
|  4 | kaushik |  25 | Mumbai    |  6500.00 | NULL |
|  5 | Hardik  |  27 | Bhopal    |  8500.00 | NULL |
|  6 | Komal   |  22 | MP        |  4500.00 | NULL |
|  7 | Muffy   |  24 | Indore    | 10000.00 | NULL |
+----+---------+-----+-----------+----------+------+
Following is the example to DROP sex column from the existing table.

ALTER TABLE CUSTOMERS DROP SEX;
Now, the CUSTOMERS table is changed and following would be the output from the SELECT statement.

+----+---------+-----+-----------+----------+
| ID | NAME    | AGE | ADDRESS   | SALARY   |
+----+---------+-----+-----------+----------+
|  1 | Ramesh  |  32 | Ahmedabad |  2000.00 |
|  2 | Ramesh  |  25 | Delhi     |  1500.00 |
|  3 | kaushik |  23 | Kota      |  2000.00 |
|  4 | kaushik |  25 | Mumbai    |  6500.00 |
|  5 | Hardik  |  27 | Bhopal    |  8500.00 |
|  6 | Komal   |  22 | MP        |  4500.00 |
|  7 | Muffy   |  24 | Indore    | 10000.00 |
+----+---------+-----+-----------+----------+

        3.1.27 TRUNCATE TABLE
The SQL TRUNCATE TABLE command is used to delete complete data from an existing table.

You can also use DROP TABLE command to delete complete table but it would remove complete table structure form the database and you would need to re-create this table once again if you wish you store some data.

Syntax
The basic syntax of a TRUNCATE TABLE command is as follows.

TRUNCATE TABLE  table_name;
Example
Consider a CUSTOMERS table having the following records −

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
Following is the example of a Truncate command.

SQL > TRUNCATE TABLE CUSTOMERS;
Now, the CUSTOMERS table is truncated and the output from SELECT statement will be as shown in the code block below −

SQL> SELECT * FROM CUSTOMERS;
Empty set (0.00 sec)

        3.1.28 Table views
A view is nothing more than a SQL statement that is stored in the database with an associated name. A view is actually a composition of a table in the form of a predefined SQL query.

A view can contain all rows of a table or select rows from a table. A view can be created from one or many tables which depends on the written SQL query to create a view.

Views, which are a type of virtual tables allow users to do the following −

Structure data in a way that users or classes of users find natural or intuitive.

Restrict access to the data in such a way that a user can see and (sometimes) modify exactly what they need and no more.

Summarize data from various tables which can be used to generate reports.

Creating Views
Database views are created using the CREATE VIEW statement. Views can be created from a single table, multiple tables or another view.

To create a view, a user must have the appropriate system privilege according to the specific implementation.

The basic CREATE VIEW syntax is as follows −

CREATE VIEW view_name AS
SELECT column1, column2.....
FROM table_name
WHERE [condition];
You can include multiple tables in your SELECT statement in a similar way as you use them in a normal SQL SELECT query.

Example
Consider the CUSTOMERS table having the following records −

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
Following is an example to create a view from the CUSTOMERS table. This view would be used to have customer name and age from the CUSTOMERS table.

SQL > CREATE VIEW CUSTOMERS_VIEW AS
SELECT name, age
FROM  CUSTOMERS;
Now, you can query CUSTOMERS_VIEW in a similar way as you query an actual table. Following is an example for the same.

SQL > SELECT * FROM CUSTOMERS_VIEW;
This would produce the following result.

+----------+-----+
| name     | age |
+----------+-----+
| Ramesh   |  32 |
| Khilan   |  25 |
| kaushik  |  23 |
| Chaitali |  25 |
| Hardik   |  27 |
| Komal    |  22 |
| Muffy    |  24 |
+----------+-----+
The WITH CHECK OPTION
The WITH CHECK OPTION is a CREATE VIEW statement option. The purpose of the WITH CHECK OPTION is to ensure that all UPDATE and INSERTs satisfy the condition(s) in the view definition.

If they do not satisfy the condition(s), the UPDATE or INSERT returns an error.

The following code block has an example of creating same view CUSTOMERS_VIEW with the WITH CHECK OPTION.

CREATE VIEW CUSTOMERS_VIEW AS
SELECT name, age
FROM  CUSTOMERS
WHERE age IS NOT NULL
WITH CHECK OPTION;
The WITH CHECK OPTION in this case should deny the entry of any NULL values in the view's AGE column, because the view is defined by data that does not have a NULL value in the AGE column.

Updating a View
A view can be updated under certain conditions which are given below −

The SELECT clause may not contain the keyword DISTINCT.

The SELECT clause may not contain summary functions.

The SELECT clause may not contain set functions.

The SELECT clause may not contain set operators.

The SELECT clause may not contain an ORDER BY clause.

The FROM clause may not contain multiple tables.

The WHERE clause may not contain subqueries.

The query may not contain GROUP BY or HAVING.

Calculated columns may not be updated.

All NOT NULL columns from the base table must be included in the view in order for the INSERT query to function.

So, if a view satisfies all the above-mentioned rules then you can update that view. The following code block has an example to update the age of Ramesh.

SQL > UPDATE CUSTOMERS_VIEW
   SET AGE = 35
   WHERE name = 'Ramesh';
This would ultimately update the base table CUSTOMERS and the same would reflect in the view itself. Now, try to query the base table and the SELECT statement would produce the following result.

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  35 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
Inserting Rows into a View
Rows of data can be inserted into a view. The same rules that apply to the UPDATE command also apply to the INSERT command.

Here, we cannot insert rows in the CUSTOMERS_VIEW because we have not included all the NOT NULL columns in this view, otherwise you can insert rows in a view in a similar way as you insert them in a table.

Deleting Rows into a View
Rows of data can be deleted from a view. The same rules that apply to the UPDATE and INSERT commands apply to the DELETE command.

Following is an example to delete a record having AGE = 22.

SQL > DELETE FROM CUSTOMERS_VIEW
   WHERE age = 22;
This would ultimately delete a row from the base table CUSTOMERS and the same would reflect in the view itself. Now, try to query the base table and the SELECT statement would produce the following result.

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  35 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
Dropping Views
Obviously, where you have a view, you need a way to drop the view if it is no longer needed. The syntax is very simple and is given below −

DROP VIEW view_name;
Following is an example to drop the CUSTOMERS_VIEW from the CUSTOMERS table.

DROP VIEW CUSTOMERS_VIEW;


        3.1.29 GROUP BY HAVING Clause
The HAVING Clause enables you to specify conditions that filter which group results appear in the results.

The WHERE clause places conditions on the selected columns, whereas the HAVING clause places conditions on groups created by the GROUP BY clause.

Syntax
The following code block shows the position of the HAVING Clause in a query.

SELECT
FROM
WHERE
GROUP BY
HAVING
ORDER BY
The HAVING clause must follow the GROUP BY clause in a query and must also precede the ORDER BY clause if used. The following code block has the syntax of the SELECT statement including the HAVING clause −

SELECT column1, column2
FROM table1, table2
WHERE [ conditions ]
GROUP BY column1, column2
HAVING [ conditions ]
ORDER BY column1, column2
Example
Consider the CUSTOMERS table having the following records.

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
Following is an example, which would display a record for a similar age count that would be more than or equal to 2.

SQL > SELECT ID, NAME, AGE, ADDRESS, SALARY
FROM CUSTOMERS
GROUP BY age
HAVING COUNT(age) >= 2;
This would produce the following result −

+----+--------+-----+---------+---------+
| ID | NAME   | AGE | ADDRESS | SALARY  |
+----+--------+-----+---------+---------+
|  2 | Khilan |  25 | Delhi   | 1500.00 |
+----+--------+-----+---------+---------+

        3.1.30 Transactions
A transaction is a unit of work that is performed against a database. Transactions are units or sequences of work accomplished in a logical order, whether in a manual fashion by a user or automatically by some sort of a database program.

A transaction is the propagation of one or more changes to the database. For example, if you are creating a record or updating a record or deleting a record from the table, then you are performing a transaction on that table. It is important to control these transactions to ensure the data integrity and to handle database errors.

Practically, you will club many SQL queries into a group and you will execute all of them together as a part of a transaction.

Properties of Transactions
Transactions have the following four standard properties, usually referred to by the acronym ACID.

Atomicity − ensures that all operations within the work unit are completed successfully. Otherwise, the transaction is aborted at the point of failure and all the previous operations are rolled back to their former state.

Consistency − ensures that the database properly changes states upon a successfully committed transaction.

Isolation − enables transactions to operate independently of and transparent to each other.

Durability − ensures that the result or effect of a committed transaction persists in case of a system failure.

Transaction Control
The following commands are used to control transactions.

COMMIT − to save the changes.

ROLLBACK − to roll back the changes.

SAVEPOINT − creates points within the groups of transactions in which to ROLLBACK.

SET TRANSACTION − Places a name on a transaction.

Transactional Control Commands
Transactional control commands are only used with the DML Commands such as - INSERT, UPDATE and DELETE only. They cannot be used while creating tables or dropping them because these operations are automatically committed in the database.

The COMMIT Command
The COMMIT command is the transactional command used to save changes invoked by a transaction to the database.

The COMMIT command is the transactional command used to save changes invoked by a transaction to the database. The COMMIT command saves all the transactions to the database since the last COMMIT or ROLLBACK command.

The syntax for the COMMIT command is as follows.

COMMIT;
Example

Consider the CUSTOMERS table having the following records −

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
Following is an example which would delete those records from the table which have age = 25 and then COMMIT the changes in the database.

SQL> DELETE FROM CUSTOMERS
   WHERE AGE = 25;
SQL> COMMIT;
Thus, two rows from the table would be deleted and the SELECT statement would produce the following result.

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
The ROLLBACK Command
The ROLLBACK command is the transactional command used to undo transactions that have not already been saved to the database. This command can only be used to undo transactions since the last COMMIT or ROLLBACK command was issued.

The syntax for a ROLLBACK command is as follows −

ROLLBACK;
Example

Consider the CUSTOMERS table having the following records −

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
Following is an example, which would delete those records from the table which have the age = 25 and then ROLLBACK the changes in the database.

SQL> DELETE FROM CUSTOMERS
   WHERE AGE = 25;
SQL> ROLLBACK;
Thus, the delete operation would not impact the table and the SELECT statement would produce the following result.

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
The SAVEPOINT Command
A SAVEPOINT is a point in a transaction when you can roll the transaction back to a certain point without rolling back the entire transaction.

The syntax for a SAVEPOINT command is as shown below.

SAVEPOINT SAVEPOINT_NAME;
This command serves only in the creation of a SAVEPOINT among all the transactional statements. The ROLLBACK command is used to undo a group of transactions.

The syntax for rolling back to a SAVEPOINT is as shown below.

ROLLBACK TO SAVEPOINT_NAME;
Following is an example where you plan to delete the three different records from the CUSTOMERS table. You want to create a SAVEPOINT before each delete, so that you can ROLLBACK to any SAVEPOINT at any time to return the appropriate data to its original state.

Example

Consider the CUSTOMERS table having the following records.

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
The following code block contains the series of operations.

SQL> SAVEPOINT SP1;
Savepoint created.
SQL> DELETE FROM CUSTOMERS WHERE ID=1;
1 row deleted.
SQL> SAVEPOINT SP2;
Savepoint created.
SQL> DELETE FROM CUSTOMERS WHERE ID=2;
1 row deleted.
SQL> SAVEPOINT SP3;
Savepoint created.
SQL> DELETE FROM CUSTOMERS WHERE ID=3;
1 row deleted.
Now that the three deletions have taken place, let us assume that you have changed your mind and decided to ROLLBACK to the SAVEPOINT that you identified as SP2. Because SP2 was created after the first deletion, the last two deletions are undone −

SQL> ROLLBACK TO SP2;
Rollback complete.
Notice that only the first deletion took place since you rolled back to SP2.

SQL> SELECT * FROM CUSTOMERS;
+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
6 rows selected.
The RELEASE SAVEPOINT Command
The RELEASE SAVEPOINT command is used to remove a SAVEPOINT that you have created.

The syntax for a RELEASE SAVEPOINT command is as follows.

RELEASE SAVEPOINT SAVEPOINT_NAME;
Once a SAVEPOINT has been released, you can no longer use the ROLLBACK command to undo transactions performed since the last SAVEPOINT.

The SET TRANSACTION Command
The SET TRANSACTION command can be used to initiate a database transaction. This command is used to specify characteristics for the transaction that follows. For example, you can specify a transaction to be read only or read write.

The syntax for a SET TRANSACTION command is as follows.

SET TRANSACTION [ READ WRITE | READ ONLY ];

        3.1.31 wildcard characters
We have already discussed about the SQL LIKE operator, which is used to compare a value to similar values using the wildcard operators.

SQL supports two wildcard operators in conjunction with the LIKE operator which are explained in detail in the following table.

Sr.No.	Wildcard & Description
1	
The percent sign (%)

Matches one or more characters.

Note − MS Access uses the asterisk (*) wildcard character instead of the percent sign (%) wildcard character.

2	
The underscore (_)

Matches one character.

Note − MS Access uses a question mark (?) instead of the underscore (_) to match any one character.

The percent sign represents zero, one or multiple characters. The underscore represents a single number or a character. These symbols can be used in combinations.

Syntax
The basic syntax of a '%' and a '_' operator is as follows.

SELECT * FROM table_name
WHERE column LIKE 'XXXX%'

or 

SELECT * FROM table_name
WHERE column LIKE '%XXXX%'

or

SELECT * FROM table_name
WHERE column LIKE 'XXXX_'

or

SELECT * FROM table_name
WHERE column LIKE '_XXXX'

or

SELECT * FROM table_name
WHERE column LIKE '_XXXX_'
You can combine N number of conditions using the AND or the OR operators. Here, XXXX could be any numeric or string value.

Example
The following table has a number of examples showing the WHERE part having different LIKE clauses with '%' and '_' operators.

Sr.No.	Statement & Description
1	
WHERE SALARY LIKE '200%'

Finds any values that start with 200.

2	
WHERE SALARY LIKE '%200%'

Finds any values that have 200 in any position.

3	
WHERE SALARY LIKE '_00%'

Finds any values that have 00 in the second and third positions.

4	
WHERE SALARY LIKE '2_%_%'

Finds any values that start with 2 and are at least 3 characters in length.

5	
WHERE SALARY LIKE '%2'

Finds any values that end with 2.

6	
WHERE SALARY LIKE '_2%3'

Finds any values that have a 2 in the second position and end with a 3.

7	
WHERE SALARY LIKE '2___3'

Finds any values in a five-digit number that start with 2 and end with 3.

Let us take a real example, consider the CUSTOMERS table having the following records.

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
The following code block is an example, which would display all the records from the CUSTOMERS table where the SALARY starts with 200.

SQL> SELECT * FROM CUSTOMERS
WHERE SALARY LIKE '200%';
This would produce the following result.

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
+----+----------+-----+-----------+----------+


        3.1.32 SQL Date Functions
The following table has a list of all the important Date and Time related functions available through SQL. There are various other functions supported by your RDBMS. The given list is based on MySQL RDBMS.

Sr.No.	Function & Description
1	ADDDATE()
Adds dates

2	ADDTIME()
Adds time

3	CONVERT_TZ()
Converts from one timezone to another

4	CURDATE()
Returns the current date

5	CURRENT_DATE(), CURRENT_DATE
Synonyms for CURDATE()

6	CURRENT_TIME(), CURRENT_TIME
Synonyms for CURTIME()

7	CURRENT_TIMESTAMP(), CURRENT_TIMESTAMP
Synonyms for NOW()

8	CURTIME()
Returns the current time

9	DATE_ADD()
Adds two dates

10	DATE_FORMAT()
Formats date as specified

11	DATE_SUB()
Subtracts two dates

12	DATE()
Extracts the date part of a date or datetime expression

13	DATEDIFF()
Subtracts two dates

14	DAY()
Synonym for DAYOFMONTH()

15	DAYNAME()
Returns the name of the weekday

16	DAYOFMONTH()
Returns the day of the month (1-31)

17	DAYOFWEEK()
Returns the weekday index of the argument

18	DAYOFYEAR()
Returns the day of the year (1-366)

19	EXTRACT
Extracts part of a date

20	FROM_DAYS()
Converts a day number to a date

21	FROM_UNIXTIME()
Formats date as a UNIX timestamp

22	HOUR()
Extracts the hour

23	LAST_DAY
Returns the last day of the month for the argument

24	LOCALTIME(), LOCALTIME
Synonym for NOW()

25	LOCALTIMESTAMP, LOCALTIMESTAMP()
Synonym for NOW()

26	MAKEDATE()
Creates a date from the year and day of year

27	MAKETIME
MAKETIME()

28	MICROSECOND()
Returns the microseconds from argument

29	MINUTE()
Returns the minute from the argument

30	MONTH()
Return the month from the date passed

31	MONTHNAME()
Returns the name of the month

32	NOW()
Returns the current date and time

33	PERIOD_ADD()
Adds a period to a year-month

34	PERIOD_DIFF()
Returns the number of months between periods

35	QUARTER()
Returns the quarter from a date argument

36	SEC_TO_TIME()
Converts seconds to 'HH:MM:SS' format

37	SECOND()
Returns the second (0-59)

38	STR_TO_DATE()
Converts a string to a date

39	SUBDATE()
When invoked with three arguments a synonym for DATE_SUB()

40	SUBTIME()
Subtracts times

41	SYSDATE()
Returns the time at which the function executes

42	TIME_FORMAT()
Formats as time

43	TIME_TO_SEC()
Returns the argument converted to seconds

44	TIME()
Extracts the time portion of the expression passed

45	TIMEDIFF()
Subtracts time

46	TIMESTAMP()
With a single argument this function returns the date or datetime expression. With two arguments, the sum of the arguments

47	TIMESTAMPADD()
Adds an interval to a datetime expression

48	TIMESTAMPDIFF()
Subtracts an interval from a datetime expression

49	TO_DAYS()
Returns the date argument converted to days

50	UNIX_TIMESTAMP()
Returns a UNIX timestamp

51	UTC_DATE()
Returns the current UTC date

52	UTC_TIME()
Returns the current UTC time

53	UTC_TIMESTAMP()
Returns the current UTC date and time

54	WEEK()
Returns the week number

55	WEEKDAY()
Returns the weekday index

56	WEEKOFYEAR()
Returns the calendar week of the date (1-53)

57	YEAR()
Returns the year

58	YEARWEEK()
Returns the year and week

ADDDATE(date,INTERVAL expr unit), ADDDATE(expr,days)
When invoked with the INTERVAL form of the second argument, ADDDATE() is a synonym for DATE_ADD(). The related function SUBDATE() is a synonym for DATE_SUB(). For information on the INTERVAL unit argument, see the discussion for DATE_ADD().

mysql> SELECT DATE_ADD('1998-01-02', INTERVAL 31 DAY);
+---------------------------------------------------------+
| DATE_ADD('1998-01-02', INTERVAL 31 DAY)                 |
+---------------------------------------------------------+
| 1998-02-02                                              |
+---------------------------------------------------------+
1 row in set (0.00 sec)

mysql> SELECT ADDDATE('1998-01-02', INTERVAL 31 DAY);
+---------------------------------------------------------+
| ADDDATE('1998-01-02', INTERVAL 31 DAY)                  |
+---------------------------------------------------------+
| 1998-02-02                                              |
+---------------------------------------------------------+
1 row in set (0.00 sec)
When invoked with the days form of the second argument, MySQL treats it as an integer number of days to be added to expr.

mysql> SELECT ADDDATE('1998-01-02', 31);
+---------------------------------------------------------+
| DATE_ADD('1998-01-02', INTERVAL 31 DAY)                 |
+---------------------------------------------------------+
| 1998-02-02                                              |
+---------------------------------------------------------+
1 row in set (0.00 sec)
ADDTIME(expr1,expr2)
ADDTIME() adds expr2 to expr1 and returns the result. The expr1 is a time or datetime expression, while the expr2 is a time expression.

mysql> SELECT ADDTIME('1997-12-31 23:59:59.999999','1 1:1:1.000002');
+---------------------------------------------------------+
| DATE_ADD('1997-12-31 23:59:59.999999','1 1:1:1.000002') |
+---------------------------------------------------------+
| 1998-01-02 01:01:01.000001                              |
+---------------------------------------------------------+
1 row in set (0.00 sec)
CONVERT_TZ(dt,from_tz,to_tz)
This converts a datetime value dt from the time zone given by from_tz to the time zone given by to_tz and returns the resulting value. This function returns NULL if the arguments are invalid.

mysql> SELECT CONVERT_TZ('2004-01-01 12:00:00','GMT','MET');
+---------------------------------------------------------+
| CONVERT_TZ('2004-01-01 12:00:00','GMT','MET')           |
+---------------------------------------------------------+
| 2004-01-01 13:00:00                                     |
+---------------------------------------------------------+
1 row in set (0.00 sec)

mysql> SELECT CONVERT_TZ('2004-01-01 12:00:00','+00:00','+10:00');
+---------------------------------------------------------+
| CONVERT_TZ('2004-01-01 12:00:00','+00:00','+10:00')     |
+---------------------------------------------------------+
| 2004-01-01 22:00:00                                     |
+---------------------------------------------------------+
1 row in set (0.00 sec)
CURDATE()
Returns the current date as a value in 'YYYY-MM-DD' or YYYYMMDD format, depending on whether the function is used in a string or in a numeric context.

mysql> SELECT CURDATE();
+---------------------------------------------------------+
| CURDATE()                                               |
+---------------------------------------------------------+
| 1997-12-15                                              |
+---------------------------------------------------------+
1 row in set (0.00 sec)

mysql> SELECT CURDATE() + 0;
+---------------------------------------------------------+
| CURDATE() + 0                                           |
+---------------------------------------------------------+
| 19971215                                                |
+---------------------------------------------------------+
1 row in set (0.00 sec)
CURRENT_DATE and CURRENT_DATE()
CURRENT_DATE and CURRENT_DATE() are synonyms for CURDATE()

CURTIME()
Returns the current time as a value in 'HH:MM:SS' or HHMMSS format, depending on whether the function is used in a string or in a numeric context. The value is expressed in the current time zone.

mysql> SELECT CURTIME();
+---------------------------------------------------------+
| CURTIME()                                               |
+---------------------------------------------------------+
| 23:50:26                                                |
+---------------------------------------------------------+
1 row in set (0.00 sec)

mysql> SELECT CURTIME() + 0;
+---------------------------------------------------------+
| CURTIME() + 0                                           |
+---------------------------------------------------------+
| 235026                                                  |
+---------------------------------------------------------+
1 row in set (0.00 sec)
CURRENT_TIME and CURRENT_TIME()
CURRENT_TIME and CURRENT_TIME() are synonyms for CURTIME().

CURRENT_TIMESTAMP and CURRENT_TIMESTAMP()
CURRENT_TIMESTAMP and CURRENT_TIMESTAMP() are synonyms for NOW().

DATE(expr)
Extracts the date part of the date or datetime expression expr.

mysql> SELECT DATE('2003-12-31 01:02:03');
+---------------------------------------------------------+
| DATE('2003-12-31 01:02:03')                             |
+---------------------------------------------------------+
|  2003-12-31                                             |
+---------------------------------------------------------+
1 row in set (0.00 sec)
DATEDIFF(expr1,expr2)
DATEDIFF() returns expr1 . expr2 expressed as a value in days from one date to the other. Both expr1 and expr2 are date or date-and-time expressions. Only the date parts of the values are used in the calculation.

mysql> SELECT DATEDIFF('1997-12-31 23:59:59','1997-12-30');
+---------------------------------------------------------+
| DATEDIFF('1997-12-31 23:59:59','1997-12-30')            |
+---------------------------------------------------------+
| 1                                                       |
+---------------------------------------------------------+
1 row in set (0.00 sec)
DATE_ADD(date,INTERVAL expr unit), DATE_SUB(date,INTERVAL expr unit)
These functions perform date arithmetic. The date is a DATETIME or DATE value specifying the starting date. The expr is an expression specifying the interval value to be added or subtracted from the starting date. The expr is a string; it may start with a '-' for negative intervals.

A unit is a keyword indicating the units in which the expression should be interpreted.

The INTERVAL keyword and the unit specifier are not case sensitive.

The following table shows the expected form of the expr argument for each unit value.

unit Value	Expected exprFormat
MICROSECOND	MICROSECONDS
SECOND	SECONDS
MINUTE	MINUTES
HOUR	HOURS
DAY	DAYS
WEEK	WEEKS
MONTH	MONTHS
QUARTER	QUARTERS
YEAR	YEARS
SECOND_MICROSECOND	'SECONDS.MICROSECONDS'
MINUTE_MICROSECOND	'MINUTES.MICROSECONDS'
MINUTE_SECOND	'MINUTES:SECONDS'
HOUR_MICROSECOND	'HOURS.MICROSECONDS'
HOUR_SECOND	'HOURS:MINUTES:SECONDS'
HOUR_MINUTE	'HOURS:MINUTES'
DAY_MICROSECOND	'DAYS.MICROSECONDS'
DAY_SECOND	'DAYS HOURS:MINUTES:SECONDS'
DAY_MINUTE	'DAYS HOURS:MINUTES'
DAY_HOUR	'DAYS HOURS'
YEAR_MONTH	'YEARS-MONTHS'
The values QUARTER and WEEK are available from the MySQL 5.0.0. version.

mysql> SELECT DATE_ADD('1997-12-31 23:59:59', 
   -> INTERVAL '1:1' MINUTE_SECOND);
+---------------------------------------------------------+
| DATE_ADD('1997-12-31 23:59:59', INTERVAL...             |
+---------------------------------------------------------+
| 1998-01-01 00:01:00                                     |
+---------------------------------------------------------+
1 row in set (0.00 sec)

mysql> SELECT DATE_ADD('1999-01-01', INTERVAL 1 HOUR);
+---------------------------------------------------------+
| DATE_ADD('1999-01-01', INTERVAL 1 HOUR)                 |
+---------------------------------------------------------+
| 1999-01-01 01:00:00                                     |
+---------------------------------------------------------+
1 row in set (0.00 sec)
DATE_FORMAT(date,format)
This command formats the date value as per the format string.

The following specifiers may be used in the format string. The '%' character is required before the format specifier characters.

Sr.No.	Specifier & Description
1	
%a

Abbreviated weekday name (Sun..Sat)

2	
%b

Abbreviated month name (Jan..Dec)

3	
%c

Month, numeric (0..12)

4	
%D

Day of the month with English suffix (0th, 1st, 2nd, 3rd, .)

5	
%d

Day of the month, numeric (00..31)

6	
%e

Day of the month, numeric (0..31)

7	
%f

Microseconds (000000..999999)

8	
%H

Hour (00..23)

9	
%h

Hour (01..12)

10	
%I

Hour (01..12)

11	
%i

Minutes, numeric (00..59)

12	
%j

Day of year (001..366)

13	
%k

Hour (0..23)

14	
%l

Hour (1..12)

15	
%M

Month name (January..December)

16	
%m

Month, numeric (00..12)

17	
%p

AM or PM

18	
%r

Time, 12-hour (hh:mm:ss followed by AM or PM)

19	
%S

Seconds (00..59)

20	
%s

Seconds (00..59)

21	
%T

Time, 24-hour (hh:mm:ss)

22	
%U

Week (00..53), where Sunday is the first day of the week

23	
%u

Week (00..53), where Monday is the first day of the week

24	
%V

Week (01..53), where Sunday is the first day of the week; used with %X

25	
%v

Week (01..53), where Monday is the first day of the week; used with %x

26	
%W

Weekday name (Sunday..Saturday)

27	
%w

Day of the week (0=Sunday..6=Saturday)

28	
%X

Year for the week where Sunday is the first day of the week, numeric, four digits; used with %V

29	
%x

Year for the week, where Monday is the first day of the week, numeric, four digits; used with %v

30	
%Y

Year, numeric, four digits

31	
%y

Year, numeric (two digits)

32	
%%

A literal .%. character

33	
%x

x, for any.x. not listed above

mysql> SELECT DATE_FORMAT('1997-10-04 22:23:00', '%W %M %Y');
+---------------------------------------------------------+
| DATE_FORMAT('1997-10-04 22:23:00', '%W %M %Y')          |
+---------------------------------------------------------+
| Saturday October 1997                                   |
+---------------------------------------------------------+
1 row in set (0.00 sec)

mysql> SELECT DATE_FORMAT('1997-10-04 22:23:00'
   -> '%H %k %I %r %T %S %w');
+---------------------------------------------------------+
| DATE_FORMAT('1997-10-04 22:23:00.......                 |
+---------------------------------------------------------+
|  22 22 10 10:23:00 PM 22:23:00 00 6                     |
+---------------------------------------------------------+
1 row in set (0.00 sec)
DATE_SUB(date,INTERVAL expr unit)
This is similar to the DATE_ADD() function.

DAY(date)
The DAY() is a synonym for the DAYOFMONTH() function.

DAYNAME(date)
Returns the name of the weekday for date.

mysql> SELECT DAYNAME('1998-02-05');
+---------------------------------------------------------+
| DAYNAME('1998-02-05')                                   |
+---------------------------------------------------------+
| Thursday                                                |
+---------------------------------------------------------+
1 row in set (0.00 sec)
DAYOFMONTH(date)
Returns the day of the month for date, in the range 0 to 31.

mysql> SELECT DAYOFMONTH('1998-02-03');
+---------------------------------------------------------+
| DAYOFMONTH('1998-02-03')                                |
+---------------------------------------------------------+
| 3                                                       |
+---------------------------------------------------------+
1 row in set (0.00 sec)
DAYOFWEEK(date)
Returns the weekday index for date (1 = Sunday, 2 = Monday, ., 7 = Saturday). These index values correspond to the ODBC standard.

mysql> SELECT DAYOFWEEK('1998-02-03');
+---------------------------------------------------------+
|DAYOFWEEK('1998-02-03')                                  |
+---------------------------------------------------------+
| 3                                                       |
+---------------------------------------------------------+
1 row in set (0.00 sec)
DAYOFYEAR(date)
Returns the day of the year for date, in the range 1 to 366.

mysql> SELECT DAYOFYEAR('1998-02-03');
+---------------------------------------------------------+
| DAYOFYEAR('1998-02-03')                                 |
+---------------------------------------------------------+
| 34                                                      |
+---------------------------------------------------------+
1 row in set (0.00 sec)
EXTRACT(unit FROM date)
The EXTRACT() function uses the same kinds of unit specifiers as DATE_ADD() or DATE_SUB(), but extracts parts from the date rather than performing date arithmetic.

mysql> SELECT EXTRACT(YEAR FROM '1999-07-02');
+---------------------------------------------------------+
| EXTRACT(YEAR FROM '1999-07-02')                         |
+---------------------------------------------------------+
| 1999                                                    |
+---------------------------------------------------------+
1 row in set (0.00 sec)

mysql> SELECT EXTRACT(YEAR_MONTH FROM '1999-07-02 01:02:03');
+---------------------------------------------------------+
| EXTRACT(YEAR_MONTH FROM '1999-07-02 01:02:03')          |
+---------------------------------------------------------+
| 199907                                                  |
+---------------------------------------------------------+
1 row in set (0.00 sec)
FROM_DAYS(N)
Given a day number N, returns a DATE value.

mysql> SELECT FROM_DAYS(729669);
+---------------------------------------------------------+
| FROM_DAYS(729669)                                       |
+---------------------------------------------------------+
| 1997-10-07                                              |
+---------------------------------------------------------+
1 row in set (0.00 sec)
Note − Use FROM_DAYS() with caution on old dates. It is not intended for use with values that precede the advent of the Gregorian calendar (1582).

FROM_UNIXTIME(unix_timestamp)
FROM_UNIXTIME(unix_timestamp,format)
Returns a representation of the unix_timestamp argument as a value in 'YYYY-MM-DD HH:MM:SS or YYYYMMDDHHMMSS format, depending on whether the function is used in a string or in a numeric context. The value is expressed in the current time zone. The unix_timestamp argument is an internal timestamp values, which are produced by the UNIX_TIMESTAMP() function.

If the format is given, the result is formatted according to the format string, which is used in the same way as is listed in the entry for the DATE_FORMAT() function.

mysql> SELECT FROM_UNIXTIME(875996580);
+---------------------------------------------------------+
| FROM_UNIXTIME(875996580)                                |
+---------------------------------------------------------+
| 1997-10-04 22:23:00                                     |
+---------------------------------------------------------+
1 row in set (0.00 sec)
HOUR(time)
Returns the hour for time. The range of the return value is 0 to 23 for time-of-day values. However, the range of TIME values actually is much larger, so HOUR can return values greater than 23.

mysql> SELECT HOUR('10:05:03');
+---------------------------------------------------------+
| HOUR('10:05:03')                                        |
+---------------------------------------------------------+
| 10                                                      |
+---------------------------------------------------------+
1 row in set (0.00 sec)
LAST_DAY(date)
Takes a date or datetime value and returns the corresponding value for the last day of the month. Returns NULL if the argument is invalid.

mysql> SELECT LAST_DAY('2003-02-05');
+---------------------------------------------------------+
| LAST_DAY('2003-02-05')                                  |
+---------------------------------------------------------+
| 2003-02-28                                              |
+---------------------------------------------------------+
1 row in set (0.00 sec)
LOCALTIME and LOCALTIME()
LOCALTIME and LOCALTIME() are synonyms for NOW().

LOCALTIMESTAMP and LOCALTIMESTAMP()
LOCALTIMESTAMP and LOCALTIMESTAMP() are synonyms for NOW().

MAKEDATE(year,dayofyear)
Returns a date, given year and day-of-year values. The dayofyear value must be greater than 0 or the result will be NULL.

mysql> SELECT MAKEDATE(2001,31), MAKEDATE(2001,32);
+---------------------------------------------------------+
| MAKEDATE(2001,31), MAKEDATE(2001,32)                    |
+---------------------------------------------------------+
| '2001-01-31', '2001-02-01'                              |
+---------------------------------------------------------+
1 row in set (0.00 sec)
MAKETIME(hour,minute,second)
Returns a time value calculated from the hour, minute and second arguments.

mysql> SELECT MAKETIME(12,15,30);
+---------------------------------------------------------+
| MAKETIME(12,15,30)                                      |
+---------------------------------------------------------+
| '12:15:30'                                              |
+---------------------------------------------------------+
1 row in set (0.00 sec)
MICROSECOND(expr)
Returns the microseconds from the time or datetime expression (expr) as a number in the range from 0 to 999999.

mysql> SELECT MICROSECOND('12:00:00.123456');
+---------------------------------------------------------+
| MICROSECOND('12:00:00.123456')                          |
+---------------------------------------------------------+
| 123456                                                  |
+---------------------------------------------------------+
1 row in set (0.00 sec)
MINUTE(time)
Returns the minute for time, in the range 0 to 59.

mysql> SELECT MINUTE('98-02-03 10:05:03');
+---------------------------------------------------------+
| MINUTE('98-02-03 10:05:03')                             |
+---------------------------------------------------------+
| 5                                                       |
+---------------------------------------------------------+
1 row in set (0.00 sec)
MONTH(date)
Returns the month for date, in the range 0 to 12.

mysql> SELECT MONTH('1998-02-03')
+---------------------------------------------------------+
| MONTH('1998-02-03')                                     |
+---------------------------------------------------------+
| 2                                                       |
+---------------------------------------------------------+
1 row in set (0.00 sec)
MONTHNAME(date)
Returns the full name of the month for a date.

mysql> SELECT MONTHNAME('1998-02-05');
+---------------------------------------------------------+
| MONTHNAME('1998-02-05')                                 |
+---------------------------------------------------------+
| February                                                |
+---------------------------------------------------------+
1 row in set (0.00 sec)
NOW()
Returns the current date and time as a value in 'YYYY-MM-DD HH:MM:SS' or YYYYMMDDHHMMSS format, depending on whether the function is used in a string or numeric context. This value is expressed in the current time zone.

mysql> SELECT NOW();
+---------------------------------------------------------+
| NOW()                                                   |
+---------------------------------------------------------+
| 1997-12-15 23:50:26                                     |
+---------------------------------------------------------+
1 row in set (0.00 sec)
PERIOD_ADD(P,N)
Adds N months to a period P (in the format YYMM or YYYYMM). Returns a value in the format YYYYMM. Note that the period argument P is not a date value.

mysql> SELECT PERIOD_ADD(9801,2);
+---------------------------------------------------------+
| PERIOD_ADD(9801,2)                                      |
+---------------------------------------------------------+
| 199803                                                  |
+---------------------------------------------------------+
1 row in set (0.00 sec)
PERIOD_DIFF(P1,P2)
Returns the number of months between periods P1 and P2. These periods P1 and P2 should be in the format YYMM or YYYYMM. Note that the period arguments P1 and P2 are not date values.

mysql> SELECT PERIOD_DIFF(9802,199703);
+---------------------------------------------------------+
| PERIOD_DIFF(9802,199703)                                |
+---------------------------------------------------------+
| 11                                                      |
+---------------------------------------------------------+
1 row in set (0.00 sec)
QUARTER(date)
Returns the quarter of the year for date, in the range 1 to 4.

mysql> SELECT QUARTER('98-04-01');
+---------------------------------------------------------+
| QUARTER('98-04-01')                                     |
+---------------------------------------------------------+
| 2                                                       |
+---------------------------------------------------------+
1 row in set (0.00 sec)
SECOND(time)
Returns the second for time, in the range 0 to 59.

mysql> SELECT SECOND('10:05:03');
+---------------------------------------------------------+
| SECOND('10:05:03')                                      |
+---------------------------------------------------------+
| 3                                                       |
+---------------------------------------------------------+
1 row in set (0.00 sec)
SEC_TO_TIME(seconds)
Returns the seconds argument, converted to hours, minutes and seconds, as a value in 'HH:MM:SS' or HHMMSS format, depending on whether the function is used in a string or numeric context.

mysql> SELECT SEC_TO_TIME(2378);
+---------------------------------------------------------+
| SEC_TO_TIME(2378)                                       |
+---------------------------------------------------------+
| 00:39:38                                                |
+---------------------------------------------------------+
1 row in set (0.00 sec)
STR_TO_DATE(str,format)
This is the inverse of the DATE_FORMAT() function. It takes a string str and a format string format. The STR_TO_DATE() function returns a DATETIME value if the format string contains both date and time parts. Else, it returns a DATE or TIME value if the string contains only date or time parts.

mysql> SELECT STR_TO_DATE('04/31/2004', '%m/%d/%Y');
+---------------------------------------------------------+
| STR_TO_DATE('04/31/2004', '%m/%d/%Y')                   |
+---------------------------------------------------------+
| 2004-04-31                                              |
+---------------------------------------------------------+
1 row in set (0.00 sec)
SUBDATE(date,INTERVAL expr unit) and SUBDATE(expr,days)
When invoked with the INTERVAL form of the second argument, SUBDATE() is a synonym for DATE_SUB(). For information on the INTERVAL unit argument, see the discussion for DATE_ADD().

mysql> SELECT DATE_SUB('1998-01-02', INTERVAL 31 DAY);
+---------------------------------------------------------+
| DATE_SUB('1998-01-02', INTERVAL 31 DAY)                 |
+---------------------------------------------------------+
| 1997-12-02                                              |
+---------------------------------------------------------+
1 row in set (0.00 sec)

mysql> SELECT SUBDATE('1998-01-02', INTERVAL 31 DAY);
+---------------------------------------------------------+
| SUBDATE('1998-01-02', INTERVAL 31 DAY)                  |
+---------------------------------------------------------+
| 1997-12-02                                              |
+---------------------------------------------------------+
1 row in set (0.00 sec)
SUBTIME(expr1,expr2)
The SUBTIME() function returns expr1 . expr2 expressed as a value in the same format as expr1. The expr1 value is a time or a datetime expression, while the expr2 value is a time expression.

mysql> SELECT SUBTIME('1997-12-31 23:59:59.999999',
   -> '1 1:1:1.000002');
+---------------------------------------------------------+
| SUBTIME('1997-12-31 23:59:59.999999'...                 |
+---------------------------------------------------------+
| 1997-12-30 22:58:58.999997                              |
+---------------------------------------------------------+
1 row in set (0.00 sec)
SYSDATE()
Returns the current date and time as a value in 'YYYY-MM-DD HH:MM:SS' or YYYYMMDDHHMMSS format, depending on whether the function is used in a string or in a numeric context.

mysql> SELECT SYSDATE();
+---------------------------------------------------------+
| SYSDATE()                                               |
+---------------------------------------------------------+
| 2006-04-12 13:47:44                                     |
+---------------------------------------------------------+
1 row in set (0.00 sec)
TIME(expr)
Extracts the time part of the time or datetime expression expr and returns it as a string.

mysql> SELECT TIME('2003-12-31 01:02:03');
+---------------------------------------------------------+
| TIME('2003-12-31 01:02:03')                             |
+---------------------------------------------------------+
| 01:02:03                                                |
+---------------------------------------------------------+
1 row in set (0.00 sec)
TIMEDIFF(expr1,expr2)
The TIMEDIFF() function returns expr1 . expr2 expressed as a time value. These expr1 and expr2 values are time or date-and-time expressions, but both must be of the same type.

mysql> SELECT TIMEDIFF('1997-12-31 23:59:59.000001',
   -> '1997-12-30 01:01:01.000002');
+---------------------------------------------------------+
| TIMEDIFF('1997-12-31 23:59:59.000001'.....              |
+---------------------------------------------------------+
|  46:58:57.999999                                        |
+---------------------------------------------------------+
1 row in set (0.00 sec)
TIMESTAMP(expr), TIMESTAMP(expr1,expr2)
With a single argument, this function returns the date or datetime expression expr as a datetime value. With two arguments, it adds the time expression expr2 to the date or datetime expression expr1 and returns the result as a datetime value.

mysql> SELECT TIMESTAMP('2003-12-31');
+---------------------------------------------------------+
| TIMESTAMP('2003-12-31')                                 |
+---------------------------------------------------------+
| 2003-12-31 00:00:00                                     |
+---------------------------------------------------------+
1 row in set (0.00 sec)
TIMESTAMPADD(unit,interval,datetime_expr)
This function adds the integer expression interval to the date or datetime expression datetime_expr. The unit for interval is given by the unit argument, which should be one of the following values −

FRAC_SECOND
SECOND, MINUTE
HOUR, DAY
WEEK
MONTH
QUARTER or
YEAR
The unit value may be specified using one of the keywords as shown or with a prefix of SQL_TSI_.

For example, DAY and SQL_TSI_DAY both are legal.

mysql> SELECT TIMESTAMPADD(MINUTE,1,'2003-01-02');
+---------------------------------------------------------+
| TIMESTAMPADD(MINUTE,1,'2003-01-02')                     |
+---------------------------------------------------------+
| 2003-01-02 00:01:00                                     |
+---------------------------------------------------------+
1 row in set (0.00 sec)
TIMESTAMPDIFF(unit,datetime_expr1,datetime_expr2)
Returns the integer difference between the date or datetime expressions datetime_expr1 and datetime_expr2. The unit for the result is given by the unit argument. The legal values for the unit are the same as those listed in the description of the TIMESTAMPADD() function.

mysql> SELECT TIMESTAMPDIFF(MONTH,'2003-02-01','2003-05-01');
+---------------------------------------------------------+
| TIMESTAMPDIFF(MONTH,'2003-02-01','2003-05-01')          |
+---------------------------------------------------------+
| 3                                                       |
+---------------------------------------------------------+
1 row in set (0.00 sec)
TIME_FORMAT(time,format)
This function is used like the DATE_FORMAT() function, but the format string may contain format specifiers only for hours, minutes and seconds.

If the time value contains an hour part that is greater than 23, the %H and %k hour format specifiers produce a value larger than the usual range of 0 to 23. The other hour format specifiers produce the hour value modulo 12.

mysql> SELECT TIME_FORMAT('100:00:00', '%H %k %h %I %l');
+---------------------------------------------------------+
| TIME_FORMAT('100:00:00', '%H %k %h %I %l')              |
+---------------------------------------------------------+
| 100 100 04 04 4                                         |
+---------------------------------------------------------+
1 row in set (0.00 sec)
TIME_TO_SEC(time)
Returns the time argument converted to seconds.

mysql> SELECT TIME_TO_SEC('22:23:00');
+---------------------------------------------------------+
| TIME_TO_SEC('22:23:00')                                 |
+---------------------------------------------------------+
| 80580                                                   |
+---------------------------------------------------------+
1 row in set (0.00 sec)
TO_DAYS(date)
Given a date, returns a day number (the number of days since year 0).

mysql> SELECT TO_DAYS(950501);
+---------------------------------------------------------+
| TO_DAYS(950501)                                         |
+---------------------------------------------------------+
| 728779                                                  |
+---------------------------------------------------------+
1 row in set (0.00 sec)
UNIX_TIMESTAMP(), UNIX_TIMESTAMP(date)
If called with no argument, this function returns a Unix timestamp (seconds since '1970-01-01 00:00:00' UTC) as an unsigned integer. If UNIX_TIMESTAMP() is called with a date argument, it returns the value of the argument as seconds since '1970-01-01 00:00:00' UTC. date may be a DATE string, a DATETIME string, a TIMESTAMP, or a number in the format YYMMDD or YYYYMMDD.

mysql> SELECT UNIX_TIMESTAMP();
+---------------------------------------------------------+
| UNIX_TIMESTAMP()                                        |
+---------------------------------------------------------+
| 882226357                                               |
+---------------------------------------------------------+
1 row in set (0.00 sec)

mysql> SELECT UNIX_TIMESTAMP('1997-10-04 22:23:00');
+---------------------------------------------------------+
| UNIX_TIMESTAMP('1997-10-04 22:23:00')                   |
+---------------------------------------------------------+
| 875996580                                               |
+---------------------------------------------------------+
1 row in set (0.00 sec)
UTC_DATE, UTC_DATE()
Returns the current UTC date as a value in 'YYYY-MM-DD' or YYYYMMDD format, depending on whether the function is used in a string or numeric context.

mysql> SELECT UTC_DATE(), UTC_DATE() + 0;
+---------------------------------------------------------+
| UTC_DATE(), UTC_DATE() + 0                              |
+---------------------------------------------------------+
| 2003-08-14, 20030814                                    |
+---------------------------------------------------------+
1 row in set (0.00 sec)
UTC_TIME, UTC_TIME()
Returns the current UTC time as a value in 'HH:MM:SS' or HHMMSS format, depending on whether the function is used in a string or numeric context.

mysql> SELECT UTC_TIME(), UTC_TIME() + 0;
+---------------------------------------------------------+
| UTC_TIME(), UTC_TIME() + 0                              |
+---------------------------------------------------------+
| 18:07:53, 180753                                        |
+---------------------------------------------------------+
1 row in set (0.00 sec)
UTC_TIMESTAMP, UTC_TIMESTAMP()
Returns the current UTC date and time as a value in 'YYYY-MM-DD HH:MM:SS' or in a YYYYMMDDHHMMSS format, depending on whether the function is used in a string or in a numeric context.

mysql> SELECT UTC_TIMESTAMP(), UTC_TIMESTAMP() + 0;
+---------------------------------------------------------+
| UTC_TIMESTAMP(), UTC_TIMESTAMP() + 0                    |
+---------------------------------------------------------+
| 2003-08-14 18:08:04, 20030814180804                     |
+---------------------------------------------------------+
1 row in set (0.00 sec)
WEEK(date[,mode])
This function returns the week number for date. The two-argument form of WEEK() allows you to specify whether the week starts on a Sunday or a Monday and whether the return value should be in the range from 0 to 53 or from 1 to 53. If the mode argument is omitted, the value of the default_week_format system variable is used

Mode	First Day of week	Range	Week 1 is the first week.
0	Sunday	0-53	with a Sunday in this year
1	Monday	0-53	with more than 3 days this year
2	Sunday	1-53	with a Sunday in this year
3	Monday	1-53	with more than 3 days this year
4	Sunday	0-53	with more than 3 days this year
5	Monday	0-53	with a Monday in this year
6	Sunday	1-53	with more than 3 days this year
7	Monday	1-53	with a Monday in this year
mysql> SELECT WEEK('1998-02-20');
+---------------------------------------------------------+
| WEEK('1998-02-20')                                      |
+---------------------------------------------------------+
| 7                                                       |
+---------------------------------------------------------+
1 row in set (0.00 sec)
WEEKDAY(date)
Returns the weekday index for date (0 = Monday, 1 = Tuesday, . 6 = Sunday).

mysql> SELECT WEEKDAY('1998-02-03 22:23:00');
+---------------------------------------------------------+
| WEEKDAY('1998-02-03 22:23:00')                          |
+---------------------------------------------------------+
| 1                                                       |
+---------------------------------------------------------+
1 row in set (0.00 sec)
WEEKOFYEAR(date)
Returns the calendar week of the date as a number in the range from 1 to 53. WEEKOFYEAR() is a compatibility function that is equivalent to WEEK(date,3).

mysql> SELECT WEEKOFYEAR('1998-02-20');
+---------------------------------------------------------+
| WEEKOFYEAR('1998-02-20')                                |
+---------------------------------------------------------+
| 8                                                       |
+---------------------------------------------------------+
1 row in set (0.00 sec)
YEAR(date)
Returns the year for date, in the range 1000 to 9999, or 0 for the .zero. date.

mysql> SELECT YEAR('98-02-03');
+---------------------------------------------------------+
| YEAR('98-02-03')                                        |
+---------------------------------------------------------+
| 1998                                                    |
+---------------------------------------------------------+
1 row in set (0.00 sec)
YEARWEEK(date), YEARWEEK(date,mode)
Returns the year and the week for a date. The mode argument works exactly like the mode argument to the WEEK() function. The year in the result may be different from the year in the date argument for the first and the last week of the year.

mysql> SELECT YEARWEEK('1987-01-01');
+---------------------------------------------------------+
| YEAR('98-02-03')YEARWEEK('1987-01-01')                  |
+---------------------------------------------------------+
| 198653                                                  |
+---------------------------------------------------------+
1 row in set (0.00 sec)
Note − The week number is different from what the WEEK() function would return (0) for optional arguments 0 or 1, as WEEK() then returns the week in the context of the given year.

        3.1.33 SQL - temporary tables
What are Temporary Tables?
There are RDBMS, which support temporary tables. Temporary Tables are a great feature that lets you store and process intermediate results by using the same selection, update, and join capabilities that you can use with typical SQL Server tables.

The temporary tables could be very useful in some cases to keep temporary data. The most important thing that should be known for temporary tables is that they will be deleted when the current client session terminates.

Temporary tables are available in MySQL version 3.23 onwards. If you use an older version of MySQL than 3.23, you can't use temporary tables, but you can use heap tables.

As stated earlier, temporary tables will only last as long as the session is alive. If you run the code in a PHP script, the temporary table will be destroyed automatically when the script finishes executing. If you are connected to the MySQL database server through the MySQL client program, then the temporary table will exist until you close the client or manually destroy the table.

Example
Here is an example showing you the usage of a temporary table.

mysql> CREATE TEMPORARY TABLE SALESSUMMARY (
   -> product_name VARCHAR(50) NOT NULL
   -> , total_sales DECIMAL(12,2) NOT NULL DEFAULT 0.00
   -> , avg_unit_price DECIMAL(7,2) NOT NULL DEFAULT 0.00
   -> , total_units_sold INT UNSIGNED NOT NULL DEFAULT 0
);
Query OK, 0 rows affected (0.00 sec)

mysql> INSERT INTO SALESSUMMARY
   -> (product_name, total_sales, avg_unit_price, total_units_sold)
   -> VALUES
   -> ('cucumber', 100.25, 90, 2);

mysql> SELECT * FROM SALESSUMMARY;
+--------------+-------------+----------------+------------------+
| product_name | total_sales | avg_unit_price | total_units_sold |
+--------------+-------------+----------------+------------------+
| cucumber     |      100.25 |          90.00 |                2 |
+--------------+-------------+----------------+------------------+
1 row in set (0.00 sec)
When you issue a SHOW TABLES command, then your temporary table will not be listed out in the list. Now, if you log out of the MySQL session and then issue a SELECT command, you will find no data available in the database. Even your temporary table will not be existing.

Dropping Temporary Tables
By default, all the temporary tables are deleted by MySQL when your database connection gets terminated. Still if you want to delete them in between, then you can do so by issuing a DROP TABLE command.

Following is an example on dropping a temporary table.

mysql> CREATE TEMPORARY TABLE SALESSUMMARY (
   -> product_name VARCHAR(50) NOT NULL
   -> , total_sales DECIMAL(12,2) NOT NULL DEFAULT 0.00
   -> , avg_unit_price DECIMAL(7,2) NOT NULL DEFAULT 0.00
   -> , total_units_sold INT UNSIGNED NOT NULL DEFAULT 0
);
Query OK, 0 rows affected (0.00 sec)

mysql> INSERT INTO SALESSUMMARY
   -> (product_name, total_sales, avg_unit_price, total_units_sold)
   -> VALUES
   -> ('cucumber', 100.25, 90, 2);

mysql> SELECT * FROM SALESSUMMARY;
+--------------+-------------+----------------+------------------+
| product_name | total_sales | avg_unit_price | total_units_sold |
+--------------+-------------+----------------+------------------+
| cucumber     |      100.25 |          90.00 |                2 |
+--------------+-------------+----------------+------------------+
1 row in set (0.00 sec)
mysql> DROP TABLE SALESSUMMARY;
mysql>  SELECT * FROM SALESSUMMARY;
ERROR 1146: Table 'TUTORIALS.SALESSUMMARY' doesn't exist

        3.1.34 SQL - clone tables
There may be a situation when you need an exact copy of a table and the CREATE TABLE ... or the SELECT... commands does not suit your purposes because the copy must include the same indexes, default values and so forth.

If you are using MySQL RDBMS, you can handle this situation by adhering to the steps given below −

Use SHOW CREATE TABLE command to get a CREATE TABLE statement that specifies the source table's structure, indexes and all.

Modify the statement to change the table name to that of the clone table and execute the statement. This way you will have an exact clone table.

Optionally, if you need the table contents copied as well, issue an INSERT INTO or a SELECT statement too.

Example
Try out the following example to create a clone table for TUTORIALS_TBL whose structure is as follows −

Step 1 − Get the complete structure about the table.

SQL> SHOW CREATE TABLE TUTORIALS_TBL \G; 
*************************** 1. row *************************** 
      Table: TUTORIALS_TBL 
Create Table: CREATE TABLE 'TUTORIALS_TBL' ( 
  'tutorial_id' int(11) NOT NULL auto_increment, 
  'tutorial_title' varchar(100) NOT NULL default '', 
  'tutorial_author' varchar(40) NOT NULL default '', 
  'submission_date' date default NULL, 
  PRIMARY KEY  ('tutorial_id'), 
  UNIQUE KEY 'AUTHOR_INDEX' ('tutorial_author') 
) TYPE = MyISAM 
1 row in set (0.00 sec)
Step 2 − Rename this table and create another table.

SQL> CREATE TABLE `CLONE_TBL` ( 
  -> 'tutorial_id' int(11) NOT NULL auto_increment, 
  -> 'tutorial_title' varchar(100) NOT NULL default '', 
  -> 'tutorial_author' varchar(40) NOT NULL default '', 
  -> 'submission_date' date default NULL, 
  -> PRIMARY KEY  (`tutorial_id'), 
  -> UNIQUE KEY 'AUTHOR_INDEX' ('tutorial_author') 
-> ) TYPE = MyISAM; 
Query OK, 0 rows affected (1.80 sec) 
Step 3 − After executing step 2, you will clone a table in your database. If you want to copy data from an old table, then you can do it by using the INSERT INTO... SELECT statement.

SQL> INSERT INTO CLONE_TBL (tutorial_id, 
   ->                        tutorial_title, 
   ->                        tutorial_author, 
   ->                        submission_date) 
   -> SELECT tutorial_id,tutorial_title, 
   ->        tutorial_author,submission_date, 
   -> FROM TUTORIALS_TBL; 
Query OK, 3 rows affected (0.07 sec) 
Records: 3  Duplicates: 0  Warnings: 0 
Finally, you will have an exact clone table as you wanted to have.


        3.1.35 Inner Queries , sub queries
A Subquery or Inner query or a Nested query is a query within another SQL query and embedded within the WHERE clause.

A subquery is used to return data that will be used in the main query as a condition to further restrict the data to be retrieved.

Subqueries can be used with the SELECT, INSERT, UPDATE, and DELETE statements along with the operators like =, <, >, >=, <=, IN, BETWEEN, etc.

There are a few rules that subqueries must follow −

Subqueries must be enclosed within parentheses.

A subquery can have only one column in the SELECT clause, unless multiple columns are in the main query for the subquery to compare its selected columns.

An ORDER BY command cannot be used in a subquery, although the main query can use an ORDER BY. The GROUP BY command can be used to perform the same function as the ORDER BY in a subquery.

Subqueries that return more than one row can only be used with multiple value operators such as the IN operator.

The SELECT list cannot include any references to values that evaluate to a BLOB, ARRAY, CLOB, or NCLOB.

A subquery cannot be immediately enclosed in a set function.

The BETWEEN operator cannot be used with a subquery. However, the BETWEEN operator can be used within the subquery.

Subqueries with the SELECT Statement
Subqueries are most frequently used with the SELECT statement. The basic syntax is as follows −

SELECT column_name [, column_name ]
FROM   table1 [, table2 ]
WHERE  column_name OPERATOR
   (SELECT column_name [, column_name ]
   FROM table1 [, table2 ]
   [WHERE])
Example
Consider the CUSTOMERS table having the following records −

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  35 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
Now, let us check the following subquery with a SELECT statement.

SQL> SELECT * 
   FROM CUSTOMERS 
   WHERE ID IN (SELECT ID 
         FROM CUSTOMERS 
         WHERE SALARY > 4500) ;
This would produce the following result.

+----+----------+-----+---------+----------+
| ID | NAME     | AGE | ADDRESS | SALARY   |
+----+----------+-----+---------+----------+
|  4 | Chaitali |  25 | Mumbai  |  6500.00 |
|  5 | Hardik   |  27 | Bhopal  |  8500.00 |
|  7 | Muffy    |  24 | Indore  | 10000.00 |
+----+----------+-----+---------+----------+
Subqueries with the INSERT Statement
Subqueries also can be used with INSERT statements. The INSERT statement uses the data returned from the subquery to insert into another table. The selected data in the subquery can be modified with any of the character, date or number functions.

The basic syntax is as follows.

INSERT INTO table_name [ (column1 [, column2 ]) ]
   SELECT [ *|column1 [, column2 ]
   FROM table1 [, table2 ]
   [ WHERE VALUE OPERATOR ]
Example
Consider a table CUSTOMERS_BKP with similar structure as CUSTOMERS table. Now to copy the complete CUSTOMERS table into the CUSTOMERS_BKP table, you can use the following syntax.

SQL> INSERT INTO CUSTOMERS_BKP
   SELECT * FROM CUSTOMERS 
   WHERE ID IN (SELECT ID 
   FROM CUSTOMERS) ;
Subqueries with the UPDATE Statement
The subquery can be used in conjunction with the UPDATE statement. Either single or multiple columns in a table can be updated when using a subquery with the UPDATE statement.

The basic syntax is as follows.

UPDATE table
SET column_name = new_value
[ WHERE OPERATOR [ VALUE ]
   (SELECT COLUMN_NAME
   FROM TABLE_NAME)
   [ WHERE) ]
Example
Assuming, we have CUSTOMERS_BKP table available which is backup of CUSTOMERS table. The following example updates SALARY by 0.25 times in the CUSTOMERS table for all the customers whose AGE is greater than or equal to 27.

SQL> UPDATE CUSTOMERS
   SET SALARY = SALARY * 0.25
   WHERE AGE IN (SELECT AGE FROM CUSTOMERS_BKP
      WHERE AGE >= 27 );
This would impact two rows and finally CUSTOMERS table would have the following records.

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  35 | Ahmedabad |   125.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  2125.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
Subqueries with the DELETE Statement
The subquery can be used in conjunction with the DELETE statement like with any other statements mentioned above.

The basic syntax is as follows.

DELETE FROM TABLE_NAME
[ WHERE OPERATOR [ VALUE ]
   (SELECT COLUMN_NAME
   FROM TABLE_NAME)
   [ WHERE) ]
Example
Assuming, we have a CUSTOMERS_BKP table available which is a backup of the CUSTOMERS table. The following example deletes the records from the CUSTOMERS table for all the customers whose AGE is greater than or equal to 27.

SQL> DELETE FROM CUSTOMERS
   WHERE AGE IN (SELECT AGE FROM CUSTOMERS_BKP
      WHERE AGE >= 27 );
This would impact two rows and finally the CUSTOMERS table would have the following records.

+----+----------+-----+---------+----------+
| ID | NAME     | AGE | ADDRESS | SALARY   |
+----+----------+-----+---------+----------+
|  2 | Khilan   |  25 | Delhi   |  1500.00 |
|  3 | kaushik  |  23 | Kota    |  2000.00 |
|  4 | Chaitali |  25 | Mumbai  |  6500.00 |
|  6 | Komal    |  22 | MP      |  4500.00 |
|  7 | Muffy    |  24 | Indore  | 10000.00 |
+----+----------+-----+---------+----------+

        3.1.36 sequences
A sequence is a set of integers 1, 2, 3, ... that are generated in order on demand. Sequences are frequently used in databases because many applications require each row in a table to contain a unique value and sequences provide an easy way to generate them.

This chapter describes how to use sequences in MySQL.

Using AUTO_INCREMENT column
The simplest way in MySQL to use sequences is to define a column as AUTO_INCREMENT and leave the rest to MySQL to take care.

Example
Try out the following example. This will create a table and after that it will insert a few rows in this table where it is not required to give a record ID because its auto-incremented by MySQL.

mysql> CREATE TABLE INSECT
   -> (
   -> id INT UNSIGNED NOT NULL AUTO_INCREMENT,
   -> PRIMARY KEY (id),
   -> name VARCHAR(30) NOT NULL, # type of insect
   -> date DATE NOT NULL, # date collected
   -> origin VARCHAR(30) NOT NULL # where collected
);
Query OK, 0 rows affected (0.02 sec)
mysql> INSERT INTO INSECT (id,name,date,origin) VALUES
   -> (NULL,'housefly','2001-09-10','kitchen'),
   -> (NULL,'millipede','2001-09-10','driveway'),
   -> (NULL,'grasshopper','2001-09-10','front yard');
Query OK, 3 rows affected (0.02 sec)
Records: 3  Duplicates: 0  Warnings: 0
mysql> SELECT * FROM INSECT ORDER BY id;
+----+-------------+------------+------------+
| id | name        | date       | origin     |
+----+-------------+------------+------------+
|  1 | housefly    | 2001-09-10 | kitchen    |
|  2 | millipede   | 2001-09-10 | driveway   |
|  3 | grasshopper | 2001-09-10 | front yard |
+----+-------------+------------+------------+
3 rows in set (0.00 sec)
Obtain AUTO_INCREMENT Values
The LAST_INSERT_ID( ) is an SQL function, so you can use it from within any client that understands how to issue SQL statements. Otherwise PERL and PHP scripts provide exclusive functions to retrieve auto-incremented value of last record.

PERL Example
Use the mysql_insertid attribute to obtain the AUTO_INCREMENT value generated by a query. This attribute is accessed through either a database handle or a statement handle, depending on how you issue the query. The following example references it through the database handle.

$dbh->do ("INSERT INTO INSECT (name,date,origin)
VALUES('moth','2001-09-14','windowsill')");
my $seq = $dbh->{mysql_insertid};
PHP Example
After issuing a query that generates an AUTO_INCREMENT value, retrieve the value by calling the mysql_insert_id( ) function.

mysql_query ("INSERT INTO INSECT (name,date,origin)
VALUES('moth','2001-09-14','windowsill')", $conn_id);
$seq = mysql_insert_id ($conn_id);
Renumbering an Existing Sequence
There may be a case when you have deleted many records from a table and you want to re-sequence all the records. This can be done by using a simple trick, but you should be very careful to do this and check if your table is having a join with another table or not.

If you determine that resequencing an AUTO_INCREMENT column is unavoidable, the way to do it is to drop the column from the table, then add it again.

The following example shows how to renumber the id values in the insect table using this technique.

mysql> ALTER TABLE INSECT DROP id;
mysql> ALTER TABLE insect
   -> ADD id INT UNSIGNED NOT NULL AUTO_INCREMENT FIRST,
   -> ADD PRIMARY KEY (id);
Starting a Sequence at a Particular Value
By default, MySQL will start the sequence from 1, but you can specify any other number as well at the time of table creation.

The following code block has an example where MySQL will start sequence from 100.

mysql> CREATE TABLE INSECT
   -> (
   -> id INT UNSIGNED NOT NULL AUTO_INCREMENT = 100,
   -> PRIMARY KEY (id),
   -> name VARCHAR(30) NOT NULL, # type of insect
   -> date DATE NOT NULL, # date collected
   -> origin VARCHAR(30) NOT NULL # where collected
);
Alternatively, you can create the table and then set the initial sequence value with ALTER TABLE.

mysql> ALTER TABLE t AUTO_INCREMENT = 100;


        3.1.37 DISTINCT, handle duplicates
There may be a situation when you have multiple duplicate records in a table. While fetching such records, it makes more sense to fetch only unique records instead of fetching duplicate records.

The SQL DISTINCT keyword, which we have already discussed is used in conjunction with the SELECT statement to eliminate all the duplicate records and by fetching only the unique records.

Syntax
The basic syntax of a DISTINCT keyword to eliminate duplicate records is as follows.

SELECT DISTINCT column1, column2,.....columnN 
FROM table_name
WHERE [condition]
Example
Consider the CUSTOMERS table having the following records.

+----+----------+-----+-----------+----------+
| ID | NAME     | AGE | ADDRESS   | SALARY   |
+----+----------+-----+-----------+----------+
|  1 | Ramesh   |  32 | Ahmedabad |  2000.00 |
|  2 | Khilan   |  25 | Delhi     |  1500.00 |
|  3 | kaushik  |  23 | Kota      |  2000.00 |
|  4 | Chaitali |  25 | Mumbai    |  6500.00 |
|  5 | Hardik   |  27 | Bhopal    |  8500.00 |
|  6 | Komal    |  22 | MP        |  4500.00 |
|  7 | Muffy    |  24 | Indore    | 10000.00 |
+----+----------+-----+-----------+----------+
First, let us see how the following SELECT query returns duplicate salary records.

SQL> SELECT SALARY FROM CUSTOMERS
   ORDER BY SALARY;
This would produce the following result where the salary of 2000 is coming twice which is a duplicate record from the original table.

+----------+
| SALARY   |
+----------+
|  1500.00 |
|  2000.00 |
|  2000.00 |
|  4500.00 |
|  6500.00 |
|  8500.00 |
| 10000.00 |
+----------+
Now, let us use the DISTINCT keyword with the above SELECT query and see the result.

SQL> SELECT DISTINCT SALARY FROM CUSTOMERS
   ORDER BY SALARY;
This would produce the following result where we do not have any duplicate entry.

+----------+
| SALARY   |
+----------+
|  1500.00 |
|  2000.00 |
|  4500.00 |
|  6500.00 |
|  8500.00 |
| 10000.00 |
+----------+

        3.1.38 preventing SQL injection
If you take a user input through a webpage and insert it into a SQL database, there is a chance that you have left yourself wide open for a security issue known as the SQL Injection. This chapter will teach you how to help prevent this from happening and help you secure your scripts and SQL statements in your server side scripts such as a PERL Script.

Injection usually occurs when you ask a user for input, like their name and instead of a name they give you a SQL statement that you will unknowingly run on your database. Never trust user provided data, process this data only after validation; as a rule, this is done by Pattern Matching.

In the example below, the name is restricted to the alphanumerical characters plus underscore and to a length between 8 and 20 characters (modify these rules as needed).

if (preg_match("/^\w{8,20}$/", $_GET['username'], $matches)) {
   $result = mysql_query("SELECT * FROM CUSTOMERS 
      WHERE name = $matches[0]");
} else {
   echo "user name not accepted";
}
To demonstrate the problem, consider this excerpt −

// supposed input
$name = "Qadir'; DELETE FROM CUSTOMERS;";
mysql_query("SELECT * FROM CUSTOMSRS WHERE name='{$name}'");
The function call is supposed to retrieve a record from the CUSTOMERS table where the name column matches the name specified by the user. Under normal circumstances, $name would only contain alphanumeric characters and perhaps spaces, such as the string ilia. But here, by appending an entirely new query to $name, the call to the database turns into disaster; the injected DELETE query removes all records from the CUSTOMERS table.

Fortunately, if you use MySQL, the mysql_query() function does not permit query stacking or executing multiple SQL queries in a single function call. If you try to stack queries, the call fails.

However, other PHP database extensions, such as SQLite and PostgreSQL happily perform stacked queries, executing all the queries provided in one string and creating a serious security problem.

Preventing SQL Injection
You can handle all escape characters smartly in scripting languages like PERL and PHP. The MySQL extension for PHP provides the function mysql_real_escape_string() to escape input characters that are special to MySQL.

if (get_magic_quotes_gpc()) {
   $name = stripslashes($name);
}
$name = mysql_real_escape_string($name);
mysql_query("SELECT * FROM CUSTOMERS WHERE name='{$name}'");
The LIKE Quandary
To address the LIKE quandary, a custom escaping mechanism must convert user-supplied '%' and '_' characters to literals. Use addcslashes(), a function that lets you specify a character range to escape.

$sub = addcslashes(mysql_real_escape_string("%str"), "%_");
// $sub == \%str\_
mysql_query("SELECT * FROM messages 
   WHERE subject LIKE '{$sub}%'");

        3.1.39 Copy from one table to another , backup table 
Copy all columns into a new table:

SELECT *
INTO newtable [IN externaldb]
FROM oldtable
WHERE condition;
Copy only some columns into a new table:

SELECT column1, column2, column3, ...
INTO newtable [IN externaldb]
FROM oldtable
WHERE condition;
        3.1.40
    3.2 advanced

        3.2.1 coalesce
return first non null value
ex:
SELECT COALESCE(NULL, NULL, NULL, 'W3Schools.com', NULL, 'Example.com');
W3Schools.com

        3.2.2

    3.3

    3.4 LLMs, Chatgpt, Gemini  etc, tags: LLMs Chatgpt Gemini

        3.4.1 q. difference between union and inner join sql operations
a.
In SQL, both UNION and INNER JOIN are used to combine data from two or more tables. However, they work in different ways and have different purposes.

UNION is used to combine the results of two or more SELECT statements into a single result set. The result set of a UNION operation contains all the distinct rows that appear in either or both of the SELECT statements. The syntax for a UNION operation is as follows:

sql
SELECT column1, column2, ... FROM table1
UNION
SELECT column1, column2, ... FROM table2;
INNER JOIN, on the other hand, is used to combine rows from two or more tables based on a common column between them. It returns only the rows that have matching values in both tables, according to the specified join condition. The syntax for an INNER JOIN operation is as follows:

sql
SELECT table1.column1, table1.column2, ..., table2.column1, table2.column2, ...
FROM table1
INNER JOIN table2
ON table1.common_column = table2.common_column;
So, the main difference between UNION and INNER JOIN is that UNION combines rows from two or more SELECT statements into a single result set, while INNER JOIN combines rows from two or more tables based on a common column

        3.4.2 q. can inner join be used on same table

a. Yes, an INNER JOIN can be used on the same table in SQL. This is known as a self-join.

A self-join can be useful when you need to compare rows within the same table, for example, when you have a table that contains hierarchical data, such as an organizational chart. In a self-join, you use two or more instances of the same table in the FROM clause and specify different aliases for each instance.

Here's an example of a self-join:

SELECT e1.employee_id, e1.employee_name, e2.employee_name AS manager_name
FROM employees e1
INNER JOIN employees e2
ON e1.manager_id = e2.employee_id;
In this example, the employees table has a manager_id column that contains the ID of the employee's manager. By joining the table to itself using this column, we can retrieve each employee's name and the name of their manager. The first instance of the table is aliased as "e1", and the second instance is aliased as "e2". The join condition is specified using the manager_id column in e1 and the employee_id column in e2.

        3.4.3 q. other use cases?
a. Here are some other use cases where a self-join (an INNER JOIN on the same table) can be helpful in SQL:

Finding all pairs of rows that have matching values in a certain column - For example, if you have a table of sales transactions and you want to find all pairs of transactions that have the same customer name and date.

Hierarchical data - As I mentioned earlier, self-joins are useful for working with hierarchical data such as organizational charts, family trees, or product categories.

Calculating running totals or averages - If you have a table of transactions with a date column and a value column, you can use a self-join to calculate running totals or averages over time. For example, you could join the table to itself on the date column and calculate the sum or average of the value column for all rows up to and including the current date.

Finding gaps or overlaps in data - If you have a table of time intervals, you can use a self-join to find any gaps or overlaps in the intervals. For example, you could join the table to itself on the start and end times of each interval and check if any intervals overlap or have gaps between them.

These are just a few examples of the many use cases for self-joins in SQL.

        3.4.4 q. can you provide sql examples for these use cases?
a. Sure, here are some SQL examples for the different use cases of self-joins:

Finding all pairs of rows that have matching values in a certain column:
SELECT t1.*
FROM transactions t1
INNER JOIN transactions t2
ON t1.customer_name = t2.customer_name AND t1.date = t2.date AND t1.transaction_id < t2.transaction_id;
This query finds all pairs of transactions that have the same customer name and date, and where the transaction ID of the first transaction is less than the transaction ID of the second transaction. This ensures that each pair is only counted once.

Hierarchical data:
SELECT e.employee_name, m.employee_name AS manager_name
FROM employees e
INNER JOIN employees m
ON e.manager_id = m.employee_id;
This query finds the name of each employee and the name of their manager, by joining the employees table to itself on the manager ID column.

Calculating running totals or averages:
SELECT t1.date, SUM(t2.value) AS running_total
FROM transactions t1
INNER JOIN transactions t2
ON t2.date <= t1.date
GROUP BY t1.date;
This query calculates the running total of the value column for each date, by joining the transactions table to itself on the date column and summing the value column for all rows with a date less than or equal to the current date.

Finding gaps or overlaps in data:
SELECT t1.*
FROM intervals t1
INNER JOIN intervals t2
ON t1.start_time < t2.end_time AND t1.end_time > t2.start_time;
This query finds any pairs of intervals that overlap, by joining the intervals table to itself on the start and end times of each interval. If the start time of the first interval is less than the end time of the second interval, and the end time of the first interval is greater than the start time of the second interval, then the intervals overlap.

        3.4.5 q. give an example involving source PG DB and nodejs client application and target PG DB and nodejs client application

            3.4.5.1  using pg-copy-streams
ChatGPT
Sure, here's an example scenario:

Let's say you have a source PostgreSQL database (PG DB) containing customer information for an e-commerce website. You also have a Node.js client application that interacts with this database to retrieve and display customer data.

Now, you want to migrate this customer data to a new target PostgreSQL database. You have set up a new PG DB and a Node.js client application that will interact with this new database.

To accomplish this migration, you can use a Node.js library like "pg-copy-streams" to stream the data from the source database to the target database. Here's an example code snippet that demonstrates how this can be done:

const { Client } = require('pg');
const { copyFrom } = require('pg-copy-streams');

// Connect to source database
const srcClient = new Client({
  user: 'source_user',
  host: 'source_host',
  database: 'source_db',
  password: 'source_password',
  port: 5432,
});
await srcClient.connect();

// Connect to target database
const tgtClient = new Client({
  user: 'target_user',
  host: 'target_host',
  database: 'target_db',
  password: 'target_password',
  port: 5432,
});
await tgtClient.connect();

// Stream data from source to target database
const query = copyFrom('COPY customers TO STDOUT');
const stream = srcClient.query(query);
stream.pipe(tgtClient.query(query));

// Close connections
stream.on('end', () => {
  srcClient.end();
  tgtClient.end();
});
In this example, we first create two PostgreSQL client instances: srcClient for the source database and tgtClient for the target database. We then connect to both databases using the connect() method.

Next, we use the pg-copy-streams library to create a query object that copies the customer data from the customers table in the source database to the standard output (STDOUT).

We then create a stream by executing the query on the srcClient and piping it to the same query executed on the tgtClient, which inserts the data into the customers table in the target database.

Finally, we close both database connections when the stream ends using the end() method.

With this code, you can migrate the customer data from the source database to the target database using a Node.js client application.

            3.4.5.2 without using pg-copy-streams

ChatGPT Sure, here's an example of how you can migrate data between two PostgreSQL databases using a Node.js client application without using the pg-copy-streams library:

const { Client } = require('pg');

// Connect to source database
const srcClient = new Client({
  user: 'source_user',
  host: 'source_host',
  database: 'source_db',
  password: 'source_password',
  port: 5432,
});
await srcClient.connect();

// Connect to target database
const tgtClient = new Client({
  user: 'target_user',
  host: 'target_host',
  database: 'target_db',
  password: 'target_password',
  port: 5432,
});
await tgtClient.connect();

// Select data from source database
const { rows: customers } = await srcClient.query('SELECT * FROM customers');

// Insert data into target database
for (const customer of customers) {
  const { first_name, last_name, email } = customer;
  const query = {
    text: 'INSERT INTO customers (first_name, last_name, email) VALUES ($1, $2, $3)',
    values: [first_name, last_name, email],
  };
  await tgtClient.query(query);
}

// Close connections
srcClient.end();
tgtClient.end();
In this example, we first create two PostgreSQL client instances: srcClient for the source database and tgtClient for the target database. We then connect to both databases using the connect() method.

Next, we select the customer data from the customers table in the source database using the query() method on the srcClient.

We then loop through the customers array and insert each customer record into the customers table in the target database using the query() method on the tgtClient. We create a query object that contains an INSERT statement with parameterized values, and we pass the values from each customer record to the values property of the query object.

Finally, we close both database connections using the end() method.

With this code, you can migrate the customer data from the source database to the target database using a Node.js client application without using the pg-copy-streams library.

            3.4.5.3 give an example of a nodejs application that interacts with a PG DB and exposes and endpoint that allows to download a file which is the result of running a select query on the DB
ChatGPT
Sure! Here's an example of a Node.js application that interacts with a PostgreSQL database and exposes an endpoint that allows users to download a file that is the result of running a SELECT query on the database:

const express = require('express');
const { Client } = require('pg');
const fs = require('fs');
const { pipeline } = require('stream');
const app = express();
const port = 3000;

const client = new Client({
  user: 'myuser',
  host: 'localhost',
  database: 'mydb',
  password: 'mypassword',
  port: 5432,
});

async function getResults(query) {
  const { rows } = await client.query(query);
  return rows;
}

app.get('/download', async (req, res) => {
  try {
    const results = await getResults('SELECT * FROM mytable');
    const filename = 'results.csv';

    const fileStream = fs.createWriteStream(filename);
    results.forEach((result) => {
      fileStream.write(`${result.id},${result.name}\n`);
    });
    fileStream.end();

    res.setHeader('Content-Disposition', `attachment; filename="${filename}"`);
    res.setHeader('Content-Type', 'text/csv');

    const readStream = fs.createReadStream(filename);
    pipeline(readStream, res, (err) => {
      if (err) {
        console.error(err);
        res.status(500).send('Server error');
      } else {
        fs.unlink(filename, (err) => {
          if (err) {
            console.error(err);
          }
        });
      }
    });
  } catch (err) {
    console.error(err);
    res.status(500).send('Server error');
  }
});

app.listen(port, () => {
  console.log(`Server running at http://localhost:${port}`);
});
In this example, we are using the Express.js framework to create an HTTP server that listens on port 3000. We are also using the pg library to connect to a PostgreSQL database.

We define a getResults() function that takes a SQL query as an argument and returns the result set as an array of objects.

In the /download endpoint handler, we call the getResults() function with a SELECT query to retrieve the results from the database. We then write the results to a CSV file named results.csv using the fs module. We set the Content-Disposition header to indicate that the response should be downloaded as an attachment with the filename results.csv, and we set the Content-Type header to text/csv.

We create a read stream from the results.csv file and use the pipeline() function from the stream module to pipe the file contents to the HTTP response. We also unlink the file after it has been sent to the client.

If an error occurs during the download process, we log the error and send a 500 status code with a message indicating a server error.

Note that this example is just one possible implementation, and you may need to modify it to fit your specific requirements and use case.



            3.4.5.4 User create an example HTTP page with a button JS code that allows user to click it in order to download the file
ChatGPT
Sure, here's an example of an HTTP page with a button that allows users to click it to download a file:

html
Copy code
<!DOCTYPE html>
<html>
  <head>
    <title>Download File Example</title>
  </head>
  <body>
    <button id="download-btn">Download File</button>

    <script>
      const downloadBtn = document.getElementById('download-btn');
      downloadBtn.addEventListener('click', async () => {
        try {
          const response = await fetch('/download');
          const filename = response.headers.get('Content-Disposition').split('filename=')[1];
          const blob = await response.blob();
          const link = document.createElement('a');
          link.href = URL.createObjectURL(blob);
          link.download = filename;
          link.click();
        } catch (err) {
          console.error(err);
          alert('Failed to download file');
        }
      });
    </script>
  </body>
</html>
In this example, we have an HTML page with a button element with an ID of download-btn. We attach a click event listener to the button that sends an HTTP GET request to the /download endpoint using the fetch() function.

When the server responds with the file, we extract the filename from the Content-Disposition header using the split() method. We then use the blob() method to convert the response body to a Blob object.

Next, we create a new <a> element, set its href attribute to a URL created from the Blob using the URL.createObjectURL() method, set its download attribute to the filename, and programmatically click the link using the click() method.

If an error occurs during the download process, we log the error and display an alert to the user indicating that the download failed.

Note that this example assumes that the /download endpoint returns a file with a Content-Disposition header indicating the filename. You may need to modify this code to handle different types of responses, such as JSON or text.

            3.4.5.5 User give an ETL example involving source PG DB a spring java client application that interacts with this DB, and a target PG DB with a  spring java client application
ChatGPT
Sure, here's an example of an ETL process involving a source PostgreSQL database, a Spring Java client application that interacts with this database, and a target PostgreSQL database with a Spring Java client application:

Connect to the source PostgreSQL database using the DataSource interface provided by Spring:
@Configuration
public class SourceDatabaseConfig {
  @Bean
  public DataSource dataSource() {
    HikariConfig config = new HikariConfig();
    config.setJdbcUrl("jdbc:postgresql://source_db_host:5432/source_db_name");
    config.setUsername("source_db_user");
    config.setPassword("source_db_password");

    return new HikariDataSource(config);
  }
}
Create a DAO (Data Access Object) that encapsulates the logic to extract data from the source PostgreSQL database. This DAO could use the JdbcTemplate provided by Spring to execute SQL queries:
@Repository
public class SourceDao {
  private final JdbcTemplate jdbcTemplate;

  public SourceDao(DataSource dataSource) {
    this.jdbcTemplate = new JdbcTemplate(dataSource);
  }

  public List<MyData> getData() {
    String sql = "SELECT * FROM my_table";
    return jdbcTemplate.query(sql, new BeanPropertyRowMapper<>(MyData.class));
  }
}
Connect to the target PostgreSQL database using another DataSource:
@Configuration
public class TargetDatabaseConfig {
  @Bean
  public DataSource dataSource() {
    HikariConfig config = new HikariConfig();
    config.setJdbcUrl("jdbc:postgresql://target_db_host:5432/target_db_name");
    config.setUsername("target_db_user");
    config.setPassword("target_db_password");

    return new HikariDataSource(config);
  }
}
Create a DAO that encapsulates the logic to insert data into the target PostgreSQL database. This DAO could use the NamedParameterJdbcTemplate provided by Spring to execute parameterized SQL statements:
@Repository
public class TargetDao {
  private final NamedParameterJdbcTemplate jdbcTemplate;

  public TargetDao(DataSource dataSource) {
    this.jdbcTemplate = new NamedParameterJdbcTemplate(dataSource);
  }

  public void insertData(MyData data) {
    String sql = "INSERT INTO my_table (column1, column2, column3) VALUES (:column1, :column2, :column3)";
    MapSqlParameterSource params = new MapSqlParameterSource()
      .addValue("column1", data.getColumn1())
      .addValue("column2", data.getColumn2())
      .addValue("column3", data.getColumn3());
    jdbcTemplate.update(sql, params);
  }
}
Create a Spring @Service that orchestrates the ETL process, extracting data from the source database, transforming it as necessary, and inserting it into the target database:
@Service
public class EtlService {
  private final SourceDao sourceDao;
  private final TargetDao targetDao;

  public EtlService(SourceDao sourceDao, TargetDao targetDao) {
    this.sourceDao = sourceDao;
    this.targetDao = targetDao;
  }

  public void etl() {
    List<MyData> data = sourceDao.getData();
    for (MyData item : data) {
      // perform any necessary data transformations
      targetDao.insertData(item);
    }
  }
}
Finally, create a controller that exposes an endpoint to trigger the ETL process:
@RestController
public class EtlController {
  private final EtlService etlService;

  public EtlController(EtlService etlService) {
    this.etlService = etlService;
  }

  @PostMapping("/etl")
  public void



            3.4.5.6 give same example using python and flask

ChatGPT
Sure, here's an example of the same ETL process using Python and Flask:

Connect to the source PostgreSQL database using the psycopg2 library:
import psycopg2
from flask import current_app

def get_source_conn():
    return psycopg2.connect(
        host=current_app.config['SOURCE_DB_HOST'],
        port=current_app.config['SOURCE_DB_PORT'],
        dbname=current_app.config['SOURCE_DB_NAME'],
        user=current_app.config['SOURCE_DB_USER'],
        password=current_app.config['SOURCE_DB_PASSWORD']
    )
Create a DAO that encapsulates the logic to extract data from the source PostgreSQL database. This DAO could use the cursor object provided by psycopg2 to execute SQL queries:
class SourceDao:
    def __init__(self):
        self.conn = get_source_conn()

    def get_data(self):
        with self.conn.cursor() as cur:
            cur.execute("SELECT * FROM my_table")
            return cur.fetchall()
Connect to the target PostgreSQL database using psycopg2:
def get_target_conn():
    return psycopg2.connect(
        host=current_app.config['TARGET_DB_HOST'],
        port=current_app.config['TARGET_DB_PORT'],
        dbname=current_app.config['TARGET_DB_NAME'],
        user=current_app.config['TARGET_DB_USER'],
        password=current_app.config['TARGET_DB_PASSWORD']
    )
Create a DAO that encapsulates the logic to insert data into the target PostgreSQL database:
class TargetDao:
    def __init__(self):
        self.conn = get_target_conn()

    def insert_data(self, data):
        with self.conn.cursor() as cur:
            cur.execute(
                "INSERT INTO my_table (column1, column2, column3) VALUES (%s, %s, %s)",
                (data.column1, data.column2, data.column3)
            )
Create a Flask Blueprint that defines the ETL endpoint:
from flask import Blueprint

bp = Blueprint('etl', __name__)

@bp.route('/etl', methods=['POST'])
def etl():
    source_dao = SourceDao()
    target_dao = TargetDao()

    data = source_dao.get_data()
    for item in data:
        # perform any necessary data transformations
        target_dao.insert_data(item)

    return '', 204
Finally, register the Blueprint with a Flask app object:
from flask import Flask

app = Flask(__name__)
app.config.update({
    'SOURCE_DB_HOST': 'source_db_host',
    'SOURCE_DB_PORT': 5432,
    'SOURCE_DB_NAME': 'source_db_name',
    'SOURCE_DB_USER': 'source_db_user',
    'SOURCE_DB_PASSWORD': 'source_db_password',
    'TARGET_DB_HOST': 'target_db_host',
    'TARGET_DB_PORT': 5432,
    'TARGET_DB_NAME': 'target_db_name',
    'TARGET_DB_USER': 'target_db_user',
    'TARGET_DB_PASSWORD': 'target_db_password'
})

app.register_blueprint(bp)


            3.4.5.7 give an example of a java spring application that interacts with a PG DB and exposes and endpoint that allows to download a file which is the result of running a select query on the DB

ChatGPT
Sure, here's an example of a Java Spring application that interacts with a PostgreSQL database and exposes an endpoint that allows users to download a file which is the result of running a select query on the database:

Configure the PostgreSQL data source in the Spring configuration file:
<bean id="dataSource" class="org.apache.commons.dbcp2.BasicDataSource" destroy-method="close">
    <property name="driverClassName" value="org.postgresql.Driver" />
    <property name="url" value="jdbc:postgresql://localhost:5432/mydatabase" />
    <property name="username" value="myuser" />
    <property name="password" value="mypassword" />
</bean>
Create a Spring MVC controller that handles the file download:
import java.io.IOException;
import java.sql.ResultSet;
import java.sql.SQLException;

import javax.servlet.http.HttpServletResponse;

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.jdbc.core.JdbcTemplate;
import org.springframework.jdbc.core.RowCallbackHandler;
import org.springframework.stereotype.Controller;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RequestMapping;
import org.springframework.web.bind.annotation.RequestParam;

@Controller
public class FileDownloadController {

    @Autowired
    private JdbcTemplate jdbcTemplate;

    @GetMapping("/download")
    public void downloadFile(@RequestParam("id") int id, HttpServletResponse response) throws IOException {
        String sql = "SELECT filename, content FROM files WHERE id = ?";
        jdbcTemplate.query(sql, new Object[] { id }, new RowCallbackHandler() {
            @Override
            public void processRow(ResultSet rs) throws SQLException, IOException {
                String filename = rs.getString("filename");
                byte[] content = rs.getBytes("content");
                response.setContentType("application/octet-stream");
                response.setHeader("Content-Disposition", "attachment; filename=\"" + filename + "\"");
                response.getOutputStream().write(content);
            }
        });
    }
}
Register the controller in the Spring configuration file:
<context:component-scan base-package="com.example.controller" />
<mvc:annotation-driven />
Create a JSP view that includes a link to download the file:
<%@ page contentType="text/html;charset=UTF-8" language="java" %>
<html>
<head>
    <title>File Download Example</title>
</head>
<body>
    <h1>File Download Example</h1>
    <ul>
        <li><a href="download?id=1">File 1</a></li>
        <li><a href="download?id=2">File 2</a></li>
        <li><a href="download?id=3">File 3</a></li>
    </ul>
</body>
</html>
Create a Spring MVC configuration file that specifies the location of the JSP view:
<bean id="viewResolver" class="org.springframework.web.servlet.view.InternalResourceViewResolver">
    <property name="prefix" value="/WEB-INF/views/" />
    <property name="suffix" value=".jsp" />
</bean>
Create a web.xml file that specifies the Spring dispatcher servlet:
<web-app>
    <display-name>File Download Example</display-name>
    <servlet>
        <servlet-name>dispatcher</servlet-name>
        <servlet-class>org.springframework.web.servlet.DispatcherServlet</servlet-class>
        <init-param>
            <param-name>contextConfigLocation</param-name>
            <param-value>/WEB-INF/spring-config.xml</param-value>
        </init-param>
    </servlet>
    <servlet-mapping>
        <servlet-name>dispatcher</s


            3.4.5.8 User give an example of a  python application that interacts with a PG DB and exposes and endpoint that allows to download a file which is the result of running a select query on the DB

ChatGPT
Sure, here's an example of a Python application that interacts with a PostgreSQL database and exposes an endpoint that allows users to download a file which is the result of running a select query on the database:

Install the psycopg2 library, which provides a Python interface to PostgreSQL:
pip install psycopg2
Create a Flask application:
from flask import Flask, request, send_file
import psycopg2
import io

app = Flask(__name__)

@app.route('/download')
def download_file():
    file_id = request.args.get('id')
    conn = psycopg2.connect("dbname=mydatabase user=myuser password=mypassword host=localhost port=5432")
    cur = conn.cursor()
    cur.execute("SELECT filename, content FROM files WHERE id = %s", (file_id,))
    row = cur.fetchone()
    if row is None:
        return "File not found", 404
    filename, content = row
    conn.close()
    return send_file(io.BytesIO(content), attachment_filename=filename, as_attachment=True)
if __name__ == '__main__':
    app.run()
Start the Flask application by running the script:
python app.py
Create a database table to store the files:
CREATE TABLE files (
    id SERIAL PRIMARY KEY,
    filename VARCHAR(255) NOT NULL,
    content BYTEA NOT NULL
);
Insert some files into the database:
import psycopg2

conn = psycopg2.connect("dbname=mydatabase user=myuser password=mypassword host=localhost port=5432")
cur = conn.cursor()
cur.execute("INSERT INTO files (filename, content) VALUES (%s, %s)", ("file1.txt", b"Hello, world!"))
cur.execute("INSERT INTO files (filename, content) VALUES (%s, %s)", ("file2.txt", b"Lorem ipsum dolor sit amet"))
conn.commit()
conn.close()
Access the file download endpoint by visiting http://localhost:5000/download?id=1 in a web browser, where 1 is the ID of the file to download.
        3.4.6 sql ON CONFLICT xxx DO UPDATE SET
The `ON CONFLICT` clause in SQL is used to handle conflicts that may arise when attempting to insert or update rows in a table. It is often used in conjunction with the `INSERT` statement. The `ON CONFLICT` clause allows you to specify how to handle conflicts, such as violating unique constraints or primary key constraints. When a conflict occurs, you can choose to perform an alternative action, such as updating the existing row.

Here's an example of using `ON CONFLICT` with the `INSERT` statement:

```sql
INSERT INTO table_name (column1, column2, ...)
VALUES (value1, value2, ...)
ON CONFLICT (unique_column) DO UPDATE
SET column1 = excluded.column1, column2 = excluded.column2, ...;
```

Explanation:

- `table_name`: The name of the table you are inserting into.
- `(column1, column2, ...)`: The columns you are inserting data into.
- `VALUES (value1, value2, ...)`: The values you are inserting.
- `ON CONFLICT (unique_column)`: Specifies the unique constraint or column(s) that, if violated, triggers the conflict resolution.
- `DO UPDATE`: Specifies that if a conflict occurs, perform an update.
- `SET column1 = excluded.column1, column2 = excluded.column2, ...`: Specifies the columns to be updated with the values from the `excluded` table, which contains the values that would have been inserted if there were no conflict.

Here's a concrete example:

```sql
-- Assuming a table named users with a unique constraint on the email column
INSERT INTO users (id, name, email)
VALUES (1, 'John Doe', 'john@example.com')
ON CONFLICT (email) DO UPDATE
SET name = excluded.name;
```

In this example, if a user with the email 'john@example.com' already exists, it will update the name column for that user.

Keep in mind that the exact syntax and behavior might vary slightly depending on the specific SQL database system you are using. The example above is compatible with databases like PostgreSQL. If you are using a different database system, you should refer to its documentation for the specific syntax and options available for the `ON CONFLICT` clause.

        3.4.7 I have a DB table of document id and the actual document. create an update statement that appends a prefix of document id to the value in json path foo.bar.id
Here's the update statement to append a document ID prefix to the value in the JSON path `foo.bar.id` of your document table:

```sql
UPDATE your_table
SET document = jsonb_set(document, '{foo, bar, id}', CONCAT(document_id, '::', document->>'foo.bar.id'));
```

**Explanation:**

* `UPDATE your_table`: This specifies the table you want to modify, assuming it's named `your_table`.
* `SET document = jsonb_set(document, '{foo, bar, id}', ...)`: This clause defines the update operation for the `document` column, which is assumed to be a JSONB data type.
    * `jsonb_set`: This function modifies a specific element within a JSONB document.
    * The first argument to `jsonb_set` is the existing JSONB document (`document`).
    * The second argument is the JSON path to the element you want to modify, which is `'{foo, bar, id}'` in this case. This selects the `id` value within the nested `foo` and `bar` objects.
    * The third argument is the new value to assign to the selected element. Here, we use `CONCAT`:
        * `document_id`: This represents the document ID column name (replace with the actual name if different).
        * `'::'`: This is a string literal used as a separator between the document ID and the existing value.
        * `document->>'foo.bar.id'`: This extracts the current value at the `foo.bar.id` path using the `->>` operator for JSONB data. The double arrow operator allows us to access the value as a text string.
* This entire expression effectively concatenates the document ID with the existing `id` value separated by the specified string and assigns it back to the `foo.bar.id` location within the JSONB document.

**Running the Query:**

You can execute this query within your SQL client tool. This will update the `document` column for each row, modifying the `id` value within the `foo.bar` object based on the document ID.

**Additional Considerations:**

* Ensure your `document` column is of the JSONB data type for this function to work correctly.
* You can adjust the separator string (`'::'`) to your preference.
* Make sure you have proper backups of your data before running update queries, especially in a production environment.

        3.4.8 I have two tables, one with item ids and types, some items can contain others. for example: locker can contain keys, phone etc. For containing there's another table for this relation with 2 columns from_id and to_id. Create a statement that counts how many items are contained in each locker and sort result descending 
Here's the SQL statement to count the number of items contained within each locker and sort the results in descending order:

```sql
SELECT c.from_id AS locker_id, COUNT(*) AS contained_items
FROM your_item_types_table AS t
INNER JOIN your_containment_relation AS c
  ON t.id = c.from_id
WHERE t.type = 'locker'
GROUP BY c.from_id
ORDER BY contained_items DESC;
```

**Explanation:**

* `SELECT c.from_id AS locker_id, COUNT(*) AS contained_items`:
    * `c.from_id AS locker_id`: This selects the `from_id` column from the `containment_relation` table (aliased as `c`) and renames it to `locker_id` for clarity. This represents the ID of the locker (containing item).
    * `COUNT(*) AS contained_items`: This counts the number of rows in the result set, effectively giving the number of items contained within each locker. The result is aliased as `contained_items`.
* `FROM your_item_types_table AS t`: This specifies the table containing item IDs and types, aliased as `t`.
* `INNER JOIN your_containment_relation AS c ON t.id = c.from_id`: This joins the `item_types_table` with the `containment_relation` table.
    * The join condition (`ON t.id = c.from_id`) ensures that items (from `item_types_table`) are matched with their containment relations (in `containment_relation`) based on the `id` from the item table matching the `from_id` (containing item) in the containment relation table.
* `WHERE t.type = 'locker'`: This filters the results to only consider items where the `type` in the `item_types_table` is 'locker'. This ensures we only count items contained within lockers.
* `GROUP BY c.from_id`: This groups the results by the `from_id` (locker ID) from the containment relation table. This allows us to aggregate the count for each unique locker.
* `ORDER BY contained_items DESC`: This sorts the final result set by the `contained_items` (count) in descending order. Lockers with the most contained items will appear first.

**Running the Query:**

You can execute this query within your SQL client tool. The result will display a list of locker IDs (containing items) and the corresponding count of items contained within each locker, sorted with the highest counts first.

**Additional Considerations:**

* Replace `your_item_types_table` and `your_containment_relation` with the actual names of your tables.
* You can modify the `WHERE` clause to filter by different item types if needed. 
        3.4.9
    3.5
4. Common concepts

    4.1  Optimistic-locking

        4.1.1  Chatgpt

            4.1.1.1  define optimistic locking
Optimistic locking is a concurrency control mechanism used in database systems to handle concurrent access to shared data. It is based on the assumption that conflicts between multiple transactions accessing the same data are rare, so it allows concurrent operations to proceed without blocking or locking resources.

In optimistic locking, each transaction that accesses the shared data does not acquire any exclusive locks initially. Instead, it retrieves a version or timestamp associated with the data. When a transaction wants to modify the data, it checks if the version or timestamp of the data matches the version it initially read. If the versions match, it indicates that no other transaction has modified the data since it was read, and the transaction can proceed with the update. However, if the versions do not match, it means that another transaction has modified the data concurrently, and the current transaction may need to be aborted or retried.

The key idea behind optimistic locking is to delay the detection of conflicts until a transaction tries to commit its changes. By allowing concurrent transactions to proceed without blocking each other, it maximizes the system's throughput and reduces contention. However, it also introduces the possibility of conflicts that need to be resolved when committing the changes.

Optimistic locking is commonly used in scenarios where conflicts are infrequent or the cost of blocking concurrent access is high. It requires careful handling of conflicts and may involve techniques such as retrying failed transactions or applying conflict resolution strategies to ensure data consistency.

Overall, optimistic locking provides a way to achieve concurrency control in database systems by assuming that conflicts are rare and allowing transactions to proceed optimistically, only detecting and resolving conflicts when they arise during commit.
            4.1.1.2 what kind of conflicts may happen when optimistic locking is used
When using optimistic locking, conflicts can occur when multiple transactions attempt to modify the same data concurrently. These conflicts arise due to the optimistic assumption that conflicts are rare and transactions can proceed without blocking each other. Here are two common types of conflicts that may happen:

1. Update Conflict:
   An update conflict occurs when two or more transactions attempt to modify the same data simultaneously. Suppose Transaction A and Transaction B both read the same data, and both intend to update it. If Transaction A commits its changes before Transaction B, there is no conflict. However, if Transaction B tries to commit its changes after Transaction A, it will detect that the data it read has been modified by another transaction, indicating an update conflict. This conflict needs to be resolved to ensure data consistency.

2. Delete-Insert Conflict:
   A delete-insert conflict, also known as a lost update conflict, occurs when a transaction reads a record and intends to update it, but another transaction deletes the same record and inserts a new one before the first transaction commits. As a result, the first transaction's update is applied to a non-existent record, leading to data loss or inconsistency. This conflict can be detected when the first transaction tries to commit and realizes that the record it intends to update has been deleted.

When conflicts occur, the optimistic locking approach typically involves rolling back or retrying the transaction that experiences the conflict. This allows the transaction to re-read the data, incorporate any changes made by other transactions, and reapply its modifications based on the updated data. Conflict resolution strategies may also be employed, such as selecting the most recent changes or merging conflicting updates.

It's important to note that the specific types of conflicts and their resolution methods may vary depending on the implementation and the data management system used. Optimistic locking is a technique to mitigate conflicts but does not completely eliminate them. Careful handling of conflicts is necessary to maintain data consistency when employing optimistic locking.
            4.1.1.3
        4.1.2

    4.2

5. Common , interesting, frequent queries , tags: Common , interesting, frequent queries , 

    5.1 I have two similar DB tables with same primary key. say orders table and orders translations table. what sql would provide all the orders that are not in orders translations?
Gemini-Advanced:
Here's the SQL query to find orders that are not in the orders translations table:

```sql
SELECT * 
FROM orders o
WHERE NOT EXISTS (
    SELECT 1 
    FROM orders_translations ot
    WHERE o.primary_key = ot.primary_key -- Assuming 'primary_key' is the shared column
);
```

**Explanation**

* **`SELECT * FROM orders o`**: Selects all columns from the `orders` table and assigns the alias `o` to it.
* **`WHERE NOT EXISTS (...)`**: Filters the results to only include rows from `orders` where there is no matching row in `orders_translations`.
* **`SELECT 1 FROM orders_translations ot`**: This subquery looks for rows in the `orders_translations` table (alias `ot`).
* **`WHERE o.primary_key = ot.primary_key`**: The subquery's condition checks if the `primary_key` in the `orders` table matches the `primary_key` in the `orders_translations` table.

**Key Points:**

* **Assumes Shared Primary Key:** This query assumes both tables have a column named `primary_key` that serves as the primary key in both tables. If the primary key column has a different name, adjust the `WHERE` clause accordingly.
* **Efficiency:** The `NOT EXISTS` clause is generally efficient for this kind of query, as it stops searching as soon as it finds a match in the subquery.
* **Alternative with `LEFT JOIN`:** You could also achieve this with a `LEFT JOIN` and filter for nulls in the right table:

```sql
SELECT o.*
FROM orders o
LEFT JOIN orders_translations ot ON o.primary_key = ot.primary_key
WHERE ot.primary_key IS NULL;
```

This query joins the two tables based on the `primary_key` and then selects rows from `orders` where there is no corresponding `primary_key` in `orders_translations`.

**Important Note**

Ensure you replace 'primary_key' with the actual name of the primary key column in your tables.

in "SELECT 1 FROM orders_translations ot" what does 1 stand for?


In the SQL query `SELECT 1 FROM orders_translations ot`, the "1" is simply a placeholder value. It doesn't represent any actual data from the `orders_translations` table.

**Purpose**

The main purpose of this subquery within the `NOT EXISTS` clause is to check for the *existence* of a matching row in the `orders_translations` table, not to retrieve any specific data from it.  

* The `SELECT 1` essentially says, "If there's at least one row in `orders_translations` that matches the condition in the `WHERE` clause, return the value 1." 
* The `NOT EXISTS` then checks if the subquery returned any rows. If it didn't (meaning there was no match), the condition is true, and that row from the main `orders` table is included in the result.

**Why 1?**

You could use any constant value here (e.g., `SELECT 'x'`, `SELECT true`).  The specific value doesn't matter, as long as it's something that will be returned if a matching row is found. `1` is commonly used because it's simple and efficient. 

**Key Points:**

* **Focus on Existence:** The core purpose of this subquery is to determine existence, not to retrieve data.
* **Placeholder Value:** The "1" is just a placeholder to indicate that a row exists if the condition is met.
* **Efficiency:** Using a constant value like "1" can be slightly more efficient than selecting an actual column from the table, as it avoids any unnecessary data retrieval.

Let me know if you have any further questions or would like more clarification on SQL queries! 



    5.2
6.
