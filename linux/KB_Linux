.........................................Table Of Contents...............................................................
1. RPMs <URL:#tn=1. RPMs>
	1.1 query installed rpm <URL:#tn=	1.1 query installed rpm>
	1.2 uninstall package <URL:#tn=	1.2 uninstall package>
	1.3 install RPM <URL:#tn=	1.3 install RPM>
	1.4  Listing Contents of an RPM Package <URL:#tn=	1.4  Listing Contents of an RPM Package>
	1.5 . Printing Out an RPM Package Information <URL:#tn=	1.5 . Printing Out an RPM Package Information>
	1.6 Simple Unpacking(extract) of an RPM Archive <URL:#tn=	1.6 Simple Unpacking(extract) of an RPM Archive>
	1.7 Creating a Private RPM Database <URL:#tn=	1.7 Creating a Private RPM Database>
	1.8  Installing an RPM Package <URL:#tn=	1.8  Installing an RPM Package>
	1.9  Upgrading RPM Packages <URL:#tn=	1.9  Upgrading RPM Packages>
	1.10  Upgrading RPM Packages <URL:#tn=	1.10  Upgrading RPM Packages>
	1.11   Dealing with Dependencies <URL:#tn=	1.11   Dealing with Dependencies>
	1.12 Listing Installed Packages <URL:#tn=	1.12 Listing Installed Packages>
	1.13  Building RPM from Source <URL:#tn=	1.13  Building RPM from Source>
	1.14 Tell to which RPM this file belongs to <URL:#tn=	1.14 Tell to which RPM this file belongs to>
	1.15 <URL:#tn=	1.15>
2. Host name, hostname <URL:#tn=2. Host name, hostname>
	2.1  How to change the hostname of a Linux system <URL:#tn=	2.1  How to change the hostname of a Linux system>
		2.1.1  Change the hostname on a running system <URL:#tn=		2.1.1  Change the hostname on a running system>
		2.1.2 Permanent hostname change on Debian based systems <URL:#tn=		2.1.2 Permanent hostname change on Debian based systems>
		2.1.3 Permanent hostname change on RedHat based systems <URL:#tn=		2.1.3 Permanent hostname change on RedHat based systems>
		2.1.4 Use sysctl to change the hostname <URL:#tn=		2.1.4 Use sysctl to change the hostname>
		2.1.5 <URL:#tn=		2.1.5>
	2.2 <URL:#tn=	2.2>
3. Determins operating system information. <URL:#tn=3. Determins operating system information.>
	3.1 kernel version, Linux version: <URL:#tn=	3.1 kernel version, Linux version:>
	3.2 gcc version <URL:#tn=	3.2 gcc version>
	3.3 Red hat version <URL:#tn=	3.3 Red hat version>
	3.4 How to get system information ? <URL:#tn=	3.4 How to get system information ?>
	3.5 <URL:#tn=	3.5>
4. DNS on Linux <URL:#tn=4. DNS on Linux>
   4.1 Introduction <URL:#tn=   4.1 Introduction>
		4.1.1 DNS Domains <URL:#tn=		4.1.1 DNS Domains>
		4.1.2 BIND <URL:#tn=		4.1.2 BIND>
		4.1.3 DNS Clients <URL:#tn=		4.1.3 DNS Clients>
		4.1.4 Authoritative DNS Servers <URL:#tn=		4.1.4 Authoritative DNS Servers>
		4.1.5 How DNS Servers Find Out Your Site Information <URL:#tn=		4.1.5 How DNS Servers Find Out Your Site Information>
		4.1.6 When To Use A DNS Caching Name Server <URL:#tn=		4.1.6 When To Use A DNS Caching Name Server>
		4.1.7 When To Use A Static DNS Server <URL:#tn=		4.1.7 When To Use A Static DNS Server>
		4.1.8 When To Use A Dynamic DNS Server <URL:#tn=		4.1.8 When To Use A Dynamic DNS Server>
		4.1.9 How To Get Your Own Domain <URL:#tn=		4.1.9 How To Get Your Own Domain>
		4.1.10 Basic DNS Testing of DNS Resolution <URL:#tn=		4.1.10 Basic DNS Testing of DNS Resolution>
			4.1.10.1 The Host Command <URL:#tn=			4.1.10.1 The Host Command>
			4.1.10.2 The nslookup Command <URL:#tn=			4.1.10.2 The nslookup Command>
				4.1.10.2.1 wiki <URL:#tn=				4.1.10.2.1 wiki>
					4.1.10.2.1.1 Background <URL:#tn=					4.1.10.2.1.1 Background>
					4.1.10.2.1.2 Usage <URL:#tn=					4.1.10.2.1.2 Usage>
				4.1.10.2.2 Some examples <URL:#tn=				4.1.10.2.2 Some examples>
				4.1.10.2.3 dig <URL:#tn=				4.1.10.2.3 dig>
					4.1.10.2.3.1 wiki <URL:#tn=					4.1.10.2.3.1 wiki>
					4.1.10.2.3.2 Examples <URL:#tn=					4.1.10.2.3.2 Examples>
					4.1.10.2.3.3 <URL:#tn=					4.1.10.2.3.3>
				4.1.10.2.4 nslookup man <URL:#tn=				4.1.10.2.4 nslookup man>
				4.1.10.2.5 <URL:#tn=				4.1.10.2.5>
		4.1.11 	Downloading and Installing the BIND Packages <URL:#tn=		4.1.11 	Downloading and Installing the BIND Packages>
		4.1.12 	How To Get BIND Started <URL:#tn=		4.1.12 	How To Get BIND Started>
			4.1.12.1 	Redhat / Fedora <URL:#tn=			4.1.12.1 	Redhat / Fedora>
			4.1.12.2 	Debian / Ubuntu <URL:#tn=			4.1.12.2 	Debian / Ubuntu>
	4.1.13 The /etc/resolv.conf File <URL:#tn=	4.1.13 The /etc/resolv.conf File>
	4.2 Important File Locations <URL:#tn=	4.2 Important File Locations>
	4.3 Configuring Your Nameserver <URL:#tn=	4.3 Configuring Your Nameserver>
	4.4 Troubleshooting BIND <URL:#tn=	4.4 Troubleshooting BIND>
	4.7 Migrating Your Web Site In-House <URL:#tn=	4.7 Migrating Your Web Site In-House>
	4.6 DHCP Considerations For DNS <URL:#tn=	4.6 DHCP Considerations For DNS>
	4.7 Simple DNS Security <URL:#tn=	4.7 Simple DNS Security>
	4.8 Conclusion <URL:#tn=	4.8 Conclusion>
	4.9 Summary <URL:#tn=	4.9 Summary>
	4.10 Make DNS changes apply, refresh DNS cache <URL:#tn=	4.10 Make DNS changes apply, refresh DNS cache>
	4.11 DNS records <URL:#tn=	4.11 DNS records>
	4.12 <URL:#tn=	4.12>
	4.2 Linux / UNIX: DNS Lookup Commands, host and dig <URL:#tn=	4.2 Linux / UNIX: DNS Lookup Commands, host and dig>
206.153.126.75.in-addr.arpa domain name pointer www.cyberciti.biz. <URL:#tn=206.153.126.75.in-addr.arpa domain name pointer www.cyberciti.biz.>
75.126.153.206 <URL:#tn=75.126.153.206>
	4.3 Active Directory DNS <URL:#tn=	4.3 Active Directory DNS>
		4.3.1 Active Directory SRV Records <URL:#tn=		4.3.1 Active Directory SRV Records>
		4.3.2 Using NSlookup.exe <URL:#tn=		4.3.2 Using NSlookup.exe>
		4.3.3 Verifying Your Basic DNS Configuration <URL:#tn=		4.3.3 Verifying Your Basic DNS Configuration>
		4.3.4 How to verify that SRV DNS records have been created for a domain controller <URL:#tn=		4.3.4 How to verify that SRV DNS records have been created for a domain controller>
		4.3.5 <URL:#tn=		4.3.5>
	4.4 <URL:#tn=	4.4>
5. xargs, build and execute command lines from standard input <URL:#tn=5. xargs, build and execute command lines from standard input>
	5.1 examples with find <URL:#tn=	5.1 examples with find>
		5.1.1 Code: Lists all directories <URL:#tn=		5.1.1 Code: Lists all directories>
		5.1.2 bash$ find ~/mail -type f | xargs grep "Linux" <URL:#tn=		5.1.2 bash$ find ~/mail -type f | xargs grep "Linux">
		5.1.3 find . -maxdepth 1 -type d -print | xargs -I {} echo Directory: {} <URL:#tn=		5.1.3 find . -maxdepth 1 -type d -print | xargs -I {} echo Directory: {}>
		5.1.4 find symbol in .so files <URL:#tn=		5.1.4 find symbol in .so files>
		5.1.5 find symbol in binary file <URL:#tn=		5.1.5 find symbol in binary file>
	5.2 Advanced usage <URL:#tn=	5.2 Advanced usage>
	5.3 run multiple commands <URL:#tn=	5.3 run multiple commands>
	5.4 advances usage <URL:#tn=	5.4 advances usage>
		5.4.1  find all files who's 9th line contains a pattern <URL:#tn=		5.4.1  find all files who's 9th line contains a pattern>
		5.4.2 xargs: unterminated quote problem <URL:#tn=		5.4.2 xargs: unterminated quote problem>
		5.4.3 <URL:#tn=		5.4.3>
	5.5 <URL:#tn=	5.5>
6. Development tools <URL:#tn=6. Development tools>
	6.1 Debugging <URL:#tn=	6.1 Debugging>
		6.1.1 The build process <URL:#tn=		6.1.1 The build process>
			6.1.1.1 ELF and DWARF <URL:#tn=			6.1.1.1 ELF and DWARF>
			6.1.1.2 Header paths <URL:#tn=			6.1.1.2 Header paths>
			6.1.1.3 Library paths <URL:#tn=			6.1.1.3 Library paths>
			6.1.1.4 Debug Information <URL:#tn=			6.1.1.4 Debug Information>
			6.1.1.5 Optimization <URL:#tn=			6.1.1.5 Optimization>
			6.1.1.6 LDD <URL:#tn=			6.1.1.6 LDD>
			6.1.1.7 Symbols <URL:#tn=			6.1.1.7 Symbols>
			6.1.1.8 NM <URL:#tn=			6.1.1.8 NM>
			6.1.1.9 objdump - ELF contents, sort of a disassembler <URL:#tn=			6.1.1.9 objdump - ELF contents, sort of a disassembler>
			6.1.1.10 Troubleshoot Static Linkage <URL:#tn=			6.1.1.10 Troubleshoot Static Linkage>
			6.1.1.11 Troubleshoot Dynamic Linkage <URL:#tn=			6.1.1.11 Troubleshoot Dynamic Linkage>
			6.1.1.12 IO, Socket usage report for a given process <URL:#tn=			6.1.1.12 IO, Socket usage report for a given process>
			6.1.1.13 <URL:#tn=			6.1.1.13>
		6.1.2 Debugging Linux Applications <URL:#tn=		6.1.2 Debugging Linux Applications>
			6.1.2.1 GDB Features <URL:#tn=			6.1.2.1 GDB Features>
			6.1.2.2 GDB Interfaces <URL:#tn=			6.1.2.2 GDB Interfaces>
			6.1.2.3 Build for debugging <URL:#tn=			6.1.2.3 Build for debugging>
			6.1.2.4 Starting GDB <URL:#tn=			6.1.2.4 Starting GDB>
			6.1.2.5 Commands <URL:#tn=			6.1.2.5 Commands>
				6.1.2.5.1 Start program <URL:#tn=				6.1.2.5.1 Start program>
				6.1.2.5.2 Program Execution <URL:#tn=				6.1.2.5.2 Program Execution>
				6.1.2.5.3 Source Code listing <URL:#tn=				6.1.2.5.3 Source Code listing>
				6.1.2.5.4 Symbolic Debugging <URL:#tn=				6.1.2.5.4 Symbolic Debugging>
			6.1.2.6 Breakpoints <URL:#tn=			6.1.2.6 Breakpoints>
			6.1.2.7 Watchpoints <URL:#tn=			6.1.2.7 Watchpoints>
			6.1.2.8 Remote Debugging <URL:#tn=			6.1.2.8 Remote Debugging>
			6.1.2.9 Automation <URL:#tn=			6.1.2.9 Automation>
			    6.1.2.9.1 https://stackoverflow.com/questions/10748501/what-are-the-best-ways-to-automate-a-gdb-debugging-session <URL:#tn=			    6.1.2.9.1 https://stackoverflow.com/questions/10748501/what-are-the-best-ways-to-automate-a-gdb-debugging-session>
			    6.1.2.9.2 My example <URL:#tn=			    6.1.2.9.2 My example>
			6.1.2.10 My frequently used commands <URL:#tn=			6.1.2.10 My frequently used commands>
				6.1.2.10.1 Show thread IDs <URL:#tn=				6.1.2.10.1 Show thread IDs>
				6.1.2.10.2 Show stacks of a list of threads <URL:#tn=				6.1.2.10.2 Show stacks of a list of threads>
				6.1.2.10.3 retrieve data from memory to file <URL:#tn=				6.1.2.10.3 retrieve data from memory to file>
				6.1.2.10.4 Print first parameter of function value <URL:#tn=				6.1.2.10.4 Print first parameter of function value>
				6.1.2.10.5 Attach to running process & set breakpoint <URL:#tn=				6.1.2.10.5 Attach to running process & set breakpoint>
				6.1.2.10.6 <URL:#tn=				6.1.2.10.6>
			6.1.2.11 <URL:#tn=			6.1.2.11>
		6.1.3 Smashing the stack <URL:#tn=		6.1.3 Smashing the stack>
			6.1.3.1 Functions of the call stack <URL:#tn=			6.1.3.1 Functions of the call stack>
			6.1.3.2 Use of the call stack <URL:#tn=			6.1.3.2 Use of the call stack>
			6.1.3.3 Buffer overflow <URL:#tn=			6.1.3.3 Buffer overflow>
				6.1.3.3.1 Buffer overflow description <URL:#tn=				6.1.3.3.1 Buffer overflow description>
				6.1.3.3.1 Buffer overflow on the stack <URL:#tn=				6.1.3.3.1 Buffer overflow on the stack>
				6.1.3.3.2 Stack Buffer overflow description <URL:#tn=				6.1.3.3.2 Stack Buffer overflow description>
				6.1.3.3.3 Protection from buffer overflows <URL:#tn=				6.1.3.3.3 Protection from buffer overflows>
		6.1.3.4 Volatile stack references <URL:#tn=		6.1.3.4 Volatile stack references>
		6.1.3.5 Symptoms <URL:#tn=		6.1.3.5 Symptoms>
	6.1.4 The heap and dynamic allocations <URL:#tn=	6.1.4 The heap and dynamic allocations>
		6.1.4.1 Dynamic allocations <URL:#tn=		6.1.4.1 Dynamic allocations>
		6.1.4.2 The heap <URL:#tn=		6.1.4.2 The heap>
		6.1.4.3 The glibc allocator <URL:#tn=		6.1.4.3 The glibc allocator>
		6.1.4.4 The malloc arena <URL:#tn=		6.1.4.4 The malloc arena>
	    		6.1.4.4.1 allocation failure <URL:#tn=	    		6.1.4.4.1 allocation failure>
		6.1.4.5  memory leaks <URL:#tn=		6.1.4.5  memory leaks>
		6.1.4.6 Allocation debugging <URL:#tn=		6.1.4.6 Allocation debugging>
		6.1.4.7 Use after free <URL:#tn=		6.1.4.7 Use after free>
		6.1.4.8 Double free <URL:#tn=		6.1.4.8 Double free>
		6.1.4.9 Free unallocated memory <URL:#tn=		6.1.4.9 Free unallocated memory>
		6.1.4.10 Heap buffer overflow <URL:#tn=		6.1.4.10 Heap buffer overflow>
		6.1.4.11 Heap consistency check <URL:#tn=		6.1.4.11 Heap consistency check>
		6.1.4.12 Hooks for malloc <URL:#tn=		6.1.4.12 Hooks for malloc>
		6.1.4.13 Summary of possible memory management errors: <URL:#tn=		6.1.4.13 Summary of possible memory management errors:>
	6.1.5 Multi-Threading and Concurrency <URL:#tn=	6.1.5 Multi-Threading and Concurrency>
	    	6.1.5.1 Threads and processes <URL:#tn=	    	6.1.5.1 Threads and processes>
	    	6.1.5.2 Threads and gdb <URL:#tn=	    	6.1.5.2 Threads and gdb>
	    	6.1.5.3 Thread Synchronization Issues <URL:#tn=	    	6.1.5.3 Thread Synchronization Issues>
	    	    	6.1.5.3.1 Race condition <URL:#tn=	    	    	6.1.5.3.1 Race condition>
	    	    	6.1.5.3.2 <URL:#tn=	    	    	6.1.5.3.2>
	    	6.1.5.4 Thread safety <URL:#tn=	    	6.1.5.4 Thread safety>
	    	6.1.5.5 achieving thread safety <URL:#tn=	    	6.1.5.5 achieving thread safety>
	    	6.1.5.6 Lock problems <URL:#tn=	    	6.1.5.6 Lock problems>
	    	6.1.5.7 POSIX mutex debugging <URL:#tn=	    	6.1.5.7 POSIX mutex debugging>
	    	6.1.5.8 Signaling between threads <URL:#tn=	    	6.1.5.8 Signaling between threads>
	    	6.1.5.9 compiler optimizations <URL:#tn=	    	6.1.5.9 compiler optimizations>
	    	6.1.5.10 CPU optimizations <URL:#tn=	    	6.1.5.10 CPU optimizations>
	    	6.1.5.11 Memory management bugs and Concurrency <URL:#tn=	    	6.1.5.11 Memory management bugs and Concurrency>
	    	6.1.5.12 posix threads <URL:#tn=	    	6.1.5.12 posix threads>
	    	    6.1.5.12.1 thread affinity <URL:#tn=	    	    6.1.5.12.1 thread affinity>
	    	        6.1.5.12.1.1 https://eli.thegreenplace.net/2016/c11-threads-affinity-and-hyperthreading/ <URL:#tn=	    	        6.1.5.12.1.1 https://eli.thegreenplace.net/2016/c11-threads-affinity-and-hyperthreading/>
	    	        6.1.5.12.1.2 <URL:#tn=	    	        6.1.5.12.1.2>
	    	    6.1.5.12.2 <URL:#tn=	    	    6.1.5.12.2>
	    	6.1.5.13 <URL:#tn=	    	6.1.5.13>
	6.1.6 programmed debug assistance <URL:#tn=	6.1.6 programmed debug assistance>
	    	6.1.6.1 System log <URL:#tn=	    	6.1.6.1 System log>
	    	6.1.6.2 assert <URL:#tn=	    	6.1.6.2 assert>
	    	6.1.6.3 signals <URL:#tn=	    	6.1.6.3 signals>
	    	6.1.6.4 Print stack dump <URL:#tn=	    	6.1.6.4 Print stack dump>
	6.1.7 Post Mortem <URL:#tn=	6.1.7 Post Mortem>
	    	6.1.7.1 Check the logs <URL:#tn=	    	6.1.7.1 Check the logs>
	    	6.1.7.2 Core dumps <URL:#tn=	    	6.1.7.2 Core dumps>
		    	    	6.1.7.2.1 Core dump configuration <URL:#tn=		    	    	6.1.7.2.1 Core dump configuration>
		    	    	6.1.7.2.2 Using cors files <URL:#tn=		    	    	6.1.7.2.2 Using cors files>
	    	6.1.7.3 <URL:#tn=	    	6.1.7.3>
	6.1.8 Debugging tools <URL:#tn=	6.1.8 Debugging tools>
	    	6.1.8.1 strace <URL:#tn=	    	6.1.8.1 strace>
	    	6.1.8.2 ltrace (1) <URL:#tn=	    	6.1.8.2 ltrace (1)>
	    	6.1.8.3 PTT <URL:#tn=	    	6.1.8.3 PTT>
	    	6.1.8.4 Dmalloc <URL:#tn=	    	6.1.8.4 Dmalloc>
	    	6.1.8.5 Valgrind <URL:#tn=	    	6.1.8.5 Valgrind>
	6.1.9 <URL:#tn=	6.1.9>
	6.2 <URL:#tn=	6.2>
7. Networking <URL:#tn=7. Networking>
	7.1 Verifying Which Ports Are Listening <URL:#tn=	7.1 Verifying Which Ports Are Listening>
	7.2 Network capturing (sniffers, sniffing) <URL:#tn=	7.2 Network capturing (sniffers, sniffing)>
		7.2.1 Wireshark <URL:#tn=		7.2.1 Wireshark>
		7.2.2 tshark <URL:#tn=		7.2.2 tshark>
	7.3 tcpdump <URL:#tn=	7.3 tcpdump>
		7.3.1 tcpdump for Dummies <URL:#tn=		7.3.1 tcpdump for Dummies>
			7.3.1.1 Introduction <URL:#tn=			7.3.1.1 Introduction>
			7.3.1.2 tcpdump uses <URL:#tn=			7.3.1.2 tcpdump uses>
			7.3.1.3 Invocation <URL:#tn=			7.3.1.3 Invocation>
			7.3.1.4 Simple filtering <URL:#tn=			7.3.1.4 Simple filtering>
			7.3.1.5 Reading tcpdump¿s output <URL:#tn=			7.3.1.5 Reading tcpdump¿s output>
			7.3.1.6 Invocation continued <URL:#tn=			7.3.1.6 Invocation continued>
			7.3.1.7 Choosing an interface <URL:#tn=			7.3.1.7 Choosing an interface>
			7.3.1.8 Turning off name resolution <URL:#tn=			7.3.1.8 Turning off name resolution>
			7.3.1.9 Limiting number of packets to intercept <URL:#tn=			7.3.1.9 Limiting number of packets to intercept>
			7.3.1.10 Saving captured data <URL:#tn=			7.3.1.10 Saving captured data>
			7.3.1.11 One of the most useful tcpdump features allows capturing incoming and outgoing packets into a file and then playing this file <URL:#tn=			7.3.1.11 One of the most useful tcpdump features allows capturing incoming and outgoing packets into a file and then playing this file>
			7.3.1.11 Changing packet size in the capture file <URL:#tn=			7.3.1.11 Changing packet size in the capture file>
			7.3.1.12 Reading from capture file <URL:#tn=			7.3.1.12 Reading from capture file>
			7.3.1.13 Looking into packets <URL:#tn=			7.3.1.13 Looking into packets>
			7.3.1.14 Seeing Ethernet header for each packet <URL:#tn=			7.3.1.14 Seeing Ethernet header for each packet>
			7.3.1.15 Controlling time stamp <URL:#tn=			7.3.1.15 Controlling time stamp>
			7.3.1.16 Controlling verbosity <URL:#tn=			7.3.1.16 Controlling verbosity>
			7.3.1.17 Printing content of the packet <URL:#tn=			7.3.1.17 Printing content of the packet>
			7.3.1.18 Packet filtering <URL:#tn=			7.3.1.18 Packet filtering>
			7.3.1.19 Packet matching <URL:#tn=			7.3.1.19 Packet matching>
			7.3.1.20 More qualifiers <URL:#tn=			7.3.1.20 More qualifiers>
			7.3.1.21 Specifying addresses <URL:#tn=			7.3.1.21 Specifying addresses>
			7.3.1.22 Other qualifiers <URL:#tn=			7.3.1.22 Other qualifiers>
			7.3.1.23 Complex filter expressions <URL:#tn=			7.3.1.23 Complex filter expressions>
			7.3.1.24 Repeating qualifiers <URL:#tn=			7.3.1.24 Repeating qualifiers>
	7.4 Linux IPv6 HOWTO (en) <URL:#tn=	7.4 Linux IPv6 HOWTO (en)>
	7.5 nmap <URL:#tn=	7.5 nmap>
		7.5.1 Linux / UNIX: Scanning network for open ports with nmap command <URL:#tn=		7.5.1 Linux / UNIX: Scanning network for open ports with nmap command>
	7.6 <URL:#tn=	7.6>
8. Time, Date <URL:#tn=8. Time, Date>
	8.1 set time <URL:#tn=	8.1 set time>
		8.1.1 Basic <URL:#tn=		8.1.1 Basic>
		8.1.2 In-Depth <URL:#tn=		8.1.2 In-Depth>
		8.1.3 Examples <URL:#tn=		8.1.3 Examples>
		8.1.4 <URL:#tn=		8.1.4>
	8.2 <URL:#tn=	8.2>
9. Manage disk space, quotas <URL:#tn=9. Manage disk space, quotas>
	9.1 Check for disk usage on my user account <URL:#tn=	9.1 Check for disk usage on my user account>
	9.2 Get a list of files and dirs sorted according to used space <URL:#tn=	9.2 Get a list of files and dirs sorted according to used space>
	9.3 Check quota <URL:#tn=	9.3 Check quota>
10. System admininstration <URL:#tn=10. System admininstration>
	10.1 File information <URL:#tn=	10.1 File information>
		10.1.1 which <URL:#tn=		10.1.1 which>
		10.1.2 alias <URL:#tn=		10.1.2 alias>
		10.1.3 locate <URL:#tn=		10.1.3 locate>
		10.1.4 whatis <URL:#tn=		10.1.4 whatis>
	10.2 Hard disks and partitions <URL:#tn=	10.2 Hard disks and partitions>
		10.2.1 list partitions <URL:#tn=		10.2.1 list partitions>
			10.2.1.1 fstab(5) - Linux man page <URL:#tn=			10.2.1.1 fstab(5) - Linux man page>
			10.2.1.2 fdisk(8) - Linux man page <URL:#tn=			10.2.1.2 fdisk(8) - Linux man page>
			10.2.1.3 df(1) - Linux man page <URL:#tn=			10.2.1.3 df(1) - Linux man page>
		10.2.2 list hard disks <URL:#tn=		10.2.2 list hard disks>
		10.2.3 Converting Ext2 Filesystems to Ext3 <URL:#tn=		10.2.3 Converting Ext2 Filesystems to Ext3>
		10.2.4 Increase partition size <URL:#tn=		10.2.4 Increase partition size>
			10.2.4.1 Example from ACS machine <URL:#tn=			10.2.4.1 Example from ACS machine>
			10.2.4.2 <URL:#tn=			10.2.4.2>
		10.2.5 Linux Partition HOWTO <URL:#tn=		10.2.5 Linux Partition HOWTO>
		10.2.6 Linux Logical Volume Extend Size <URL:#tn=		10.2.6 Linux Logical Volume Extend Size>
		10.2.7 <URL:#tn=		10.2.7>
	10.3 cron <URL:#tn=	10.3 cron>
		10.3.1 cron(8) - Linux man page <URL:#tn=		10.3.1 cron(8) - Linux man page>
		10.3.2 crontab(5) - Linux man page <URL:#tn=		10.3.2 crontab(5) - Linux man page>
		10.3.3 Linux Crontab: 15 Awesome Cron Job Examples <URL:#tn=		10.3.3 Linux Crontab: 15 Awesome Cron Job Examples>
			10.3.3.1 Scheduling a Job For a Specific Time Every Day <URL:#tn=			10.3.3.1 Scheduling a Job For a Specific Time Every Day>
			10.3.3.2 Schedule a Job For More Than One Instance (e.g. Twice a Day) <URL:#tn=			10.3.3.2 Schedule a Job For More Than One Instance (e.g. Twice a Day)>
			10.3.3.3 Schedule a Job for Specific Range of Time (e.g. Only on Weekdays) <URL:#tn=			10.3.3.3 Schedule a Job for Specific Range of Time (e.g. Only on Weekdays)>
			10.3.3.4 How to View Crontab Entries? <URL:#tn=			10.3.3.4 How to View Crontab Entries?>
			10.3.3.5 How to Edit Crontab Entries? <URL:#tn=			10.3.3.5 How to Edit Crontab Entries?>
			10.3.3.6 Schedule a Job for Every Minute Using Cron. <URL:#tn=			10.3.3.6 Schedule a Job for Every Minute Using Cron.>
			10.3.3.7 Schedule a Background Cron Job For Every 10 Minutes. <URL:#tn=			10.3.3.7 Schedule a Background Cron Job For Every 10 Minutes.>
			10.3.3.8 Schedule a Job For First Minute of Every Year using @yearly <URL:#tn=			10.3.3.8 Schedule a Job For First Minute of Every Year using @yearly>
			10.3.3.9 Schedule a Cron Job Beginning of Every Month using @monthly <URL:#tn=			10.3.3.9 Schedule a Cron Job Beginning of Every Month using @monthly>
			10.3.3.10 Schedule a Background Job Every Day using @daily <URL:#tn=			10.3.3.10 Schedule a Background Job Every Day using @daily>
			10.3.3.11 How to Execute a Linux Command After Every Reboot using @reboot? <URL:#tn=			10.3.3.11 How to Execute a Linux Command After Every Reboot using @reboot?>
			10.3.3.12 How to Disable/Redirect the Crontab Mail Output using MAIL keyword? <URL:#tn=			10.3.3.12 How to Disable/Redirect the Crontab Mail Output using MAIL keyword?>
			10.3.3.13 How to Execute a Linux Cron Jobs Every Second Using Crontab. <URL:#tn=			10.3.3.13 How to Execute a Linux Cron Jobs Every Second Using Crontab.>
			10.3.3.14 Specify PATH Variable in the Crontab <URL:#tn=			10.3.3.14 Specify PATH Variable in the Crontab>
			10.3.3.15 Installing Crontab From a Cron File <URL:#tn=			10.3.3.15 Installing Crontab From a Cron File>
		10.3.4 Run a cron job every minute <URL:#tn=		10.3.4 Run a cron job every minute>
		10.3.5 <URL:#tn=		10.3.5>
	10.4 anacron <URL:#tn=	10.4 anacron>
		10.4.1 anacron man page <URL:#tn=		10.4.1 anacron man page>
	10.5 NTP, NTPD <URL:#tn=	10.5 NTP, NTPD>
		10.5.1  Manual <URL:#tn=		10.5.1  Manual>
		10.5.2 NTP Debugging Techniques <URL:#tn=		10.5.2 NTP Debugging Techniques>
		10.5.3 ntpdc - special NTP query program <URL:#tn=		10.5.3 ntpdc - special NTP query program>
		10.5.4 NAME ntp.conf -- Network Time Protocol (NTP) daemon configuration file <URL:#tn=		10.5.4 NAME ntp.conf -- Network Time Protocol (NTP) daemon configuration file>
		10.5.5 ntpq(8) - Linux man page <URL:#tn=		10.5.5 ntpq(8) - Linux man page>
		10.5.6 logout user <URL:#tn=		10.5.6 logout user>
		10.5.7 Introduction <URL:#tn=		10.5.7 Introduction>
		10.5.8 The NTP FAQ and HOWTO: Understanding and using the Network Time Protocol (A first try on a non-technical Mini-HOWTO and FAQ on NTP) <URL:#tn=		10.5.8 The NTP FAQ and HOWTO: Understanding and using the Network Time Protocol (A first try on a non-technical Mini-HOWTO and FAQ on NTP)>
		10.5.10 Fixing NTP Refusing to Sync <URL:#tn=		10.5.10 Fixing NTP Refusing to Sync>
		10.5.11 Timekeeping best practices for Linux guests (1006427) <URL:#tn=		10.5.11 Timekeeping best practices for Linux guests (1006427)>
0.vmware.pool.ntp.org <URL:#tn=0.vmware.pool.ntp.org>
1.vmware.pool.ntp.org <URL:#tn=1.vmware.pool.ntp.org>
		10.5.12 Kernel Timer Systems <URL:#tn=		10.5.12 Kernel Timer Systems>
	10.6 Network settings <URL:#tn=	10.6 Network settings>
		10.6.1 Linux Static IP Address Configuration <URL:#tn=		10.6.1 Linux Static IP Address Configuration>
		10.6.2 <URL:#tn=		10.6.2>
	10.7 <URL:#tn=	10.7>
11. Installation from sources. <URL:#tn=11. Installation from sources.>
	11.1 General procedure. <URL:#tn=	11.1 General procedure.>
		11.1.1 open archive <URL:#tn=		11.1.1 open archive>
		11.1.2 configure and make <URL:#tn=		11.1.2 configure and make>
		11.1.3 Install <URL:#tn=		11.1.3 Install>
12. Compression <URL:#tn=12. Compression>
	12.1 TAR <URL:#tn=	12.1 TAR>
		12.1.1 Basics <URL:#tn=		12.1.1 Basics>
			12.1.1.1 Creates a GZIP-compressed Tar file <URL:#tn=			12.1.1.1 Creates a GZIP-compressed Tar file>
			12.1.1.2 To list files in a compressed Tar file <URL:#tn=			12.1.1.2 To list files in a compressed Tar file>
			12.1.1.3 To extract files from a Tar file <URL:#tn=			12.1.1.3 To extract files from a Tar file>
			12.1.1.4 Use bzip2 compression <URL:#tn=			12.1.1.4 Use bzip2 compression>
		12.1.2 Exclude or Include files <URL:#tn=		12.1.2 Exclude or Include files>
			12.1.2.1 How to exclude/include <URL:#tn=			12.1.2.1 How to exclude/include>
			12.1.2.2 Example, create a backup of ACS project, exclude non source code files <URL:#tn=			12.1.2.2 Example, create a backup of ACS project, exclude non source code files>
13. Tools <URL:#tn=13. Tools>
	13.1 sort <URL:#tn=	13.1 sort>
		13.1.1 Examples <URL:#tn=		13.1.1 Examples>
		13.1.2 Using Sort to List Directories by Size <URL:#tn=		13.1.2 Using Sort to List Directories by Size>
		13.1.3 Databases <URL:#tn=		13.1.3 Databases>
			13.1.3.1 sqlite <URL:#tn=			13.1.3.1 sqlite>
		13.1.4 FZF <URL:#tn=		13.1.4 FZF>
		13.1.5 <URL:#tn=		13.1.5>
14. sudo <URL:#tn=14. sudo>
	14.1 Description <URL:#tn=	14.1 Description>
	14.2 Tips and tricks <URL:#tn=	14.2 Tips and tricks>
15. Command line power example <URL:#tn=15. Command line power example>
	15.1 The power of the for loop <URL:#tn=	15.1 The power of the for loop>
	15.2 When `` and xargs are not enough <URL:#tn=	15.2 When `` and xargs are not enough>
16.  SSH <URL:#tn=16.  SSH>
	16.1  Disable session timeout <URL:#tn=	16.1  Disable session timeout>
	16.2 copy files, scp <URL:#tn=	16.2 copy files, scp>
		16.2.1  scp(1) - Linux man page <URL:#tn=		16.2.1  scp(1) - Linux man page>
		16.2.2 My examples <URL:#tn=		16.2.2 My examples>
		16.2.3 Tips & Tricks with ssh and scp <URL:#tn=		16.2.3 Tips & Tricks with ssh and scp>
		16.2.4 scp errors <URL:#tn=		16.2.4 scp errors>
			16.2.4.1 protocol error: bad mode <URL:#tn=			16.2.4.1 protocol error: bad mode>
		16.2.5 <URL:#tn=		16.2.5>
	16.3 Read remote file via SSH <URL:#tn=	16.3 Read remote file via SSH>
	16.4 SSH login without password <URL:#tn=	16.4 SSH login without password>
		16.4.1 explanation <URL:#tn=		16.4.1 explanation>
		16.4.2 My example <URL:#tn=		16.4.2 My example>
	16.5 <URL:#tn=	16.5>
17. IO redirection <URL:#tn=17. IO redirection>
	17.1 tee <URL:#tn=	17.1 tee>
	17.2 Manual <URL:#tn=	17.2 Manual>
	17.3 How to use tee with stdout and stderr? <URL:#tn=	17.3 How to use tee with stdout and stderr?>
	17.4 <URL:#tn=	17.4>
18. Troubleshoot <URL:#tn=18. Troubleshoot>
	18.1 login <URL:#tn=	18.1 login>
		18.1.1 Direct login fails  "GDM could not write to your authorization file" <URL:#tn=		18.1.1 Direct login fails  "GDM could not write to your authorization file">
	18.2 Scripts <URL:#tn=	18.2 Scripts>
		18.2.1 line 1: ﻿#!/usr/bin/bash: No such file or directory <URL:#tn=		18.2.1 line 1: ﻿#!/usr/bin/bash: No such file or directory>
	18.3 <URL:#tn=	18.3>
19. LDAP <URL:#tn=19. LDAP>
	19.1 LDAP search <URL:#tn=	19.1 LDAP search>
	19.2 Determine gc of domain <URL:#tn=	19.2 Determine gc of domain>
	19.3 Determine DCs of domain <URL:#tn=	19.3 Determine DCs of domain>
38.60.56.10.in-addr.arpa        name = a3-adf1.amer.acs.com. <URL:#tn=38.60.56.10.in-addr.arpa        name = a3-adf1.amer.acs.com.>
	19.3 <URL:#tn=	19.3>
20. Examples of my use cases <URL:#tn=20. Examples of my use cases>
	20.1 sort -k example <URL:#tn=	20.1 sort -k example>
21. Monit <URL:#tn=21. Monit>
	21.1 Examples <URL:#tn=	21.1 Examples>
	21.2 FAQ <URL:#tn=	21.2 FAQ>
	21.3 MAN page <URL:#tn=	21.3 MAN page>
	21.4 <URL:#tn=	21.4>
22. FTP <URL:#tn=22. FTP>
	22.1 In Unix, how can I issue batches of non-interactive FTP commands? <URL:#tn=	22.1 In Unix, how can I issue batches of non-interactive FTP commands?>
	22.2 my aliases to automate FTP access <URL:#tn=	22.2 my aliases to automate FTP access>
	22.3 Kermit FTP client <URL:#tn=	22.3 Kermit FTP client>
	22.4 Expect script to automate ftp upload <URL:#tn=	22.4 Expect script to automate ftp upload>
	22.5 <URL:#tn=	22.5>
23. grep <URL:#tn=23. grep>
	23.1 Searching Files Using UNIX grep <URL:#tn=	23.1 Searching Files Using UNIX grep>
	23.2 grep for whole word (like vim /\<word\> <URL:#tn=	23.2 grep for whole word (like vim /\<word\>>
	23.3 quick-ref <URL:#tn=	23.3 quick-ref>
	23.4 Grep examples <URL:#tn=	23.4 Grep examples>
		23.4.1.1. Search for the given string in a single file <URL:#tn=		23.4.1.1. Search for the given string in a single file>
		23.4.1.1.2. Checking for the given string in multiple files. <URL:#tn=		23.4.1.1.2. Checking for the given string in multiple files.>
            23.4.1.1.3. Case insensitive search using grep -i <URL:#tn=            23.4.1.1.3. Case insensitive search using grep -i>
            23.4.1.1.4. Match regular expression in files <URL:#tn=            23.4.1.1.4. Match regular expression in files>
            23.4.1.1.5. Checking for full words, not for sub-strings using grep -w <URL:#tn=            23.4.1.1.5. Checking for full words, not for sub-strings using grep -w>
            23.4.1.1.6. Displaying lines before/after/around the match using grep -A, -B and -C <URL:#tn=            23.4.1.1.6. Displaying lines before/after/around the match using grep -A, -B and -C>
            23.4.1.1.6.1 Display N lines after match <URL:#tn=            23.4.1.1.6.1 Display N lines after match>
            23.4.1.1.6.2 Display N lines before match <URL:#tn=            23.4.1.1.6.2 Display N lines before match>
            23.4.1.1.6.3 Display N lines around match <URL:#tn=            23.4.1.1.6.3 Display N lines around match>
            23.4.1.1.7. Highlighting the search using GREP_OPTIONS <URL:#tn=            23.4.1.1.7. Highlighting the search using GREP_OPTIONS>
            23.4.1.1.8. Searching in all files recursively using grep -r <URL:#tn=            23.4.1.1.8. Searching in all files recursively using grep -r>
            23.4.1.1.9. Invert match using grep -v <URL:#tn=            23.4.1.1.9. Invert match using grep -v>
            23.4.1.1.10. display the lines which does not matches all the given pattern. <URL:#tn=            23.4.1.1.10. display the lines which does not matches all the given pattern.>
            23.4.1.1.11. Counting the number of matches using grep -c <URL:#tn=            23.4.1.1.11. Counting the number of matches using grep -c>
            23.4.1.1.12. Display only the file names which matches the given pattern using grep -l <URL:#tn=            23.4.1.1.12. Display only the file names which matches the given pattern using grep -l>
            23.4.1.1.13. Show only the matched string <URL:#tn=            23.4.1.1.13. Show only the matched string>
            23.4.1.1.14. Show the position of match in the line <URL:#tn=            23.4.1.1.14. Show the position of match in the line>
            23.4.1.1.15. Show line number while displaying the output using grep -n <URL:#tn=            23.4.1.1.15. Show line number while displaying the output using grep -n>
	23.5 <URL:#tn=	23.5>
24. FAQS <URL:#tn=24. FAQS>
	24.1 howto kill defunct processes <URL:#tn=	24.1 howto kill defunct processes>
	24.2 logrotate and syslog Troubleshoot <URL:#tn=	24.2 logrotate and syslog Troubleshoot>
		24.2.1 CSCty31541 Log Rotation:Excess log file generated for Runtime, Mgmt & mgmtAudit log (blocker for testing?) <URL:#tn=		24.2.1 CSCty31541 Log Rotation:Excess log file generated for Runtime, Mgmt & mgmtAudit log (blocker for testing?)>
		24.2.2 logrotate keeps writing to xxx.log.1 file <URL:#tn=		24.2.2 logrotate keeps writing to xxx.log.1 file>
		24.2.3 rsyslog and log rotate problem <URL:#tn=		24.2.3 rsyslog and log rotate problem>
		24.2.4 Run bash command for exactly X seconds time <URL:#tn=		24.2.4 Run bash command for exactly X seconds time>
			24.2.4.1 simplest approach <URL:#tn=			24.2.4.1 simplest approach>
			24.2.4.2 More sophisticated <URL:#tn=			24.2.4.2 More sophisticated>
			24.2.4.3 <URL:#tn=			24.2.4.3>
		24.2.5 Limit find to only one file <URL:#tn=		24.2.5 Limit find to only one file>
		24.2.6 How to insert a date inside the filename with logrotate <URL:#tn=		24.2.6 How to insert a date inside the filename with logrotate>
	24.3 how-to-list-just-directories-the-correct-way <URL:#tn=	24.3 how-to-list-just-directories-the-correct-way>
	24.4 Linux: Find Out How Many File Descriptors Are Being Used <URL:#tn=	24.4 Linux: Find Out How Many File Descriptors Are Being Used>
	24.5 Check the open FD limit for a given process in Linux <URL:#tn=	24.5 Check the open FD limit for a given process in Linux>
	24.6 Tip: Redirecting Multiple Command Outputs <URL:#tn=	24.6 Tip: Redirecting Multiple Command Outputs>
	24.7 Question BASH: How to Redirect Output to File, AND Still Have it on Screen <URL:#tn=	24.7 Question BASH: How to Redirect Output to File, AND Still Have it on Screen>
	24.8 Use find to test whether file found and exists <URL:#tn=	24.8 Use find to test whether file found and exists>
	24.9 How to determine the Boost version on a system? <URL:#tn=	24.9 How to determine the Boost version on a system?>
	24.10 Sort du output in Human-readable format <URL:#tn=	24.10 Sort du output in Human-readable format>
	24.11 How to: Mount an ISO image under Linux <URL:#tn=	24.11 How to: Mount an ISO image under Linux>
	24.12 How big is the pipe buffer? <URL:#tn=	24.12 How big is the pipe buffer?>
19.2k54168 <URL:#tn=19.2k54168>
	24.13 Get exit status of process that's piped to another <URL:#tn=	24.13 Get exit status of process that's piped to another>
	24.14 Find which user is running a process, also get user name from ID <URL:#tn=	24.14 Find which user is running a process, also get user name from ID>
	24.15 <URL:#tn=	24.15>
	24.16 <URL:#tn=	24.16>
	24.17 <URL:#tn=	24.17>
	24.18 <URL:#tn=	24.18>
	24.19 <URL:#tn=	24.19>
	24.20 <URL:#tn=	24.20>
	24.21 <URL:#tn=	24.21>
	24.22 <URL:#tn=	24.22>
	24.23 <URL:#tn=	24.23>
	24.24 <URL:#tn=	24.24>
	24.25 <URL:#tn=	24.25>
	24.26 <URL:#tn=	24.26>
	24.27 <URL:#tn=	24.27>
	24.28 <URL:#tn=	24.28>
	24.29 <URL:#tn=	24.29>
	24.30 <URL:#tn=	24.30>
	24.31 <URL:#tn=	24.31>
	24.32 <URL:#tn=	24.32>
	24.33 <URL:#tn=	24.33>
	24.34 <URL:#tn=	24.34>
	24.35 <URL:#tn=	24.35>
	24.36 <URL:#tn=	24.36>
	24.37 <URL:#tn=	24.37>
	24.38 <URL:#tn=	24.38>
	24.39 <URL:#tn=	24.39>
	24.40 <URL:#tn=	24.40>
	24.41 <URL:#tn=	24.41>
	24.42 <URL:#tn=	24.42>
	24.43 <URL:#tn=	24.43>
	24.44 <URL:#tn=	24.44>
	24.45 <URL:#tn=	24.45>
	24.46 <URL:#tn=	24.46>
	24.47 <URL:#tn=	24.47>
	24.48 <URL:#tn=	24.48>
	24.49 <URL:#tn=	24.49>
	24.50 <URL:#tn=	24.50>
	24.51 <URL:#tn=	24.51>
	24.52 <URL:#tn=	24.52>
	24.53 <URL:#tn=	24.53>
	24.54 <URL:#tn=	24.54>
	24.55 <URL:#tn=	24.55>
	24.56 <URL:#tn=	24.56>
	24.57 <URL:#tn=	24.57>
	24.58 <URL:#tn=	24.58>
	24.59 <URL:#tn=	24.59>
	24.60 <URL:#tn=	24.60>
	24.61 <URL:#tn=	24.61>
	24.62 <URL:#tn=	24.62>
	24.63 <URL:#tn=	24.63>
25. NFS <URL:#tn=25. NFS>
	25.1 http://www.troubleshooters.com/linux/nfs.htm <URL:#tn=	25.1 http://www.troubleshooters.com/linux/nfs.htm>
	25.2 <URL:#tn=	25.2>
26. Linux system install <URL:#tn=26. Linux system install>
	26.1 Install from ISO <URL:#tn=	26.1 Install from ISO>
		26.1.1  Using kickstart <URL:#tn=		26.1.1  Using kickstart>
			26.1.1.1  How Kickstart Works http://kickstart-tools.sourceforge.net/howkickstartworks.html <URL:#tn=			26.1.1.1  How Kickstart Works http://kickstart-tools.sourceforge.net/howkickstartworks.html>
    1. You may not want all the RPMS that RedHat provides <URL:#tn=    1. You may not want all the RPMS that RedHat provides>
    2. You may want to add your own RPMS. <URL:#tn=    2. You may want to add your own RPMS.>
    3. If you list all the specific rpms you want in the %packages section of the ks.cfg then the ks.cfg file won't fit on boot floppy. <URL:#tn=    3. If you list all the specific rpms you want in the %packages section of the ks.cfg then the ks.cfg file won't fit on boot floppy.>
    4. If you put complex postinstall commands in the ks.cfg file then the file might not fit on the boot floppy. <URL:#tn=    4. If you put complex postinstall commands in the ks.cfg file then the file might not fit on the boot floppy.>
    5. There are updates available for some packages. You don't won't to install old ones that would then need to be upgraded. <URL:#tn=    5. There are updates available for some packages. You don't won't to install old ones that would then need to be upgraded.>
		26.1.2 <URL:#tn=		26.1.2>
	26.2 <URL:#tn=	26.2>
27. Ubuntu <URL:#tn=27. Ubuntu>
	27.1 Configuring <URL:#tn=	27.1 Configuring>
		27.1.1 Install guide for VNC <URL:#tn=		27.1.1 Install guide for VNC>
		27.1.2 Change Ubuntu Server from DHCP to a Static IP Address <URL:#tn=		27.1.2 Change Ubuntu Server from DHCP to a Static IP Address>
		27.1.3 Login as root <URL:#tn=		27.1.3 Login as root>
		27.1.4 Disable firewall <URL:#tn=		27.1.4 Disable firewall>
			27.1.4.1 Disable / Turn Off Firewall in Ubuntu Linux Server <URL:#tn=			27.1.4.1 Disable / Turn Off Firewall in Ubuntu Linux Server>
			27.1.4.2 <URL:#tn=			27.1.4.2>
		27.1.5 <URL:#tn=		27.1.5>
	27.2 Troubleshooting <URL:#tn=	27.2 Troubleshooting>
		27.2.1  How To Fix “Problem with MergeList /var/lib/apt/lists” Error In Ubuntu 11.04 <URL:#tn=		27.2.1  How To Fix “Problem with MergeList /var/lib/apt/lists” Error In Ubuntu 11.04>
		27.2.2 <URL:#tn=		27.2.2>
	27.3 packages <URL:#tn=	27.3 packages>
		27.3.1 istapd <URL:#tn=		27.3.1 istapd>
		27.3.2 <URL:#tn=		27.3.2>
	27.4 <URL:#tn=	27.4>
28. Logging <URL:#tn=28. Logging>
	28.1 Logging, Log File Rotation, and Syslog Tutorial <URL:#tn=	28.1 Logging, Log File Rotation, and Syslog Tutorial>
29.   Featured Articles <URL:#tn=29.   Featured Articles>
	29.1 20 Linux System Monitoring Tools Every SysAdmin Should Know <URL:#tn=	29.1 20 Linux System Monitoring Tools Every SysAdmin Should Know>
		29.1.1 #1: top - Process Activity Command <URL:#tn=		29.1.1 #1: top - Process Activity Command>
		29.1.2 #2: vmstat - System Activity, Hardware and System Information <URL:#tn=		29.1.2 #2: vmstat - System Activity, Hardware and System Information>
		29.1.3 #3: w - Find Out Who Is Logged on And What They Are Doing <URL:#tn=		29.1.3 #3: w - Find Out Who Is Logged on And What They Are Doing>
		29.1.4 #4: uptime - Tell How Long The System Has Been Running <URL:#tn=		29.1.4 #4: uptime - Tell How Long The System Has Been Running>
		29.1.5 #5: ps - Displays The Processes <URL:#tn=		29.1.5 #5: ps - Displays The Processes>
		29.1.6 #6: free - Memory Usage <URL:#tn=		29.1.6 #6: free - Memory Usage>
		29.1.7 #7: iostat - Average CPU Load, Disk Activity <URL:#tn=		29.1.7 #7: iostat - Average CPU Load, Disk Activity>
		29.1.8 #8: sar - Collect and Report System Activity <URL:#tn=		29.1.8 #8: sar - Collect and Report System Activity>
		29.1.9 #9: mpstat - Multiprocessor Usage <URL:#tn=		29.1.9 #9: mpstat - Multiprocessor Usage>
		29.1.10 #10: pmap - Process Memory Usage <URL:#tn=		29.1.10 #10: pmap - Process Memory Usage>
		29.1.11 #11 and #12: netstat and ss - Network Statistics <URL:#tn=		29.1.11 #11 and #12: netstat and ss - Network Statistics>
		29.1.12 #13: iptraf - Real-time Network Statistics <URL:#tn=		29.1.12 #13: iptraf - Real-time Network Statistics>
		29.1.13 #14: tcpdump - Detailed Network Traffic Analysis <URL:#tn=		29.1.13 #14: tcpdump - Detailed Network Traffic Analysis>
		29.1.14 #15: strace - System Calls <URL:#tn=		29.1.14 #15: strace - System Calls>
		29.1.15 #16: /Proc file system - Various Kernel Statistics <URL:#tn=		29.1.15 #16: /Proc file system - Various Kernel Statistics>
		29.1.16 #19: KDE System Guard - Real-time Systems Reporting and Graphing <URL:#tn=		29.1.16 #19: KDE System Guard - Real-time Systems Reporting and Graphing>
		29.1.17 #20: Gnome System Monitor - Real-time Systems Reporting and Graphing <URL:#tn=		29.1.17 #20: Gnome System Monitor - Real-time Systems Reporting and Graphing>
    29.2 Iptables <URL:#tn=    29.2 Iptables>
	29.2.1 Linux: 20 Iptables Examples For New SysAdmins <URL:#tn=	29.2.1 Linux: 20 Iptables Examples For New SysAdmins>
		29.2.1.1 #1: Displaying the Status of Your Firewall <URL:#tn=		29.2.1.1 #1: Displaying the Status of Your Firewall>
		29.2.1.2 #2: Stop / Start / Restart the Firewall <URL:#tn=		29.2.1.2 #2: Stop / Start / Restart the Firewall>
		29.2.1.3 #3: Delete Firewall Rules <URL:#tn=		29.2.1.3 #3: Delete Firewall Rules>
		29.2.1.4 <URL:#tn=		29.2.1.4>
		29.2.1.5 #4: Insert Firewall Rules <URL:#tn=		29.2.1.5 #4: Insert Firewall Rules>
		29.2.1.6 #5: Save Firewall Rules <URL:#tn=		29.2.1.6 #5: Save Firewall Rules>
		29.2.1.7 #6: Restore Firewall Rules <URL:#tn=		29.2.1.7 #6: Restore Firewall Rules>
		29.2.1.8 #7: Set the Default Firewall Policies <URL:#tn=		29.2.1.8 #7: Set the Default Firewall Policies>
		29.2.1.9 #8:Drop Private Network Address On Public Interface <URL:#tn=		29.2.1.9 #8:Drop Private Network Address On Public Interface>
		29.2.1.10 #9: Blocking an IP Address (BLOCK IP) <URL:#tn=		29.2.1.10 #9: Blocking an IP Address (BLOCK IP)>
		29.2.1.11 #10: Block Incoming Port Requests (BLOCK PORT) <URL:#tn=		29.2.1.11 #10: Block Incoming Port Requests (BLOCK PORT)>
		29.2.1.12 #11: Block Outgoing IP Address <URL:#tn=		29.2.1.12 #11: Block Outgoing IP Address>
		29.2.1.13 #12: Log and Drop Packets <URL:#tn=		29.2.1.13 #12: Log and Drop Packets>
		29.2.1.14 #13: Log and Drop Packets with Limited Number of Log Entries <URL:#tn=		29.2.1.14 #13: Log and Drop Packets with Limited Number of Log Entries>
		29.2.1.15 #14: Drop or Accept Traffic From Mac Address <URL:#tn=		29.2.1.15 #14: Drop or Accept Traffic From Mac Address>
		29.2.1.16 #15: Block or Allow ICMP Ping Request <URL:#tn=		29.2.1.16 #15: Block or Allow ICMP Ping Request>
		29.2.1.17 #16: Open Range of Ports <URL:#tn=		29.2.1.17 #16: Open Range of Ports>
		29.2.1.18 #17: Open Range of IP Addresses <URL:#tn=		29.2.1.18 #17: Open Range of IP Addresses>
		29.2.1.19 #18: Established Connections and Restaring The Firewall <URL:#tn=		29.2.1.19 #18: Established Connections and Restaring The Firewall>
		29.2.1.20 #19: Help Iptables Flooding My Server Screen <URL:#tn=		29.2.1.20 #19: Help Iptables Flooding My Server Screen>
		29.2.1.21 #20: Block or Open Common Ports <URL:#tn=		29.2.1.21 #20: Block or Open Common Ports>
		29.2.1.22 #21: Restrict the Number of Parallel Connections To a Server Per Client IP <URL:#tn=		29.2.1.22 #21: Restrict the Number of Parallel Connections To a Server Per Client IP>
		29.2.1.23 #22: HowTO: Use iptables Like a Pro <URL:#tn=		29.2.1.23 #22: HowTO: Use iptables Like a Pro>
	29.2.2 My examples <URL:#tn=	29.2.2 My examples>
		29.2.2.1 Block incoming traffic from given server <URL:#tn=		29.2.2.1 Block incoming traffic from given server>
		29.2.2.2 <URL:#tn=		29.2.2.2>
	29.2.3 <URL:#tn=	29.2.3>
    29.3 <URL:#tn=    29.3>
30. Performance Optimization, tools and Analysis <URL:#tn=30. Performance Optimization, tools and Analysis>
	30.1  Tools <URL:#tn=	30.1  Tools>
		30.1.1  uptime <URL:#tn=		30.1.1  uptime>
		30.1.2  cat /proc/cpuinfo <URL:#tn=		30.1.2  cat /proc/cpuinfo>
		30.1.3 vmstat <URL:#tn=		30.1.3 vmstat>
		30.1.4 top and gtop <URL:#tn=		30.1.4 top and gtop>
		30.1.5 sar <URL:#tn=		30.1.5 sar>
		30.1.6 Memory Utilization tools <URL:#tn=		30.1.6 Memory Utilization tools>
			30.1.6.1 /proc/meminfo and  /proc/slabinfo <URL:#tn=			30.1.6.1 /proc/meminfo and  /proc/slabinfo>
			30.1.6.2 ps memory info <URL:#tn=			30.1.6.2 ps memory info>
			30.1.6.3 /proc/pid/maps <URL:#tn=			30.1.6.3 /proc/pid/maps>
			30.1.6.4 vmstat <URL:#tn=			30.1.6.4 vmstat>
		30.1.7 I/O Utilization <URL:#tn=		30.1.7 I/O Utilization>
			30.1.7.1 iostat <URL:#tn=			30.1.7.1 iostat>
			30.1.7.2 sar <URL:#tn=			30.1.7.2 sar>
		30.1.8 Network Utilization <URL:#tn=		30.1.8 Network Utilization>
		30.1.9 perf stat <URL:#tn=		30.1.9 perf stat>
		    30.1.9.1 user perf test to troubelshoot cache misses <URL:#tn=		    30.1.9.1 user perf test to troubelshoot cache misses>
	30.2 <URL:#tn=	30.2>
31. Services <URL:#tn=31. Services>
	31.1 Configuring services in Linux. <URL:#tn=	31.1 Configuring services in Linux.>
		31.1.1 Overview of services in Linux. <URL:#tn=		31.1.1 Overview of services in Linux.>
		31.1.2 chkconfig and service commands. <URL:#tn=		31.1.2 chkconfig and service commands.>
		31.1.3 GUI tool - system-config-services. <URL:#tn=		31.1.3 GUI tool - system-config-services.>
		31.1.4 Commands <URL:#tn=		31.1.4 Commands>
			31.1.4.1 Finding which services start on boot using chkconfig <URL:#tn=			31.1.4.1 Finding which services start on boot using chkconfig>
			31.1.4.2 View only services not started on boot. <URL:#tn=			31.1.4.2 View only services not started on boot.>
			31.1.4.3 Changing whether a service starts on boot. <URL:#tn=			31.1.4.3 Changing whether a service starts on boot.>
			31.1.4.4 Starting and stopping services using the service command. <URL:#tn=			31.1.4.4 Starting and stopping services using the service command.>
			31.1.4.5 <URL:#tn=			31.1.4.5>
		31.1.5 <URL:#tn=		31.1.5>
	31.2 Automatically load services (and scripts) after reboot <URL:#tn=	31.2 Automatically load services (and scripts) after reboot>
		31.2.1 Get To Know Linux: The /etc/init.d Directory <URL:#tn=		31.2.1 Get To Know Linux: The /etc/init.d Directory>
		31.2.2 Linux Init Process / PC Boot Procedure <URL:#tn=		31.2.2 Linux Init Process / PC Boot Procedure>
			31.2.2.1 PC Boot and Linux Init Process: <URL:#tn=			31.2.2.1 PC Boot and Linux Init Process:>
			31.2.2.2 The Linux Init Processes: <URL:#tn=			31.2.2.2 The Linux Init Processes:>
			31.2.2.3 Linux init Run Levels: <URL:#tn=			31.2.2.3 Linux init Run Levels:>
			31.2.2.4 Run Level Commands: <URL:#tn=			31.2.2.4 Run Level Commands:>
			31.2.2.5 Init Script Activation: <URL:#tn=			31.2.2.5 Init Script Activation:>
			31.2.2.6 Init Script: <URL:#tn=			31.2.2.6 Init Script:>
			31.2.2.7 chkconfig: <URL:#tn=			31.2.2.7 chkconfig:>
			31.2.2.8 Related Commands: <URL:#tn=			31.2.2.8 Related Commands:>
		31.2.3 <URL:#tn=		31.2.3>
	31.3 <URL:#tn=	31.3>
32. HowTO <URL:#tn=32. HowTO>
	32.1  HowTo: Debug Crashed Linux Application Core Files Like A Pro <URL:#tn=	32.1  HowTo: Debug Crashed Linux Application Core Files Like A Pro>
	32.2 30 Handy Bash Shell Aliases For Linux / Unix / Mac OS X <URL:#tn=	32.2 30 Handy Bash Shell Aliases For Linux / Unix / Mac OS X>
	32.3 Top 30 Nmap Command Examples For Sys/Network Admins <URL:#tn=	32.3 Top 30 Nmap Command Examples For Sys/Network Admins>
	32.4 <URL:#tn=	32.4>
33. My examples and snippets <URL:#tn=33. My examples and snippets>
	33.1 use xargs for multiple commands and handle empty results <URL:#tn=	33.1 use xargs for multiple commands and handle empty results>
	33.2 search all files for pattern in specific line only <URL:#tn=	33.2 search all files for pattern in specific line only>
	33.3 <URL:#tn=	33.3>
34. Shared objects <URL:#tn=34. Shared objects>
	34.1 ld-linux(8) - Linux man page <URL:#tn=	34.1 ld-linux(8) - Linux man page>
		34.1.1 Description <URL:#tn=		34.1.1 Description>
		34.1.2 Shared object load order <URL:#tn=		34.1.2 Shared object load order>
		34.1.3 Options <URL:#tn=		34.1.3 Options>
		34.1.4 <URL:#tn=		34.1.4>
	34.2 <URL:#tn=	34.2>
35. Connectivity <URL:#tn=35. Connectivity>
	35.1 VNC <URL:#tn=	35.1 VNC>
		35.1.1 GUI problem - can't open gnome session <URL:#tn=		35.1.1 GUI problem - can't open gnome session>
		35.1.2 Login via SSH but show GUI on VNC <URL:#tn=		35.1.2 Login via SSH but show GUI on VNC>
		35.1.3 tmux <URL:#tn=		35.1.3 tmux>
		    35.1.3.1 What is it <URL:#tn=		    35.1.3.1 What is it>
		    35.1.3.2 install from source <URL:#tn=		    35.1.3.2 install from source>
		    35.1.3.3 <URL:#tn=		    35.1.3.3>
	35.2 screen, similar tool to tmux <URL:#tn=	35.2 screen, similar tool to tmux>
31917.pts-5.office      (Detached) <URL:#tn=31917.pts-5.office      (Detached)>
31844.pts-0.office      (Detached) <URL:#tn=31844.pts-0.office      (Detached)>
	35.3 My screen commands <URL:#tn=	35.3 My screen commands>
	35.4 Screen status line <URL:#tn=	35.4 Screen status line>
	35.5 <URL:#tn=	35.5>
36. Cookbook <URL:#tn=36. Cookbook>
	36.1 Diff <URL:#tn=	36.1 Diff>
		36.1.1 find difference between two text files with one item per line <URL:#tn=		36.1.1 find difference between two text files with one item per line>
		36.1.2 <URL:#tn=		36.1.2>
	36.2 <URL:#tn=	36.2>
37. Display <URL:#tn=37. Display>
38. korn shell ksh <URL:#tn=38. korn shell ksh>
    38.1 Tutorial <URL:#tn=    38.1 Tutorial>
        38.1.1  intro <URL:#tn=        38.1.1  intro>
        38.1.2 Ksh basics <URL:#tn=        38.1.2 Ksh basics>
        38.1.3 Advanced variable usage <URL:#tn=        38.1.3 Advanced variable usage>
        38.1.4 Ksh Functions <URL:#tn=        38.1.4 Ksh Functions>
        38.1.5 Ksh built-in functions <URL:#tn=        38.1.5 Ksh built-in functions>
        38.1.6 Redirection and Pipes <URL:#tn=        38.1.6 Redirection and Pipes>
        38.1.7 Other Stuff <URL:#tn=        38.1.7 Other Stuff>
    38.2 command history <URL:#tn=    38.2 command history>
        38.2.1 run last command <URL:#tn=        38.2.1 run last command>
        38.2.2 run cmd by number <URL:#tn=        38.2.2 run cmd by number>
        38.2.3 <URL:#tn=        38.2.3>
    38.3 kshrc <URL:#tn=    38.3 kshrc>
        38.3.1 GE linux example <URL:#tn=        38.3.1 GE linux example>
    38.4 The fc Command <URL:#tn=    38.4 The fc Command>
39. Docker <URL:#tn=39. Docker>
    39.1  Introduction <URL:#tn=    39.1  Introduction>
        39.1.1  First steps <URL:#tn=        39.1.1  First steps>
        39.1.2 Create a container <URL:#tn=        39.1.2 Create a container>
        39.1.3 cleanup <URL:#tn=        39.1.3 cleanup>
            39.1.3.1 How To Remove Docker Containers, Images, Volumes, and Networks <URL:#tn=            39.1.3.1 How To Remove Docker Containers, Images, Volumes, and Networks>
                39.1.3.1.1 Removing All Unused Objects <URL:#tn=                39.1.3.1.1 Removing All Unused Objects>
                39.1.3.1.2 Removing Docker Containers <URL:#tn=                39.1.3.1.2 Removing Docker Containers>
                39.1.3.1.3 Removing Docker Images <URL:#tn=                39.1.3.1.3 Removing Docker Images>
                39.1.3.1.4 Removing Docker Volumes <URL:#tn=                39.1.3.1.4 Removing Docker Volumes>
                39.1.3.1.5 Removing Docker Networks <URL:#tn=                39.1.3.1.5 Removing Docker Networks>
                39.1.3.1.6 Conclusion <URL:#tn=                39.1.3.1.6 Conclusion>
                39.1.3.1.7 <URL:#tn=                39.1.3.1.7>
            39.1.3.2 <URL:#tn=            39.1.3.2>
        39.1.4 <URL:#tn=        39.1.4>
    39.2 Cheatsheet <URL:#tn=    39.2 Cheatsheet>
    39.3 docker-compose <URL:#tn=    39.3 docker-compose>
        39.3.1  intro <URL:#tn=        39.3.1  intro>
            39.3.1.1  https://docs.docker.com/compose/ <URL:#tn=            39.3.1.1  https://docs.docker.com/compose/>
            39.3.1.2 https://hackernoon.com/practical-introduction-to-docker-compose-d34e79c4c2b6 <URL:#tn=            39.3.1.2 https://hackernoon.com/practical-introduction-to-docker-compose-d34e79c4c2b6>
        39.3.2 <URL:#tn=        39.3.2>
    39.4 <URL:#tn=    39.4>
40. cgroups <URL:#tn=40. cgroups>
41. find, find command, <URL:#tn=41. find, find command,>
    41.1 find cheatsheet <URL:#tn=    41.1 find cheatsheet>
    41.2 My useful find commands <URL:#tn=    41.2 My useful find commands>
        41.2.1 find recent files - newer than 1 hour / 1 day <URL:#tn=        41.2.1 find recent files - newer than 1 hour / 1 day>
        41.2.2 exclude directory <URL:#tn=        41.2.2 exclude directory>
        41.2.3 <URL:#tn=        41.2.3>
    41.3 <URL:#tn=    41.3>
42.  rm, delete files <URL:#tn=42.  rm, delete files>
    42.1  rm -fr <URL:#tn=    42.1  rm -fr>
    42.2 Delete All Files in a Directory Except One or Few Files with Extensions <URL:#tn=    42.2 Delete All Files in a Directory Except One or Few Files with Extensions>
        42.2.1 background <URL:#tn=        42.2.1 background>
        42.2.2 To delete all files in a directory except filename, type the command below: <URL:#tn=        42.2.2 To delete all files in a directory except filename, type the command below:>
        42.2.3 To delete all files with the exception of filename1 and filename2: <URL:#tn=        42.2.3 To delete all files with the exception of filename1 and filename2:>
        42.2.4 The example below shows how to remove all files other than all .zip files interactively: <URL:#tn=        42.2.4 The example below shows how to remove all files other than all .zip files interactively:>
        42.2.5 delete all files in a directory apart from all .zip and .odt files as follows, while displaying what is being done: <URL:#tn=        42.2.5 delete all files in a directory apart from all .zip and .odt files as follows, while displaying what is being done:>
        42.2.6 Delete Files Using Linux find Command <URL:#tn=        42.2.6 Delete Files Using Linux find Command>
            42.2.6.1 The following command will delete all files apart from .gz files in the current directory: <URL:#tn=            42.2.6.1 The following command will delete all files apart from .gz files in the current directory:>
            42.2.6.2 Using a pipeline and xargs, you can modify the case above as follows: <URL:#tn=            42.2.6.2 Using a pipeline and xargs, you can modify the case above as follows:>
            42.2.6.3 wipe out all files excluding .gz, .odt, and .jpg files in the current directory: <URL:#tn=            42.2.6.3 wipe out all files excluding .gz, .odt, and .jpg files in the current directory:>
            42.2.6.4 <URL:#tn=            42.2.6.4>
        42.2.7 Delete Files Using Bash GLOBIGNORE Variable <URL:#tn=        42.2.7 Delete Files Using Bash GLOBIGNORE Variable>
        42.2.8 <URL:#tn=        42.2.8>
    42.3 <URL:#tn=    42.3>
43. ag silver searcher <URL:#tn=43. ag silver searcher>
    43.1  basics <URL:#tn=    43.1  basics>
    43.2 advanced <URL:#tn=    43.2 advanced>
        43.2.1  https://www.alexwheeler.io/command-line/2018/08/02/ag.html <URL:#tn=        43.2.1  https://www.alexwheeler.io/command-line/2018/08/02/ag.html>
        43.2.2 <URL:#tn=        43.2.2>
    43.3 <URL:#tn=    43.3>
44. <URL:#tn=44.>
.................................................END TOC..............................................

























1. RPMs

	1.1 query installed rpm 
	rpm -qa | grep <rpm name>

	1.2 uninstall package
	rpm -e <rpm name>

	1.3 install RPM
	rpm -ivh <rpm name>

	1.4  Listing Contents of an RPM Package

To list files archived in an RPM package, issue an RPM query command:

rpm -qlp myfile.rpm

Here command line parameters have the following meaning:
q	-	perform a query
l	-	list files
p	-	use the specified package file (myfile.rpm in this example)

	1.5 . Printing Out an RPM Package Information

To retrieve general information about a given RPM package, issue the following RPM query:

rpm -qip myfile.rpm

Here command line parameters have the following meaning:
q	-	perform a query
i	-	print out package information
p	-	use the specified package file (myfile.rpm in this example)

This query will print out all the essential information stored by the creator of the RPM package in question. This may contain such fields as package name and version, creator's name, build date and  what's most important for users  relocations. Section 7 explains how to make use of this information. 
	1.6 Simple Unpacking(extract) of an RPM Archive

In many cases you would only need to extract files from an RPM archive, without performing complex operations with an RPM database. There is a simple way to do it:

rpm2cpio < myfile.rpm | cpio -i --make-directories

This method makes use of the generic GNU CPIO utility, which deals with copying files to and from archives. The rpm2cpio command converts from RPM to CPIO format, and the output of this conversion is redirected to the cpio command. Command line parameters for the latter have the following meaning:
i	-	extract contents
make-directories	-	create directories when necessary

As a result, all the contents of the RPM package will be extracted into the current directory, creating relative paths. 

	1.7 Creating a Private RPM Database

To achieve a higher flexibility and to get access to more RPM functionality, you may want to make a step forward and create your own RPM database. You only have to do it once, by issuing the following instructions:

mkdir $HOME/myrpmdb
rpmdb --initdb --dbpath $HOME/myrpmdb

or, for older versions of RPM:

mkdir $HOME/myrpmdb
rpm --initdb --dbpath $HOME/myrpmdb

Mentioned in this example directory $HOME/myrpmdb can of course be substituted with any other location which suits you best. The only requirement is that you must have write permissions in this location.

Having such a personal RPM database allows you to perform more complex operations with RPM packages, without resorting to simple unpacking and manual installation. To simplify your life even further, do the following:

echo '%_dbpath /home/yourname/myrpmdb' | cat >> $HOME/.rpmmacros

This will create a file .rpmmacros in your $HOME directory, and add there the line specifying the database location. In this example, /home/yourname/myrpmdb should be substituted with your RPM database path. This will instruct RPM to use always your personal database. If you don't want to use this feature (for example, if you have several databases), take care to append

--dbpath $HOME/myrpmdb

to each RPM instruction which refers to such a database (e.g., package installation or removal) .

It may be useful to copy the system-wide database into your own one. RPM does not provide any tool for database synchronization (yet?), thus you have to do it hard way:

cp /var/lib/rpm/* $HOME/myrpmdb/.

Here /var/lib/rpm/ is a typical default location of the system-wide database (it could be different though on different systems). Having thus your private database initialized with the existing system information may help you in dealing with dependencies (see Section 8), but only for a while, since any system-wide upgrades will not be registered in your database, and there is no way to synchronize RPM databases in a clever manner.

	1.8  Installing an RPM Package

Having defined your private database (Section 4), you can attempt to install any RPM package by doing the following:

rpm -ivh myfile.rpm

Here command line parameters have the following meaning:
i	-	perform an installation procedure
v	-	print extra information
h	-	print progress bar with hash-marks

Keep in mind that if you didn't define a default database in $HOME/.rpmmacros, you'll have to append --dbpath $HOME/myrpmdb (or whatever is your private database location) to the directive.

Most likely, however, such a simple installation instruction will fail. There are two main reasons:

   -> RPM packages attempt to install themselves in locations defined by their creators, not by you. How to get around, read in Section 1.10.
   -> RPM checks for other software needed by the package  dependencies  in your private database; however, most likely, such software was not installed by you and can not be found in your database. How to deal with it, read in Section 1.11. 

	1.9  Upgrading RPM Packages

If you happened to have a package installed and simply want to upgrade it, issue the following instruction:

rpm -Uvh myfile.rpm

Here command line parameters have the following meaning:
U	-	upgrade the installation
v	-	print extra information
h	-	print progress bar with hash-marks

Since this operation needs to access a database, you either have to have the default database location specified in the $HOME/.rpmmacros file, or to append --dbpath $HOME/myrpmdb (or whatever is your private database location) to the directive (see Section 1.7 for details). 

	1.10  Upgrading RPM Packages

If you happened to have a package installed and simply want to upgrade it, issue the following instruction:

rpm -Uvh myfile.rpm

Here command line parameters have the following meaning:
U	-	upgrade the installation
v	-	print extra information
h	-	print progress bar with hash-marks

Since this operation needs to access a database, you either have to have the default database location specified in the $HOME/.rpmmacros file, or to append --dbpath $HOME/myrpmdb (or whatever is your private database location) to the directive (see Section 1.7 for details). 

	1.11   Dealing with Dependencies

Even when relocations proceeded smoothly, your package may still fail to get installed due to unsatisfied dependencies (this is the name for any other software needed by the package), which are absent from your private database (Section 4). A bold way to deal with it is to force the installation, e.g.:

rpm -ivh myfile.rpm --relocate /oldlocation=/newlocation --nodeps

The command line parameters have the following meaning:
i	-	perform an installation procedure
v	-	print extra information
h	-	print progress bar with hash-marks
relocate	-	instruct RPM to install files from /oldlocation into /newlocation
nodeps	-	ignore dependencies

This is an admittedly non-elegant way. A better solution may be to enter the necessary information into the database by installing "virtual" RPMs which only register a database record without actually installing software; however this is clearly package-dependent, thus you should request such a "fake" RPM from the package provider. An example of a shell script building such a virtual RPM can be found here.

	1.12 Listing Installed Packages

To list all the packages installed via RPM mechanisms, do:

rpm -qai

Here command line parameters have the following meaning:
q	-	perform a query
a	-	query all installed packages
i	-	print out package information

This command prints out the information stored in your RPM database, hence you either have to have the default database location specified in the $HOME/.rpmmacros file, or to append --dbpath $HOME/myrpmdb (or whatever is your private database location) to the directive

	1.13  Building RPM from Source

It may happen so that binary RPM packages for your system are unavailable, but a source RPM or a tarball is provided. You can build a binary RPM yourself from such sources, but some extra manipulations should be made first.

The default RPM top directory is /usr/src/redhat (for a RedHat system). Naturally, as a user, you have no write permission there. To solve the problem, start by creating a new directory structure in any place which has enough disk space and write permission:

mkdir -p rpmtop/RPMS/i386; mkdir rpmtop/SRPMS; mkdir rpmtop/SOURCES; mkdir rpmtop/BUILD; mkdir rpmtop/SPECS

Now you should instruct RPM to use this directory instead of the system-wide one (assuming you created the rpmtop structure in /home/yourname/):

echo '%_topdir /home/yourname/rpmtop' | cat >> $HOME/.rpmmacros

Next thing you should take care of, is the temporary directory for RPM builds, which by default is /var/tmp, also unavailable for users. The solution is similar to the above, to create your own directory in any place which has enough disk space and write permission:

mkdir mytmp

This directory should also be specified in your .rpmmacros file:

echo '%_tmppath /home/yourname/mytmp' | cat >> $HOME/.rpmmacros

That's it; from now on you should be able to build binary RPMs from source by using the command:

rpmbuild --rebuild mypackage.src.rpm

Here mypackage.src.rpm is a source RPM. If a source is provided as a tarball, e.g., mypackage.tar.gz, use

rpmbuild -ta mypackage.tar.gz

Your RPMs will be placed in /home/yourname/rpmtop/RPMS/i386/ directory of the example above. In case your architecture is different from i386, please take care of creating a corresponding subdirectory in rpmtop/RPMS (e.g., rpmtop/RPMS/alpha).

2  Read PDF
xpdf --- which should already be on your machine

acroread --- the official pdf viewer from Adobe.

 You can find RPM's from http://rpmfind.net/linux/rpm2html/search.php?query=acroread.

 To get acroread from Adobe and then set it up:
  -> Go to http://www.adobe.com/products/acrobat/readstep2.html 
      and choose "LINUX" as the platform under Step ->  
      This will let you download 
      a 6MB "tarball" with a name like linux-ar-40->tar.gz.  
  -> Open the archive:
           zcat linux-ar-40->tar.gz | tar xfv -
  -> In some subdirectory like ILINXR.install you'll find an INSTALL script.
      Run it as root:
          ./INSTALL
  -> By default, acroread will be installed as
          /usr/local/Acrobat4/bin/acroread
     You will probably want to put a link in /usr/local/bin:
        ln -s /usr/local/Acrobat4/bin/acroread /usr/local/bin/acroread

	1.14 Tell to which RPM this file belongs to

	rpm -qf <file>

	1.15
id=__hostname_lnx__
2. Host name, hostname

	2.1  How to change the hostname of a Linux system
Normally we will set the hostname of a system during the installation process. Many peoples don’t care about this, and don’t change the hostname even if for example this was set to something really stupid by the datacenter that installed the system (most likely they will set this to “debian” on any debian installation, etc). For me, it is important to see on each one of the ssh screens I will have open at any time a different hostname that is relevant and will give me quickly the information on what system I am logged in.

		2.1.1  Change the hostname on a running system
On any Linux system you can change its hostname with the command ‘hostname‘ (surprised?)… Here are some quick usages of the command line hostname:
hostname
without any parameter it will output the current hostname of the system.
hostname --fqd
it will output the fully qualified domain name (or FQDN) of the system.
hostname NEW_NAME
will set the hostname of the system to NEW_NAME. This is active right away and will remain like that until the system will be rebooted (because at system boot it will set this from some particular file configurations – see bellow how to set this permanently). You will most probably need to exit the current shell in order to see the change in your shell prompt.


		2.1.2 Permanent hostname change on Debian based systems
Debian based systems use the file /etc/hostname to read the hostname of the system at boot time and set it up using the init script /etc/init.d/hostname.sh
/etc/hostname
server
So on a Debian based system we can edit the file /etc/hostname and change the name of the system and then run:
/etc/init.d/hostname.sh start
to make the change active. The hostname saved in this file (/etc/hostname) will be preserved on system reboot (and will be set using the same script we used hostname.sh).

		2.1.3 Permanent hostname change on RedHat based systems
RedHat based system use the file /etc/sysconfig/network to read the saved hostname at system boot. This is set using the init script /etc/rc.d/rc.sysinit
/etc/sysconfig/network
NETWORKING=yes
HOSTNAME="plain.domainname.com"
GATEWAY="192.168.0.1"
GATEWAYDEV="eth0"
FORWARD_IPV4="yes"
So in order to preserve your change on system reboot edit this file and enter the appropriate name using the HOSTNAME variable.

		2.1.4 Use sysctl to change the hostname
Why would someone need a different method of doing the same thing as above? No idea, but here is anyway: use sysctl to change the variable kernel.hostname:
Use:
sysctl kernel.hostname
to read the current hostname, and
sysctl kernel.hostname=NEW_HOSTNAME
to change it.

		2.1.5

	2.2

3. Determins operating system information.

	3.1 kernel version, Linux version:
	$ unname -a
Linux yizaq-lnx 2.6.9-42.7.ELsmp #1 SMP Tue Sep 5 18:29:39 EDT 2006 i686 i686 i386 GNU/Linux

	3.2 gcc version
	$ gcc --ver

	3.3 Red hat version
[yizaq@yizaq-lnx:Sun Nov 11:~]$ cat /etc/redhat-release 
Red Hat Enterprise Linux AS release 4 (Nahant Update 4)

	3.4 How to get system information ?
What is the command to get details system information 
about CPU, Memory, Hard Disk etc ? 

try: 
dmesg 
the above will work on ANY UNIX, linux, *bsd :-) 

cat /cpuinfo 
cat / proc/meminfo 
lspci and lspci -vv 
top 


	3.5
4. DNS on Linux 

   4.1 Introduction

Domain Name System (DNS) converts the name of a Web site (www.linuxhomenetworking.com) to an IP address (65.115.71.34). This step is important, because the IP address of a Web site's server, not the Web site's name, is used in routing traffic over the Internet. This chapter will explain how to configure your own DNS server to help guide Web surfers to your site.
Introduction to DNS

Before you dig too deep in DNS, you need to understand a few foundation concepts on which the rest of the chapter will be built.

		4.1.1 DNS Domains

Everyone in the world has a first name and a last, or family, name. The same thing is true in the DNS world: A family of Web sites can be loosely described a domain. For example, the domain linuxhomenetworking.com has a number of children, such as www.linuxhomenetworking.com and mail.linuxhomenetworking.com for the Web and mail servers, respectively.

		4.1.2 BIND

BIND is an acronym for the Berkeley Internet Name Domain project, which is a group that maintains the DNS-related software suite that runs under Linux. The most well known program in BIND is named, the daemon that responds to DNS queries from remote machines.

		4.1.3 DNS Clients

A DNS client doesn't store DNS information; it must always refer to a DNS server to get it. The only DNS configuration file for a DNS client is the /etc/resolv.conf file, which defines the IP address of the DNS server it should use. You shouldn't need to configure any other files. You'll become well acquainted with the /etc/resolv.conf file soon.

		4.1.4 Authoritative DNS Servers

Authoritative servers provide the definitive information for your DNS domain, such as the names of servers and Web sites in it. They are the last word in information related to your domain.

		4.1.5 How DNS Servers Find Out Your Site Information

There are 13 root authoritative DNS servers (super duper authorities) that all DNS servers query first. These root servers know all the authoritative DNS servers for all the main domains - .com, .net, and the rest. This layer of servers keep track of all the DNS servers that Web site systems administrators have assigned for their sub domains.

For example, when you register your domain my-site.com, you are actually inserting a record on the .com DNS servers that point to the authoritative DNS servers you assigned for your domain. (More on how to register your site later.).

		4.1.6 When To Use A DNS Caching Name Server

Most servers don¿t ask authoritative servers for DNS directly, they usually ask a caching DNS server to do it on their behalf. These servers, through a process called recursion, sequentially query the authoritative servers at the root, main domain and sub domain levels to get eventually get the specific information requested. The most frequently requested information is then stored (or cached) to reduce the lookup overhead of subsequent queries.

If you want to advertise your Web site www.my-site.com to the rest of the world, then a regular DNS server is what you require. Setting up a caching DNS server is fairly straightforward and works whether or not your ISP provides you with a static or dynamic Internet IP address.

After you set up your caching DNS server, you must configure each of your home network PCs to use it as their DNS server. If your home PCs get their IP addresses using DHCP, then you have to configure your DHCP server to make it aware of the IP address of your new DNS server, so that the DHCP server can advertise the DNS server to its PC clients. Off-the-shelf router/firewall appliances used in most home networks usually can act as both the caching DNS and DHCP server, rendering a separate DNS server is unnecessary.

You can find the configuration steps for a Linux DHCP server in Chapter 8, "Configuring the DHCP Server".

		4.1.7 When To Use A Static DNS Server

If your ISP provides you with a fixed or static IP address, and you want to host your own Web site, then a regular authoritative DNS server would be the way to go. A caching DNS name server is used as a reference only, regular name servers are used as the authoritative source of information for your Web site's domain.

Note: Regular name servers are also caching name servers by default.

		4.1.8 When To Use A Dynamic DNS Server

If your ISP provides your router/firewall with its Internet IP address using DHCP then you must consider dynamic DNS covered in Chapter 19, "Dynamic DNS". For now, I'm assuming that you are using static Internet IP addresses.

		4.1.9 How To Get Your Own Domain

Whether or not you use static or dynamic DNS, you need to register a domain.

Dynamic DNS providers frequently offer you a subdomain of their own site, such as my-site.dnsprovider.com, in which you register your domain on their site.

If you choose to create your very own domain, such as my-site.com, you have to register with a company specializing in static DNS registration and then point your registration record to the intended authoritative DNS for your domain. Popular domain registrars include VeriSign, Register Free, and Yahoo.

If you want to use a dynamic DNS provider for your own domain, then you have to point your registration record to the DNS servers of your dynamic DNS provider. (More details on domain registration are coming later in the chapter.).

		4.1.10 Basic DNS Testing of DNS Resolution

As you know, DNS resolution maps a fully qualified domain name (FQDN), such as www.linuxhomenetworking.com, to an IP address. This is also known as a forward lookup. The reverse is also true: By performing a reverse lookup, DNS can determining the fully qualified domain name associated with an IP address.

Many different Web sites can map to a single IP address, but the reverse isn't true; an IP address can map to only one FQDN. This means that forward and reverse entries frequently don't match. The reverse DNS entries are usually the responsibility of the ISP hosting your site, so it is quite common for the reverse lookup to resolve to the ISP's domain. This isn't an important factor for most small sites, but some e-commerce applications require matching entries to operate correctly. You may have to ask your ISP to make a custom DNS change to correct this.

There are a number of commands you can use do these lookups. Linux uses the host command, for example, but Windows uses nslookup.

			4.1.10.1 The Host Command

The host command accepts arguments that are either the fully qualified domain name or the IP address of the server when providing results. To perform a forward lookup, use the syntax:

[root@bigboy tmp]# host www.linuxhomenetworking.com
www.linuxhomenetworking.com has address 65.115.71.34
[root@bigboy tmp]#

To perform a reverse lookup

[root@bigboy tmp]# host 65.115.71.34
-> 34.71.115.65.in-addr.arpa domain name pointer 65-115-71-34.myisp.net.
[root@bigboy tmp]#

As you can see, the forward and reverse entries don't match. The reverse entry matches the entry of the ISP.

			4.1.10.2 The nslookup Command

				4.1.10.2.1 wiki
nslookup is a network administration command-line tool available for many computer operating systems for querying the Domain Name System (DNS) to obtain domain name or IP address mapping or for any other specific DNS record.

					4.1.10.2.1.1 Background

The name nslookup means name server lookup. Nslookup uses the operating system's local Domain Name System resolver library to perform its queries. Thus, it is configured automatically by the contents of the operating system file resolv.conf.[1]

The nslookup utility is generally considered obsolete from historical and technical point of view.[2] Modern alternatives to nslookup are the host and dig programs.

					4.1.10.2.1.2 Usage
 
nslookup operates in interactive or non-interactive mode. When used interactively, when the program is invoked without arguments (another option exists), the user issues parameter configurations or requests when presented the nslookup prompt ('>') in line by line fashion. In non-interactive mode parameters and the query are specified as command line arguments in the invocation of the program.
        
The general command syntax is:
        
 nslookup [-option] [name | -] [server]
        
[edit] Example
        
The following example queries the Domain Name System for the IP address of the domain name example.com by issue the command nslookup example.com to the command line interpreter (shell):
        
$ nslookup example.com
Server:         192.168.0.254
Address:        192.168.0.254#53
        
Non-authoritative answer:
Name:   example.com
Address: 192.0.32.10
        
It first lists the DNS server it is using and then the address
The next paragraph contains the answer

This is an example of interactive, prompted use of the nslookup application. The user executes the program without any arguments and issues parameters and queries at the program prompt ('>'):
 
 $ nslookup
 Default Server:  dsldevice.lan
 Address:  192.168.0.254
 
 > server 8.8.8.8
 Default Server:  google-public-dns-a.google.com
 Address:  8.8.8.8
 
 > set type=mx
 > wikipedia.org
 Server:  google-public-dns-a.google.com
 Address:  8.8.8.8
 
 Non-authoritative answer:
 wikipedia.org   MX preference = 10, mail exchanger = mchenry.wikimedia.org
 wikipedia.org   MX preference = 50, mail exchanger = lists.wikimedia.org
 > exit
 
In this example, the program first displays its default name server configuration, after which the user changes the name server to be used to the host at IP address 8.8.8.8. The type of query to be performed is specified by setting the type of record to be fetched (mx record). Finally, the user simply issues the domain name at the prompt to receive the query results. The command exit ends the interactive session and terminate the program.
				4.1.10.2.2 Some examples
 
The nslookup command provides the same results on Windows PCs. To perform forward lookup, use.
 
C:\> nslookup www.linuxhomenetworking.com
Server:  192-168-1-200.my-site.com
Address:  192.168.1.200
 
Non-authoritative answer:
Name:    www.linuxhomenetworking.com
Address:  65.115.71.34
 
C:\>
 
To perform a reverse lookup
 
C:\> nslookup 65.115.71.34
Server:  192-168-1-200.my-site.com
Address:  192.168.1.200

Name:    65-115-71-34.my-isp.com
Address:  65.115.71.34

C:\>

				4.1.10.2.3 dig

					4.1.10.2.3.1 wiki
Domain Information Groper (dig) is a network administration command-line tool for querying Domain Name System (DNS) name servers for any desired DNS records.

Dig is useful for network troubleshooting and for educational purposes. Dig can operate in interactive command line mode or in batch mode by reading requests from an operating system file. When a specific name server is not specified in the command invocation, it will use the operating systems default resolver, usually configured via the resolv.conf file. Without any arguments it queries the DNS root zone.

Dig supports Internationalized Domain Name (IDN) queries.

Dig is part of the BIND domain name server software suite. Dig replaces older tools such as nslookup and the host program.
Contents

Example usage

In this example, dig is used to query for any type of record information in the domain example.com:

    * Command line:

dig any example.com

    * Output:

; <<>> DiG 9.6.1 <<>> any example.com
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 4016
;; flags: qr rd ra; QUERY: 1, ANSWER: 4, AUTHORITY: 0, ADDITIONAL: 0

;; QUESTION SECTION:
;example.com.                   IN      ANY

;; ANSWER SECTION:
example.com.            172719  IN      NS      a.iana-servers.net.
example.com.            172719  IN      NS      b.iana-servers.net.
example.com.            172719  IN      A       208.77.188.166
example.com.            172719  IN      SOA     dns1.icann.org. hostmaster.icann.org. 2007051703 7200 3600 1209600 86400

;; Query time: 1 msec
;; SERVER: ::1#53(::1)
;; WHEN: Wed Aug 12 11:40:43 2009
;; MSG SIZE  rcvd: 154

Queries may be directed to designated DNS servers for specific records; in this example, MX records:

    * Command:

dig MX wikimedia.org @ns0.wikimedia.org

    * Output:

; <<>> DiG 9.6.1 <<>> MX wikimedia.org @ns0.wikimedia.org
;; global options: +cmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 61144
;; flags: qr aa rd; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 2
;; WARNING: recursion requested but not available

;; QUESTION SECTION:
;wikimedia.org.                 IN      MX

;; ANSWER SECTION:
wikimedia.org.          3600    IN      MX      10 mchenry.wikimedia.org.
wikimedia.org.          3600    IN      MX      50 lists.wikimedia.org.

;; ADDITIONAL SECTION:
mchenry.wikimedia.org.  3600    IN      A       208.80.152.186
lists.wikimedia.org.    3600    IN      A       91.198.174.5

;; Query time: 73 msec
;; SERVER: 208.80.152.130#53(208.80.152.130)
;; WHEN: Wed Aug 12 11:51:03 2009
;; MSG SIZE  rcvd: 109


					4.1.10.2.3.2 Examples

- SRV record for LDAP on AD DC
dig  _ldap._tcp.canada.north.amer.acs.com SRV

; <<>> DiG 9.2.4 <<>> _ldap._tcp.canada.north.amer.acs.com SRV
;; global options:  printcmd
;; Got answer:
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 23133
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; QUESTION SECTION:
;_ldap._tcp.canada.north.amer.acs.com. IN SRV

;; ANSWER SECTION:
_ldap._tcp.canada.north.amer.acs.com. 600 IN SRV 0 100 389 a3-adf1-b-2.canada.north.amer.acs.com.

;; ADDITIONAL SECTION:
a3-adf1-b-2.canada.north.amer.acs.com. 1200 IN A 10.56.60.46

;; Query time: 1 msec
;; SERVER: 10.56.60.150#53(10.56.60.150)
;; WHEN: Thu Oct 21 20:10:46 2010
;; MSG SIZE  rcvd: 127


					4.1.10.2.3.3

				4.1.10.2.4 nslookup man

nslookup

Query Internet name servers

Syntax:
       nslookup

       nslookup host-to-find 

       nslookup server 

interactive mode:

       nslookup -server 

       nslookup [-options] [host-to-find ]

Options:

   host [server ]
         Look up information for host using the current default server or using server,
         if specified. If host is an Internet address and the query type is A or PTR ,
         the name of the host is returned. If host is a name and does not have a trailing
         period, the default domain name is appended to the name. (This behavior depends
         on the state of the set options domain , srchlist , defname , and search.

         To look up a host not in the current domain, append a period to the name.

   server domain
   lserver domain
         Change the default server to domain ; lserver uses the initial server to look up
         information about domain while server uses the current default server.
         If an authoritative answer can't be found, the names of servers that might have 
         the answer are returned. 
   root
         Change the default server to the server for the root of the domain name space.
         Currently, the host ns.internic.net is used. (This command is a synonym for
          `lserver ns.internic.net' The name of the root server can be changed with
         the `set root ' command. 

   finger [name ] [> filename ] 
   finger [name ] [>> filename ]
          Connects with the finger server on the current host. The current host is
          defined when a previous lookup for a host was successful and returned address
          information (see the `set querytype=A ' command). The name is optional. > and
          >> can be used to redirect output in the usual manner.

   ls [option ] domain [> filename ]
   ls [option ] domain [>> filename ]
          List the information available for domain , optionally creating or appending
          to filename The default output contains host names and their Internet addresses.
          Option can be one of the following:

             -t querytype  list all records of the specified type (see querytype below). 
             -a            list aliases of hosts in the domain; synonym for `-t CNAME ' 
             -d            list all records for the domain; synonym for `-t ANY' 
             -h            list CPU and operating system information for the domain; synonym for `-t HINFO' 
             -s            list well-known services of hosts in the domain; synonym for `-t WKS'

          When output is directed to a file, hash marks are printed for every 50 records
          received from the server. 

   view filename
          Sort and lists the output of previous ls command(s) with more(1). 

   set keyword [= value ]
          This command is used to change state information that affects the lookups.
          run man nslookup for a full list of valid keywords.

   set all   Print the current value of the frequently-used options
             to set Information about the current default server and host is also printed.

   help
   ?      Print a brief summary of commands. 

   exit   Exit the program.
Nslookup has two modes: interactive and non-interactive.

Interactive mode allows the user to query name servers for information about various hosts and domains or to print a list of hosts in a domain.

Non-interactive mode is used to print just the name and requested information for a host or domain.

The options listed under the `set ' command can be specified in the .nslookuprc file in the user's home directory (listed one per line). Options can also be specified on the command line if they precede the arguments and are prefixed with a hyphen.

"He who listens to truth is not less than he who utters truth" - Kahlil Gibran

Related:

named(8), resolver(3), resolver(5)
Equivalent Windows command: NSLOOKUP - Lookup IP addresses on a NameServer.

				4.1.10.2.5
		4.1.11 	Downloading and Installing the BIND Packages

Most RedHat and Fedora Linux software products are available in a package format. When searching for the file, remember that the BIND package's filename usually starts with the word ¿bind¿ followed by a version number, as in bind-9.2.2.P3-9.i386.rpm. (For more details on downloading RPMs, see Chapter 6, "Installing Linux Software").

Note: Unless otherwise stated, the sample configurations covered in this chapter will be for Redhat / Fedora distributions. If you use Debian / Ubuntu, don¿t worry, there will be annotations to make you aware of the differences.

		4.1.12 	How To Get BIND Started

Setting up your DNS server is easy to do, but the procedure differs between Linux distributions.

			4.1.12.1 	Redhat / Fedora

You can use the chkconfig command to get BIND configured to start at boot

[root@bigboy tmp]# chkconfig named on

To start, stop, and restart BIND after booting, use:

[root@bigboy tmp]# /etc/init.d/named start
[root@bigboy tmp]# /etc/init.d/named stop
[root@bigboy tmp]# /etc/init.d/named restart

Remember to restart the BIND process every time you make a change to the configuration file for the changes to take effect on the running process.

			4.1.12.2 	Debian / Ubuntu

You can use the sysv-rc-conf command to get BIND configured to start at boot

[root@bigboy tmp]# sysv-rc-conf bind on

To start, stop, and restart BIND after booting, use:

[root@bigboy tmp]# /etc/init.d/bind start
[root@bigboy tmp]# /etc/init.d/bind stop
[root@bigboy tmp]# /etc/init.d/bind restart

Even though the startup script and installation package name refers to bind, the name of the daemon that runs is named just like it is with Redhat / Fedora. Also remember to restart the BIND process every time you make a change to the configuration file for the changes to take effect on the running process.

	4.1.13 The /etc/resolv.conf File

DNS clients (servers not running BIND) use the /etc/resolv.conf file to determine both the location of their DNS server and the domains to which they belong. The file generally has two columns; the first contains a keyword, and the second contains the desired values separated by commas. See Table for a list of keywords.
Table Keywords In /etc/resolv.conf
	|Keyword 	|Value
_____________________________________________________________________________
	|Nameserver 	|IP address of your DNS nameserver. There should be only one entry per "nameserver" keyword. If there is more than one nameserver, you¿ll need to have multiple "nameserver" lines.
_____________________________________________________________________________
	|Domain 	|The local domain name to be used by default. If the server is bigboy.my-web-site.org, then the entry would just be my-web-site.org
_____________________________________________________________________________
	|Search 	|If you refer to another server just by its name without the domain added on, DNS on your client will append the server name to each domain in this list and do an DNS lookup on each to get the remote servers¿ IP address. This is a handy time saving feature to have so that you can refer to servers in the same domain by only their servername without having to specify the domain. The domains in this list must separated by spaces.
_____________________________________________________________________________

Take a look at a sample configuration in which the client server's main domain is my-site.com, but it also is a member of domains my-site.net and my-site.org, which should be searched for shorthand references to other servers. Two name servers, 192.168.1.100 and 192.168.1.102, provide DNS name resolution:

search my-site.com my-site.net my-site.org
nameserver 192.168.1.100
nameserver 192.168.1.102

The first domain listed after the search directive must be the home domain of your network, in this case my-site.com. Placing a domain and search entry in the /etc/resolv.conf is redundant, therefore.



	4.2 Important File Locations

The locations of the BIND configuration files vary by Linux distribution, as you will soon see.
RedHat / Fedora

RedHat / Fedora BIND normally runs as the named process owned by the unprivileged named user.

Sometimes BIND is also installed using Linux's chroot feature to not only run named as user named, but also to limit the files named can see. When installed, named is fooled into thinking that the directory /var/named/chroot is actually the root or / directory. Therefore, named files normally found in the /etc directory are found in /var/named/chroot/etc directory instead, and those you'd expect to find in /var/named are actually located in /var/named/chroot/var/named.

The advantage of the chroot feature is that if a hacker enters your system via a BIND exploit, the hacker's access to the rest of your system is isolated to the files under the chroot directory and nothing else. This type of security is also known as a chroot jail.

You can determine whether you have the chroot add-on RPM by using this command, which returns the name of the RPM.

[root@bigboy tmp]# rpm -q bind-chroot
bind-chroot-9.2.3-13
[root@bigboy tmp]#

There can be confusion with the locations: Regular BIND installs its files in the normal locations, and the chroot BIND add-on RPM installs its own versions in their chroot locations. Unfortunately, the chroot versions of some of the files are empty. Before starting Fedora BIND, copy the configuration files to their chroot locations:

[root@bigboy tmp]# cp -f /etc/named.conf /var/named/chroot/etc/
[root@bigboy tmp]# cp -f /etc/rndc.* /var/named/chroot/etc/

Before you go to the next step of configuring a regular name server, it is important to understand exactly where the files are located. Table 18.2 provides a map.
Table 18.2 Differences In Fedora And Redhat DNS File Locations
File 	Purpose 	BIND chroot Location 	Regular BIND Location
named.conf 	Tells the names of the zone files to be used for each of your website domains. 	/var/named/chroot/etc 	/etc
rndc.key

rndc.conf
	Files used in named authentication 	/var/named/chroot/etc 	/etc
zone files 	Links all the IP addresses in your domain to their corresponding server 	/var/named/chroot/var/named 	/var/named

Note: Fedora Core installs BIND chroot by default. RedHat 9 and earlier don't.
Debian / Ubuntu

With Debian / Ubuntu, all the configuration files, the primary named.conf file and all the DNS zone files reside in the /etc/bind directory.

Unlike in Redhat / Fedora, references to other files within these configuration files should include the full path. The named daemon won't automatically assume they are located in the /etc/bind directory.

	4.3 Configuring Your Nameserver

For the purposes of this tutorial, assume your ISP assigned you the subnet 97.158.253.24 with a subnet mask of 255.255.255.248 (/29).
Configuring resolv.conf

You'll have to make your DNS server refer to itself for all DNS queries by configuring the /etc/resolv.conf file to reference localhost only.

nameserver 127.0.0.1

Creating a named.conf Base Configuration

The /etc/named.conf file contains the main DNS configuration and tells BIND where to find the configuration, or zone files for each domain you own. This file usually has two zone areas:

    * Forward zone file definitions list files to map domains to IP addresses.
    * Reverse zone file definitions list files to map IP addresses to domains. 

Some versions of BIND will come with a /etc/amed.conf file configured to work as a caching nameserver which can be converted to an authoritative nameserver by adding the correct references to your zone files. Please proceed to the next section if this is the case with your version of BIND.

In other cases the named.conf configuration file may be hard to find. Some versions of Linux install BIND as a default caching nameserver using a file names /etc/named.caching-nameserver.conf for its configuration. In such cases BIND becomes an authoritative nameserver when a correctly configured /etc/named.conf file is created.

Fortunately BIND comes with samples of all the primary files you need. Table 18.3 explains their names and purpose in more detail.
Table 18.3 The Primary BIND Configuration Files
File 	Description
/etc/named.conf 	The main configuration file that lists the location of all your domain's zone files
/etc/named.root.hints 	A file describing the location of the named.root file
/var/named/named.root 	A list of the 13 root authoritative DNS servers.

The creation of these files in the correct locations and their subsequent configuration isn't hard. Here's what you need to do:

a. The first task is to locate these files and place them in the correct locations. In the examples that follow we assume a chroot version of BIND and add the appropriate symbolic links to the /etc directory.

[root@bigboy tmp]# locate named.conf
/etc/dbus-1/system.d/named.conf
/usr/share/doc/bind-9.3.3/sample/etc/named.conf
[root@bigboy tmp]# cp /usr/share/doc/bind-9.3.3/sample/etc/named.conf \
    /var/named/chroot/etc 
[root@bigboy tmp]# ln -s /var/named/chroot/etc/named.conf /etc/named.conf
[root@bigboy tmp]# locate named.root.hints
/usr/share/doc/bind-9.3.3/sample/etc/named.root.hints
[root@bigboy tmp]# cp /usr/share/doc/bind-9.3.3/sample/etc/named.root.hints \
    /var/named/chroot/etc 
[root@bigboy tmp]# ln -s /var/named/chroot/etc/named.root.hints /etc/named.root.hints
[root@bigboy tmp]# locate named.root
/usr/share/doc/bind-9.3.3/sample/etc/named.root.hints
/usr/share/doc/bind-9.3.3/sample/var/named/named.root
[root@bigboy tmp]# 
[root@bigboy tmp]# cp /usr/share/doc/bind-9.3.3/sample/var/named/named.root \
    /var/named/chroot/var/named/named.root
[root@bigboy tmp]# 

b. Though it is beyond the scope of this book, it is possible to create a master and redundant slave authoritative name servers for your domains. You can configure your master to use a secret identification key when updating its slaves to help ensure the security of the updates. The next step requires you to generate a key for your named.conf file, and this can be done using the dns-keygen or dnskeygen commands.

[root@bigboy tmp]# /usr/sbin/dns-keygen
u8yNvOLHovDstA8lFHRvQl0XnjlxL1q1JCP5OaDHw4sgssgzRxKNkB7kKbON
[root@bigboy tmp]#

Edit your /etc/named.conf file and add your secret key to the ddns_key section. Though it is not required, it is a good practice to configure your DNS server's named.conf file to support BIND views. This will be discussed next.
Configuring BIND Views in named.conf

Our sample scenario assumes that DNS queries will be coming from the Internet and that the zone files will return information related to the external 97.158.253.26 address of the Web server. What do the PCs on your home network need to see? They need to see DNS references to the real IP address of the Web server, 192.168.1.100, because NAT won¿t work properly if a PC on your home network attempts to connect to the external 97.158.253.26 NAT IP address of your Web server. Don¿t worry. BIND figures this out using its views feature which allows you to use predefined zone files for queries from certain subnets. This means it¿s possible to use one set of zone files for queries from the Internet and another set for queries from your home network. Here¿s a summary of how it¿s done:

a. If your DNS server is also acting as a caching DNS server, then you'll also need a view for localhost to use. A view called localhost_resolver is a predefined view in Fedora's sample named.conf file and will be explained later.

b. Place your zone statements in the /etc/named.conf file in one of two other view sections. The first section is called internal and lists the zone files to be used by your internal network. The second view called external lists the zone files to be used for Internet users.

For example; you could have a reference to a zone file called my-site.zone for lookups related to the 97.158.253.X network which Internet users would see. This /etc/named.conf entry would be inserted in the external section. You could also have a file called my-site-home.zone for lookups by home users on the 192.168.1.0 network. This entry would be inserted in the internal section. Creating the my-site-home.zone file is fairly easy: Copy it from the my-site.zone file and replace all references to 97.158.253.X with references to 192.168.1.X.

c. You must also tell the DNS server which addresses you feel are internal and external. To do this, you must first define the internal and external networks with access control lists (ACLs) and then refer to these lists within their respective view section with the match-clients statement. Some built-in ACLs can save you time:

    * localhost: Refers to the DNS server itself
    * localnets: Refers to all the networks to which the DNS server is directly connected
    * any: which is self explanatory. 

Let's examine BIND views more carefully using a number of sample configuration snippets from the /etc/named.conf file I use for my home network. All the statements below were inserted after the options and controls sections in the file. I have selected generic names internal, for views given to trusted hosts (home, non-internet or corporate users), and external for the views given to Internet clients, but they can be named whatever you wish.

First let's talk about how we should refer to the zone files in each view.
Zone File References in named.conf

In this section we'll set up the forward zone reference for the my-web-site.org domain by placing entries for it in the named.conf file.

In our example the zone file is named my-site.zone, and, although not explicitly stated, the file my-site.zone should be located in the default directory of /var/named/chroot/var/named in a chroot configuration or in /var/named in a regular one. With Debian / Ubuntu, references to the full file path will have to be used. Use the code:

zone ¿my-web-site.org¿ {

   type master;
   notify no;
   allow-query { any; };
   file ¿my-site.zone¿;

};

In addition, you can insert additional entries in the named.conf file to reference other Web domains you host. Here is an example for another-site.com using a zone file named another-site.zone.

zone ¿another-site.com¿ {

   type master;
   notify no;
   allow-query { any; };
   file ¿another-site.zone¿;

};

Note: The allow-query directive defines the networks that are allowed to query your DNS server for information on any zone. For example, to limit queries to only your 192.168.1.0 network, you could modify the directive to:

allow-query { 192.168.1.0/24; };

Next, you have to format entries to handle the reverse lookups for your IP addresses. In most cases, your ISP handles the reverse zone entries for your public IP addresses, but you will have to create reverse zone entries for your SOHO/home environment using the 192.168.1.0/24 address space. This isn¿t important for the Windows clients on your network, but some Linux applications require valid forward and reverse entries to operate correctly.

The forward domain lookup process for mysite.com scans the FQDN from right to left to get to get increasingly more specific information about the authoritative servers to use. Reverse lookups operate similarly by scanning an IP address from left to right to get increasingly specific information about an address.

The similarity in both methods is that increasingly specific information is sought, but the noticeable difference is that for forward lookups the scan is from right to left, and for reverse lookups the scan is from left to right. This difference can be seen in the formatting of the zone statement for a reverse zone in /etc/named.conf file where the main in-addr.arpa domain, to which all IP addresses belong, is followed by the first 3 octets of the IP address in reverse order. This order is important to remember or else the configuration will fail. This reverse zone definition for named.conf uses a reverse zone file named 192-168-1.zone for the 192.168.1.0/24 network.

zone ¿1.168.192.in-addr.arpa¿ {
   type master;
   notify no;
   file ¿192-168-1.zone¿;
};

Your patience will soon be rewarded. It's time to talk about the views! Let's go!
The Caching Nameserver localhost_resolver View

The localhost_resolver view is used for your caching DNS server configuration and should look like this:

view "localhost_resolver"
{
/* This view sets up named to be a localhost resolver 
 * ( caching only nameserver ). If all you want is a 
 * caching-only nameserver, then you need only define this view:
 */
        match-clients           { localhost; };
        match-destinations      { localhost; };
        recursion yes;
 
        /* these are zones that contain definitions for all the localhost
         * names and addresses, as recommended in RFC1912 - these names should
         * ONLY be served to localhost clients:
         */
        include "/etc/named.rfc1912.zones";

        /*
         *  Include zonefiles for internal zones
         */
        include "/var/named/zones/internal/internal_zones.conf";
};

There are some quick facts you should be aware of with your caching name server configuration:

a. Make sure this section only refers to including the /etc/named.rfc1912.zones file. Other include references to files such as named.root.hints could cause errors when starting the named daemon.

b. If you want your server to be only a caching DNS server, then delete all other views in named.conf and restart the named daemon.

[root@bigboy tmp]# /etc/init.d/named restart

c. Make all the other machines on your network point to the caching DNS server as their primary DNS server.

d. Remember that all DNS queries done on your DNS server appear to come from localhost. If your server is also an authoritative server for your domain, you will have to include a reference to your domain's zone files in this section for the server's own DNS lookups to work. If not, queries from clients defined by the internal and external ACLs will work correctly, but queries for the domain from the server itself will fail. In this example we have included a reference to the internal_zones.conf zone file which we'll visit again soon. This line can be deleted if your server isn't an authoritative server for your domain.

Note: If you have a localhost only view like this, make sure you don't reference localhost in any of your other views as one view will take precedence over the other for queries from your server. This could lead to unpredictable results.
The Internal View

In this example I included an ACL for network 192.168.17.0 /24 called safe-subnet to help clarify the use of ACLs in more complex environments. Once the ACL was defined, I then inserted a reference to the safe-subnet in the match-clients statement in the internal view. Therefore the local network (192.168.1.0 /24), the other trusted network (192.168.17.0), and localhost get DNS data from the zone files in the internal view.

// ACL statement

acl ¿safe-subnet¿ { 192.168.17.0/24; };

view ¿internal¿ { // What the home network will see
 
   match-clients      { localnets; localhost; safe-subnet; };
   match-destinations { localnets; localhost; safe-subnet; };

   recursion yes;
 
   // all views must contain the root hints zone:
   include "/etc/named.root.hints";

   // These are your "authoritative" internal zones, and would probably
   // also be included in the "localhost_resolver" view above :

   /*
   *  Include zonefiles for internal zones
   */
   include "/var/named/zones/internal/internal_zones.conf";

};


The question you may have on your mind is, "Where are the zone file definitions?". Don't worry, there is an include statement that refers to a file named internal_zones.conf that contains them all as we see here:

// File internal_zones.conf

zone "1.168.192.in-addr.arpa" IN {
   type master;
   file "/var/named/zones/internal/192.168.1.zone";
   allow-update { none; };
};

zone "my-web-site.org" IN {
   type master;
   file "/var/named/zones/internal/my-web-site.org.zone";
   allow-update { none; };
};

I'll discuss how to handle queries from clients outside your trusted networks in the next section where an external view can be used.
The External View

You can also setup an external view that will be used for DNS queries from clients outside your network, such as the Internet. In this case external queries get results from zone files in the /var/named/zones/external directory.

view ¿external¿ { // What the Internet will see
 
   /* This view will contain zones you want to serve only to "external"
    * clients that have addresses that are not on your directly attached 
    * LAN interface subnets:
    */

   match-clients      { any; };
   match-destinations { any; };
 
   recursion no;
   // you'd probably want to deny recursion to external clients, so you don't
   // end up providing free DNS service to all takers
 
   // all views must contain the root hints zone:
   include "/etc/named.root.hints";

   // These are your "authoritative" external zones, and would probably
   // contain entries for just your web and mail servers:

   zone "253.158.97.in-addr.arpa" IN {
      type master;
      file "/var/named/zones/external/97.158.253.zone";
      allow-update { none; };
   };
  
   zone "my-web-site.org" IN {
      type master;
      file "/var/named/zones/external/my-web-site.org.zone";
      allow-update { none; };
   };
};

Notice that the reverse zone file gives results for public internet addresses, and of course, the forward zone file should only provide responses with Internet accessible addresses.

Note: In the external view, you may be tempted to use an exclamation mark (!) to eliminate networks used in the internal view like this. Be careful, it is best to use "any;" for your external view as the exclamation mark (!) is not honored with some versions of BIND in views named "external".

; !!! CAUTION !!!

match-clients      { !localnets; !localhost; !safe-subnet; };
match-destinations { !localnets; !localhost; !safe-subnet; };


The views listed here are purely to illustrate their use. The sample home network we have been using doesn¿t need to have the ACL statement at all as the built in ACLs localnets and localhost are sufficient. The sample network won¿t need the safe-subnet section in the match-clients line either as there is only one subnet in the configuration.

Views are also not just for NAT. If you run an Internet data center, you can set up your DNS server to act as a caching server to servers on all the Internet networks you own and no one else, and then provide authoritative responses to your customers' domains to everyone. Views can be very useful.
Configuring The Zone Files

You need to keep a number of things in mind when configuring DNS zone files:

    * In all zone files, you can place a comment at the end of any line by inserting a semi-colon character then typing in the text of your comment.
    * By default, your zone files are located in the /var/named or /var/named/chroot/var/named or /etc/bind directories depending on your Linux distribution.
    * Each zone file contains a variety of records (SOA, NS, MX, A, and CNAME) that govern different areas of BIND. 

Take a closer look at these entries in the zone file.
Time to Live Value

The very first entry in the zone file is usually the zone's time to live (TTL) value. Caching DNS servers cache the responses to their queries from authoritative DNS servers. The authoritative servers not only provide the DNS answer but also provide the information's time to live, which is the period for which it's valid.

The purpose of a TTL is to reduce the number of DNS queries the authoritative DNS server has to answer. If the TTL is set to three days, then caching servers use the original stored response for three days before making the query again.

$TTL 3D

BIND recognizes several suffixes for time-related values. A D signifies days, a W signifies weeks, and an H signifies hours. In the absence of a suffix, BIND assumes the value is in seconds.
DNS Resource Records

The rest of the records in a zone file are usually BIND resource records. They define the nature of the DNS information in your zone files that's presented to querying DNS clients. They all have the general format:

Name    Class    Type    Data

There are different types of records for mail (MX), forward lookups (A), reverse lookups (PTR), aliases (CNAME) and overall zone definitions, Start of Authority (SOA). The data portion is formatted according to the record type and may consist of several values separated by spaces. Similarly, the name is also subject to interpretation based on this factor.
The SOA Record

The first resource record is the Start of Authority (SOA) record, which contains general administrative and control information about the domain. It has the format:

Name Class Type Name-Server Email-Address Serial-No Refresh Retry Expiry Minimum-TTL

The record can be long, and will sometimes wrap around on your screen. For the sake of formatting, you can insert new line characters between the fields as long as you insert parenthesis at the beginning and end of the insertion to alert BIND that part of the record will straddle multiple lines. You can also add comments to the end of each new line separated by a semicolon when you do this. Here is an example:

@       IN      SOA     ns1.my-site.com. hostmaster.my-site.com. (
                       2004100801      ; serial #
                       4H              ; refresh
                       1H              ; retry
                       1W              ; expiry
                       1D )            ; minimum

Table 18.4 explains what each field in the record means.
Table 18.4 The SOA Record Format
Field 	Description
Name 	The root name of the zone. The ¿@¿ sign is a shorthand reference to the current origin (zone) in the /etc/named.conf file for that particular database file.
Class 	There are a number of different DNS classes. Home/SOHO will be limited to the IN or Internet class used when defining IP address mapping information for BIND. Other classes exist for non Internet protocols and functions but are very rarely used.
Type 	The type of DNS resource record. In the example, this is an SOA resource record. Other types of records exist, which I¿ll cover later.
Name-server 	Fully qualified name of your primary name server. Must be followed by a period.
Email-address 	The e-mail address of the name server administrator. The regular @ in the e-mail address must be replaced with a period instead. The e-mail address must also be followed by a period.
Serial-no 	A serial number for the current configuration. You can use the date format YYYYMMDD with an incremented single digit number tagged to the end. This will allow you to do multiple edits each day with a serial number that both increments and reflects the date on which the change was made.
Refresh 	Tells the slave DNS server how often it should check the master DNS server. Slaves aren¿t usually used in home / SOHO environments.
Retry 	The slave¿s retry interval to connect the master in the event of a connection failure. Slaves aren¿t usually used in home / SOHO environments.
Expiry 	Total amount of time a slave should retry to contact the master before expiring the data it contains. Future references will be directed towards the root servers. Slaves aren¿t usually used in home/SOHO environments.
Minimum-TTL 	There are times when remote clients will make queries for subdomains that don¿t exist. Your DNS server will respond with a no domain or NXDOMAIN response that the remote client caches. This value defines the caching duration your DNS includes in this response.

So in the example, the primary name server is defined as ns1.my-site.com with a contact e-mail address of hostmaster@my-site.com. The serial number is 2004100801 with refresh, retry, expiry, and minimum values of 4 hours, 1 hour, 1 week, and 1 day, respectively.
NS, MX, A And CNAME Records

Like the SOA record, the NS, MX, A, PTR and CNAME records each occupy a single line with a very similar general format. Table 18.5 outlines the way they are laid out.
Table 18.5 NS, MX, A, PTR and CNAME Record Formats
Record Type 	Name Field 	Class Field2 	Type Field 	Data Field
NS 	Usually blank1 	IN 	NS 	IP address or CNAME of the name server
MX 	Domain to be used for mail. Usually the same as the domain of the zone file itself. 	IN 	MX 	Mail server DNS name
A 	Name of a server in the domain 	IN 	A 	IP address of server
CNAME 	Server name alias 	IN 	CNAME 	"A" record name for the server
PTR 	Last octet of server¿s IP address 	IN 	PTR 	Fully qualified server name

       a. If the search key to a DNS resource record is blank it reuses the search key from the previous record which in this case of is the SOA @ sign.
       b. For most home / SOHO scenarios, the Class field will always be IN or Internet. You should also be aware that IN is the default Class, and BIND will assume a record is of this type unless otherwise stated. 

If you don't put a period at the end of a host name in a SOA, NS, A, or CNAME record, BIND will automatically tack on the zone file's domain name to the name of the host. So, BIND assumes an A record with www refers to www.my-site.com. This may be acceptable in most cases, but if you forget to put the period after the domain in the MX record for my-site.com, BIND attaches the my-site.com at the end, and you will find your mail server accepting mail only for the domain my-site.com.mysite.com.
TXT Records

There is also a less frequently used DNS TXT record that can be configured to contain additional generic information. The data section of the record typically has the format "name=value", where "name" is the name to be given to the type of data, and "value" is the value assigned to the name as seen in this example.

my-web-site.org. TXT "v=spf1 -all"

TXT records are increasingly being used to help fight SPAM using the Sender Policy Framework (SPF) method. SPF TXT records are used by systems receiving mail to interrogate the DNS of the domain which appears in the email (the sender) and determine if the originating IP address of the mail (the source) is authorized to send mail for the sender's domain.

Further description of the use of TXT records is beyond the scope of this book, but you should at least be aware that they can be up to 255 characters in length and that this feature is often exploited in distributed denial of service (DDoS) attacks. The section on "Simple DNS Security" explains how to configure your DNS server to not participate in such an event.
Sample Forward Zone File

Now that you know the key elements of a zone file, it's time to examine a working example for the domain my-site.com.

;
; Zone file for my-site.com
;
; The full zone file
;
$TTL 3D
@       IN      SOA     ns1.my-site.com. hostmaster.my-site.com. (
                       200211152       ; serial#
                       3600            ; refresh, seconds
                       3600            ; retry, seconds
                       3600            ; expire, seconds
                       3600 )          ; minimum, seconds

                NS      www             ; Inet Address of nameserver
my-site.com.    MX      10 mail         ; Primary Mail Exchanger
  
localhost       A       127.0.0.1
bigboy          A       97.158.253.26
mail            CNAME   bigboy
ns1             CNAME   bigboy
www             CNAME   bigboy


Notice that in this example:

    * Server ns1.my-site.com is the name server for my-site.com. In corporate environments there may be a separate name server for this purpose. Primary name servers are more commonly called ns1 and secondary name servers ns2. 

    * The minimum TTL value ($TTL) is three days, therefore remote DNS caching servers will store learned DNS information from your zone for three days before flushing it out of their caches. 

    * The MX record for my-site.com points to the server named mail.my-site.com. 

    * ns1 and mail are actually CNAMEs or aliases for the Web server www. So here you have an example of the name server, mail server, and Web server being the same machine. If they were all different machines, then you'd have an A record entry for each. 

www                 A          97.158.253.26
mail                A          97.158.253.134
ns                  A          97.158.253.125

It is a required practice to increment your serial number whenever you edit your zone file. When DNS is setup in a redundant configuration, the slave DNS servers periodically poll the master server for updated zone file information, and use the serial number to determine whether the data on the master has been updated. Failing to increment the serial number, even though the contents of the zone file have been modified, could cause your slaves to have outdated information.
Sample Reverse Zone File

Now you need to make sure that you can do a host query on all your home network's PCs and get their correct IP addresses. This is very important if you are running a mail server on your network, because sendmail typically relays mail only from hosts whose IP addresses resolve correctly in DNS. NFS, which is used in network-based file access, also requires valid reverse lookup capabilities.

This is an example of a zone file for the 192.168.1.x network. All the entries in the first column refer to the last octet of the IP address for the network, so the IP address 192.168.1.100 points to the name bigboy.my-site.com.

Notice how the main difference between forward and reverse zone files is that the reverse zone file only has PTR and NS records. Also the PTR records cannot have CNAME aliases.

;
; Filename: 192-168-1.zone
;
; Zone file for 192.168.1.x
;
$TTL 3D
@       IN        SOA        www.my-site.com.  hostmaster.my-site.com. (
                            200303301          ; serial number
                            8H                 ; refresh, seconds
                            2H                 ; retry, seconds
                            4W                 ; expire, seconds
                            1D )               ; minimum, seconds

                  NS         www                ; Nameserver Address

|100                PTR        bigboy.my-site.com.
|103                PTR        smallfry.my-site.com.
|102                PTR        ochorios.my-site.com.
|105                PTR        reggae.my-site.com.
|
|32                 PTR        dhcp-192-168-1-32.my-site.com.
|33                 PTR        dhcp-192-168-1-33.my-site.com.
|34                 PTR        dhcp-192-168-1-34.my-site.com.
|35                 PTR        dhcp-192-168-1-35.my-site.com.
|36                 PTR        dhcp-192-168-1-36.my-site.com.

I included entries for addresses 192.168.1.32 to 192.168.1.36, which are the addresses the DHCP server issues. SMTP mail relay wouldn't work for PCs that get their IP addresses via DHCP if these lines weren't included.

You may also want to create a reverse zone file for the public NAT IP addresses for your home network. Unfortunately, ISPs won't usually delegate this ability for anyone with less than a Class C block of 256 IP addresses. Most home DSL sites wouldn't qualify.
Loading Your New Configuration Files

Make sure your configuration files are in the correct locations and the serial numbers of the zone files you may have modified have been updated. If all seems correct, restart BIND named daemon for the configuration to become active.

[root@bigboy tmp]# /etc/init.d/named restart

Take a look at the end of your /var/log/messages file to make sure there are no errors.
Make Sure Your /etc/hosts File Is Correctly Updated

Chapter 3, "Linux Networking", explains how to correctly configure your /etc/hosts file. Some programs, such as sendmail, require a correctly configured /etc/hosts file even though DNS is correctly configured.
Configure Your Firewall

The sample network assumes that the BIND name server and Apache Web server software run on the same machine protected by a router/firewall. The actual IP address of the server is 192.168.1.100, which is a private IP address. You'll have to use NAT for Internet users to be able to gain access to the server via the chosen public IP address, namely 97.158.253.26. If your firewall is a Linux box, you may want to consider taking a look at Chapter 14, "Linux Firewalls Using iptables", describes how to do the network address translation and allow DNS traffic through to your name server.
Fix Your Domain Registration

Remember to edit your domain registration for my-site.com, or whatever it is, so that at least one of the name servers is your new name server (97.158.253.26 in this case). Domain registrars, such as VeriSign and RegisterFree, usually provide a Web interface to help you manage your domain.

Once you've logged in with the registrar's username and password, you'll have take two steps:

    1) Create a new name server record entry for the IP address 97.158.253.26 to map to ns.my-site.com or www.my-site.com or whatever your name server is called. (This screen prompts you for both the server's IP address and name.) 

    2) Assign ns.my-site.com to handle your domain. This screen will prompt you for the server name only. 

    Sometimes, the registrar requires at least two registered name servers per domain. If you only have one, then you could either create a second name server record entry with the same IP address, but different name, or you could give your Web server a second IP address using an IP alias, create a second NAT entry on your firewall and then create the second name server record entry with the new IP address, and different name. 

It normally takes about three to four days for your updated DNS information to be propagated to all 13 of the world's root name servers. You'll therefore have to wait about this amount of time before starting to notice people hitting your new Web site.

You can use the chapter's troubleshooting section to test specific DNS servers for the information they have on your site. You'll most likely want to test your new DNS server, which should be up to date, plus a few well known ones, which should have delayed values.

	4.4 Troubleshooting BIND

BIND troubleshooting is usually easy to do. The named daemon updates the /var/log/messages file with detailed status messages that are frequently easy to interpret when you suspect a configuration error. The usual troubleshooting steps for network problems are also applicable. Both methodologies will be covered next.
Configuration Troubleshooting Steps

Always check your /var/logs/messages file and console output file for errors. Here are a couple examples you may come across:

    * The named daemon is started with an unedited version of the sample named.conf file which causes unusual errors on the screen. References to the nonexistent sample zone files create errors. References to both the named.rfc1912.zones and named.root files in the localhost_resolver section cause errors related to duplicate definitions. 

[root@bigboy tmp]# service named restart
Starting named: 
Error in named configuration:
/etc/named.rfc1912.zones:10: zone '.': already exists previous definition: /etc/named.root.hints:12
zone localdomain/IN: loaded serial 42
zone localhost/IN: loaded serial 42
zone 0.0.127.in-addr.arpa/IN: loaded serial 1997022700
zone 0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa/IN: loaded serial 1997022700
zone 255.in-addr.arpa/IN: loaded serial 42
zone 0.in-addr.arpa/IN: loaded serial 42
zone my.internal.zone/IN: loading master file my.internal.zone.db: file not found
internal/my.internal.zone/IN: file not found
zone my.ddns.internal.zone/IN: loading master file slaves/my.ddns.internal.zone.db: file not found
internal/my.ddns.internal.zone/IN: file not found
zone my.external.zone/IN: loading master file my.external.zone.db: file not found
external/my.external.zone/IN: file not found
[FAILED]
[root@bigboy tmp]#


    * The named.conf file refers to an undefined secret key in the ddns_key of named.conf. Use the dns-keygen or dnskeygen commands to create a correct entry. 

Feb 25 20:38:49 bigboy named[4593]: /etc/named.conf:99: configuring key 'ddns_key': bad base64 encoding
Feb 25 20:38:49 bigboy named[4593]: loading configuration: bad base64 encoding

    * The named.root.hints file referred to in named.conf isn't present in the /etc or the chroot /etc directory. 

[root@bigboy tmp]# service named start
Starting named: 
Error in named configuration:
/etc/named.conf:58: open: /etc/named.root.hints: file not found
[FAILED]
[root@bigboy tmp]#


    * The named.root file referred to in the named.root.hints file isn't present. 

Feb 25 21:33:41 bigboy named[5007]: could not configure root hints from 'named.root': file not found
Feb 25 21:33:41 bigboy named[5007]: loading configuration: file not found
Feb 25 21:33:41 bigboy named[5007]: exiting (due to fatal error)

    * You are using a chroot version of BIND with a sample rndc.key file located in the /etc directory instead of the /var/named/chroot/etc/ directory. Copy the file to the correct location and restart named to fix the problem. 

[root@bigboy tmp]# service named restart
Stopping named: rndc: connect failed: connection refused
[  OK  ]
Starting named: [  OK  ]
[root@bigboy tmp]#

    * In your named.conf file you refer to a zone file that doesn't exist. This example includes both errors to the console screen and errors in the /var/log/messages file. 

[root@bigboy tmp]# service named start
Starting named: 
Error in named configuration:
zone localdomain/IN: loaded serial 42
zone localhost/IN: loaded serial 42
zone 0.0.127.in-addr.arpa/IN: loaded serial 1997022700
zone 0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa/IN: loaded serial 1997022700
zone 255.in-addr.arpa/IN: loaded serial 42
zone 0.in-addr.arpa/IN: loaded serial 42
zone 2.168.192.in-addr.arpa/IN: loaded serial 2006052301
zone my-web-site.org/IN: loaded serial 2006052302
zone my-web-site.com/IN: loading master file /var/named/zones/internal/my-web-site.com.zone: file not found
internal/my-web-site.com/IN: file not found
zone 1.168.192.in-addr.arpa/IN: loaded serial 2006052301
zone my-web-site.org/IN: loaded serial 2006052302
[FAILED]
[root@bigboy tmp]#

Feb 26 01:47:10 smallfry named: zone my-web-site.com/IN: loading master file /var/named/zones/internal/my-web-site.com.zone: file not found
Feb 26 01:47:10 smallfry named: internal/my-web-site.com/IN: file not found

    * This is a tricky one that would occur in some early versions of Fedora. BIND would appear to start correctly, but none of the zone files would be loaded. In this scenario could be using a chroot version of BIND with a sample named.conf file located in the /etc directory instead of the /var/named/chroot/etc/ directory. Copy the file to the correct location and restart named to fix the problem. Delete the /etc and create a symbolic link to /var/named/chroot/etc/named.conf from /etc to ensure you always edit the correct file. 

Nov  9 17:35:41 bigboy named[1157]: starting BIND 9.2.3 -u named -t /var/named/chroot
Nov  9 17:35:41 bigboy named[1157]: using 1 CPU
Nov  9 17:35:41 bigboy named[1157]: loading configuration from ¿/etc/named.conf¿
Nov  9 17:35:41 bigboy named[1157]: listening on IPv4 interface lo, 127.0.0.1#53
Nov  9 17:35:41 bigboy named[1157]: listening on IPv4 interface eth0, 10.41.32.71#53
Nov  9 17:35:41 bigboy named[1157]: command channel listening on 127.0.0.1#953
Nov  9 17:35:41 bigboy named[1157]: command channel listening on ::1#953
Nov  9 17:35:41 bigboy named[1157]: running

    * If there are no named errors to the screen or /var/log/messages, and your domain doesn't resolve correctly when queried using the host command when you are logged into your new nameserver, then the problem could be due to you forgetting to add a zone file entry for the domain in named.conf; there could be a typographical error in your zone file; or you could have forgotten to update your zone file serial numbers. 

This isn't a comprehensive configuration error list, but it covers some common mistakes with a new configuration.
Network Troubleshooting Steps

Once configuration troubleshooting this is completed, you can continue with the following troubleshooting steps:

1) Determine whether your DNS server is accessible on DNS UDP/TCP port 53. Lack of connectivity could be caused by a firewall with incorrect, permit, NAT, or port forwarding rules to your DNS server. Failure could also be caused by the named process being stopped. It is best to test this from both inside your network and from the Internet.

Troubleshooting with TELNET is covered in Chapter 4, "Simple Network Troubleshooting".

2) Linux status messages are logged to the file /var/log/messages. Use it to make sure all your zone files are loaded when you start BIND/named. Check your /etc/named.conf file if they fail to do so. (Linux logging is covered in Chapter 5, "Troubleshooting Linux with syslog".

Feb 21 09:13:13 bigboy named: named startup succeeded
Feb 21 09:13:13 bigboy named[12026]: loading configuration from '/etc/named.conf'
Feb 21 09:13:13 bigboy named[12026]: no IPv6 interfaces found
Feb 21 09:13:13 bigboy named[12026]: listening on IPv4 interface lo, 127.0.0.1#53
Feb 21 09:13:13 bigboy named[12026]: listening on IPv4 interface wlan0, 192.168.1.100#53
Feb 21 09:13:13 bigboy named[12026]: listening on IPv4 interface eth0, 172.16.1.100#53
Feb 21 09:13:14 bigboy named[12026]: command channel listening on 127.0.0.1#953
Feb 21 09:13:14 bigboy named[12026]: zone 0.0.127.in-addr.arpa/IN: loaded serial 1997022700
Feb 21 09:13:14 bigboy named[12026]: zone 1.16.172.in-addr.arpa/IN: loaded serial 51
Feb 21 09:13:14 bigboy named[12026]: zone 1.168.192.in-addr.arpa/IN: loaded serial 51
Feb 21 09:13:14 bigboy named[12026]: zone simiya.com/IN: loaded serial 2004021401
Feb 21 09:13:14 bigboy named[12026]: zone localhost/IN: loaded serial 42
Feb 21 09:13:14 bigboy named[12026]: zone simiya.com/IN: loaded serial 200301114
Feb 21 09:13:14 bigboy named[12026]: running

3) Use the host (nslookup in Windows) command for both forward and reverse lookups to make sure the zone files were configured correctly.

If this fails, try:

    * Double check for your updated serial numbers in the modified files and also inspect the individual records within the files for mistakes.
    * Ensure there isn't a firewall that could be blocking DNS traffic on TCP and/or UDP port 53 between your server and the DNS server.
    * Use the dig command to determine whether the name server for your domain is configured correctly. 

Here is an example of querying DNS server ns1.my-site.com for the IP address of www.linuxhomenetworking.com. (You can also replace the name server's name with its IP address.)

[root@bigboy tmp]# host www.linuxhomenetworking.com ns1.my-site.com
Using domain server:
Name: ns1.my-site.com
Address: 192.168.1.100#53
Aliases:

www.linuxhomenetworking.com has address 65.115.71.34

[root@bigboy tmp]#

Here is an example of querying your default DNS server for the IP address of www.linuxhomenetworking.com. As you can see, the name of the specific DNS server to query has been left off the end. Failure in this case could be due not only to an error on your BIND configuration or domain registration but also to an error in your DNS client's DNS server entry in your Linux /etc/resolv.conf file or the Windows TCP/IP properties for your NIC.

[root@bigboy tmp]# host www.linuxhomenetworking.com
www.linuxhomenetworking.com has address 65.115.71.34
[root@bigboy tmp]#

4) You can also use the dig command to determine whether known DNS servers on the Internet have received a valid update for your zone. (Remember if you decide to change the DNS servers for your domain that it could take up to four days for it to propagate across the Internet.)

The format for the command is:

dig <domain-name> <name-server> soa

The name server is optional. If you specify a name server, then dig queries that name server instead of the Linux server's default name server. It is sometimes good to query both your name server, as well as a well known name server such as ns1.yahoo.com to make sure your DNS records have propagated properly. The dig command only works with fully qualified domain names only, because it doesn't refer to the /etc/resolv.conf file.

This command uses the local DNS server for the query. It returns the SOA record information and the addresses of the domain's DNS servers in the authority section.

[root@bigboy tmp]# dig linuxhomenetworking.com SOA
...
...
;; AUTHORITY SECTION:
linuxhomenetworking.com. 3600   IN      NS      ns1.myisp.net.
linuxhomenetworking.com. 3600   IN      NS      ns2.myisp.net.

;; ADDITIONAL SECTION:
ns1.myisp.net.      3600    IN      A       65.115.70.68
ns2.myisp.net.      3600    IN      A       65.115.70.69
...
...
[root@bigboy tmp]#

Here is a successful dig using DNS server ns1.yahoo.com for the query. As before, it returns the SOA record for the zone.

[root@bigboy tmp]# dig ns1.yahoo.com linuxhomenetworking.com SOA
...
...
;; AUTHORITY SECTION:
linuxhomenetworking.com. 3600   IN      NS      ns2.myisp.net.
linuxhomenetworking.com. 3600   IN      NS      ns1.myisp.net.
 
;; ADDITIONAL SECTION:
ns1.myisp.net.      3600    IN      A       65.115.70.68
ns2.myisp.net.      3600    IN      A       65.115.70.69
...
...
[root@bigboy tmp]#

Sometimes your SOA dig will fail. This command uses the DNS server ns1.yahoo.com for the query. In this case the authority section doesn't know of the domain and points to the name server for the entire .com domain at VeriSign.

[root@bigboy tmp]# dig  ns1.yahoo.com linuxhomeqnetworking.com SOA
...
...
;; QUESTION SECTION:
;linuxhomeqnetworking.com.      IN      SOA
;; AUTHORITY SECTION:
com.                    0       IN      SOA     a.gtld-servers.net. nstld.verisign-grs.com.  1077341254 1800 900 604800 900
...
...
[root@bigboy tmp]#

Possible causes of failure include:

    * Typographical errors. In this case the misspelling "linuxhomeqnetworking.com" was entered on the command line.
    * Incorrect domain registration.
    * Correct domain registration, but there is a lag in the propagation of the domain information across the Internet. Delays of up to four days are not uncommon.
    * A firewall could be blocking DNS traffic on TCP and/or UDP port 53 between your server and the DNS server. 


	4.7 Migrating Your Web Site In-House

It is important to have a detailed migration plan if you currently use an external company to host your Web site and wish to move the site to a server at home or in your office. At the very least your plan should include these steps:

   a. There is no magic bullet that will allow you to tell all the caching DNS servers in the world to flush their caches of your zone file entries. Your best alternative is to request your existing service provider to set the TTL on my-site.com in the DNS zone file to a very low value, say one minute. As the TTL is usually set to a number of days, it will take at least three to five days for all remote DNS servers to recognize the change. Once the propagation is complete, it will take only one minute to see the results of the final DNS configuration switch to your new server. If anything goes wrong, you can then revert to the old configuration, knowing it will rapidly recover within minutes rather than days.
   b. Set up your test server in house. Edit the /etc/hosts file to make www.my-site.com refer to its own IP address, not that of the www.my-site.com site that is currently in production. This file is usually given a higher priority than DNS, therefore the test server will begin to think that www.my-site.com is really hosted on itself. You may also want to add an entry for mail.my-site.com if the new Web server is going to also be your new mail server.
   c. Test your server based applications from the server itself. This should include mail, Web, and so on.
   d. Test the server from a remote client. You can test the server running as www.my-site.com even though DNS hasn't been updated. Just edit your /etc/hosts file on your Web browsing Linux PC to make www.my-site.com map to the IP address of the new server. In the case of Windows, the file would be C:\WINDOWS\system32\drivers\etc\hosts. You may also want to add an entry for mail.my-site.com if the new Web server is going to also be your new mail server. Your client will usually refer to these files first before checking DNS, hence you can use them to predefine some DNS lookups at the local client level only.
   e. Once testing is completed, coordinate with your Web hosting provider to update your domain registration's DNS records for www.my-site.com to point to your new Web server. As the TTLs were set to one minute previously, you'll be able to see results of the migration within minutes.
   f. Once complete, you can set the TTL back to the original value to help reduce the volume of DNS query traffic hitting your DNS server.
   h. Fix your /etc/hosts files by deleting the test entries you had before.
   i. You may also want to take over your own DNS. Edit your my-site.com DNS entries with VeriSign, RegisterFree or whoever you bought your domain from to point to your new DNS servers. 

Remember, you don't have to host DNS or mail in-house, this could be left in the hands of your service provider. You can then migrate these services in-house as your confidence in hosting becomes greater.

Finally, if you have concerns that your service provider won't cooperate, then you could explain to the provider that you want to test its failover capabilities to a duplicate server that you host in-house. You can then decide whether the change will be permanent once you have failed over back and forth a few times.

	4.6 DHCP Considerations For DNS

If you have a DHCP server on your network, you'll need to make it assign the IP address of the Linux box as the DNS server it tells the DHCP clients to use. If your Linux box is the DHCP server, then you may need to refer to Chapter 8, "Configuring the DHCP Server".

	4.7 Simple DNS Security

DNS can reveal a lot about the nature of your domain. You should take some precautions to conceal some of the information for the sake of security.
Zone Transfer Protection

The host command does one DNS query at a time, but the dig command is much more powerful. When given the right parameters it can download the entire contents of your domain's zone file.

In this example, the AFXR zone transfer parameter is used to get the contents of the my-site.com zone file.

|   [root@smallfry tmp]# dig my-site.com AXFR
|   ; <<>> DiG 9.2.3 <<>> my-site.com AXFR
|   ;; global options:  printcmd
|   my-site.com.            3600    IN      SOA     www.my-site.com. hostmaster.my-site.com. 2004110701  3600 3600 3600 3600
|   my-site.com.            3600    IN      NS      ns1.my-site.com.
|   my-site.com.            3600    IN      MX      10 mail.my-site.com.
|   192-168-1-96.my-site.com. 3600  IN      A       192.168.1.96
|   192-168-1-97.my-site.com. 3600  IN      A       192.168.1.97
|   192-168-1-98.my-site.com. 3600  IN      A       192.168.1.98
|   bigboy.my-site.com.     3600    IN      A       192.168.1.100
|   gateway.my-site.com.    3600    IN      A       192.168.1.1
|   localhost.my-site.com.  3600    IN      A       127.0.0.1
|   mail.my-site.com.       3600    IN      CNAME   www.my-site.com.
|   ns1.my-site.com.        3600    IN      CNAME   www.my-site.com.
|   ntp.my-site.com.        3600    IN      CNAME   www.my-site.com.
|   smallfry.my-site.com.   3600    IN      A       192.168.1.102
|   www.my-site.com.        3600    IN      A       192.168.1.100
|   my-site.com.            3600    IN      SOA     www.my-site.com. hostmaster.my-site.com. 2004110701  3600 3600 3600 3600
|   ;; Query time: 16 msec
|   ;; SERVER: 192.168.1.100#53(192.168.1.100)
|   ;; WHEN: Sun Nov 14 20:21:07 2004
|   ;; XFR size: 16 records
|   [root@smallfry tmp]#

This may not seem like an important security threat at first glance, but it is. Anyone can use this command to determine all your server's IP addresses and from the names determine what type of server it is and then launch an appropriate cyber attack.

In a simple home network, without master and slave servers, zone transfers should be disabled. You can do this by applying the allow-transfer directive to the global options section of your named.conf file.

options {
   allow-transfer {none;};
};

Once applied, your zone transfer test should fail.

[root@smallfry tmp]# dig my-site.com AXFR
...
...
 ; <<>> DiG 9.2.3 <<>> my-site.com AXFR
 ;; global options:  printcmd
 ; Transfer failed.
 [root@smallfry tmp]#


Selectively Disabling Recursion

Your caching DNS server can unknowingly participate in a form of DDoS attack if recursive lookups are globally allowed.

Say for example that for political, religious, competitive or otherwise malicious reasons your web site is targeted for an attack. First, a hacker breaks into the authoritative DNS server for a sub domain, like my-web-site.org, and adds a large TXT record to the sub domain. The hacker then sends thousands of queries to unsecured caching DNS servers requesting the TXT record, but there is a catch. The queries use a false source IP address that corresponds to the IP address of the DNS server for your website. The queries are small, but the responses are amplified by the size of the TXT information, and your DNS server quickly becomes overwhelmed by the flurry of replies. Without DNS, your web site goes off the air. For the administrator of the caching DNS servers, the additional load of the queries can be unnoticeable, but when multiplied by thousands of other poorly configured servers, the attack on your site becomes lethal.

The allow-recursion directive placed in the options section of your named.conf file can be used to restrict the networks to which recursive lookups are allowed. In this example an ACL is also used to limit lookups to localhost and the 192.168.1.0/24 network.

|    acl "recursive_subnets" {	
|           192.168.1.0/24;
|           localhost;
|    };
|    
|    options {
|             allow-recursion { "recursive_subnets"; };
|    };

Note: This does not restrict forward or reverse lookups defined by the zone files on the server. The server will answer all queries for my-web-site.org if it owns that domain, but it won't respond to queries for servers in another domain such as google.com.
Naming Convention Security

Your my-site.com domain will probably have a www and a mail subdomain, and they should remain obvious to all. You may want to adjust your DNS views so that to external users, your MySQL database server doesn't have the letters "DB" or "SQL" in the name, or that your firewall doesn't have the letters "FW" in its name either. This may good for ease of reference within the company, but to the Internet these names provide rapid identifiaction of the types of malicious exploits a hacker could use to break in. Web site security refers to anything that helps to guarantee the availability of the site, this is just one of many methods you can use.

	4.8 Conclusion

DNS management is a critical part of the maintenance of any Web site. Fortunately, although it can be a little complicated, DNS modifications are usually infrequent, because the IP address of a server is normally fixed or static. This is not always the case. There are situations in which a server's IP address will change unpredictably and frequently, making DNS management extremely difficult. Dynamic DNS was created as a solution to this and is explained in Chapter 19, "Dynamic DNS".

Retrieved from "http://www.linuxhomenetworking.com/wiki/index.php/Quick_HOWTO_:_Ch18_:_Configuring_DNS"


	4.9 Summary
	Test DNS resolution:
	$host <name> 

	Perform lookup:
	$host <IP address> 

	on windows host command is replaced by nslookup

	List of DNS servers for current machine:
	$ cat /etc/resolv.conf

	4.10 Make DNS changes apply, refresh DNS cache
	/etc/rc.d/init.d/nscd restart

	4.11 DNS records
This List of DNS record types provides an overview of types of resource records (database records) stored in the zone files of the Domain Name System (DNS).

The DNS implements a distributed, hierarchical, and redundant database for information associated with Internet domain names and addresses. In these domain servers, different record types are used for different purposes.

For an introduction to the domain name system, see Domain Name System.
Contents

Resource records
Code¿ 	Number¿ 	Defining RFC¿ 	Description 	Function
A 	1 	RFC 1035 	address record 	Returns a 32-bit IPv4 address, most commonly used to map hostnames to an IP address of the host, but also used for DNSBLs, storing subnet masks in RFC 1101, etc.
AAAA 	28 	RFC 3596 	IPv6 address record 	Returns a 128-bit IPv6 address, most commonly used to map hostnames to an IP address of the host.
AFSDB 	18 	RFC 1183 	AFS database record 	Location of database servers of an AFS cell. This record is commonly used by AFS clients to contact AFS cells outside their local domain. A subtype of this record is used by the obsolete DCE/DFS file system.
APL 	42 	RFC 3123 	Address Prefix List 	Specify lists of address ranges, e.g. in CIDR format, for various address families. Experimental.
CERT 	37 	RFC 4398 	Certificate record 	Stores PKIX, SPKI, PGP, etc.
CNAME 	5 	RFC 1035 	Canonical name record 	Alias of one name to another: the DNS lookup will continue by retrying the lookup with the new name.
DHCID 	49 	RFC 4701 	DHCP identifier 	Used in conjunction with the FQDN option to DHCP
DLV 	32769 	RFC 4431 	DNSSEC Lookaside Validation record 	For publishing DNSSEC trust anchors outside of the DNS delegation chain. Uses the same format as the DS record. RFC 5074 describes a way of using these records.
DNAME 	39 	RFC 2672 	delegation name 	DNAME creates an alias for a name and all its subnames, unlike CNAME, which aliases only the exact name in its label. Like the CNAME record, the DNS lookup will continue by retrying the lookup with the new name.
DNSKEY 	48 	RFC 4034 	DNS Key record 	The key record used in DNSSEC. Uses the same format as the KEY record.
DS 	43 	RFC 4034 	Delegation signer 	The record used to identify the DNSSEC signing key of a delegated zone
HIP 	55 	RFC 5205 	Host Identity Protocol 	Method of separating the end-point identifier and locator roles of IP addresses.
IPSECKEY 	45 	RFC 4025 	IPSEC Key 	Key record that can be used with IPSEC
KEY 	25 	RFC 2535[1] and RFC 2930[2] 	key record 	Used only for SIG(0) (RFC 2931) and TKEY (RFC 2930).[3] RFC 3445 eliminated their use for application keys and limited their use to DNSSEC.[4] RFC 3755 designates DNSKEY as the replacement within DNSSEC.[5]
KX 	36 	RFC 2230 	Key eXchanger record 	Used with some cryptographic systems (not including DNSSEC) to identify a key management agent for the associated domain-name. Note that this has nothing to do with DNS Security. It is Informational status, rather than being on the IETF standards-track. It has always had limited deployment, but is still in use.
LOC 	29 	RFC 1876 	Location record 	Specifies a geographical location associated with a domain name
MX 	15 	RFC 1035 	mail exchange record 	Maps a domain name to a list of message transfer agents for that domain
NAPTR 	35 	RFC 3403 	Naming Authority Pointer 	Allows regular expression based rewriting of domain names which can then be used as URIs, further domain names to lookups, etc.
NS 	2 	RFC 1035 	name server record 	Delegates a DNS zone to use the given authoritative name servers
NSEC 	47 	RFC 4034 	Next-Secure record 	Part of DNSSEC¿used to prove a name does not exist. Uses the same format as the (obsolete) NXT record.
NSEC3 	50 	RFC 5155 	NSEC record version 3 	An extension to DNSSEC that allows proof of nonexistence for a name without permitting zonewalking
NSEC3PARAM 	51 	RFC 5155 	NSEC3 parameters 	Parameter record for use with NSEC3
PTR 	12 	RFC 1035 	pointer record 	Pointer to a canonical name. Unlike a CNAME, DNS processing does NOT proceed, just the name is returned. The most common use is for implementing reverse DNS lookups, but other uses include such things as DNS-SD.
RRSIG 	46 	RFC 4034 	DNSSEC signature 	Signature for a DNSSEC-secured record set. Uses the same format as the SIG record.
RP 	17 	RFC 1183 	Responsible person 	Information about the responsible person(s) for the domain. Usually a email adress with the @ replaced by a .
SIG 	24 	RFC 2535 	Signature 	Signature record used in SIG(0) (RFC 2931) and TKEY (RFC 2930).[5] RFC 3755 designated RRSIG as the replacement for SIG for use within DNSSEC.[5]
SOA 	6 	RFC 1035 	start of authority record 	Specifies authoritative information about a DNS zone, including the primary name server, the email of the domain administrator, the domain serial number, and several timers relating to refreshing the zone.
SPF 	99 	RFC 4408 	Sender Policy Framework 	Specified as part of the SPF protocol in preference to the earlier provisional practice of storing SPF data in TXT records. Uses the same format as the earlier TXT record.
SRV 	33 	RFC 2782 	Service locator 	Generalized service location record, used for newer protocols instead of creating protocol-specific records such as MX.
SSHFP 	44 	RFC 4255 	SSH Public Key Fingerprint 	Resource record for publishing SSH public host key fingerprints in the DNS System, in order to aid in verifying the authenticity of the host.
TA 	32768 	None 	DNSSEC Trust Authorities 	Part of a deployment proposal for DNSSEC without a signed DNS root. See the IANA database and Weiler Spec for details. Uses the same format as the DS record.
TKEY 	249 	RFC 2930 	secret key record 	A method of providing keying material to be used with TSIG that is encrypted under the public key in an accompanying KEY RR.[6]
TSIG 	250 	RFC 2845 	Transaction Signature 	Can be used to authenticate dynamic updates as coming from an approved client, or to authenticate responses as coming from an approved recursive name server[7] similar to DNSSEC.
TXT 	16 	RFC 1035 	Text record 	Originally for arbitrary human-readable text in a DNS record. Since the early 1990s, however, this record more often carries machine-readable data, such as specified by RFC 1464, opportunistic encryption, Sender Policy Framework (although this provisional use of TXT records is deprecated in favor of SPF records), DomainKeys, DNS-SD, etc.

Other types and pseudo resource records

Other types of records simply provide some types of information (for example, an HINFO record gives a description of the type of computer/OS a host uses), or others return data used in experimental features. The "type" field is also used in the protocol for various operations.
Code¿ 	Number¿ 	Defining RFC¿ 	Description 	Function
* 	255 	RFC 1035 	All cached records 	Returns all records of all types known to the name server. If the name server does not have any information on the name, the request will be forwarded on. The records returned may not be complete. For example, if there is both an A and an MX for a name, but the name server has only the A record cached, only the A record will be returned. Sometimes referred to as "ANY", for example in Windows nslookup and Wireshark.
AXFR 	252 	RFC 1035 	Full Zone Transfer 	Transfer entire zone file from the master name server to secondary name servers.
IXFR 	251 	RFC 1995 	Incremental Zone Transfer 	Requests a zone transfer of the given zone but only differences from a previous serial number. This request may be ignored and a full (AXFR) sent in response if the authoritative server is unable to fulfill the request due to configuration or lack of required deltas.
OPT 	41 	RFC 2671 	Option 	This is a "pseudo DNS record type" needed to support EDNS

	4.12
	4.2 Linux / UNIX: DNS Lookup Commands, host and dig

by Vivek Gite on November 8, 2010 · 5 comments

How do I perform dns lookup under Linux or UNIX or Apple OS X operating systems without using 3rd party web sites for troubleshooting DNS usage?

You can use any one of the following dns lookup utility under Linux / UNIX. You can skip all 3rd party websites and use the following to debug your dns servers and lookup issues:

[a] host command - DNS lookup utility.

[b] dig command - DNS lookup utility.

Both commands will allow you to get answer to various dns queries such as the IP address (A), mail exchanges (MX), name servers (NS), text annotations (TXT), or ANY (all) type.
host DNS Lookup Examples

host command is a simple utility for performing DNS lookups. It is normally used to convert names to IP addresses and vice versa. When no arguments or options are given, host command displays a short summary of its command line arguments and options. The syntax is as follows:

host example.com
host -t TYPE example.com
host -t a example.com

Task: Find Out the Domain IP

$ host -t a cyberciti.biz
Sample outputs:

cyberciti.biz has address 75.126.153.206

Task: Find Out the Domain Mail Server

$ host -t mx cyberciti.biz
Sample outputs:

cyberciti.biz mail is handled by 2 CYBERCITI.BIZ.S9A2.PSMTP.com.
cyberciti.biz mail is handled by 3 CYBERCITI.BIZ.S9B1.PSMTP.com.
cyberciti.biz mail is handled by 4 CYBERCITI.BIZ.S9B2.PSMTP.com.
cyberciti.biz mail is handled by 1 CYBERCITI.BIZ.S9A1.PSMTP.com.

Task: Find Out the Domain Name Servers

$ host -t ns cyberciti.biz
Sample outputs:

cyberciti.biz name server ns2.nixcraft.net.
cyberciti.biz name server ns1.nixcraft.net.
cyberciti.biz name server ns5.nixcraft.net.
cyberciti.biz name server ns4.nixcraft.net.

Task: Find Out the Domain TXT Recored (e.g., SPF)

$ host -t txt cyberciti.biz
Sample outputs:

cyberciti.biz descriptive text "v=spf1 a mx ip4:74.86.48.99 ip4:74.86.48.98 ip4:74.86.48.102 ip4:74.86.48.101 ip4:74.86.48.100 ip4:72.26.218.170 ip4:93.89.92.12 ip4:180.92.186.178 include:_spf.google.com ~all"

Task: Find Out the Domain CNAME Record

$ host -t cname files.cyberciti.biz
Sample outputs:

files.cyberciti.biz is an alias for files.cyberciti.biz.edgesuite.net.

Task: Find Out the Domain SOA Record

$ host -t soa cyberciti.biz
Sample outputs:

cyberciti.biz has SOA record ns1.nixcraft.net. vivek.nixcraft.com. 2008072353 10800 3600 604800 3600

Task: Query Particular Name Server

Query ns2.nixcraft.net:
$ host cyberciti.biz ns2.nixcraft.net
Sample outputs:

Using domain server:
Name: ns2.nixcraft.net
Address: 75.126.168.152#53
Aliases:
cyberciti.biz has address 75.126.153.206
cyberciti.biz has IPv6 address 2607:f0d0:1002:51::4
cyberciti.biz mail is handled by 3 CYBERCITI.BIZ.S9B1.PSMTP.com.
cyberciti.biz mail is handled by 4 CYBERCITI.BIZ.S9B2.PSMTP.com.
cyberciti.biz mail is handled by 1 CYBERCITI.BIZ.S9A1.PSMTP.com.
cyberciti.biz mail is handled by 2 CYBERCITI.BIZ.S9A2.PSMTP.com.

Task: Display All Information About Domain Records and Zone

You need to pass the -a (all) option and asking host command to make a query of type ANY:
$ host -a cyberciti.biz
OR
$ host -t any cyberciti.biz
Sample outputs:

Trying "cyberciti.biz"
;; Truncated, retrying in TCP mode.
Trying "cyberciti.biz"
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 34079
;; flags: qr rd ra; QUERY: 1, ANSWER: 14, AUTHORITY: 0, ADDITIONAL: 0
;; QUESTION SECTION:
;cyberciti.biz.			IN	ANY
;; ANSWER SECTION:
cyberciti.biz.		3423	IN	AAAA	2607:f0d0:1002:51::4
cyberciti.biz.		3600	IN	SOA	ns1.nixcraft.net. vivek.nixcraft.com. 2008072353 10800 3600 604800 3600
cyberciti.biz.		3600	IN	TXT	"v=spf1 a mx ip4:74.86.48.99 ip4:74.86.48.98 ip4:74.86.48.102 ip4:74.86.48.101 ip4:74.86.48.100 ip4:72.26.218.170 ip4:93.89.92.12 ip4:180.92.186.178 include:_spf.google.com ~all"
cyberciti.biz.		3600	IN	MX	2 CYBERCITI.BIZ.S9A2.PSMTP.com.
cyberciti.biz.		3600	IN	MX	3 CYBERCITI.BIZ.S9B1.PSMTP.com.
cyberciti.biz.		3600	IN	MX	4 CYBERCITI.BIZ.S9B2.PSMTP.com.
cyberciti.biz.		3600	IN	MX	1 CYBERCITI.BIZ.S9A1.PSMTP.com.
cyberciti.biz.		2805	IN	A	75.126.153.206
cyberciti.biz.		3423	IN	NS	ns2.nixcraft.net.
cyberciti.biz.		3423	IN	NS	ns5.nixcraft.net.
cyberciti.biz.		3423	IN	NS	ns1.nixcraft.net.
cyberciti.biz.		3423	IN	NS	ns4.nixcraft.net.
cyberciti.biz.		84092	IN	RRSIG	NSEC 8 2 86400 20101125013720 20101026010313 50568 biz. OjDv09mccTZR2bYCl4D57QcnNEkBq6bNEa20ExsI6NC2sI9pmiKLnq+w UnCYxWMnkMi7WNXwIhhUWtNhV48X3wJGj1Mufrhq8MnO25JIcRE6UJF2 y12TTZHHE0UJV6HSkw1sac3XlZKXLi/oSvE/IXTsdj2SckPh+pMlaieQ jAA=
cyberciti.biz.		84092	IN	NSEC	CYBERCITIZEN.biz. NS RRSIG NSEC
Received 749 bytes from 192.168.1.254#53 in 0.1 ms

Task: Use IPv6 Query Transport

Test your dns lookup using IPv6 query transport (you must have IPV6 based connectivity including IPv6 enabled resolving name servers):

$ host -6 cyberciti.biz
$ host -6 -a cyberciti.biz
$ host -6 cyberciti.biz ns1.nixcraft.net
$ host -6 -t ns cyberciti.biz ns3.nixcraft.net

Sample outputs:

Using domain server:
Name: ns3.nixcraft.net
Address: 2001:48c8:10:1::2#53
Aliases:
cyberciti.biz name server ns2.nixcraft.net.
cyberciti.biz name server ns4.nixcraft.net.
cyberciti.biz name server ns5.nixcraft.net.
cyberciti.biz name server ns1.nixcraft.net.

Task: Reverse IP Lookup

Type the command:
$ host {IP-Address-Here}
$ host 75.126.153.206
Sample outputs:

206.153.126.75.in-addr.arpa domain name pointer www.cyberciti.biz.

Task: Get TTL Information

Type the command as follows:
$ host -v -t {TYPE} {example.com}
$ host -v -t a cyberciti.biz
$ host -v -t a i.hexindia.net
Sample outputs:

Trying "cyberciti.biz"
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 17431
;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 4, ADDITIONAL: 7
;; QUESTION SECTION:
;cyberciti.biz.			IN	A
;; ANSWER SECTION:
cyberciti.biz.		1866	IN	A	75.126.153.206
;; AUTHORITY SECTION:
cyberciti.biz.		3850	IN	NS	NS1.NIXCRAFT.NET.
cyberciti.biz.		3850	IN	NS	NS4.NIXCRAFT.NET.
cyberciti.biz.		3850	IN	NS	NS5.NIXCRAFT.NET.
cyberciti.biz.		3850	IN	NS	NS2.NIXCRAFT.NET.
;; ADDITIONAL SECTION:
NS1.NIXCRAFT.NET.	85669	IN	A	72.26.218.170
NS1.NIXCRAFT.NET.	85689	IN	AAAA	2001:48c8:7::2
NS2.NIXCRAFT.NET.	85669	IN	A	75.126.168.152
NS2.NIXCRAFT.NET.	85669	IN	AAAA	2607:f0d0:1002:51::3
NS4.NIXCRAFT.NET.	85669	IN	A	93.89.92.12
NS4.NIXCRAFT.NET.	85669	IN	AAAA	2a01:348:0:15:5d59:50c:0:1
NS5.NIXCRAFT.NET.	85669	IN	AAAA	2001:48c8:10:1::2
Received 291 bytes from 10.0.80.11#53 in 2 ms

If you run the same command again, you’ll notice that the TTL number (1866) reduced.
dig DNS Lookup Examples

dig (domain information groper) or host command is a flexible tool for interrogating DNS name servers. It performs DNS lookups and displays the answers that are returned from the name server(s) that were queried. Most DNS administrators use dig to troubleshoot DNS problems because of its flexibility, ease of use and clarity of output. hos dns lookup tool have less functionality than dig.
Examples

dig @{ns1.example.com} {example.com}
dig @{ns1.example.com} {example.com} {TYPE}
dig cyberciti.biz a
dig cyberciti.biz mx
dig cyberciti.biz ns
dig cyberciti.biz txt
dig @ns1.nixcraft.net cyberciti.biz a

Task: Trace Usage

See how domains are resolved using root servers i.e. turn on tracing of the delegation path from the root name servers for the name being looked up. When tracing is enabled, dig makes iterative queries to resolve the name being looked up. It will follow referrals from the root servers, showing the answer from each server that was used to resolve the lookup:
$ dig +trace cyberciti.biz
Sample outputs:

 <<>> DiG 9.3.6-P1-RedHat-9.3.6-4.P1.el5_4.2 <<>> +trace cyberciti.biz
;; global options:  printcmd
.			41219	IN	NS	b.root-servers.net.
.			41219	IN	NS	e.root-servers.net.
.			41219	IN	NS	i.root-servers.net.
.			41219	IN	NS	d.root-servers.net.
.			41219	IN	NS	g.root-servers.net.
.			41219	IN	NS	k.root-servers.net.
.			41219	IN	NS	l.root-servers.net.
.			41219	IN	NS	c.root-servers.net.
.			41219	IN	NS	m.root-servers.net.
.			41219	IN	NS	a.root-servers.net.
.			41219	IN	NS	h.root-servers.net.
.			41219	IN	NS	j.root-servers.net.
.			41219	IN	NS	f.root-servers.net.
;; Received 436 bytes from 10.0.80.11#53(10.0.80.11) in 2 ms
biz.			172800	IN	NS	h.gtld.biz.
biz.			172800	IN	NS	c.gtld.biz.
biz.			172800	IN	NS	e.gtld.biz.
biz.			172800	IN	NS	b.gtld.biz.
biz.			172800	IN	NS	g.gtld.biz.
biz.			172800	IN	NS	a.gtld.biz.
biz.			172800	IN	NS	f.gtld.biz.
;; Received 316 bytes from 192.228.79.201#53(b.root-servers.net) in 34 ms
cyberciti.biz.		7200	IN	NS	NS5.NIXCRAFT.NET.
cyberciti.biz.		7200	IN	NS	NS1.NIXCRAFT.NET.
cyberciti.biz.		7200	IN	NS	NS2.NIXCRAFT.NET.
cyberciti.biz.		7200	IN	NS	NS4.NIXCRAFT.NET.
;; Received 115 bytes from 2001:503:8028:ffff:ffff:ffff:ffff:ff7e#53(h.gtld.biz) in 23 ms
cyberciti.biz.		3600	IN	A	75.126.153.206
cyberciti.biz.		3600	IN	NS	ns4.nixcraft.net.
cyberciti.biz.		3600	IN	NS	ns5.nixcraft.net.
cyberciti.biz.		3600	IN	NS	ns1.nixcraft.net.
cyberciti.biz.		3600	IN	NS	ns2.nixcraft.net.
;; Received 307 bytes from 2001:48c8:10:1::2#53(NS5.NIXCRAFT.NET) in 222 ms

Task: Get Only Short Answer

A quick way to just get the answer is to type the following command:
$ dig +short cyberciti.biz
Sample outputs:

75.126.153.206

Task: Display All Records

$ dig +noall +answer cyberciti.biz any
Sample outputs:

cyberciti.biz.		3490	IN	A	75.126.153.206
cyberciti.biz.		2733	IN	NS	NS2.NIXCRAFT.NET.
cyberciti.biz.		2733	IN	NS	NS1.NIXCRAFT.NET.
cyberciti.biz.		2733	IN	NS	NS4.NIXCRAFT.NET.
cyberciti.biz.		2733	IN	NS	NS5.NIXCRAFT.NET.
cyberciti.biz.		85668	IN	RRSIG	NSEC 8 2 86400 20101125013720 20101026010313 50568 biz. OjDv09mccTZR2bYCl4D57QcnNEkBq6bNEa20ExsI6NC2sI9pmiKLnq+w UnCYxWMnkMi7WNXwIhhUWtNhV48X3wJGj1Mufrhq8MnO25JIcRE6UJF2 y12TTZHHE0UJV6HSkw1sac3XlZKXLi/oSvE/IXTsdj2SckPh+pMlaieQ jAA=
cyberciti.biz.		85668	IN	NSEC	CYBERCITIZEN.biz. NS RRSIG NSEC

Task: Reverse IP Lookup

Type the following command:
$ dig -x +short {IP-Address-here}
$ dig -x 75.126.153.206 +short
Sample outputs

www.cyberciti.biz.

Task: Find Domain SOA Record

$ dig +nssearch cyberciti.biz
Sample outputs:

SOA ns1.nixcraft.net. vivek.nixcraft.com. 2008072353 10800 3600 604800 3600 from server ns5.nixcraft.net in 81 ms.
SOA ns1.nixcraft.net. vivek.nixcraft.com. 2008072353 10800 3600 604800 3600 from server ns4.nixcraft.net in 216 ms.
SOA ns1.nixcraft.net. vivek.nixcraft.com. 2008072353 10800 3600 604800 3600 from server ns1.nixcraft.net in 347 ms.
SOA ns1.nixcraft.net. vivek.nixcraft.com. 2008072353 10800 3600 604800 3600 from server ns2.nixcraft.net in 316 ms.

Task: Find Out TTL Value Using dig

$ dig +nocmd +noall +answer {TYPE} {example.com}
$ dig +nocmd +noall +answer a cyberciti.biz
Sample outputs:

cyberciti.biz.		1642	IN	A	75.126.153.206

Run again, enter:
$ dig +nocmd +noall +answer a cyberciti.biz
Sample outputs:

cyberciti.biz.		1629	IN	A	75.126.153.206

See also:

See man page for more information:
man dig
man host
Featured articles:

    20 Linux System Monitoring Tools Every SysAdmin Should Know
    20 Linux Server Hardening Security Tips
    My 10 UNIX Command Line Mistakes
    Linux: 20 Iptables Examples For New SysAdmins
    25 PHP Security Best Practices For Sys Admins
    The Novice Guide To Buying A Linux Laptop
    10 Greatest Open Source Software Of 2009
    Top 5 Email Client For Linux, Mac OS X, and Windows Users
    Top 20 OpenSSH Server Best Security Practices
    Top 10 Open Source Web-Based Project Management Software

Facebook it - Tweet it - Print it -

{ 5 comments... Please Comment & Share Your Expertise! }

kiranjith November 9, 2010 at 8:48 am

    Cool work…
    Love this one…

    Reply
Prasanth November 9, 2010 at 10:14 am

    Good Work! Nice to see all the main DNS troubleshooting commands clubbed in one page. It will be really helpful for the newbies…

    PS: Found one typo in the following command:

    Task: Get TTL Information

    ”
    $ host -t -t a cyberciti.biz
    $ host -t -t a i.hexindia.net
    ”

    It should be host -v -t a cyberciti.biz

    Reply

    Vivek Gite November 9, 2010 at 11:11 am

        Thanks for the heads up!

        Reply

Colin Brace November 9, 2010 at 2:02 pm

    Super useful, Vivek. I’m definitely bookmarking this page.

    FWIW, I find ‘whois IP number’ really useful for determining who owns a given IP number (helpful when tracking abuse). An example:

    $ whois 75.126.153.206
    [Querying whois.arin.net]
    [Redirected to rwhois.softlayer.com:4321]
    [Querying rwhois.softlayer.com]
    [rwhois.softlayer.com]
    %rwhois V-1.5:003fff:00 rwhois.softlayer.com (by Network Solutions, Inc. V-1.5.9.5)
    network:Class-Name:network
    network:ID:NETBLK-SOFTLAYER.75.126.128.0/19
    network:Auth-Area:75.126.128.0/19
    network:Network-Name:SOFTLAYER-75.126.128.0
    network:IP-Network:75.126.153.200/29
    network:IP-Network-Block:75.126.153.200-75.126.153.207
    network:Organization;I:SoftLayer Technologies, Inc.
    network:Street-Address:1950 Stemmons Freeway Suite 2043
    network:City:Dallas
    network:State:TX
    network:Postal-Code:75207
    network:Country-Code:US
    network:Tech-Contact;I:sysadmins@softlayer.com
    network:Abuse-Contact;I:abuse@softlayer.com
    network:Admin-Contact;I:IPADM258-ARIN
    network:Created:20070218
    network:Updated:20091220
    network:Updated-By:ipadmin@softlayer.com

    %referral rwhois://root.rwhois.net:4321/auth-area=.
    %ok

    Reply
Cristian November 14, 2010 at 5:55 pm

    excelent job !!!

    Reply

Leave a Comment

Name *

E-mail *

Website

You can use these HTML tags and attributes for your code and commands: <strong> <em> <ol> <li> <u> <ul> <blockquote> <pre> <a href="" title="">

Notify me of followup comments via e-mail
Security Question:
What is 2 + 7 ?
Solve the simple math so we know that you are a human and not a bot.





Tagged as: alias, descriptive text, dig command, dig command ttl find, dns lookup utility, dns queries, dns servers, domain ip, domain mail, domain name, host command, host command ttl find, host dns lookup, ip address, mail exchanges, mail server, mx, name server, server host, text annotations

Previous post: Linux / UNIX: Deleting Files In Many Subdirectories

Next post: Linux Network IP Accounting
GET FREE TIPS
Make the most of Linux Sysadmin work!
    

    Facebook   |   Twitter   |   Google +

    	
    	
    Related FAQs
        How to test or check reverse DNS
        Reverse nslookup Command
        How to Set Up DNS Lookup in Linux
        Linux / Unix: Look Up IP Addresses
        Linux / UNIX Find Out What Program / Service is Listening on a Specific TCP Port

©2006-2012 nixCraft. All rights reserved. Cannot be reproduced without written permission.
Privacy Policy | Terms of Service | Questions or Comments | Sitemap

	4.3 Active Directory DNS

		4.3.1 Active Directory SRV Records
by Daniel Petri - January 7, 2009
What DNS entries (SRV Records) does Windows 2000/2003 add when you create a domain?


In order for Active Directory to function properly, DNS servers must provide support for Service Location (SRV) resource records described in RFC 2052, A DNS RR for specifying the location of services (DNS SRV). SRV resource records map the name of a service to the name of a server offering that service. Active Directory clients and domain controllers use SRV records to determine the IP addresses of domain controllers. Although not a technical requirement of Active Directory, it is highly recommended that DNS servers provide support for DNS dynamic updates described in RFC 2136, Observations on the use of Components of the Class A Address Space within the Internet.

The Windows 2000 DNS service provides support for both SRV records and dynamic updates. If a non-Windows 2000 DNS server is being used, verify that it at least supports the SRV resource record. If not, it must be upgraded to a version that does support the use of the SRV resource record. For example, Windows NT Server 4.0 DNS servers must be upgraded to Service Pack 4 or later to support SRV resource records. A DNS server that supports SRV records but does not support dynamic update must be updated with the contents of the Netlogon.dns file created by the Active Directory Installation wizard while promoting a Windows 2000 Server to a domain controller. The Netlogon.dns file is described in the following section.

So now you understand that Windows 2000 domains rely heavily on DNS entries. If you enable dynamic update on the relevant DNS zones, W2K creates these entries automatically:

    _ldap._tcp.<DNSDomainName>

Enables a client to locate a W2K domain controller in the domain named by <DNSDomainName>. A client searching for a domain controller in the domain dpetri.net would query the DNS server for _ldap._tcp.dpetri.net.

    _ldap._tcp.<SiteName>._sites.<DNSDomainName>

Enables a client to find a W2K domain controller in the domain and site specified (e.g., _ldap._tcp.lab._sites.dpetri.net for a domain controller in the Lab site of dpetri.net).

    _ldap._tcp.pdc._ms-dcs.<DNSDomainName>

Enables a client to find the PDC flexible single master object (FSMO) role holder of a mixed-mode domain. Only the PDC of the domain registers this record.

    _ldap._tcp.gc._msdcs.<DNSTreeName>

Enables a client to find a Global Catalog (GC) server. Only domain controllers serving as GC servers for the tree will register this name. If a server ceases to be a GC server, the server will deregister the record.

    _ldap._tcp. ._sites.gc._msdcs.<DNSTreeName>

Enables a client to find a GC server in the specified site (e.g., _ldap._tcp.lab._sites.gc._msdcs.dpetri.net).

    _ldap._tcp.<DomainGuid>.domains._msdcs.<DNSTreeName>

Enables a client to find a domain controller in a domain based on the domain controller’s globally unique ID. A GUID is a 128-bit (8 byte) number that generates automatically for referencing Active Directory objects.

    <DNSDomainName>

Enables a client to find a domain controller through a normal Host record.

		4.3.2 Using NSlookup.exe
View products that this article applies to.
	System TipThis article applies to a different version of Windows than the one you are using. Content in this article may not be relevant to you. Visit the Windows 7 Solution Center	
This article was previously published under Q200525
On This Page
Expand all | Collapse all
SUMMARY
Nslookup.exe is a command-line administrative tool for testing and troubleshooting DNS servers. This tool is installed along with the TCP/IP protocol through Control Panel. This article includes several tips for using Nslookup.exe.
Back to the top
MORE INFORMATION
To use Nslookup.exe, please note the following:

    The TCP/IP protocol must be installed on the computer running Nslookup.exe
    At least one DNS server must be specified when you run the IPCONFIG /ALL command from a command prompt.
    Nslookup will always devolve the name from the current context. If you fail to fully qualify a name query (that is, use trailing dot), the query will be appended to the current context. For example, the current DNS settings are att.com and a query is performed on www.microsoft.com; the first query will go out as www.microsoft.com.att.com because of the query being unqualified. This behavior may be inconsistent with other vendor's versions of Nslookup, and this article is presented to clarify the behavior of Microsoft Windows NT Nslookup.exe
    If you have implemented the use of the search list in the Domain Suffix Search Order defined on the DNS tab of the Microsoft TCP/IP Properties page, devolution will not occur. The query will be appended to the domain suffixes specified in the list. To avoid using the search list, always use a Fully Qualified Domain Name (that is, add the trailing dot to the name).


Nslookup.exe can be run in two modes: interactive and noninteractive. Noninteractive mode is useful when only a single piece of data needs to be returned. The syntax for noninteractive mode is:

   nslookup [-option] [hostname] [server]
				


To start Nslookup.exe in interactive mode, simply type "nslookup" at the command prompt:

   C:\> nslookup
   Default Server:  nameserver1.domain.com
   Address:  10.0.0.1
   >
				


Typing "help" or "?" at the command prompt will generate a list of available commands. Anything typed at the command prompt that is not recognized as a valid command is assumed to be a host name and an attempt is made to resolve it using the default server. To interrupt interactive commands, press CTRL+C. To exit interactive mode and return to the command prompt, type exit at the command prompt.

The following is the help output and contains the complete list of options:

Commands:   (identifiers are shown in uppercase, [] means optional)

 NAME            - print info about the host/domain NAME using default 
                   server
 NAME1 NAME2     - as above, but use NAME2 as server
 help or ?       - print info on common commands
 set OPTION      - set an option

    all                 - print options, current server and host
    [no]debug           - print debugging information
    [no]d2              - print exhaustive debugging information
    [no]defname         - append domain name to each query
    [no]recurse         - ask for recursive answer to query
    [no]search          - use domain search list
    [no]vc              - always use a virtual circuit
    domain=NAME         - set default domain name to NAME
    srchlist=N1[/N2/.../N6] - set domain to N1 and search list to N1, N2, 
                          and so on
    root=NAME           - set root server to NAME
    retry=X             - set number of retries to X
    timeout=X           - set initial time-out interval to X seconds
    type=X              - set query type (for example, A, ANY, CNAME, MX, 
                          NS, PTR, SOA, SRV)
    querytype=X         - same as type
    class=X             - set query class (for example, IN (Internet), ANY)
    [no]msxfr           - use MS fast zone transfer
    ixfrver=X           - current version to use in IXFR transfer request

 server NAME     - set default server to NAME, using current default server
 lserver NAME    - set default server to NAME, using initial server
 finger [USER]   - finger the optional NAME at the current default host
 root            - set current default server to the root
 ls [opt] DOMAIN [> FILE] - list addresses in DOMAIN (optional: output to 
                  FILE)

    -a          -  list canonical names and aliases
    -d          -  list all records
    -t TYPE     -  list records of the given type (for example, A, CNAME, 
                   MX, NS, PTR, and so on)

 view FILE       - sort an 'ls' output file and view it with pg
 exit            - exit the program
				


A number of different options can be set in Nslookup.exe by running the set command at the command prompt. A complete listing of these options is obtained by typing set all. See above, under the set command for a printout of the available options.


Back to the top
Looking up Different Data Types
To look up different data types within the domain name space, use the set type or set q[uerytype] command at the command prompt. For example, to query for the mail exchanger data, type the following:

   C:\> nslookup
   Default Server:  ns1.domain.com
   Address:  10.0.0.1

   > set q=mx
   > mailhost
   Server:  ns1.domain.com
   Address:  10.0.0.1

   mailhost.domain.com     MX preference = 0, mail exchanger =
                           mailhost.domain.com
   mailhost.domain.com     internet address = 10.0.0.5
   >
				


The first time a query is made for a remote name, the answer is authoritative, but subsequent queries are nonauthoritative. The first time a remote host is queried, the local DNS server contacts the DNS server that is authoritative for that domain. The local DNS server will then cache that information, so that subsequent queries are answered nonauthoritatively out of the local server's cache.


Back to the top
Querying Directly from Another Name Server
To query another name server directly, use the server or lserver commands to switch to that name server. The lserver command uses the local server to get the address of the server to switch to, while the server command uses the current default server to get the address.

Example:

   C:\> nslookup

   Default Server:  nameserver1.domain.com
   Address:  10.0.0.1

   > server 10.0.0.2

   Default Server:  nameserver2.domain.com
   Address:  10.0.0.2
   >
				

Back to the top
Using Nslookup.exe to Transfer Entire Zone
Nslookup can be used to transfer an entire zone by using the ls command. This is useful to see all the hosts within a remote domain. The syntax for the ls command is:

   ls [- a | d | t type] domain [> filename]
				


Using ls with no arguments will return a list of all address and name server data. The -a switch will return alias and canonical names, -d will return all data, and -t will filter by type.

Example:

   >ls domain.com
   [nameserver1.domain.com]
    nameserver1.domain.com.    NS     server = ns1.domain.com
    nameserver2.domain.com                 NS     server = ns2.domain.com
    nameserver1                            A      10.0.0.1
    nameserver2                            A      10.0.0.2

   >
				


Zone transfers can be blocked at the DNS server so that only authorized addresses or networks can perform this function. The following error will be returned if zone security has been set:
*** Can't list domain example.com.: Query refused

For additional information, see the following article or articles in the Microsoft Knowledge Base:
193837  Windows NT 4.0 DNS Server Default Zone Security Settings
Back to the top
Troubleshooting Nslookup.exe
Default Server Timed Out
When starting the Nslookup.exe utility, the following errors may occur:
*** Can't find server name for address w.x.y.z: Timed out

NOTE: w.x.y.z is the first DNS server listed in the DNS Service Search Order list.

*** Can't find server name for address 127.0.0.1: Timed out

The first error indicates that the DNS server cannot be reached or the service is not running on that computer. To correct this problem, either start the DNS service on that server or check for possible connectivity problems.

The second error indicates that no servers have been defined in the DNS Service Search Order list. To correct this problem, add the IP address of a valid DNS server to this list.

For additional information, see the following article or articles in the Microsoft Knowledge Base:
172060  NSLOOKUP: Can't Find Server Name for Address 127.0.0.1
Can't Find Server Name when Starting Nslookup.exe
When starting the Nslookup.exe utility, the following error may occur:

*** Can't find server name for address w.x.y.z: Non-existent domain


This error occurs when there is no PTR record for the name server's IP address. When Nslookup.exe starts, it does a reverse lookup to get the name of the default server. If no PTR data exists, this error message is returned. To correct make sure that a reverse lookup zone exists and contains PTR records for the name servers.

For additional information, see the following article or articles in the Microsoft Knowledge Base:
172953  How to Install and Configure Microsoft DNS Server
Nslookup on Child Domain Fails
When querying or doing a zone transfer on a child domain, Nslookup may return the following errors:

*** ns.domain.com can't find child.domain.com.: Non-existent domain
*** Can't list domain child.domain.com.: Non-existent domain


In DNS Manager, a new domain can be added under the primary zone, thus creating a child domain. Creating a child domain this way does not create a separate db file for the domain, thus querying that domain or running a zone transfer on it will produce the above errors. Running a zone transfer on the parent domain will list data for both the parent and child domains. To work around this problem, create a new primary zone on the DNS server for the child domain. 

		4.3.3 Verifying Your Basic DNS Configuration
5 out of 9 rated this helpful - Rate this topic

If you use a third-party DNS server to support Active Directory, you must perform configuration tasks manually, and doing so, you might cause common configuration errors that prevent DNS and Active Directory from working properly. The following sections describe tests that you can perform to verify that your DNS server is working properly, that the forward and reverse lookup zones are properly configured, and that DNS can support Active Directory.

If you use either the Configure DNS Server wizard or the Active Directory Installation wizard to install your Windows 2000 DNS server, most configuration tasks are performed automatically and you can avoid many common configuration errors, but you might still want to perform the tests in this section.

Before checking anything else, check the event log for errors. For more information about Event Viewer, see "Troubleshooting Tools" earlier in this chapter.
Verifying That Your DNS Server Can Answer Queries

Use the following process to verify that your DNS server is started and can answer queries.

    Make sure that your server has basic network connectivity. For more information about verifying basic network connectivity, see "Checking the DNS Server for Problems" later in this chapter.

    Make sure that the server can answer both simple and recursive queries from the Monitoring tab in the DNS console. For more information about the Monitoring tab, see "Troubleshooting Tools" earlier in this chapter.

    From a client, use Nslookup to look up a domain name and the name of a host in the domain. For more information about using Nslookup, see "Troubleshooting Tools" earlier in this chapter.

    On the server, run netdiag to make sure the server is working properly and that the resource records Netlogon needs are registered on a DNS server. For more information about Netdiag, see "Troubleshooting Tools" earlier in this chapter.

    Make sure that the server can reach a root server by typing the following:
    nslookup
    server < IP address of server >
    set querytype=NS
    .

    Make sure that there is an A and PTR resource record configured for the server. For information about PTR resource records, see "Testing for Reverse Lookup Zones and PTR Records" later in this chapter.

Top Of Page
Verifying That the Forward Lookup Zone Is Properly Configured

After you create a forward lookup zone, you can use Nslookup to make sure it is properly configured and to test its integrity to host Active Directory. To start Nslookup, type the following

Nslookup server < IP address of server on which you created zone > set querytype=any

Nslookup starts. If the resolver cannot locate a PTR resource record for the server, you see an error message, but you are still able to perform the tests in this section.

To verify the zone is responding correctly, simulate a zone transfer by typing the following:

ls -d < domain name >

If the server is configured to restrict zone transfers, you might see an error message in Event Viewer. (For more information about Event Viewer, see "Troubleshooting Tools" earlier in this chapter.) Otherwise, you see a list of all the records in the domain.

Next, query for the SOA record by typing the following and pressing ENTER:

< domain name >

If your server is configured correctly, you see an SOA record. The SOA record includes a "primary name server" field. To verify that the primary name server has registered an NS record, type the following:

set type=ns < domain name >

If your server is configured correctly, you see an NS record for the name server.

Make sure that the authoritative name server listed in the NS record can be contacted to request queries by typing the following:

server <server name or IP address>

Next, query the server for any name for which it is authoritative.

If these tests are successful, the NS record points to the correct hostname, and the hostname has the correct IP address associated with it.
Top Of Page
Testing for Reverse Lookup Zones and PTR Resource Records

You do not need reverse lookup zones and PTR resource records for Active Directory to function. However, you need them if you want clients to be able to resolve FQDNs from IP addresses. Also, PTR resource records are commonly used by some applications for security purposes, to verify the identity of the client.

You do not need to have the reverse lookup zones and PTR resource records on your own servers; instead, another DNS server can contain these zones.

After you have configured your reverse lookup zones and PTR resource records, manually examine them in the DNS console. A reverse lookup zone must exist for each subnet, and the parent reverse lookup zone must have a delegation to your reverse lookup zone. For example, if you have a private root and the subnets 172.32.16.x and 172.32.17.x, the private root can host all reverse lookup zones, or it can contain the reverse lookup zone 172.32.x and delegate the reverse lookup zones 172.32.16.x and 172.32.17.x to other servers. Also, PTR resource records must exist for all the computers in your network. For more information about adding a reverse lookup zone, see "Adding a Reverse Lookup Zone" earlier in this chapter.

You can also use Nslookup to verify that the reverse lookup zones and PTR resource records are configured correctly.

To make sure your reverse lookup zones and PTR resource records are configured correctly

    Start Nslookup by typing Nslookup at the command prompt and then pressing ENTER.

    Switch to the server you want to query by typing the following:
    server < Server IP Address >

    Enter the IP address of the computer whose PTR resource record you want to verify, and then press ENTER.
    If the reverse lookup zone and PTR resource record are configured correctly, Nslookup returns the name of the computer.

    To quit Nslookup, type exit and then press ENTER.

Top Of Page
Verifying Your DNS Configuration After Installing Active Directory

When you use third-party DNS servers to support Active Directory, you can verify the registration of domain controller locator resource records. If the server does not support dynamic update, you need to add these records manually.

The Netlogon service creates a log file that contains all the locator resource records and places the log file in the following location:

% SystemRoot %\System32\Config\Netlogon.dns

You can check this file to find out which locator resource records are created for the domain controller.

The locator resource records are stored in a text file, compliant with RFC specifications. If your server is configured correctly, you see the LDAP SRV record for the domain controller:

_ldap._tcp. <Active Directory domain name> IN SRV < priority > < weight > 389 < domain controller name >

For example:

_ldap._tcp.reskit.com. IN SRV 0 0 389 dc1.reskit.com

Next, use the Nslookup command-line tool to verify that the domain controller registered the SRV resource records that were listed in Netlogon.dns.

note-icon Note

During the following test, if you have not configured a reverse lookup zone and PTR resource record for the DNS server you are querying, you might see several time-outs. This is not a problem.

To verify that SRV resource records are registered for the domain controller

    At the command prompt, type nslookup and then press ENTER.

    To set the DNS query type to filter for SRV records only, type set type=SRV and then press ENTER.

    To send a query for the registered SRV record for a domain controller in your Active Directory domain, type _ ldap._tcp. < Active Directory domain name > and then press ENTER.

    You should see the SRV records listed in Netlogon.dns. If you do not, SRV resource records might not be registered for the domain controller.

The following example shows a full Nslookup session, used to verify SRV resource records that are registered for locating two domain controllers on a network. In this example, the two domain controllers (DC1 and DC2) are registered for the domain noam.reskit.com.

C:\>nslookup

Default Server: dc1.noam.reskit.com

Address: 10.0.0.14

> set type=SRV

> _ldap._tcp.noam.reskit.com

Server: dc1.noam.reskit.com

Address: 10.0.0.14

_ldap._tcp.noam.reskit.com SRV service location:

priority = 0

weight = 0

port = 389

svr hostname = dc1.noam.reskit.com

_ldap._tcp.noam.reskit.com SRV service location:

priority = 0

weight = 0

port = 389

svr hostname = dc2.noam.reskit.com

dc1.noam.reskit.com internet address = 10.0.0.14

dc2.noam.reskit.com internet address = 10.0.0.15 

		4.3.4 How to verify that SRV DNS records have been created for a domain controller
View products that this article applies to.
	System TipThis article applies to a different version of Windows than the one you are using. Content in this article may not be relevant to you. Visit the Windows 7 Solution Center	
For a Microsoft Windows 2000 version of this article, see 241515  .
Expand all | Collapse all
SUMMARY
This article describes how to verify Service Location (SRV) locator resource records for a domain controller after you install the Active Directory directory service.
Back to the top
MORE INFORMATION
The SRV record is a Domain Name System (DNS) resource record that is used to identify computers that host specific services. SRV resource records are used to locate domain controllers for Active Directory. To verify SRV locator resource records for a domain controller, use one of the following methods.
DNS Manager
After you install Active Directory on a server running the Microsoft DNS service, you can use the DNS Management Console to verify that the appropriate zones and resource records are created for each DNS zone.

Active Directory creates its SRV records in the following folders, where Domain_Name is the name of your domain:
Forward Lookup Zones/Domain_Name/_msdcs/dc/_sites/Default-First-Site-Name/_tcp Forward Lookup Zones/Domain_Name/_msdcs/dc/_tcp

In these locations, an SRV record should appear for the following services:
_kerberos
_ldap
Netlogon.dns
If you are using non-Microsoft DNS servers to support Active Directory, you can verify SRV locator resource records by viewing Netlogon.dns. Netlogon.dns is located in the %systemroot%\System32\Config folder. You can use a text editor, such as Microsoft Notepad, to view this file.

The first record in the file is the domain controller's Lightweight Directory Access Protocol (LDAP) SRV record. This record should appear similar to the following:
_ldap._tcp.Domain_Name
Nslookup
Nslookup is a command-line tool that displays information you can use to diagnose Domain Name System (DNS) infrastructure.
To use Nslookup to verify the SRV records, follow these steps:

    On your DNS, click Start, and then click Run.
    In the Open box, type cmd.
    Type nslookup, and then press ENTER.
    Type set type=all, and then press ENTER.
    Type _ldap._tcp.dc._msdcs.Domain_Name, where Domain_Name is the name of your domain, and then press ENTER.

Nslookup returns one or more SRV service location records that appear in the following format, where Server_Name is the host name of a domain controller, and where Domain_Name is the domain the domain controller belongs to, and Server_IP_Address is the domain controller's Internet Protocol (IP) address:

Server: localhost
Address:  127.0.0.1
_ldap._tcp.dc._msdcs.Domain_Name
SRV service location:
	priority	= 0
	weight		= 100
	port		= 389
	srv hostname	= Server_Name.Domain_NameServer_Name.Domain_Name		internet address = Server_IP_Address

Back to the top
REFERENCES
For more information about the SRV records that are registered by Netlogon, please see the "SRV Records Registered by NetLogon" section in the TechNet document How DNS Support for Active Directory Works. To view this document, visit the following Microsoft web site:
http://technet2.microsoft.com/windowsserver/en/library/9D62E91D-75C3-4A77-AE93-A8804E9FF2A11033.mspx 

		4.3.5
	4.4

5. xargs, build and execute command lines from standard input

	5.1 examples with find

		5.1.1 Code: Lists all directories


	recall find . -exec echo '{}';
	xargs can do the same find . | xargs echo $1

	find . -type d -name ".svn" -print | xargs rm -rf

The above command will execute rm on each file found by 'find'. The above construct can be used to execute a command on multiple files. This is similar to the -exec argument find has but doesn't suffer from the "Too Many Arguments" problem. And xargs is easier to read than -exec in most cases.

Code: Lists all directories

find . -maxdepth 1 -type d -print | xargs echo Directories:

This will print all directories in the current folder. The command (echo) specified will recived the input from find and be executed once. That's why this code will output all directories on one line.
Code: Lists all directories using -I

A filter for feeding arguments to a command, and also a tool for assembling the commands themselves. It breaks a data stream into small enough chunks for filters and commands to process. Consider it as a powerful replacement for backquotes. In situations where command substitution fails with a too many arguments error, substituting xargs often works. [1]  Normally, xargs reads from stdin or from a pipe, but it can also be given the output of a file.

The default command for xargs is echo. This means that input piped to xargs may have linefeeds and other whitespace characters stripped out.

bash$ ls -l
total 0
 -rw-rw-r--    1 bozo  bozo         0 Jan 29 23:58 file1
 -rw-rw-r--    1 bozo  bozo         0 Jan 29 23:58 file2



bash$ ls -l | xargs
total 0 -rw-rw-r-- 1 bozo bozo 0 Jan 29 23:58 file1 -rw-rw-r-- 1 bozo bozo 0 Jan...



		5.1.2 bash$ find ~/mail -type f | xargs grep "Linux"
./misc:User-Agent: slrn/0.9.8.1 (Linux)
 ./sent-mail-jul-2005: hosted by the Linux Documentation Project.
 ./sent-mail-jul-2005: (Linux Documentation Project Site, rtf version)
 ./sent-mail-jul-2005: Subject: Criticism of Bozo's Windows/Linux article
 ./sent-mail-jul-2005: while mentioning that the Linux ext2/ext3 filesystem
 . . .
	      

ls | xargs -p -l gzip gzips every file in current directory, one at a time, prompting before each operation.

Note	

Note that xargs processes the arguments passed to it sequentially, one at a time.

bash$ find /usr/bin | xargs file
/usr/bin:          directory
 /usr/bin/foomatic-ppd-options:          perl script text executable
 . . .

		5.1.3 find . -maxdepth 1 -type d -print | xargs -I {} echo Directory: {}

This time we added -I {} to xargs. This is the replacement argument. The command specifed to xargs will now be executed once for each line of output from find and replace {} with that line.

So this time we get each directory printed on a separate line.
Note: If xargs -i fails you might need a newer version of xargs. The Gentoo package findutils contains xargs.


Keep this in mind when using xargs. When creating more complex commands you often need to use the -I argument. Like if you want to run multiple commands and/or pipe to another command.

		5.1.4 find symbol in .so files
find /opt/pbis/lib64/ -name '*.so' -print | xargs -I '{}' sh -c 'echo searching lib {} ; nm -C {} | grep LwWin32ExtErrorToName'

		5.1.5 find symbol in binary file
[yizaq@yizaq-dev01:Thu Aug 27:1044:53:/view/yizaq3__yizaq2.int.pbis.lx/vob/nm_acs/pbis/cisco/build/stage/opt/pbis/sbin]$ for f in $( ls ); do echo searching $f; nm $f | grep LsaDmpThreadRoutine ; done
searching lwsmd
00000000000cdc60 t LsaDmpThreadRoutine

	5.2 Advanced usage
An interesting xargs  option is -n NN, which limits to NN the number of arguments passed.

ls | xargs -n 8 echo lists the files in the current directory in 8 columns.

Tip	

Another useful option is -0, in combination with find -print0 or grep -lZ. This allows handling arguments containing whitespace or quotes.

find / -type f -print0 | xargs -0 grep -liwZ GUI | xargs -0 rm -f

grep -rliwZ GUI / | xargs -0 rm -f

Either of the above will remove any file containing "GUI". (Thanks, S.C.)

Or:

cat /proc/"$pid"/"$OPTION" | xargs -0 echo
#  Formats output:         ^^^^^^^^^^^^^^^
#  From Han Holl's fixup of "get-commandline.sh"
#+ script in "/dev and /proc" chapter.

	


	5.3 run multiple commands
	ex:
find . -name GNUmakefile | xargs -i bash -c 'echo {} ; grep lrpc {} '

	5.4 advances usage

		5.4.1  find all files who's 9th line contains a pattern
[yizaq@yizaq-lnx:Sun Aug 28:/view/yizaq__yizaq_5_3_cpr_fix.int.acs5_0.lx/vob/nm_acs/acs]$ find . -type f -name pom.xml  | xargs -ixxx echo xxx ; sed -e '9p' -n xxx | grep 2011 

use -i to specify to string to replace the list of files

To get the file name do:
[yizaq@yizaq-lnx:Sun Aug 28:/view/yizaq__yizaq_5_3_cpr_fix.int.acs5_0.lx/vob/nm_acs/acs]$ find . -type f -name pom.xml   -exec  echo '{}' \;  -exec sed -e '9p' -n '{}' \; | grep -B 1 2011


		5.4.2 xargs: unterminated quote problem
- cause - a file containing '
ex:
[yizaq@YIZAQ-M-D1BW:Thu Feb 26:~/Desktop/Work:]$ search_in_files  CSCun25815
executing: find . -type f | xargs grep -s CSCun25815 
xargs: unterminated quote

see:
[yizaq@YIZAQ-M-D1BW:Thu Feb 26:~/Desktop/Work:]$ find . | grep \"
[yizaq@YIZAQ-M-D1BW:Thu Feb 26:~/Desktop/Work:]$ find . | grep \'
./acs/5.0/AD/troubleshoot/machine auth doesn't work 28_07_08
./acs/5.0/AD/troubleshoot/machine auth doesn't work 28_07_08/machine auth doesn't work 28_07_08.zip
./acs/5.0/AD/troubleshoot/Shachar's PEAP AUTH issue25_09
./acs/5.0/AD/troubleshoot/Shachar's PEAP AUTH issue25_09/ACSADAgent.zip
./acs/5.0/AD/troubleshoot/Shachar's PEAP AUTH issue25_09/acsRuntime.zip
...


- fix 
 find . -type f -print0 | xargs -0 grep -s CSCun25815 
		5.4.3
	5.5
6. Development tools

	6.1 Debugging

		6.1.1 The build process 

			6.1.1.1 ELF and DWARF
			-> Executable and Linking Format (ELF) is the file format standard for executables, objects, shared libraries and core dumps in Unix.
			-> Use readelf to obtain ELF information.
Usage: readelf <option(s)> elf-file(s)
 Display information about the contents of ELF format files
 Options are:
  -a --all               Equivalent to: -h -l -S -s -r -d -V -A -I
  -h --file-header       Display the ELF file header
  -l --program-headers   Display the program headers
     --segments          An alias for --program-headers
  -S --section-headers   Display the sections' header
     --sections          An alias for --section-headers
  -g --section-groups    Display the section groups
  -e --headers           Equivalent to: -h -l -S
  -s --syms              Display the symbol table
      --symbols          An alias for --syms
  -n --notes             Display the core notes (if present)
  -r --relocs            Display the relocations (if present)
  -u --unwind            Display the unwind info (if present)
  -d --dynamic           Display the dynamic section (if present)
  -V --version-info      Display the version sections (if present)
  -A --arch-specific     Display architecture specific information (if any).
  -D --use-dynamic       Use the dynamic section info when displaying symbols
  -x --hex-dump=<number> Dump the contents of section <number>
  -w[liaprmfFso] or
  --debug-dump[=line,=info,=abbrev,=pubnames,=ranges,=macro,=frames,=str,=loc]
                         Display the contents of DWARF2 debug sections
  -I --histogram         Display histogram of bucket list lengths
  -W --wide              Allow output width to exceed 80 characters
  -H --help              Display this information
  -v --version           Display the version number of readelf

  Example:
  [ 3] .hash             HASH            08048148 000148 00002c 04   A  4   0  4
  [ 4] .dynsym           DYNSYM          08048174 000174 000060 10   A  5   1  4
  [ 5] .dynstr           STRTAB          080481d4 0001d4 000060 00   A  0   0  1
  [ 6] .gnu.version      VERSYM          08048234 000234 00000c 02   A  4   0  2
  [ 7] .gnu.version_r    VERNEED         08048240 000240 000020 00   A  5   1  4
  [ 8] .rel.dyn          REL             08048260 000260 000008 08   A  4   0  4
  [ 9] .rel.plt          REL             08048268 000268 000010 08   A  4  11  4
  [10] .init             PROGBITS        08048278 000278 000017 00  AX  0   0  4
  [11] .plt              PROGBITS        08048290 000290 000030 04  AX  0   0  4
  Source Code
  [12] .text             PROGBITS        080482c0 0002c0 00027c 00  AX  0   0  4
  [13] .fini             PROGBITS        0804853c 00053c 00001a 00  AX  0   0  4
  [14] .rodata           PROGBITS        08048558 000558 000151 00   A  0   0  4
  [15] .eh_frame         PROGBITS        080486ac 0006ac 000004 00   A  0   0  4
  CTORS and DTORS
  [16] .ctors            PROGBITS        080496b0 0006b0 000008 00  WA  0   0  4
  [17] .dtors            PROGBITS        080496b8 0006b8 000008 00  WA  0   0  4
  [18] .jcr              PROGBITS        080496c0 0006c0 000004 00  WA  0   0  4
  [19] .dynamic          DYNAMIC         080496c4 0006c4 0000c8 08  WA  5   0  4
  [20] .got              PROGBITS        0804978c 00078c 000004 04  WA  0   0  4
  [21] .got.plt          PROGBITS        08049790 000790 000014 04  WA  0   0  4
  [22] .data             PROGBITS        080497a4 0007a4 000010 00  WA  0   0  4
  [23] .bss              NOBITS          080497b4 0007b4 00000c 00  WA  0   0  4
  [24] .comment          PROGBITS        00000000 0007b4 000168 00      0   0  1
  DEBUG Sections
  [25] .debug_aranges    PROGBITS        00000000 00091c 000060 00      0   0  1
  [26] .debug_pubnames   PROGBITS        00000000 00097c 000098 00      0   0  1
  [27] .debug_info       PROGBITS        00000000 000a14 000598 00      0   0  1
  [28] .debug_abbrev     PROGBITS        00000000 000fac 000189 00      0   0  1
  [29] .debug_line       PROGBITS        00000000 001135 0000ad 00      0   0  1
  [30] .debug_frame      PROGBITS        00000000 0011e4 0000cc 00      0   0  4
  [31] .debug_str        PROGBITS        00000000 0012b0 000041 00      0   0  1
  [32] .debug_loc        PROGBITS        00000000 0012f1 0000dc 00      0   0  1
  [33] .shstrtab         STRTAB          00000000 0013cd 00013f 00      0   0  1
  [34] .symtab           SYMTAB          00000000 001aac 0005b0 10     35  61  4
  [35] .strtab           STRTAB          00000000 00205c 0002ec 00      0   0  1

			-> A newer format, compatible with ELF is DWARF 3.

			6.1.1.2 Header paths
			-> GCC will look for system headers (included with <>) in predefined paths.
			-> After that GCC searches user headers (included with "") according to the -I options:
			$gcc -I/path/to/my/headers src.c

			6.1.1.3 Library paths
			Two things are needed in order to link with a library:
				-> -larchive: link against archive, either a statically linking library (.a) or dynamically linking shared object (.so)
				-> -Lsearchdir: Add searchdir to the list of paths the linker will search. The directory is added before the system default directories.

			6.1.1.4 Debug Information
			-> -g flag instructs GCC to add debug Information.
			-> Even without -g, GCC still includes minimal DWARF debug information.
			-> Prior to shipping an application, it may be a good idea to strip the executable from all debug symbols:
			-> executables with debug information are significantly larger and occupy more disk space. However the debug sections are not loaded to RAM so there's no effect on runtime performance.
				$strip -d application

			Note, Debug information resides inside special ELF sections and is stored on the disk only. It is not loaded into memory. It contains the path to the sources (you can look for it with $strings -a application). This is way the same source code, if compiled on different machines, may generate two different object files.

			-> executables that are shipped to customers should be stripped, however its important to keep a copy of the corresponding exec compiled with -g (for debugging if an issue occurs).

			6.1.1.5 Optimization
			-> -On flag determines the optimization level, n.
			-> Linux kernel compiles with -o2
			-> Debugging and optimization don't go well together.
			Note, gdb will allow to debug optimized binaries but things will work awkwardly.
			line numbers may not match; variables may disappear (optimized);

			Note, In some cases its not possible not to optimize either because the application must be optimized (for example, an application that is using the inb() or outb() functions) or when a bug happens only in optimized application.

			-> A full listing of optimization options in gnu gcc site,http://gcc.gnu.org/onlinedocs/gcc-3.4.3/gcc/Optimize-Options.html 

			6.1.1.6 LDD
			-> View dynamic library dependencies of an application.
			$ ldd head
		        libc.so.6 => /lib/tls/libc.so.6 (0x00be3000)
		        /lib/ld-linux.so.2 (0x001b9000)

			Note, ldd -v will give recursive dependencies.

			Note, ldd shows runtime information. The results depend on the system you're checking the binary on.

			Example:
[yizaq@yizaq-lnx:Wed Apr 09:~/work/linux_debug/912/static_and_dynamic_linkage]$ ldd main_stat 
        libc.so.6 => /lib/tls/libc.so.6 (0x00be3000)
        /lib/ld-linux.so.2 (0x001b9000)
[yizaq@yizaq-lnx:Wed Apr 09:~/work/linux_debug/912/static_and_dynamic_linkage]$ ldd main_dyn 
        libdyn.so.1 => /users/yizaq/work/linux_debug/912/static_and_dynamic_linkage/libdyn.so.1 (0x007c1000)
        libc.so.6 => /lib/tls/libc.so.6 (0x00be3000)
        /lib/ld-linux.so.2 (0x001b9000)

			6.1.1.7 Symbols
			-> Global functions and variables in the sources become symbols in the object files.
			-> The linker statically links object files together using symbols
			-> Dynamic linkage uses the GOT (Global Offset Table) and the PLT (Procedure Linkage Table)

			Note, a function or variable with "static" modifier is *not* global.
			Note, The ELF includes all the GOT and PLT information. The GOT holds all variables addresses. All addresses in GOT are referenced using an offset known at link time.
			The PLT does lazy evaluation of function addresses, using the dynamic linker the first time a function in a DSO (.so) is called and storing the address so that subsequent calls don't have to go through the dynamic linker again.

			-> Symbols usually have the same name as their function or variable name.
			-> However, dynamic linkage (that allows multiple version of the same DSO) and c++ (that allows function overloading) create possible name conflicts, which is why symbol names will sometimes be mangled.

			6.1.1.8 NM
			Used in order to list symbols from an ELF object.
			Example:
$ nm libActiveDirectoryIDStore.so 
00018310 r AD_MAX_TIME
0001a338 r AD_MAX_TIME
0001d8e0 A __bss_start
0000c388 t call_gmon_start
         U cdcFreeObject

			The symbol types:
				--> Capital symbol type letter are for exported symbols, that the linker works with. Lowercase letters for the type are used for internal symbols (static or hidden).
				--> B/b the symbol is in the uninitialized data section (BSS)
				Note, a good method of finding unintialized variables in the code is 
				$ nm application  | grep -i " b "
				--> D/d the symbol is in the initialized data section (BSS)
				--> T/t the symbol is in the text (source code) section
				--> U the symbol is undefined
				--> The other types are less important, see man page for further details.

				--> BSS
In computer programming, .bss or bss is used by many compilers and linkers as the name of the data segment containing static variables that are filled solely with zero-valued data initially (ie, when execution begins). It is often referred to as the "bss section" or "bss segment". The program loader initializes the memory allocated for the bss section when it loads the program.

In an object module compiled from C, the bss section contains the local variables (but not functions) that were declared with the static keyword, except for those with non-zero initial values. (In C, static variables are initialized to zero by default.) It also contains the non-local (both extern and static) variables that are also initialized to zero (either explicitly or by default).

Historically, BSS (from Block Started by Symbol) was a pseudo-operation in UA-SAP (United Aircraft Symbolic Assembly Program), the assembler developed in the mid-1950s for the IBM 704 by Roy Nutt, Walter Ramshaw, and others at United Aircraft Corporation.

The BSS keyword was later incorporated into FAP (FORTRAN Assembly Program), IBM's standard assembler for its 709 and 7090/94 computers. It defined a label and reserved uninitialized space for a given number of words.

			6.1.1.9 objdump - ELF contents, sort of a disassembler
				-> Flags
					--> -d/-D, disassemble/disassemble all
					--> -S, intermix source code with assembly
					--> -x, display all available header information
					--> -C, demangle symbol names into user level names (C++)

				-> Usage
					-> most commonly used when we only have a crash address (EIP) and need to understand where we crashed.
					-> NM OToH is used to resolve linkage problems - in order to understand which symbols are defined where.

				-> Example:
$ objdump -DSx  main_stat
...
//int lib_a_global_var;

int main(void)
{
 8048368:   55                      push   %ebp
 8048369:   89 e5                   mov    %esp,%ebp
 804836b:   83 ec 08                sub    $0x8,%esp
 804836e:   83 e4 f0                and    $0xfffffff0,%esp
 8048371:   b8 00 00 00 00          mov    $0x0,%eax
 8048376:   83 c0 0f                add    $0xf,%eax
 8048379:   83 c0 0f                add    $0xf,%eax
 804837c:   c1 e8 04                shr    $0x4,%eax
 804837f:   c1 e0 04                shl    $0x4,%eax
 8048382:   29 c4                   sub    %eax,%esp
//    lib_a_global_var = 1234;
//    printf("lib_a_global_var = %d\n", lib_a_global_var);

    lib_a_print_msg("Message A");
 8048384:   83 ec 0c                sub    $0xc,%esp
 8048387:   68 60 85 04 08          push   $0x8048560
 804838c:   e8 3d 00 00 00          call   80483ce <lib_a_print_msg>
 8048391:   83 c4 10                add    $0x10,%esp
    lib_b_print_msg("Message B");
 8048394:   83 ec 0c                sub    $0xc,%esp
 8048397:   68 6a 85 04 08          push   $0x804856a
 804839c:   e8 ad 00 00 00          call   804844e <lib_b_print_msg>
 80483a1:   83 c4 10                add    $0x10,%esp

    return 0;
 80483a4:   b8 00 00 00 00          mov    $0x0,%eax
}
...

			6.1.1.10 Troubleshoot Static Linkage

				-> Missing symbols

				-> Using the same name for a static symbol in two different files by mistake. For example, while intending to have only one variable, it is by mistake defined as static in a header file. This is because a static variable can be defined in a header file, but this would cause each source file that included the header file to have its own private copy of the variable.

				-> Symbol collisions. For example, sometimes preprocessor macros are used to change function names.

				-> Link order, the order of -l placements is important!
				example,
				$ gcc foo.o -lz bar.o
				If bar.o uses symbols from z.a the link will fail due to undefined symbols.

				-> Linker for searches for lins in the default paths and only then in the -L paths
				-> The order of -l or -L is important, gcc links according to this order.

				-> To make an executable that is fully statically linked use the -static flag for gcc. 
Note, linking with libc can be done statically, however libc depends on libnss that has only .so version.

			6.1.1.11 Troubleshoot Dynamic Linkage

				The same as in static plus:

				-> Library interface changes require change to the DSO version

				-> Debug dynamic linker
				$ export LD_DEBUG=help
				$ ./main_dyn
Valid options for the LD_DEBUG environment variable are:

  libs        display library search paths
  reloc       display relocation processing
  files       display progress for input file
  symbols     display symbol table processing
  bindings    display information about symbol binding
  versions    display version dependencies
  all         all previous options combined
  statistics  display relocation statistics
  unused      determined unused DSOs
  help        display this help message and exit

To direct the debugging output into a file instead of standard output
a filename can be specified using the LD_DEBUG_OUTPUT environment variable.

[yizaq@yizaq-lnx:Wed Apr 09:~/work/linux_debug/912/static_and_dynamic_linkage]$ LD_DEBUG=files; ./main_dyn 
     10009:
     10009:     file=libdyn.so.1 [0];  needed by ./main_dyn [0]
     10009:     file=libdyn.so.1 [0];  generating link map
     10009:       dynamic: 0x007c687c  base: 0x007c5000   size: 0x00001984
     10009:         entry: 0x007c54f0  phdr: 0x007c5034  phnum:          4
     10009:
     10009:
     10009:     file=libc.so.6 [0];  needed by ./main_dyn [0]
     10009:     file=libc.so.6 [0];  generating link map
     10009:       dynamic: 0x00d09d3c  base: 0x00000000   size: 0x0012acdc
     10009:         entry: 0x00bf7ed0  phdr: 0x00be3034  phnum:         10
     10009:
     10009:
     10009:     calling init: /lib/tls/libc.so.6
     10009:
     10009:
     10009:     calling init: /users/yizaq/work/linux_debug/912/static_and_dynamic_linkage/libdyn.so.1
     10009:
     10009:
     10009:     initialize program: ./main_dyn
     10009:
     10009:
     10009:     transferring control: ./main_dyn
     10009:
lib_a.c - lib_a_print_msg(): Calling lib_a_static_print_msg()
lib_a.c - lib_a_static_print_msg(): Message A
lib_a.c - lib_a_print_msg(): lib_a_global_var = 0, lib_a_global_inited_var = 42, lib_a_static_var = 0
lib_b.c - lib_b_print_msg(): Calling lib_b_static_print_msg()
lib_b.c - lib_b_static_print_msg(): Message B
     10009:
     10009:     calling fini: /users/yizaq/work/linux_debug/912/static_and_dynamic_linkage/libdyn.so.1 [0]
     10009:
     10009:
     10009:     calling fini: /lib/tls/libc.so.6 [0]
     10009:


	   -> A process virtual map
	   Virtual address space of 0-4GB
addresses > 0xc0000000 are inside the kernal virtual mapping to the process
So crash address larger than 0xc0000000 is in the crash
local pointer that has address larger than 0xc0000000 is invalid and may cause unexpected behaviour


		   HW		CPU<->NMU<->Memory buss		SW generated interrupt
4GB		   Kernel		Sys calls		sys_open()
3G 0xc0000000	   Stack(s)		C lib			open()
		    ||
		   Heap
	   	   .bss (uninitialized data)
	   	   Data (initialized)
0 GB		   Text (source code)

Note, MT processes has multiple stacks (per threads)

	-> Getting information about process
	processes directory
[yizaq@yizaq-lnx:Wed Apr 09:~]$ ls /proc
1/      10502/  12151/  2331/   28359/  34/    4801/  6840/  7663/        fs/         pc
10212/  10503/  13022/  2332/   28548/  3456/  4812/  6864/  7722/        ide/        sc
10224/  10505/  1532/   2349/   2870/   3521/  4821/  6873/  8/           interrupts  se
10445/  10506/  15370/  2350/   2871/   3638/  4822/  6891/  9/           iomem       sl
10447/  10507/  15381/  24813/  29814/  3647/  4823/  6912/  9012/        ioports     st
10448/  10508/  15621/  26101/  3/      3703/  4825/  6913/  9315/        irq/        sw
10485/  10517/  15624/  26102/  30067/  3719/  4831/  6916/  9582/        kallsyms    sy
10487/  10549/  15625/  2681/   30559/  3812/  4839/  6918/  acpi/        kcore       sy
10488/  10550/  15626/  2682/   308/    3843/  4840/  6922/  asound/      keys        sy
10489/  10551/  15627/  2702/   309/    3868/  5/     6924/  buddyinfo    key-users   tt
10490/  10552/  15630/  2739/   313/    3869/  53/    6926/  bus/         kmsg        up
10491/  10554/  15633/  28296/  31621/  4/     54/    6928/  cmdline      loadavg     ve
10492/  10556/  15634/  28297/  3174/   4251/  5495/  6930/  cpuinfo      locks       vm
10493/  10558/  15635/  28298/  3179/   4427/  55/    6934/  crypto       mdstat
10494/  10560/  16027/  28299/  3189/   4658/  5526/  6948/  devices      meminfo
10495/  10562/  199/    28300/  32/     4666/  5836/  6950/  diskstats    misc
10496/  11173/  2/      28303/  3208/   4677/  6/     6952/  dma          modules
10498/  11174/  2004/   28304/  3228/   4709/  6773/  6954/  driver/      mounts@
10499/  11175/  2098/   28306/  3258/   4746/  6775/  7/     execdomains  mtrr
10500/  12147/  22006/  28308/  33/     4764/  6777/  7487/  fb           net/
10501/  12149/  22262/  28309/  332/    4790/  6785/  7489/  filesystems  partitions

1 is init process 
Get its command line
[yizaq@yizaq-lnx:Wed Apr 09:~]$ cat /proc/1/cmdline 

Address break down of the process
init [5][yizaq@yizaq-lnx:Wed Apr 09:~]$ cat /proc/1/maps
cat: /proc/1/maps: Permission denied
[yizaq@yizaq-lnx:Wed Apr 09:~]$ sudo cat /proc/1/maps
Password:
001b9000-001ce000 r-xp 00000000 08:02 770174     /lib/ld-2.3.4.so
001ce000-001cf000 r--p 00015000 08:02 770174     /lib/ld-2.3.4.so
001cf000-001d0000 rw-p 00016000 08:02 770174     /lib/ld-2.3.4.so
001d2000-001df000 r-xp 00000000 08:02 770137     /lib/libsepol.so.1
001df000-001e0000 rw-p 0000c000 08:02 770137     /lib/libsepol.so.1
001e0000-001e8000 rw-p 001e0000 00:00 0 
00590000-0059d000 r-xp 00000000 08:02 776587     /lib/libselinux.so.1
0059d000-0059e000 rw-p 0000d000 08:02 776587     /lib/libselinux.so.1
C lib
00be3000-00d08000 r-xp 00000000 08:02 776436     /lib/tls/libc-2.3.4.so
00d08000-00d09000 r--p 00124000 08:02 776436     /lib/tls/libc-2.3.4.so
00d09000-00d0c000 rw-p 00125000 08:02 776436     /lib/tls/libc-2.3.4.so
00d0c000-00d0e000 rw-p 00d0c000 00:00 0 
Source code
08048000-0804f000 r-xp 00000000 08:02 835645     /sbin/init
0804f000-08050000 rw-p 00007000 08:02 835645     /sbin/init
08a7c000-08a9d000 rw-p 08a7c000 00:00 0 
b7f36000-b7f38000 rw-p b7f36000 00:00 0 
bfe4a000-c0000000 rw-p bfe4a000 00:00 0 
ffffe000-fffff000 ---p 00000000 00:00 0 

Notice 4k segments (last three digits 000)


			6.1.1.12 IO, Socket usage report for a given process
ls -l "/proc/"`pidof proc_name`"/fd"
ex:
[root@acs-sus-e2a ~]# ls -l "/proc/"`pidof adclient`"/fd"
total 19
lrwx------  1 root gadmin 64 Nov  7 15:00 0 -> /dev/null
lrwx------  1 root gadmin 64 Nov  7 15:00 1 -> /dev/null
lrwx------  1 root gadmin 64 Nov  7 15:00 11 -> /var/centrifydc/uid.idx
lrwx------  1 root gadmin 64 Nov  7 15:00 12 -> /var/centrifydc/uname.idx
lrwx------  1 root gadmin 64 Nov  7 15:00 13 -> /var/centrifydc/gname.idx
lrwx------  1 root gadmin 64 Nov  7 15:00 14 -> /var/centrifydc/gid.idx
lrwx------  1 root gadmin 64 Nov  7 15:00 15 -> /var/centrifydc/extmgr.idx
lrwx------  1 root gadmin 64 Nov  7 15:00 17 -> socket:[42252340]
lrwx------  1 root gadmin 64 Nov  7 15:00 18 -> socket:[42252342]
lr-x------  1 root gadmin 64 Nov  7 15:00 19 -> pipe:[42252344]
lrwx------  1 root gadmin 64 Nov  7 15:00 2 -> /dev/null
l-wx------  1 root gadmin 64 Nov  7 15:00 20 -> pipe:[42252344]
lrwx------  1 root gadmin 64 Nov  7 15:00 3 -> socket:[42252315]
lrwx------  1 root gadmin 64 Nov  7 15:00 4 -> /var/run/adclient.lock
lrwx------  1 root gadmin 64 Nov  7 15:00 5 -> /var/centrifydc/gcdn.idx
lrwx------  1 root gadmin 64 Nov  7 15:00 6 -> /var/centrifydc/dcdn.idx
lrwx------  1 root gadmin 64 Nov  7 15:00 7 -> /var/centrifydc/gc.cache
lrwx------  1 root gadmin 64 Nov  7 15:00 8 -> /var/centrifydc/dc.cache
lrwx------  1 root gadmin 64 Nov  7 15:00 9 -> /var/centrifydc/search.idx

			6.1.1.13
		6.1.2 Debugging Linux Applications

			6.1.2.1 GDB Features
				-> Starting programs, attaching to running programs or debugging crashed programs
				-> Debugging locally or remotely (via gdbserver)
				-> Setting breakpoints and watchpoints
				-> Examining variables, registers and call stack
				-> Changing data and calling functions
				-> Automating debug tasks
				-> Multi threaded programs

			6.1.2.2 GDB Interfaces
				-> GDB can be used in two ways:
					--> Standalone interactive program running in the shell
					--> Back-end to a GUI program, communicating in the MI protocol

				-> The text interface is more difficult to use, but can do things not possible in GUI, like automation

			6.1.2.3 Build for debugging
				
				-> In order to effectively debug a program, we need to build it with debug information
				-> Debug information saves the relation between the program binary and its sources and is required for proper GDB operation
				-> To build a binary with debug information, pass the -g argument to GCC

				-> Note, on most compilers -o (optimization) and -g (debug) or mutually exclusive.
				-> Note, GCC, G++, support compiling with both -o and -g. However, keep in mind, that -o rearranges the code so you might see some discrepancies between the sources and the actual program
				-> Note, It may happen that some things will not work well with -g and -O, particularly on machines with instruction scheduling. In case of doubt recompile only with -g
				-> Note, Most compilers will not show information regarding preprocessor macros. GCC (versions > 3.1) can show this information if you specify the flags -gdwarf-2 and -g3. 

			6.1.2.4 Starting GDB
				
				-> Invoke:
				$ gdb application

				-> Core file:
				$ gdb application core

				-> Attach to a running process
				$gdb application pid

			6.1.2.5 Commands

				-> GDB command is a single line of input
				-> A command may be truncated if that abbreviation is unambigouos
				Note, In special cases ambigouos abbreviations are allowed. For example 's' for step
				-> A blank line (typing just RETURN) means to repeat the previous command
				Note, Certain commands (for example, run) will not repeat since they're unintentional repetition might cause unexpected behaviour.
				Note, The list and x commands when repeated with RETURN construct new arguments rather than repeating exactly as typed. This permits easy scanning of source code or memory

				-> Any text after # is comment
				-> Ctrl+o repeats the last command
				
				6.1.2.5.1 Start program
					-> To start the program use 'run'
					-> 'run' can accept arguments the same as the original program
					-> The program to run is the one specifed on the GDB command line

				6.1.2.5.2 Program Execution
					-> Ctrl+C stopes the program (if it is multi threaded, all threads are stoepd)
					-> 'step' steps into current function
						--> 'stepi' steps one machine instruction
					-> 'next' advances to next source line
					-> 'cont' continues the program

				6.1.2.5.3 Source Code listing
					-> 'list' shows the source code at the current position
						--> Additional list commands show the next lines of source
					-> Its possible to list a specific file or function
						--> function: list my_func
						(gdb) list 'ActiveDirectoryClientTest::checkConnection()'

						--> file: list file:line
						(gdb) list ActiveDirectoryClientTest.cpp:15

				6.1.2.5.4 Symbolic Debugging
				-> 'p', 'print' shows the value of a variable, address or expression
				-> 'display'/'undisplay' follow/stop following changes in above while stepping through the code
					--> Can be abbreviated to 'disp'/'undisp'
				-> 'x','examine' enables to examine memory
					$ x/20w $sp
				The command accepts three optional switches after the / - the repeat count (20), the display format and the unit size (w - word, 4 bytes). The example will print 20 words starting at the address of the stack pointer 

				-> 'call' funcname(params...) will call the function
				Usefull mainly for read only functions or functions that don't change the current state

				-> 'backtrace','bt', 'where', prints the call stack, containing all frames leading to the current function
				Note, use 'fr' to select a frame from the trace and print information from that frame.
				-> 'info threads' shows information on current threads
				-> 'info registers' will list the CPU registers and their contents, for a selected stack frame
					--> It can receive a register name as argument to print only it
				-> More info commands are available, see help info for details
					--> help info
info address -- Describe where symbol SYM is stored
info all-registers -- List of all registers and their contents
info args -- Argument variables of current stack frame
info auxv -- Display the inferior's auxiliary vector
info breakpoints -- Status of user-settable breakpoints
info catch -- Exceptions that can be caught in the current stack frame
info classes -- All Objective-C classes
info common -- Print out the values contained in a Fortran COMMON block
info copying -- Conditions for redistributing copies of GDB
info dcache -- Print information on the dcache performance
info display -- Expressions to display when program stops
info extensions -- All filename extensions associated with a source language
info files -- Names of targets and files being debugged
info float -- Print the status of the floating point unit
info frame -- All about selected stack frame
info functions -- All function names
info handle -- What debugger does when program gets various signals
info line -- Core addresses of the code for a source line
info linkmap -- Display the inferior's linkmap
info locals -- Local variables of current stack frame
info macro -- Show the definition of MACRO
info mem -- Memory region attributes
info proc -- Show /proc process information about any running process
info program -- Execution status of the program
info registers -- List of integer registers and their contents
info scope -- List the variables local to a scope
info selectors -- All Objective-C selectors
info set -- Show all GDB settings
info sharedlibrary -- Status of loaded shared object libraries
info signals -- What debugger does when program gets various signals
info source -- Information about the current source file
info sources -- Source files in the program
info stack -- Backtrace of the stack
info symbol -- Describe what symbol is at location ADDR
info target -- Names of targets and files being debugged
info terminal -- Print inferior's saved terminal status
info threads -- IDs of currently known threads
info tracepoints -- Status of tracepoints
info types -- All type names
info udot -- Print contents of kernel ``struct user'' for current child
info variables -- All global and static variable names
info vector -- Print the status of the vector unit
info warranty -- Various kinds of warranty you do not have
info watchpoints -- Synonym for ``info breakpoints''
info win -- List of all displayed windows

			6.1.2.6 Breakpoints
			-> break [LOCATION] [thread #] [if condition] will set a breakpoint
				--> LOCATION, line number, function name or an address
				--> # referes to the thread number. The brearkpoint will only be valid for that thread
				--> condition is a boolean expression. The breakpoint will only happen if its true

			-> Examples:
				-->	(gdb) break main
			break at entrance to main()

				-->	(gdb) break hello.cpp:3
			break in third line of hello.cpp

				-->	(gdb) break thread 3 *0xdeadbeef
			break when EIP value is 0xdeadbeef in thread 3
			Its useful if working with tools that allow to overwrite the process memory addresses with this value, so that if the EIP is invalid (meaning it gets out of memory boundary value that we've set earlier) the breakpoint will spring.

				-->	(gdb) break func if (my_global == 40 )
			break in func() if variable my_global equals 40

			6.1.2.7 Watchpoints
			Get an interrupt when a certain value changes.

			-> Use watch command to set watchpoints
			-> This will stop execution whenever the value of an expression changes
			-> GDB supports hardware watchpoints. usually limited to 32.
			-> GDB also supports software watchpoints but they are very slow
			-> Example, 
			(gdb) watch g_flag
			-> Use info watchpoints to see current watchpoint. The same breakpoints syntax applies for enabling and disabling watchpoints

			-> Example, bussybox crash
Crash example:
[yizaq@yizaq-lnx:Wed Apr 09:~/work/linux_debug/912/busybox/busybox-1.2.2]$ ./busybox  df
Filesystem           1k-blocks      Used Available Use% Mounted on
Floating point exception

Debug executable with debug information
[yizaq@yizaq-lnx:Wed Apr 09:~/work/linux_debug/912/busybox/busybox-1.2.2]$ gdb ./busybox_unstripped 
GNU gdb Red Hat Linux (6.3.0.0-1.132.EL4rh)
Copyright 2004 Free Software Foundation, Inc.
GDB is free software, covered by the GNU General Public License, and you are
welcome to change it and/or distribute copies of it under certain conditions.
Type "show copying" to see the conditions.
There is absolutely no warranty for GDB.  Type "show warranty" for details.
This GDB was configured as "i386-redhat-linux-gnu"...Using host libthread_db library "/lib/tls/libthread_db.so.1".

(gdb) run df 
Starting program: /users/yizaq/work/linux_debug/912/busybox/busybox-1.2.2/busybox_unstripped df
Filesystem           1k-blocks      Used Available Use% Mounted on

Program received signal SIGFPE, Arithmetic exception from CPU FPU.
0x0804e788 in __divdi3 ()
(gdb) bt
__ prefix suggests a libc function, This is the exception signal handler that GDB planted in the debug executable.
#0  0x0804e788 in __divdi3 ()

This is the point of crash
#1  0x0804ace5 in df_main (argc=1, argv=0xbfe98afc)
    at /users/yizaq/work/linux_debug/912/busybox/busybox-1.2.2/coreutils/df.c:110

#2  0x080498e4 in run_applet_by_name (name=0xbff4c63a "df", argc=1, argv=0xbfe98af8)
    at /users/yizaq/work/linux_debug/912/busybox/busybox-1.2.2/applets/applets.c:484
#3  0x08049aa5 in busybox_main (argc=2, argv=0xbfe98af4)
    at /users/yizaq/work/linux_debug/912/busybox/busybox-1.2.2/applets/busybox.c:151
#4  0x0804987c in run_applet_by_name (name=0xbff4c627 "busybox_unstripped", argc=2, argv=0xbfe98af4)
    at /users/yizaq/work/linux_debug/912/busybox/busybox-1.2.2/applets/applets.c:477
#5  0x08049963 in main (argc=2, argv=0xbfe98af4)
    at /users/yizaq/work/linux_debug/912/busybox/busybox-1.2.2/applets/busybox.c:81
(gdb) fr 1
#1  0x0804ace5 in df_main (argc=1, argv=0xbfe98afc)
    at /users/yizaq/work/linux_debug/912/busybox/busybox-1.2.2/coreutils/df.c:110
110                                     blocks_percent_used = (((long long) blocks_used) * 100 /
(gdb) list
105
106                     if ((s.f_blocks > 0) || !mount_table){
107                             blocks_used = s.f_blocks - s.f_bfree;
108                             blocks_percent_used = 0;
109                             if (blocks_used + s.f_bavail) {
110                                     blocks_percent_used = (((long long) blocks_used) * 100 /
111                                                                                blocks_percent_used
112                                                                                + (blocks_used + s.f_bavail)/2
113                                                                                ) / (blocks_used + s.f_bavail);
114                             }

			6.1.2.8 Remote Debugging
			-> Sometimes its neccessary to debug a program on a different machine
				--> Problem occurs on a remote customer server accessible only via network
				--> Can't put source on customer machine
				--> Don't have room on customer machine for source and debug information
				--> No debugger on customer machine
				--> Embedded service
				--> Its possible to debug when host and target are different platforms. 
				For example host can be X86 and target can be MIPS.
				This is achieved by GDB's cross_toolchain lib that can generate target assembly instructions and pass them to gdbserver

			-> Terms:
				--> Our workstation with debugger and source is called "host"
				--> Customer machine with the executable is called "target"

			-> On host we need:
				 Debugger, Source, Executable and libraries with debug information

			-> On target we need:
				Executable and Debugger Agent
				Note, no debug information is needed on target machine
				Note, we can generate the executable with debug information, save a copy for debugging and use "strip" to remove the debug information from the binary before installing in on customer machine
				$strip my_exe
				Note, debug information significantly increases the executable size

			-> Debugging Agent
				--> Named GDBServer, like GDB it can either start a program or attach to an already running program
				--> GDB and GDBServer communicate either via TCP or via serial port
			
			-> Example
				--> On target
					---> $gdbserver /dev/ttyS0 my_prog 12 3
					Load my_prog with parameters 12 and 3 and wait for the debugger to connect on the first serial port

					---> $gdbserver 0.0.0.0:9999 my_prog 12 3
					Load my_prog with parameters 12 and 3 and wait for the debugger to connect on TCP port 9999

					---> $gdbserver 0.0.0.0:9999 -attach 112
					Attach agent to the process with PID 112 and wait for the debugger to connect on TCP port 9999

				--> On host
					---> $gdb my_prog
					start gdb

					---> (gdb) set solib-absolute-prefix /dev/null
					     (gdb) set solib-search-path /paths/to/target/libs
					     This will set the host patch to target libraries with debug information
					     The paths are a colon separated list of directories in which to search for shared libraries.
					     Note, solib-search-path is used after solib-absolute-prefix fails to locate the library, or if the path to the library is relative instead of absolute. If you want to use solib-search-path instead of solib-absolute-prefix be sure to set solib-absolute-prefix to a nonexistent directory to prevent GDB from finding your host's libraries

					     Note, this is usually part of the cross platform toolchain directory (this being a copy of the target libs for most common platforms). It isn't included in default RH image so need to install it manually. 

					---> target remote <ip>:9999
Example:
Target:
$ gdbs
erver  :9999  ./busybox ls xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxx
Process ./busybox created; pid = 18757
Listening on port 9999
^[[DRemote debugging from host 127.0.0.1

Host:
(gdb) target remote localhost:9999
A program is being debugged already.  Kill it? (y or n) y
Remote debugging using localhost:9999
0x001b97c0 in ?? ()
(gdb) b main
Note: breakpoints 1 and 2 also set at pc 0x804990c.
Breakpoint 3 at 0x804990c: file /users/yizaq/work/linux_debug/912/busybox/busybox-1.2.2/applets/busybox.c, line 72.
(gdb) c
Continuing.

Breakpoint 1, main (argc=3, argv=0xbff62734) at /users/yizaq/work/linux_debug/912/busybox/busybox-1.2.2/applets/busybox.c:72
72              bb_applet_name=argv[0];
(gdb) c
Continuing.

Program received signal SIGSEGV, Segmentation fault.
0x0804c300 in ls_main (argc=Cannot access memory at address 0x78787880
) at /users/yizaq/work/linux_debug/912/busybox/busybox-1.2.2/coreutils/ls.c:1041
1041            dn = NULL;
(gdb) x/80w $sp

look at the address 0x78787880, check if its in the process memory address
$ cat /proc/18798/maps

			-> Tips
				--> Your first automatic breakpoint will be before main() in the guts of the C library
				for this just do, "b main" and "c"
				--> If the path to the copy of the target libraries on the host is wrong, or that the target and the host files do not match, you will see many strange errors
				--> If the target gdbserver is not installed or the file libthread_db.so is missing, you will not be able to see or debug threads

			6.1.2.9 Automation


			    6.1.2.9.1 https://stackoverflow.com/questions/10748501/what-are-the-best-ways-to-automate-a-gdb-debugging-session
I was just going through something similar, and came up with a basic example - and knowing I'll forget about it soon, I thought I'd better post it :) So I'll post it here, as it looks related to the question.

Basically, in this example I wanted to get some variable values in particular places of the code; and have them output until the program crashes. So here is first a little program which is guaranteed to crash in a few steps, test.c:

#include <stdio.h>
#include <stdlib.h>

int icount = 1; // default value

main(int argc, char *argv[])
{
  int i;

  if (argc == 2) {
    icount = atoi(argv[1]);
  }

  i = icount;
  while (i > -1) {
    int b = 5 / i;
    printf(" 5 / %d = %d \n", i, b );
    i = i - 1;
  }

  printf("Finished\n");
  return 0;
}
The only reason the program accepts command-line arguments is to be able to choose the number of steps before crashing - and to show that gdb ignores --args in batch mode. This I compile with:

gcc -g test.c -o test.exe
Then, I prepare the following script - the main trick here is to assign a command to each breakpoint, which will eventually continue (see also Automate gdb: show backtrace at every call to function puts). This script I call test.gdb:

# http://sourceware.org/gdb/wiki/FAQ: to disable the
# "---Type <return> to continue, or q <return> to quit---"
# in batch mode:
set width 0
set height 0
set verbose off

# at entry point - cmd1
b main
commands 1
  print argc
  continue
end

# printf line - cmd2
b test.c:17
commands 2
  p i
  p b
  continue
end

# int b = line - cmd3
b test.c:16
commands 3
  p i
  p b
  continue
end

# show arguments for program
show args
printf "Note, however: in batch mode, arguments will be ignored!\n"

# note: even if arguments are shown;
# must specify cmdline arg for "run"
# when running in batch mode! (then they are ignored)
# below, we specify command line argument "2":
run 2     # run

#start # alternative to run: runs to main, and stops
#continue
Note that, if you intend to use it in batch mode, you have to "start up" the script at the end, with run or start or something similar.

With this script in place, I can call gdb in batch mode - which will generate the following output in the terminal:

$ gdb --batch --command=test.gdb --args ./test.exe 5
Breakpoint 1 at 0x804844d: file test.c, line 10.
Breakpoint 2 at 0x8048485: file test.c, line 17.
Breakpoint 3 at 0x8048473: file test.c, line 16.
Argument list to give program being debugged when it is started is "5".
Note, however: in batch mode, arguments will be ignored!

Breakpoint 1, main (argc=2, argv=0xbffff424) at test.c:10
10    if (argc == 2) {
$1 = 2

Breakpoint 3, main (argc=2, argv=0xbffff424) at test.c:16
16      int b = 5 / i;
$2 = 2
$3 = 134513899

Breakpoint 2, main (argc=2, argv=0xbffff424) at test.c:17
17      printf(" 5 / %d = %d \n", i, b );
$4 = 2
$5 = 2
 5 / 2 = 2 

Breakpoint 3, main (argc=2, argv=0xbffff424) at test.c:16
16      int b = 5 / i;
$6 = 1
$7 = 2

Breakpoint 2, main (argc=2, argv=0xbffff424) at test.c:17
17      printf(" 5 / %d = %d \n", i, b );
$8 = 1
$9 = 5
 5 / 1 = 5 

Breakpoint 3, main (argc=2, argv=0xbffff424) at test.c:16
16      int b = 5 / i;
$10 = 0
$11 = 5

Program received signal SIGFPE, Arithmetic exception.
0x0804847d in main (argc=2, argv=0xbffff424) at test.c:16
16      int b = 5 / i;
Note that while we specify command line argument 5, the loop still spins only two times (as is the specification of run in the gdb script); if run didn't have any arguments, it spins only once (the default value of the program) confirming that --args ./test.exe 5 is ignored.

However, since now this is output in a single call, and without any user interaction, the command line output can easily be captured in a text file using bash redirection, say:

gdb --batch --command=test.gdb --args ./test.exe 5 > out.txt
There is also an example of using python for automating gdb in c - GDB auto stepping - automatic printout of lines, while free running?
			    6.1.2.9.2 My example 
			-> Its possible to specify GDB commands to be run automatically when a breakpoint is hit using:
			"command <breakpoint number> "
			Example:
			(gdb) b main
			(gdb) commands
			Type command ...
			end line with "end"
			>print my_var1
			>call my_func(0)
			>end

			-> Its possible to put GDB command in a text file and run it by using "source <file name> "
			Example:
$ cat gdb_adclient_cmds
b 'ActiveDirectoryClientTest::checkConnection()'
define my_command
b main
run
c
end

$gdb my_prog
			(gdb) source gdb_adclient_cmds
Breakpoint 1 at 0x805a8de: file ./test/ActiveDirectoryClientTest.cpp, line 109.

			(gdb) my_command
			-> The file .gdbinit is called in such away automatically every time GDB starts
For example:
define my_start
b main
run
c
end

More complex example:
In .gdbinit:
define ppp_cp1
source cmd.gdb
end

In cmd.gdb
b file.c:525
commands
silent
printf "event="
output event
printf "NPCNextStates="
output NPCNextStates[event][ncpp->ncp_state]
printf "\n"
end
			
Which sets a command for breakpoint on file.c:525 to print event= the event and NPCNextStates the actual value

			6.1.2.10 My frequently used commands

				6.1.2.10.1 Show thread IDs
info threads

				6.1.2.10.2 Show stacks of a list of threads
thread apply 2-56 bt

select thread
thread 1

				6.1.2.10.3 retrieve data from memory to file
a. go to relevant frame
Ex: 
fr 22 
(#22 0xb7bbd3ca in XmlManager::loadConfig (this=0x82a9de8, versionID=2, encryptKey=0xb270e3f0 "\030cK202$\006\001210e") at XmlManager.cpp:307)

b. Check source code, what vars or on stack frame?
        m_debugXmlBuffer.swap(inputSource.getDebugBuffer());
        if (!m_debugXmlBuffer.empty())
        {
            pXml = (const char*)&m_debugXmlBuffer.front();
            xmlSize = m_debugXmlBuffer.size(); 
        }

c. get size of buffer
(gdb) print xmlSize
$3 = 3908881

d. dump buffer to file
(gdb) dump memory /tmp/config_xml pXml pXml+xmlSize

				6.1.2.10.4 Print first parameter of function value

gdb) bt
#0  0xb7f26410 in ?? ()
#1  0xaf8433bc in ?? ()
#2  0x00000006 in ?? ()
#3  0x00001f09 in ?? ()
#4  0x442e9825 in _nl_load_domain () from /lib/tls/libc.so.6
#5  0x442eb289 in __gettext_free_exp () from /lib/tls/libc.so.6
#6  0x4431dcda in _IO_wstr_overflow () from /lib/tls/libc.so.6
#7  0x44323e2d in _IO_file_seekoff_mmap () from /lib/tls/libc.so.6
#8  0x44324c30 in _IO_switch_to_get_mode_internal () from /lib/tls/libc.so.6
#9  0x44326aa1 in _IO_str_pbackfail_internal () from /lib/tls/libc.so.6
#10 0x001a13b7 in operator new () from /usr/share/centrifydc/lib/libstdc++.so.6
#11 0xb78e8b7d in cims::WriteMessage () from
/usr/share/centrifydc/lib/liblrpc.so.0
#12 0xb78e9bdf in cims::Logger::log () from
/usr/share/centrifydc/lib/liblrpc.so.0
#13 0xb7b34c7b in getCredentials () from /usr/share/centrifydc/lib/libeda.so.0
#14 0xb7b485ad in ADAgent::getPlainTextUser ()
   from /usr/share/centrifydc/lib/libeda.so.0
#15 0x0807aba7 in std::_Rb_tree<ThreadPool::PooledThread*,
ThreadPool::PooledThread*, std::_Identity<ThreadPool::PooledThread*>,
std::less<ThreadPool::PooledThread*>, std::allocator<ThreadPool::PooledThread*>
>::_M_erase ()
#16 0x0807eba0 in std::_Rb_tree<ThreadPool::PooledThread*,
ThreadPool::PooledThread*, std::_Identity<ThreadPool::PooledThread*>,
std::less<ThreadPool::PooledThread*>, std::allocator<ThreadPool::PooledThread*>
>::_M_erase ()
#17 0x0808723d in std::_Rb_tree<ThreadPool::PooledThread*,
ThreadPool::PooledThread*, s---Type <return> to continue, or q <return> to quit---q Quit
(gdb) frame 14
#14 0xb7b485ad in ADAgent::getPlainTextUser ()
   from /usr/share/centrifydc/lib/libeda.so.0


(gdb) x/10xw $ebp
0xaf8442e8:     0xaf8445a8      0x0807aba7      0xaf844540      0x081ba780
0xaf8442f8:     0xaf844c40      0xaf844c50      0xaf844c70      0xaf844ba0
0xaf844308:     0x08152120      0x0821d48c
(gdb) x/10xw  0xaf844c40
0xaf844c40:     0x0821d48c      0x00000000      0xaf844c68      0x0814dfbd
0xaf844c50:     0x081f892c      0x00000002      0xaf844c82      0x0814f3d9
0xaf844c60:     0x081ec6b0      0x081efdd0
(gdb) x/s  0x0821d48c
0x821d48c:       "TEST" <=== I THINK THIS IS THE USER
(gdb) x/10xw 0xaf844c50
0xaf844c50:     0x081f892c      0x00000002      0xaf844c82      0x0814f3d9
0xaf844c60:     0x081ec6b0      0x081efdd0      0xaf844c88      0x0814e079
0xaf844c70:     0x08260d80      0x081f22d8
(gdb) x/s 0x081f892c
0x81f892c:      
"\uffff\016\uffff\uffff\uffff\uffff\u0363\uffff\223\034A\uffff\uffff\227\uffff"
(gdb) x/10xb 0x081f892c  <=== AND THIS IS THE PASSWORD
0x81f892c:      0xe5    0x0e    0xe4    0xd1    0xdd    0xe3    0xcd    0xa3
0x81f8934:      0xda    0x93

				6.1.2.10.5 Attach to running process & set breakpoint
gdb -p PID 
...
(gdb) b Certificate.cpp:2107
Breakpoint 1 at 0x9fb1d9: file Certificate.cpp, line 2107.

hit CTRL-C
(gdb) Quit
(gdb) cont
Continuing.
[Switching to Thread 0xb59c4b90 (LWP 18673)]

Breakpoint 1, Crypto::Certificate::getSANs (this=0x9d2eae0, vec=std::vector of length 0, capacity 0, requestedTypeOfSAN=SAN_TYPE_NONE) at Certificate.cpp:2107
2107    Certificate.cpp: No such file or directory.
        in Certificate.cpp

				6.1.2.10.6

			6.1.2.11
		6.1.3 Smashing the stack
		The call stack stores information about the active sobroutines of a computer program. The active sobroutines are those which have been called but have not yet completed execution by returning. This stack is also known as, "execution stack", "control stack", "function stack" or "run-time stack", and is often abbreviated to just "the stack".

			6.1.3.1 Functions of the call stack
			-> Storing the return address. To know which function to return to.
			Storing the return address on the stack has two advantages:
				First, Since each task (thread) has its own stack, the subroutine can be reenrant.
				Second, Recursion is automatically supported.

			-> Local data storage
			The local variables can be stored either on the stack, by moving the stack pointer by enough bytes to allow for the needed space, or on the heap (like new in C++, which is much slower).

			-> Parameter passing
			Usually If there are a few small parameters, processor registers are used to pass the values, but if there are more parameters then memory space will be allocated for them.

			-> Pointer to current instance. like C++ this.
			C++ (and other OO languages) store the this pointer along with function arguments in the call stack when invoking methods.

			-> This is how the call stack structure is:

			Stack pointer		Top of stack     
int DrawLine		----------------------->--+--+-----------------+-+-+
(int x1, int y1, int y2)			| Locals of            |   +           
{						| DrawLine             |    +
	int tmp;				|                      |     +
	char myarr[25];				|                      |      +
	...					+----------------------+      ++
	return 0;				| Return address       |       | Stack frame
						|                      |       ++ of 
			Frame pointer		+----------------------+        | DrawLine
			---------------------->	|Parameters of         |      +-+
}						|DrawLine              |   +|-+
						+----------------------++--+|
int DrawSquare					| Locals of            |    |
(int x1, int y1, int y2)			| DrawSquare           |    ++
{						|                      |     ++
	int tmp;				+----------------------+      ++ Stack Frame 
	char myarr[25];				| Return address       |       ++ of
	...					|                      |       -+ DrawSquare
	return 0;				+----------------------+        |
						| Parameters of        |       ++
						| DrawSquare           |      ++
						+----------------------++-----+
}						|                      |
						+----------------------+
						|                      |
						|                      |
						|                      |
						|                      |
						+----------------------+

A call stack is composed of stack frames (AKA activation records). Each stack frames corresponds to a call to a subroutine which has not yet terminated with a return. 

The stack frame at the top of the stack is the currently executing routine, DrawLine that was called by DrawSquare. Usually there are two registers used to control the stack, the stack pointer which points to the top of the stack and the frame pointer which typically points to a fixed point in the frame structure, such as the location for the return address.

Stack frames are not all the same size. Number of parameters are different. Local variables space as well. Some languages support dynamic allocations of memeory on the call stack, which makes the size of the locals on the subroutine frame vary from one activation to another (and of course unknown in compile time). For those languages, access via the framve pointer, rather than via the stack pointer, is usually neccessary since the offsets from the stack top to values such as the return address are not known at compile time.

On many systems a stack frame has a field to contain the previous value of the frame pointer register, the value it had while the caller was executing.  This is useful for accessing previous frames from the currently executing routine's frame.

			6.1.3.2 Use of the call stack
			-> Call site processing
			Push parameters into the stack and call.
			Usually the call stack manipulation needed at the site of a call to a subroutine is minimal (which is a good thing since there can be many call sites for each subroutin to be called). The values of the actual arguments are evaluated at the call site, since they are specific to the particular call, and either pushed onto the stack or placed into registers, as determined by thje calling convention being used. The actual call instruction, such as "Branch and link" is then typically executed to transfer control to the code of the targer subroutine.

			-> Callee processing
			Subroutine prologue gets parameters, saves room for locals.
			In the called subroutine, the first code executed is usually termed the subroutine prologue, since it does the necessary housekeeping before the code for the statements of the routine is begun.
			The prologue will commonly save the return address left in a register by the call instruction by pushing the value onto the stack. Similarly, the current stack pointer and/or frame pointer values may be pushed. Alternately, some instruction set architectures automatically provide comparable functionality as part of the action of the call instruction itself, and in such adn environment the prologue need not do this
			If frame pointers are being used, the prologue will typically set the new value of the frame pointer register from the stack pointer. Space on the stack for local variables can then be allocated by incrementally changing the stack pointer.

			-> Return processing
			Subroutine epilogue undo processing, pop stack, return to caller
			
			When a subroutine is rerady to return, it executes an epilogue that undoes the steps of the prologue. This will typically restore save register value (such as the frame pointer value) from the stack frame, pop the entire stack frame off the stack by changing the stack pointer value, and finally branch to the instruction at the return address. Under many calling conventions the items popped off the stack by the epilogue include the original argument values, in which case there usually are no further stack manipulations that need to be done by the caller. With some calling conventions, however, it is the caller's responsinblity to remove the arguments from the stack after the return.

			-> Unwinding
			Exception handling

			Some languages, such as C++, provide exception handling, which requires unwinding the stack. The stack frame of a function contains one or more entries specifying exception handlers. When an exception is thrown, the stack is unwound until an exception handler is found that is prepared to handle (catch) the exception. 

			6.1.3.3 Buffer overflow

				6.1.3.3.1 Buffer overflow description
			A buffer overflow is an anomalous condition where a process attempts to store data beyond the boundaries of a fixed length buffer. The result is that the extra data overwrites adjacent memeory locations. The overwritten data may include other buffers, variables and program flow data and may cause a process to crash or produce incorrect results.

				6.1.3.3.1 Buffer overflow on the stack

int main(void)				Buffer before strcpy...
{                                       +---+--+---+---+---+--+---+--+---+---+
	int B = 3;                      |A  |A |A  |A  |A  |A |A  |A |A  |A  |
	char A[8];                      +   |  |   |   |   |  |   |  |   |   |
                                        |+--+--+---+---+---+--+---+--+---+---+
	printf("%d\n", B);              |0  |0 |0  |0  |0  |0 |0  |0 |0  |3  |
	strcpy(A, "excessive");         +   |  |   |   |   |  |   |  |   |   |      
	printf("%d\n", B);              |+--+--+---+---+---+--+---+--+---+---+      
                                        
					Buffer after strcpy...
	return 0;                       +---+--+---+---+---+--+---+--+---+---+   
}                                       |A  |A |A  |A  |A  |A |A  |A |A  |A  |
                                        +   |  |   |   |   |  |   |  |   |   |
$./program                              |+--+--+---+---+---+--+---+--+---+---+
3                                       |e  |x |c  |e  |s  |s |i  |v |e  |0  |
228                                     +   |  |   |   |   |  |   |  |   |   |
                                        |+--+--+---+---+---+--+---+--+---+---+

In the example the program defines two data items which are adjacent in memeory: an 8 byte long string buffer, A and a two byte integer, B. Initially, A contains nothing but zero bytes, and B contains the number 3.

Now, the program attempts to store the character string "excessive" in the A buffer, followed by a zero byte to mark the end of the string. By not checking the length of the string, it overwrites the value of B.

although the programmer did not intend to change B at all, B's value has now been replaced by a number formed from a part of the character string. In this example, a big-endian system that uses ASCII, 'e' followed by a zero byte would become the number 25856
See, 'e' ASCII value is 101, followd by 0:
>>> int2bin(101)
'000000000000000001100101'
>>> int2bin(25856)
'000000000110010100000000'
---------|-------|-------
    0	   'e'      0

Writing a longer string could well go past the end of the memory space allocated to the program and cause an error such as a segmentation fault.

				6.1.3.3.2 Stack Buffer overflow description

An example of overflow on the stack:
foo(Z,N)				bar(X,Y)
{					{
	int A;					char A[4];
	int B;					int B;
	klunki(0,0);				strcpy(a, "HAXOR!#@!%");
	...					return;
}					}

	Normal stack (return address points to foo)
	|bar local variables  |bar ret addr  | bar parameters   | 
        +---+--+---+---+---+--+---+--+---+---+---+--+---+---+---+
        |A  |A |A  |A  |B  |B |return address| X |X |Y  |Y  |...|
        +   |  |   |   |   |  |              |   |  |   |   |   |
        |+--+--+---+---+---+--+---+--+---+---++--+--+---+---+---+
        |0  |0 |0  |0  |4  |2 |3  |2 |1  |5  |0  |0 |11 |12 |...|
        +   |  |   |   |   |  |   |  |   |   |   |  |   |   |   |
        |+--+--+---+---+---+--+---+--+---+---++--+--+---+---+---+

        Normal after overflow (return address points to somewhere else)
        |bar local variables  |bar ret addr  | bar parameters   |
        +---+--+---+---+---+--+---+--+---+---+---+--+---+---+---+
        |A  |A |A  |A  |B  |B |return address| X |X |Y  |Y  |...|
        +   |  |   |   |   |  |              |   |  |   |   |   |
        |+--+--+---+---+---+--+---+--+---+---++--+--+---+---+---+
        |H  |A |X  |O  |R  |! |6  |6 |6  |6  |0  |0 |11 |12 |...|
        +   |  |   |   |   |  |   |  |   |   |   |  |   |   |   |
        |+--+--+---+---+---+--+---+--+---+---++--+--+---+---+---+			
	
Besides changing values of unrelated variables, buffer overflows can be exploited by hackers to cause a running program to execute arbitrary supplied code. The technique available to an attacker to seek control over a process depends on the memory region where the buffer resides, for example the stack memory region, where data can be temporarily "pushed" onto the "top" of the stack, and later "popped" to read the value of the variable. Typically, when a function begins executing, temporary data items (local variables) are pushed, which remain accessible only during execution of that function.

In the example, foo() calls bar(), passing it two parameters on the stack, X and Y. bar() has two local variables, B - an integar and A- a buffer. If the function bar() causes a buffer overflow, it could write date that belongs to function foo() or even to the main program.

In the example, the return address has changed. In case this is caused by a mistake the return address will be some random point that will crash the process. However a malicious attacker could change the return address to point to program instructions that he wants to run (like give him root access).
	
				6.1.3.3.3 Protection from buffer overflows
				-> Some programming languages have internal bound checking and are fail proof to buffer overflows

				-> Use of strncpy rather than strcpy, etc

				-> Use verified libraries, such as Vstr or Erwin

				-> SSP, Stack-Smaching Protection
					compiler or library extension that adds automatic checks against stack overruns
				gcc ver > 3.3 has SSP by default. To disable it pass -fno-stack-protector

		6.1.3.4 Volatile stack references

	-> Local variables are stored on the stack
	-> After returning from the function the stack is unwound
	-> Returning a reference (pointer to) information on the stack is a bug
	-> It may be that the information will still be valid until another function is call that overwrites it.
	In any case, its a common cause for a bug.


		6.1.3.5 Symptoms
		usually buffer overflow results in crash due to override of the stack pointer or the return address. Although at some case only local (stack) variables will be garbaged.
		Crashes will usually be at the point of returning from the offending function

		Volatile stack reference symptom is usually garbaged variables.

	6.1.4 The heap and dynamic allocations

		6.1.4.1 Dynamic allocations
		-> Reffers to the allocation of memory storage during runtime
		-> The amount of memory allocated is determined at runtime
		-> A dynamic allocation exists until it is explicitly released, either "manually" by the programmer or by a garbage collector

		6.1.4.2 The heap
		-> Usually, memory is allocated from a large pool of unused memory area called the heap (since its implemented as heap array)
		-> The allocated memory is accessed via pointers (since its not possible to know the location in advance)
		-> The allocator can expand and contract the heap to fulfill allocation requests
		-> The heap method suffers form a few inherent flaws, stemming from fragmentation

		6.1.4.3 The glibc allocator
		-> The GNU C library (glibc) uses the ptmalloc allocator which uses both brk (2) and nmap (2)
		-> The brk (2) system call changes the size of the heap to be larger or smaller as needed
		-> The nmap (2) system call is used when extremely large segments are allocated
		-> The data structure used to keep track of allocation is called the "malloc arena"
		Note the (#) means you can use man # <keyword>, example man 2 brk

		6.1.4.4 The malloc arena
		Something like a linked list of memeory chunks, each chunk is made of:
		Example of allocated chunk: prev size, size, flags, Allocated memeory, ...
		Example of free chunk: prev size, size, flags, prev, next 

	    		6.1.4.4.1 allocation failure
			malloc() can fail. When no memeory is available or if the program has exceeded its memeory limit. When failed malloc() returns NULL.

			Trying to use such a NULL pointer will result in program crash.
			memory leaks and system quatas are cheif reason for malloc failures.
		
		6.1.4.5  memory leaks
		Each allocation should be at some point freed. If not freed a memory leak will occur.

		Example:
		ptr = realloc(ptr, size)

		When realloc fails it returns a null, it does not free ptr. Now since it returns a null it actually sets ptr to null preventing any possiblity to free it.

		6.1.4.6 Allocation debugging

		-> Tools, purify, valgrind etc.

		-> Or, tracing malloc:
		mtrace()
		Nees to export MALLOC_TRACE shell variable, export MALLOC_TRACE="output_malloc_traces_log".
		All uses of malloc realloc and free are traced.

		muntrace()
		Close the trace


		6.1.4.7 Use after free
		After a buffer is freed it can't be used anymore.

		int * ptr = malloc (sizeof(int))
		free ptr;

		...

		*ptr = 0

		Will cause a crash in the allocator (in libc) since such usage garbages to allocator arena.

		6.1.4.8 Double free
		free first time.
		Allocator gives another code section this memory block.
		free second time.
		The other code section is working with a free memory block.
		result, allocator crash (libc)


		6.1.4.9 Free unallocated memory

		int local_var = 3;
		free(local_var);

		6.1.4.10 Heap buffer overflow
		char *p = malloc(2);
		*p = "aaaaaa";

		Will cause a crash.

		Example:
		p = malloc(strnlen(mystr, MAX_LEN));
		strncpy(p, mystr, MAX_LEN);
		In case mystr len < MAX_LEN p will be alloced mystr len.
		Then strncpy will pad with 0 to p 

		6.1.4.11 Heap consistency check
		-> mcheck()
		-> mprobe()

		Can call those functions from gdb. They will only work on a live process (not core).
		(gdb) call mcheck()
		...
		(gdb) call mprobe(myptr)

		-> Set MALLOC_CHECK_ environment variable.
		auses glibc to do simple checks on malloc()

		6.1.4.12 Hooks for malloc
		check malloc.h, __malloc__hook


	
		6.1.4.13 Summary of possible memory management errors:
		-> Stack buffer overflow.
		Crash on return from pointer

		-> Malloc failure

		-> Memory leaks

		-> Use after free 
		Crash on use

		-> Double free
		Crash on another code segment

		-> Freeing unallocated memory
		Crash that looks to be in libc

		-> Heap buffer overflow
		Crash on write

	6.1.5 Multi-Threading and Concurrency

	
	    	6.1.5.1 Threads and processes
		Pthreads lib is not included in kernel.

		Each thread has its own:
		-> Thread id (TID), when doing ps -eLF you'll see that under column LWP.
		-> Priority, 
			Linux Kernel scheduler was changed since version 2.6 to a more RT oriented scheduler.
			The old scheduler uses the priority field (nice, 19, -20) and the new one uses the RT priority field
			RT priority:
			0		Nice levels 19 to -20		Non RT processes SCHED_OTHER
			1
			2
			...						RT processes SCHED_FIFO, SCHED_RR
			99

			SCHED_OTHER, all priority 0, fair, good for non mission critical tasks
			SCHED_FIFO, First In First Out, RT priorities 1-99, preemptive
			SCHED_RR,   Round Robin

			Example for a good MT source free project, asterisk (VOIP call switch)
		-> Register values
		-> Stack (Although pointers to data on the stack will be accessible across threads)
		-> Signal mask
		-> errno
		-> Thread specific data

		Threads share:
		-> Code
		-> Off-stack data
		-> File Descriptors
		-> Signal handlers
		-> User and group ID
		-> Current working directory

		Note, fork in MT process will create a new process with clone of the current running thread

	    	6.1.5.2 Threads and gdb

		-> thread apply [all | threadnum]
		<command>

		Example:,
		(gdb) thread apply 1 bt

		all threads :
		(gdb) thread apply all bt

		gdb print output to file:
gdb -q -n -ex "thread apply all bt" -batch /opt/pbis/sbin/lwsmd ./core.cd-ibm-19.lwsmd.30005  > ./stack-core.cd-ibm-19.lwsmd.30005

	    	6.1.5.3 Thread Synchronization Issues

	    	    	6.1.5.3.1 Race condition
			Issues that are time dependant. 
	
			Example:
			-> Static int num = 0;
			void AddOne(void){
				num++
			}

			The function is compiled to:
			-> Read value of num to register
			-> Increment the temporary register
			-> Set the new value from temporary register to num

	    	    	6.1.5.3.2

	    	6.1.5.4 Thread safety
		-> Problems happen mainly in:
		Globals on heap
		Allocating resource with global limits (files etc)
		Indirect access through pointers

		-> A function is considered thread safe it
		It only calls thread safe functions
		and accesses variables on the stack and arguments passed in

	    	6.1.5.5 achieving thread safety
		-> Mutual exclusion
		Use locks to synchronize access to shared data

		futex lib defines two primitives: sem_t and pthread_mutex_t
			--> Mutex
			pthread_mutex_init(), creates a mutex
			pthread_mutex_lock(), lock a code segment
			pthread_mutex_unlock(), unlock a code segment

		-> Re-entrancy
		Don't use statci or global variables

		-> Thread-local storage
		Use only variables that are local to each thread
		Two ways to do it, GNU __thread, or POSIX pthread_key_create() and then pthread_setspecific()


	    	6.1.5.6 Lock problems
		-> Deadlock,
		try to lock a locked mutex.

		-> Multiple locks
		Thread1		Thread2
		lock 1		lock2
		lock 2		lock1		DEADLOCK!

		Best practice, always maintaind a fixed and consistent order of locking.


	    	6.1.5.7 POSIX mutex debugging
		-> Initialize mutex with attribute PTHREAD_MUTEX_ERRORCHECK
		pthread_mutexattr_settype(&attr, PTHREAD_MUTEX_ERRORCHECK);
		Need to add -D_GNU_SOURCE compiler flag

		-> Will report common errors like double lock. The locking functions will be slower


	    	6.1.5.8 Signaling between threads
		Use either semaphores or pthread_cond_t condition variables
		
	    	6.1.5.9 compiler optimizations
		Sometimes those optimizations can have an adverse effect on concurrent code.
		Two notable things that optimizing can do:
		-> Put variables into registers
		-> re-order code

		Examples:
		static int flag = 0;

Thread A
do_something();
flag=1;

Thread B
// wait for thread A
while (!flag){
	sleep(1);
}
do_something_else();

		solution, "volatile" modifier. Tell the compiler not to optimize this variable.
		Cons, considerable slow down.

		A better solution, "memory barriers".
		Tells the compiler to make sure that all the stores prior to this point are seen.
		Tells the compiler not to reorder code between the two sides of the barrier.

		To put a barrier, asm volatile ("" : : : "memory");
		Which tells GCC to:
		-> Not move code around across barrier
		-> Flush all register values to memory 
		-> Assume that variable content may have changed after the barrier

		Note, mutexes are the better solution for locking, however they're siginficantly slower since when mutex is locked accessing threads will have to wait (sleep()) which inccurs a time expensive system call.

	    	6.1.5.10 CPU optimizations
		Resembling compiler optimizations.
		There are HW memory barriers (different from CPU to CPU).

	    	6.1.5.11 Memory management bugs and Concurrency
		Memory management bugs are enhanced by concurrency, race conditions and deadlocks.

	    	6.1.5.12 posix threads

	    	    6.1.5.12.1 thread affinity

	    	        6.1.5.12.1.1 https://eli.thegreenplace.net/2016/c11-threads-affinity-and-hyperthreading/
git: https://github.com/eliben/code-for-blog.git
code: /Users/i500695/work/code/C11/threads/threadsAffinityBlog/code-for-blog/2016/threads-affinity

	    	        6.1.5.12.1.2
	    	    6.1.5.12.2

	    	6.1.5.13
	6.1.6 programmed debug assistance
	-> Good logging practices
	-> preprocessor
	-> Signals
	-> Write stuck dump function


	    	6.1.6.1 System log
		/var/log/logfilename

		syslog (3) function to log messages via syslogd(8)
		syslogd supports remote logging, its configuration file is syslog.conf (5)

		-> logger can be used from shell to log:
		logger -p err "this is error severity syslog print"

		-> Use syslog (3) to write messages to system log
#include <syslog.h>

		void openlog(char * ident, int option, int facility)
		void syslog (int priority, const char *format, ...)
		void closelog(void)

		openlog allows to set an "identity" prefix (like application/module name)

		->Syslog typs
		Use correct log level for messages.
		Add prefix per module
		syslog is implemented using a unix domain socket, the most efficient IPC.


	    	6.1.6.2 assert
		Use glibc's assert() in debug builds. Defined in <assert.h>

		In production builds disable assert by defining the preprocessor flag NDEBUG


	    	6.1.6.3 signals
		signals are asynchronous notifications sent to a process by kernel or another process
		They interrupt the process
		The application may register a signal handler for each signal.

		Example:
		Process A sends a signal to process B (via kill)
		C lib writes that there is a signal pending for B
		When the kernel's scheduler runs B it reads that there is a pending signal for it and so runs the appropriate signal hander for B.

		-> Catching signals the old way
		register a signal handler by calling signal (3), from <signal.h>

		void (*signal(int sig,  void (*func)(int)))(int);

		-> Real Time signals
		Additional 32 signals.
		And a newer API also from <signal.h> sigaction (3)
		Its better than signal (3) since
		a. It is more portable (Sys V)
		b. It has more features

		-> There are two signal handlers by default.
		SIG_IGN, ignore signal
		SIG_DFL, use the system's default handler

		There are two signals which cannot be ignored, SIG_KILL and SIG_STOP

	    	6.1.6.4 Print stack dump

		Supported in glibc
#include <execinfo.h>

int backtrace(void **buffer, int size);
char **backtrace_symbols(void **buffer, int size);

	6.1.7 Post Mortem


		
	    	6.1.7.1 Check the logs
		tail -f /var/log/messages

		if not enough information edit to /etc/syslog.conf and change *.info to *.debug
		and /etc/init.d/syslog restart
		or pkill -1 syslogd

	    	6.1.7.2 Core dumps
		-> Some exceptions are defined to terminate the process with a core dump, a record of the process state at the point of crash
		-> Since cors take up lots of space some linux distributions disable them
		-> To enable cors of up to 500 MB (1024 blocks of 512 bytes)
		$ulimit -c 1024
		or unlimited
		$ulimit -c unlimited

		Limits are set in (for PAM)
$ cat /etc/security/limits.conf 

			--> Login process
			Linux has tty1-6 "hard" terminals
			getty is what set ups the login process
			login -> /etc/passwd (defines shell)
			passwd hashes are used for user verfication and stored in /etc/shadow
			shell

		    	    	6.1.7.2.1 Core dump configuration
				-> /proc/sys/kernel/core_pattern
				+ /proc/sys/kernel/core_uses_pid
[yizaq@yizaq-lnx:Thu Apr 10:~]$ cat /proc/sys/kernel/core_pattern 
core
Can use %e and %p (executable and pid) $echo "core.%e.%p" > /proc/sys/kernel/core_pattern
will produce the following dump files names: core.<exec name>.<pid>
format flags:
%u, uid
%g, gid
%s, signal #
%t, UNIX time of dump
%h, hostname

		    	    	6.1.7.2.2 Using cors files
				-> use file the make sure this is your dump
				$file <core file>

				-> gdb exec core

[yizaq@yizaq-lnx:Thu Apr 10:~]$ cat /proc/sys/kernel/core_uses_pid 
1

Note, those files are actually variables of the kernel process and this is actually part of a general mechanism to set values to kernel variables

				Usually the default is core.<pid>

	    	6.1.7.3

	6.1.8 Debugging tools

	
	    	6.1.8.1 strace
		Use it to trace system calls and signals when:
			-> The source code is not available
			-> The application hangs or fails and you wish to quickly get more information
		
		-> Example:
$ strace ls
core.12380
dump
dump.c
Makefile
stam
execve("/bin/ls", ["ls"], [/* 36 vars */]) = 0
uname({sys="Linux", node="yizaq-lnx", ...}) = 0
brk(0)                                  = 0x85f4000
access("/etc/ld.so.preload", R_OK)      = -1 ENOENT (No such file or directory)
open("/opt/CiscoACS/runtime/lib/tls/i686/sse2/librt.so.1", O_RDONLY) = -1 ENOENT (No such file or directory)
stat64("/opt/CiscoACS/runtime/lib/tls/i686/sse2", 0xbff02740) = -1 ENOENT (No such file or directory)
open("/opt/CiscoACS/runtime/lib/tls/i686/librt.so.1", O_RDONLY) = -1 ENOENT (No such file or directory)
stat64("/opt/CiscoACS/runtime/lib/tls/i686", 0xbff02740) = -1 ENOENT (No such file or directory)
open("/opt/CiscoACS/runtime/lib/tls/sse2/librt.so.1", O_RDONLY) = -1 ENOENT (No such file or directory)
stat64("/opt/CiscoACS/runtime/lib/tls/sse2", 0xbff02740) = -1 ENOENT (No such file or directory)
open("/opt/CiscoACS/runtime/lib/tls/librt.so.1", O_RDONLY) = -1 ENOENT (No such file or directory)
stat64("/opt/CiscoACS/runtime/lib/tls", 0xbff02740) = -1 ENOENT (No such file or directory)
open("/opt/CiscoACS/runtime/lib/i686/sse2/librt.so.1", O_RDONLY) = -1 ENOENT (No such file or directory)

		-> strace (1) can attach to a running process using -p <pid> flag
		-> you can tell strace to follow forked children with the -f option

		-> You can use strace to analyze performance by using the -c flag. Example:
		strace -c -p <PID>

	    	6.1.8.2 ltrace (1)
		Same as strace but allows to trace dynamic library calls
		It can't trace DSOs which were opened by dlopen()


	    	6.1.8.3 PTT
		POSIX Threads Trace Toolkit

		-> Building it requires patching gcc and rebuilding it

		-> Provides a very informative graphical trace 


	    	6.1.8.4 Dmalloc
		Debug Malloc library

		Dmalloc replace malloc(), free() etc with its own macros when you #include <dmalloc.h>

		-> Designed mainly for C, not C++
		-> Produces a log explaining the problems found
		-> By default it will immediately core dump when it finds heap memory problem.


	    	6.1.8.5 Valgrind
		Toolset that emulated CPU instructions and can perform:
		-> Memcheck, can find
			--> memory leaks
			--> Illegal read/write operations
			--> Uses of uninitialized variables
			--> Bad system call parameters
			--> Overlapping blocks in memcpy operations

		-> Cachegrind - Cache profiler for l1, D1 and L2 CPU caches
		-> Helgrind   - multi-threading data race detector
			--> Misuse of POSIX API
			--> deadlocks (potential)
			--> race conditions

		-> Callgrind  - heavyweight profiler
		Usage: $ valgrind --tool=callgrind proc_name

		Note, oprofile is better suited for profiling

		-> Massif     - heap profiler

		By default memcheck will be activated.
	6.1.9

	6.2

7. Networking
	7.1 Verifying Which Ports Are Listening

Once you have configured services on the network, it is important to keep tabs on which ports are actually listening on the system's network interfaces. Any open ports can be evidence of an intrusion.

There are two basic approaches for listing the ports that are listening on the network. The less reliable approach is to query the network stack by typing commands such as netstat -an or usof -i. This method is less reliable since these programs do not connect to the machine from the network, but rather check to see what is running on the system. For this reason, these applications are frequent targets for replacement by attackers. In this way, crackers attempt to cover their tracks if they open unauthorized network ports.

A more reliable way to check which ports are listening on the network is to use a port scanner such as nmap.

The following command issued from the console determines which ports are listening for TCP connections from the network:

nmap -sT -O localhost

The output of this command looks like the following:

Starting nmap V. 3.00 ( www.insecure.org/nmap/ )
Interesting ports on localhost.localdomain (127.0.0.1):
(The 1596 ports scanned but not shown below are in state: closed)
Port       State       Service
22/tcp     open        ssh
111/tcp    open        sunrpc
515/tcp    open        printer
834/tcp    open        unknown
6000/tcp   open        X11
Remote OS guesses: Linux Kernel 2.4.0 or Gentoo 1.2 Linux 2.4.19 rc1-rc7)

Nmap run completed -- 1 IP address (1 host up) scanned in 5 seconds

This output shows the system is running portmap due to the presence of the sunrpc service. However, there is also a mystery service on port 834. To check if the port is associated with the official list of known services, type:

cat /etc/services | grep 834

This command returns no output. This indicates that while the port is in the reserved range (meaning 0 through 1023) and requires root access to open, it is not associated with a known service.

Next, you can check for information about the port using netstat or lsof. To check for port 834 using netstat, use the following command:

netstat -anp | grep 834

The command returns the following output:

tcp   0    0 0.0.0.0:834    0.0.0.0:*   LISTEN   653/ypbind

The presence of the open port in netstat is reassuring because a cracker opening a port surreptitiously on a hacked system would likely not allow it to be revealed through this command. Also, the [p] option reveals the process id (PID) of the service which opened the port. In this case the open port belongs to ypbind (NIS), which is an RPC service handled in conjunction with the portmap service.

The lsof command reveals similar information since it is also capable of linking open ports to services:

lsof -i | grep 834

Below is the relevant portion of the output for this command:

ypbind      653        0    7u  IPv4       1319                 TCP *:834 (LISTEN)
ypbind      655        0    7u  IPv4       1319                 TCP *:834 (LISTEN)
ypbind      656        0    7u  IPv4       1319                 TCP *:834 (LISTEN)
ypbind      657        0    7u  IPv4       1319                 TCP *:834 (LISTEN)

As you can see, these tools can reveal a great about the status of the services running on a machine. These tools are flexible and can provide a wealth of information about network services and configuration. Consulting the man pages for lsof, netstat, nmap, and services is therefore highly recommended


	7.2 Network capturing (sniffers, sniffing)

		7.2.1 Wireshark

		7.2.2 tshark
Usage example:
tshark -i lo -x -R udp.port==20514

capture all packets on local loop sent on UDP port 20514 (ACS view syslog collector)
You need to run this as root!

	7.3 tcpdump

		7.3.1 tcpdump for Dummies
Posted on May 18, 2008, 4:29 pm, by Alexander Sandler, under System Administrator Articles.
Table of contents

Introduction
tcpdump uses
Invocation
Simple filtering
Reading tcpdump¿s output
Invocation continued
Choosing an interface
Turning off name resolution
Limiting number of packets to intercept
Saving captured data
Changing packet size in the capture file
Reading from capture file
Looking into packets
Seeing Ethernet header for each packet
Controlling time stamp
Controlling verbosity
Printing content of the packet
Packet filtering
Packet matching
More qualifiers
Specifying addresses
Other qualifiers
Complex filter expressions
Repeating qualifiers
Afterword

			7.3.1.1 Introduction

In this article  I would like to talk about one of the most useful tools in my networking toolbox and that is tcpdump. Unfortunately mastering this tool completely is not an easy task. Yet stuff you do the most is relatively simple and may become a good springboard when diving into more complex topics.

			7.3.1.2 tcpdump uses

tcpdump is a packet sniffer. It is able to capture traffic that passes through a machine. It operates on a packet level, meaning that it captures the actual packets that fly in and out of your computer. It can save the packets into a file. You can save whole packets or only the headers. Later you can ¿play¿ recorded file and apply different filters on the packets, telling tcpdump to ignore packets that you are not interested to see.

Under the hood, tcpdump understands protocols and host names. It will do all in its power to see what host sent each packet and will tell you its name instead of the IP address.

It is exceptionally useful tool for debugging what might have caused certain networking related problem. It is an excellent tool to learn new things.

			7.3.1.3 Invocation

Invoking tcpdump is easy. First thing that you have to remember is that you should either be logged in as root or  be a sudoer on the computer ¿ sudoer is someone who is entitled to gain administrator rights on computer for short period of time using sudo command.

Running tcpdump without any arguments makes it capture packets on first network interface (excluding lo) and print short description of each packet to output. This may cause a bit of a headache in case you are using network to connect to the machine. If you are connected with SSH or telnet (rlogin?), running tcpdump will produce a line of text for each incoming or outgoing packet. This line of text will cause SSH daemon to send a packet with this line, thus causing tcpdump to produce another line of text. And this will not stop until you do something about it.

			7.3.1.4 Simple filtering

So first thing that we will learn about tcpdump is how to filter out SSH and telnet packets. We will study the basics of tcpdump filtering later in this guide, but for now just remember this syntax.
view source
print?
# tcpdump not port 22

¿not port 22¿ is a filter specification that tells tcpdump to filter out packets with IP source or destination port 22. As you know port 22 is SSH port. Basically, when you tell tcpdump something like this, it will make tcpdump ignore all SSH packets ¿ exactly what we needed.

Telnet on the other hand, uses port 23. So if you are connecting via telnet, you can filter that out with:
view source
print?
# tcpdump not port 23

Clear and simple!

			7.3.1.5 Reading tcpdump¿s output

By default tcpdump produces one line of text per every packet it intercepts. Each line starts with a time stamp. It tells you very precise time when packet arrived.

Next comes protocol name. Unfortunately, tcpdump understands very limited number of protocols. It won¿t tell you the difference between packets belonging to HTTP and for instance FTP stream. Instead, it will mark such packets as IP packets. It does have some limited understanding of TCP. For instance it identifies TCP synchronization packets such as SYN, ACK, FIN and others. This information printed after source and destination IP addresses (if it IP packet).

Source and destination addresses follow protocol name. For IP packets, these are IP addresses. For other protocols, tcpdump does not print any identifiers unless explicitly asked to do so (see -e command line switch below).

Finally, tcpdump prints some information about the packet. For instance, it prints TCP sequence numbers, flags, ARP/ICMP commands, etc.

Here¿s an example of typical tcpdump output.
view source
print?
17:50:03.089893 IP 69.61.72.101.www > 212.150.66.73.48777: P 1366488174:1366488582
(408) ack 2337505545 win 7240 <nop,nop,timestamp 1491222906 477679143>

This packet is part of HTTP data stream. You can see meaning of each and every field in the packet description in tcpdump¿s manual page.

Here¿s another example
view source
print?
17:50:00.718266 arp who-has 69.61.72.185 tell 69.61.72.1

This is ARP packet. It¿s slightly more self explanatory than TCP packets. Again, to see exact meaning of each field in the packet description see tcpdump¿s manual page.

			7.3.1.6 Invocation continued

Now, when we know how to invoke tcpdump even when connecting to the computer over some net, let¿s see what command line switches are available for us.

			7.3.1.7 Choosing an interface

We¿ll start with a simple one. How to dump packets that arrived and sent through a certain network interface. -i command line argument does exactly this.
view source
print?
# tcpdump -i eth1

Will cause tcpdump to capture packets from network interface eth1. Or, considering our SSH/telnet experience:
view source
print?
# tcpdump -i eth1 not port 22

Finally, you can specify any as interface name, to tell tcpdump to listen to all interfaces.
view source
print?
# tcpdump -i any not port 22

			7.3.1.8 Turning off name resolution

As we debug networking issues, we may encounter a problem with how tcpdump works out of the box. The problem is that it tries to resolve every single IP address that it meets. I.e. when it sees an IP packet it asks DNS server for names of the computers behind IP address. It works flawlessly most of the time. However, there are two problems.

First, it slows down packet interception. It¿s not a big deal when there are only few packets, but when there are thousands and tens of thousands it introduces a delay into the process. Amount of delay can be different, depending on the traffic.

Another, much more serious problem occurs when there is no DNS server around or when DNS server is not working properly. If this is the case, tcpdump spends few seconds trying to figure out two hostnames for each IP packet. This means virtually stopping intercepting the traffic.

Luckily there is a way around. There is an option that causes tcpdump to stop detecting hostnames and that is -n.
view source
print?
# tcpdump -n

And here are few variations of how you can use this option in conjunction with options that we have learned already.
view source
print?
# tcpdump -n -i eth1
# tcpdump -ni eth1 not port 22

			7.3.1.9 Limiting number of packets to intercept

Here are few more useful options. Sometimes amount of traffic that goes in and out of your computer is very high, while all you want to see is just few packets. Often you want to see who sends you the traffic, but when you try to capture anything with tcpdump it dumps so many packets that you cannot understand anything. This is the case when -c command line switch becomes handy.

It tells tcpdump to limit number of packets it intercepts. You specify number of packets you want to see. tcpdump will capture that number of packets and exit. This is how you use it.
view source
print?
# tcpdump -c 10

Or with options that we¿ve learned before.
view source
print?
# tcpdump -ni eth1 -c 10 not port 22

This will limit number of packets that tcpdump will receive to 10. Once received 10 packets, tcpdump will exit.

			7.3.1.10 Saving captured data


			7.3.1.11 One of the most useful tcpdump features allows capturing incoming and outgoing packets into a file and then playing this file 

You can do this with -w command line switch. It should be followed by the name of the file that will contain the packets. Like this:
view source
print?
# tcpdump -w file.cap

Or adding options that we¿ve already seen
view source
print?
# tcpdump -ni eth1 -w file.cap not port 22

			7.3.1.11 Changing packet size in the capture file

By default, when capturing packets into a file, it will save only 68 bytes of the data from each packet. Rest of the information will be thrown away.

One of the things I do often when capturing traffic into a file, is to change the saved packet size. The thing is that disk space that is required to save the those few bytes is very cheap and available most of the time. Spending few spare megabytes of your disk space on capture isn¿t too painful. On the other hand, loosing valuable portion of packets might be very critical.

So, what I usually do when capturing into a file is running tcpdump with -s command line switch. It tells tcpdump how many bytes for each packet to save. Specifying 0 as a packet¿s snapshot length tells tcpdump to save whole packet. Here how it works:
view source
print?
# tcpdump -w file.cap -s 0

And with conjunction with options that we already saw:
view source
print?
# tcpdump -ni eth1 -w file.cap -s 0 -c 1000 not port 22

Obviously you can save as much data as you want. Specifying 1000 bytes will do the job for you. Just keep in mind that there are so called jumbo frames those size can be as big as 8Kb.

			7.3.1.12 Reading from capture file

Now, when we have captured some traffic into a file, we would like to play it back. -r command like switch tells tcpdump that it should read the data from a file, instead of capturing packets from interfaces. This is how it works.
view source
print?
# tcpdump -r file.cap

With capture file, we can easily analyze the packets and understand what¿s inside. tcpdump introduces several options that will help us with this task. Lets see few of them.

			7.3.1.13 Looking into packets

There are several options that allow one to see more information about the packet. There is a problem though. tcpdump in general isn¿t giving you too much information about packets. It doesn¿t understand different protocols.

If you want to see packet¿s content, it is better to use tools like Wireshark. It does understand protocols, analyzes them and allows you to see different fields, not only in TCP header, but in layer 7 protocols headers.

tcpdump is a command line tool and as most of the command line tools, its ability to present information is quiet limited. Yet, it still has few options that control the way packets presented.

			7.3.1.14 Seeing Ethernet header for each packet

-e command line switch, causes tcpdump to present Ethernet (link level protocol) header for each printed packet. Lets see an example.
view source
print?
# tcpdump -e -n not port 22

			7.3.1.15 Controlling time stamp

There are four command line switches that control the way how tcpdump prints time stamp. First, there is -t option. It makes tcpdump not to print time stamps. Next comes -tt. It causes tcpdump to print time stamp as number of seconds since Jan. 1st 1970 and a fraction of a second. -ttt prints the delta between this line and a previous one. Finally, -tttt causes tcpdump to print time stamp in it¿s regular format preceeded by date.

			7.3.1.16 Controlling verbosity

-v causes tcpdump to print more information about each packet. With -vv tcpdump prints even more information. As you could guess, -vvv produces even more information. Finally -vvvv will produce an error message telling you there is no such option :D

			7.3.1.17 Printing content of the packet

-x command line switch will make tcpdump to print each packet in hexadecimal format. Number of bytes that will be printed remains somewhat a mystery. As is, it will print first 82 bytes of the packet, excluding ethernet header. You can control number of bytes printed using -s command line switch.

In case you want to see ethernet header as well, use -xx. It will cause tcpdump to print extra 14 bytes for ethernet header.

Similarily -X and -XX will print contents of packet in hexadecimal and ASCII formats. The later will cause tcpdump to include ethernet header into printout.

			7.3.1.18 Packet filtering

We already saw a simple filter. It causes tcpdump to ignore SSH packets, allowing us to run tcpdump from remote. Now lets try to understand the language that tcpdump uses to evaluate filter expressions.

			7.3.1.19 Packet matching

We should understand that tcpdump applies our filter on every single incoming and outgoing packet. If packet matches the filter, tcpdump aknownledges the packet and depending on command line switches either saves it to file or dumps it to the screen. Otherwise, tcpdump will ignore the packet and account it only when telling how many packets received, dropped and filtered out when it exits.

To demostrate this, lets go back to not port 22 expression. tcpdump ignores packets that either sourced or destined to port 22. When such packet arrives, tcpdump applies filter on it and since the result is false, it will drop the packet.

			7.3.1.20 More qualifiers

So, from what we¿ve seen so far, we can conclude that tcpdump understands a word port and understands expression negation with not. Actually, negating an expression is part of complex expressions syntax and we will talk about complex expressions a little later. In the meantime, lets see few more packet qualifiers that we can use in tcpdump expressions.

We¿ve seen that port qualifier specifies either source or destination port number. In case we want to specify only the source port or only the destination port we can use src port or dst port. For instance, using following expression we can see all outgoing HTTP packets.
view source
print?
# tcpdump -n dst port 80

We can also specify ranges of ports. portrange, src portrange and dst portrange qualifiers do exactly this. For instance, lets see a command that captures all telnet and SSH packets.
view source
print?
# tcpdump -n portrange 22-23

			7.3.1.21 Specifying addresses

Using dst host, src host and host qualifiers you can specify source, destination or any of them IP addresses. For example
view source
print?
# tcpdump src host alexandersandler.net

Will print all packets originating from alexandersandler.net computer.

You can also specify Ethernet addresses. You do that with ether src, ether dst and ether host qualifiers. Each should be followed by MAC address of either source, destination or source or destination machines.

You can specify networks as well. The net, src net and dst net qualifiers do exactly this. Their syntax however slighly more complex than those of a single host. This is due to a netmask that has to be specified.

You can use two basic forms of network specifications. One using netmask and the other so called CIDR notation. Here are few examples.
view source
print?
# tcpdump src net 67.207.148.0 mask 255.255.255.0

Or same command using CIDR notation.
view source
print?
# tcpdump src net 67.207.148.0/24

Note the word mask that does the job of specifying the network in first example. Second example is much shorter.

			7.3.1.22 Other qualifiers

There are several useful qualifiers that don¿t fall under any of the categories I already covered.

For instance, you can specify that you are interested in packets with specific length. length qualifier does this. less and greater qualifiers tell tcpdump that you are interested in packets whose length is less or greater than value you specified.

Here¿s an example that demonstrates these qualifiers in use.
view source
print?
# tcpdump -ni eth1 greater 1000

Will capture only packets whose size is greater than 1000 bytes.


			7.3.1.23 Complex filter expressions

As we already saw we can build more complex filter expressions using tcpdump filters language. Actually, tcpdump allows exceptionally complex filtering expressions.

We¿ve seen not port 22 expression. Applying this expression on certain packet will produce logical true for packets that are not sourced or destined to port 22. Or in two words, not negates the expression.

In addition to expression negation, we can build more complex expressions combining two smaller expression into one large using and and or keywords. In addition, you can use brackets to group several expressions together.

For example, lets see a tcpdump filter that causes tcpdump to capture packets larger then 100 bytes originating from google.com or from microsoft.com.
view source
print?
# tcpdump -XX greater 100 and \(src host google.com or src host microsoft.com\)

and and or keywords in tcpdump filter language have same precedence and evaluated left to right. This means that without brackets, tcpdump could have captured packets from microsoft.com disregarding packet size. With brackets, tcpdump first makes sure that all packets are greater than 100 bytes and only then checks their origin.

Note the backslash symbol (¿\¿) before brackets. We have to place them before brackets because of shell. Unix shell has special understanding of what brackets used for. Hence we have to tell shell to leave these particular brackets alone and pass them as they are to tcpdump. Backslash characters do exactly this.

Talking about precedence, we have to keep in mind that in tcpdump¿s filter expression language not has higher precedence than and and or. tcpdump¿s manual page has very nice example and emphasizes the meaning of this.
view source
print?
not host vs and host ace

and
view source
print?
not (host vs or host ace)

are two different expressions. Because not has higher precedence over and and or, filter from the first example will capture packets not to/from vs, but to/from ace. Second filter example on the other hand will capture packets that are not to/from vs and to/from ace. I.e. first will capture packet from ace to some other host (but not to vs). Yet second example won¿t capture this packet.

			7.3.1.24 Repeating qualifiers

To conclude this article, I would like to tell you one more thing that may become handy when writing complex tcpdump filter expressions.

Take a look at the following example.
view source
print?
# tcpdump -XX greater 100 and \(src host google.com or microsoft.com\)

We already saw this example, with one little exception. In previous example we had a src host qualifier before microsoft.com and now its gone. The thing is that if we want to use same qualifier two times in a row, we don¿t have to specify it twice. Instead we can just write qualifier¿s parameter and tcpdump will know what to do.

This makes tcpdump filter expression language much easier to understand and much more readable.

	7.4 Linux IPv6 HOWTO (en)
Peter Bieringer

pb at bieringer dot de
Revision History
Revision 0.65	2009-12-13	Revised by: PB
Revision 0.64	2009-06-11	Revised by: PB
Revision 0.60	2007-05-31	Revised by: PB
Revision 0.51	2006-11-08	Revised by: PB

The goal of the Linux IPv6 HOWTO is to answer both basic and advanced questions about IPv6 on the Linux operating system. This HOWTO will provide the reader with enough information to install, configure, and use IPv6 applications on Linux machines. Intermediate releases of this HOWTO are available at mirrors.bieringer.de or mirrors.deepspace6.net. See also revision history for changes.

Table of Contents
|1. General
|
|    1.1. Copyright, license and others
|    1.2. Category
|    1.3. Version, History and To-Do
|    1.4. Translations
|    1.5. Technical
|    1.6. Preface
|    1.7. Used terms, glossary and shortcuts
|    1.8. Requirements for using this HOWTO
|
|2. Basics
|
|    2.1. What is IPv6?
|    2.2. History of IPv6 in Linux
|    2.3. What do IPv6 addresses look like?
|    2.4. FAQ (Basics)
|
|3. Address types
|
|    3.1. Addresses without a special prefix
|    3.2. Network part, also known as prefix
|    3.3. Address types (host part)
|    3.4. Prefix lengths for routing
|
|4. IPv6-ready system check
|
|    4.1. IPv6-ready kernel
|    4.2. IPv6-ready network configuration tools
|    4.3. IPv6-ready test/debug programs
|    4.4. IPv6-ready programs
|    4.5. IPv6-ready client programs (selection)
|    4.6. IPv6-ready server programs
|    4.7. FAQ (IPv6-ready system check)
|
|5. Configuring interfaces
|
|    5.1. Different network devices
|    5.2. Bringing interfaces up/down
|
|6. Configuring IPv6 addresses
|
|    6.1. Displaying existing IPv6 addresses
|    6.2. Add an IPv6 address
|    6.3. Removing an IPv6 address
|
|7. Configuring normal IPv6 routes
|
|    7.1. Displaying existing IPv6 routes
|    7.2. Add an IPv6 route through a gateway
|    7.3. Removing an IPv6 route through a gateway
|    7.4. Add an IPv6 route through an interface
|    7.5. Removing an IPv6 route through an interface
|    7.6. FAQ for IPv6 routes
|
|8. Neighbor Discovery
|
|    8.1. Displaying neighbors using “ip”
|    8.2. Manipulating neighbors table using “ip”
|
|9. Configuring IPv6-in-IPv4 tunnels
|
|    9.1. Types of tunnels
|    9.2. Displaying existing tunnels
|    9.3. Setup of point-to-point tunnel
|    9.4. Setup of 6to4 tunnels
|
|10. Configuring IPv4-in-IPv6 tunnels
|
|    10.1. Displaying existing tunnels
|    10.2. Setup of point-to-point tunnel
|    10.3. Removing point-to-point tunnels
|
|11. Kernel settings in /proc-filesystem
|
|    11.1. How to access the /proc-filesystem
|    11.2. Entries in /proc/sys/net/ipv6/
|    11.3. IPv6-related entries in /proc/sys/net/ipv4/
|    11.4. IPv6-related entries in /proc/net/
|
|12. Netlink-Interface to kernel
|13. Address Resolver
|14. Network debugging
|
|    14.1. Server socket binding
|    14.2. Examples for tcpdump packet dumps
|
|15. Support for persistent IPv6 configuration in Linux distributions
|
|    15.1. Red Hat Linux and “clones”
|    15.2. SuSE Linux
|    15.3. Debian Linux
|
|16. Auto-configuration
|
|    16.1. Stateless auto-configuration
|    16.2. Stateful auto-configuration using Router Advertisement Daemon (radvd)
|    16.3. Dynamic Host Configuration Protocol v6 (DHCPv6)
|
|17. Mobility
|
|    17.1. Common information
|
|18. Firewalling
|
|    18.1. Firewalling using netfilter6
|    18.2. Preparation
|    18.3. Usage
|
|19. Security
|
|    19.1. Node security
|    19.2. Access limitations
|    19.3. IPv6 security auditing
|
|20. Encryption and Authentication
|
|    20.1. Modes of using encryption and authentication
|    20.2. Support in kernel (ESP and AH)
|    20.3. Automatic key exchange (IKE)
|    20.4. Additional informations:
|
|21. Quality of Service (QoS)
|22. Hints for IPv6-enabled daemons
|
|    22.1. Berkeley Internet Name Domain (BIND) daemon “named”
|    22.2. Internet super daemon (xinetd)
|    22.3. Webserver Apache2 (httpd2)
|    22.4. Router Advertisement Daemon (radvd)
|    22.5. Dynamic Host Configuration v6 Server (dhcp6s)
|    22.6. ISC Dynamic Host Configuration Server (dhcpd)
|    22.7. DHCP Server Dibbler
|    22.8. tcp_wrapper
|    22.9. vsftpd
|    22.10. proftpd
|    22.11. Other daemons
|
|23. Programming
|
|    23.1. Programming using C-API
|    23.2. Other programming languages
|
|24. Interoperability
|25. Further information and URLs
|
|    25.1. Paper printed books, articles, online reviews (mixed)
|    25.2. Conferences, Meetings, Summits
|    25.3. Online information
|    25.4. IPv6 Infrastructure
|    25.5. Maillists
|    25.6. Online tools
|    25.7. Trainings, Seminars
|    25.8. 'The Online Discovery' ...
|
|26. Revision history / Credits / The End
|
|    26.1. Revision history
|    26.2. Credits
|    26.3. The End
|
Chapter 1. General

Information about available translations you will find in section Translations.
|1.1. Copyright, license and others
|1.1.1. Copyright

Written and Copyright (C) 2001-2009 by Peter Bieringer
|1.1.2. License

This Linux IPv6 HOWTO is published under GNU GPL version 2:

The Linux IPv6 HOWTO, a guide how to configure and use IPv6 on Linux systems.

Copyright © 2001-2009 Peter Bieringer

This documentation is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 2 of the License, or (at your option) any later version.

This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.

You should have received a copy of the GNU General Public License along with this program; if not, write to the Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110, USA.
|1.1.3. About the author
|1.1.3.1. Internet/IPv6 history of the author

|    1993: I got in contact with the Internet using console based e-mail and news client (e.g. look for “e91abier” on groups.google.com, that's me).

|    1996: I got a request for designing a course on IPv6, including a workshop with the Linux operating system.

|    1997: Started writing a guide on how to install, configure and use IPv6 on Linux systems, called IPv6 & Linux - HowTo (see IPv6 & Linux - HowTo/History for more information).

|    2001: Started writing this new Linux IPv6 HOWTO. 

|1.1.3.2. Contact

The author can be contacted via e-mail at <pb at bieringer dot de> and also via his homepage.

He's currently living in Munich [northern part of Schwabing] / Bavaria / Germany (south) / Europe (middle) / Earth (surface/mainland).
|1.2. Category

This HOWTO should be listed in category “Networking/Protocols”.
|1.3. Version, History and To-Do
|1.3.1. Version

The current version is shown at the beginning of the document.

CVS information: CVS-ID: $Id: Linux+IPv6-HOWTO.sgml,v 1.42 2009/12/20 14:22:42 pbldp Exp $

For other available versions/translations see also http://www.bieringer.de/linux/IPv6/.
|1.3.2. History
|1.3.2.1. Major history

|2001-11-30: Starting to design new HOWTO.

|2002-01-02: A lot of content completed, first public release of chapter 1 (version 0.10).

|2002-01-14: More completed, some reviews, public release of the whole document (version 0.14).

|2002-08-16: Polish translation is in progress

|2002-10-31: Chinese translation is available (see Translations for more)

|2002-11-10: German translation is in progress

|2003-02-10: German translation is available

|2003-04-09: French translation is in progress

|2003-05-09: French translation is available

|2003-10-16: Italian translation is in progress

|2004-03-12: Italian translation is available

|2004-06-18: Greek translation is in progress

|2005-07-25: Turkish translation is availble

|2007-03-28: Portuguese-Brazil is in progress

|2008-07-30: Spanish translation is available (but still in progress)
|1.3.2.2. Full history

See revision history at the end of this document.
|1.3.3. To-Do

    Fill in missing content

    Finishing grammar checking

|1.4. Translations

Translations always have to contain the URL, version number and copyright of the original document (but yours, too). Pls. don't translate the original changelog, this is not very useful - also do not translate the full section about available translations, can be run out-of-date, add an URL to this section here in the English howto.

Looks like the document's change frequency is mostly less than once per month. Since version 0.27 it looks like that most of the content contributed by me has been written. Translations always have to use the English version as source.
|1.4.1. To language

Note: an overview with URLs can be found at http://www.bieringer.de/linux/IPv6/.
|1.4.1.1. Chinese

A Chinese translation by Burma Chen <expns at yahoo dot com> (announced to me at 2002-10-31) can be found on the TLDP: http://www.ibiblio.org/pub/Linux/docs/HOWTO/translations/zh/Linux-IPv6-HOWTO.txt.gz (g'zipped txt). It's a snapshot translation, don't know whether kept up-to-date.
|1.4.1.2. Polish

Since 2002-08-16 a Polish translation was started and is still in progress by Lukasz Jokiel <Lukasz dot Jokiel at klonex dot com dot pl>. Taken source: CVS-version 1.29 of LyX file, which was source for howto version 0.27. Status is still work-in-progress (2004-08-30).
|1.4.1.3. German

With 2002-11-10 a German translation was started by Georg Käfer <gkaefer at gmx dot at> and the first public version was published 2003-02-10. It's originally available on Deep Space 6 at http://mirrors.deepspace6.net/Linux+IPv6-HOWTO-de/ (mirrored e.g. on http://mirrors.bieringer.de/Linux+IPv6-HOWTO-de/). This version will stay up-to-date as much as possible.
|1.4.1.4. French

With 2003-04-09 a French translation was started by Michel Boucey <mboucey at free dot fr> and the first public version was published 2003-05-09. It's originally available on Deep Space 6 at http://mirrors.deepspace6.net/Linux+IPv6-HOWTO-fr/ (mirrored e.g. on http://mirrors.bieringer.de/Linux+IPv6-HOWTO-fr/).
|1.4.1.5. Spanish

A member of the MontevideoLibre, a project in Uruguay (South America) starts the translation into Spanish in wiki format: http://www.montevideolibre.org./manuales:libros:ipv6
|1.4.1.6. Italian

With 2003-10-16 a Italian translation was started by Michele Ferritto <m dot ferritto at virgilio dot it> for the ILDP (Italian Linux Documentation Project) and the first public version was published 2004-03-12. It's originally available on the ILDP at http://it.tldp.org/HOWTO/Linux+IPv6-HOWTO/.
|1.4.1.7. Japanese

On 2003-05-14 Shino Taketani <shino_1305 at hotmail dot com> send me a note that he planned to translate the HowTo into Japanese.
|1.4.1.8. Greek

On 2004-06-18 Nikolaos Tsarmpopoulos <ntsarb at uth dot gr> send me a note that he planned to translate the HowTo into Greek.
|1.4.1.9. Turkish

On 2005-07-18 Necdet Yucel <nyucel at comu dot edu dot tr> send me a note that a Turkish translation is available. It's a snapshot translation (currently of 0.61) and can be found at http://docs.comu.edu.tr/howto/ipv6-howto.html.
|1.4.1.10. Portuguese-Brazil

On 2007-03-28 Claudemir da Luz <claudemir dot daluz at virtuallink dot com dot br> send me a note that he planned to translate the HowTo in Portuguese-Brazil.
|1.5. Technical
|1.5.1. Original source of this HOWTO

This HOWTO is currently written with LyX version 1.6.1 on a Fedora 10 Linux system with template SGML/XML (DocBook book). It's available on TLDP-CVS / users / Peter-Bieringer for contribution.
|1.5.1.1. Code line wrapping

Code line wrapping is done using selfmade utility “lyxcodelinewrapper.pl”, you can get it from CVS for your own usage: TLDP-CVS / users / Peter-Bieringer
|1.5.1.2. SGML generation

SGML/XML is generated using export function in LyX.
|1.5.2. On-line references to the HTML version of this HOWTO (linking/anchors)
|1.5.2.1. Master index page

Generally, a reference to the master index page is recommended.
|1.5.2.2. Dedicated pages

Because the HTML pages are generated out of the SGML file, the HTML filenames turn out to be quite random. However, some pages are tagged in LyX, resulting in static names. These tags are useful for references and shouldn't be changed in the future.

If you think that I have forgotten a tag, please let me know, and I will add it.
|1.6. Preface

Some things first:
|1.6.1. How many versions of a Linux & IPv6 related HOWTO are floating around?

Including this, there are three (3) HOWTO documents available. Apologies, if that is too many ;-)
|1.6.1.1. Linux IPv6 FAQ/HOWTO (outdated)

The first IPv6 related document was written by Eric Osborne and called Linux IPv6 FAQ/HOWTO (please use it only for historical issues). Latest version was 3.2.1 released July, 14 1997.

Please help: if someone knows the date of birth of this HOWTO, please send me an e-mail (information will be needed in “history”).
|1.6.1.2. IPv6 & Linux - HowTo (maintained)

There exists a second version called IPv6 & Linux - HowTo written by me (Peter Bieringer) in pure HTML. It was born April 1997 and the first English version was published in June 1997. I will continue to maintain it, but it will slowly fade (but not full) in favour of the Linux IPv6 HOWTO you are currently reading.
|1.6.1.3. Linux IPv6 HOWTO (this document)

Because the IPv6 & Linux - HowTo is written in pure HTML it's not really compatible with the The Linux Documentation Project (TLDP). I (Peter Bieringer) got a request in late November 2001 to rewrite the IPv6 & Linux - HowTo in SGML. However, because of the discontinuation of that HOWTO (Future of IPv6 & Linux - HowTo), and as IPv6 is becoming more and more standard, I decided to write a new document covering basic and advanced issues which will remain important over the next few years. More dynamic and some advanced content will be still found further on in the second HOWTO (IPv6 & Linux - HowTo).
|1.7. Used terms, glossary and shortcuts
|1.7.1. Network related

Base 10

    Well known decimal number system, represent any value with digit 0-9.
Base 16

    Usually used in lower and higher programming languages, known also as hexadecimal number system, represent any value with digit 0-9 and char A-F (case insensitive).
Base 85

    Representation of a value with 85 different digits/chars, this can lead to shorter strings but never seen in the wild.
Bit

    Smallest storage unit, on/true (1) or off/false (0)
Byte

    Mostly a collection of 8 (but not really a must - see older computer systems) bits
Device

    Here, hardware of network connection, see also NIC
Dual homed host

    A dual homed host is a node with two network (physical or virtual) interfaces on two different links, but does not forward any packets between the interfaces.
Host

    Generally a single homed host on a link. Normally it has only one active network interface, e.g. Ethernet or (not and) PPP.
Interface

    Mostly same as “device”, see also NIC
IP Header

    Header of an IP packet (each network packet has a header, kind of is depending on network layer)
Link

    A link is a layer 2 network packet transport medium, examples are Ethernet, Token Ring, PPP, SLIP, ATM, ISDN, Frame Relay,...
Node

    A node is a host or a router.
Octet

    A collection of 8 real bits, today also similar to “byte”.
Port

    Information for the TCP/UDP dispatcher (layer 4) to transport information to upper layers
Protocol

    Each network layer contains mostly a protocol field to make life easier on dispatching transported information to upper layer, seen in layer 2 (MAC) and 3 (IP)
Router

    A router is a node with two or more network (physical or virtual) interfaces, capable of forwarding packets between the interfaces.
Socket

    An IP socket is defined by source and destination IP addresses and Ports and (binding) 
Stack

    Network related a collection of layers
Subnetmask

    IP networks uses bit masks to separate local networks from remote ones
Tunnel

    A tunnel is typically a point-to-point connection over which packets are exchanged which carry the data of another protocol, e.g. an IPv6-in-IPv4 tunnel.

|1.7.1.1. Shortcuts

ACL

    Access Control List
API

    Application Programming Interface
ASIC

    Application Specified Integrated Circuit
BSD

    Berkeley Software Distribution
CAN-Bus

    Controller Area Network Bus (physical bus system)
ISP

    Internet Service Provider
KAME

    Project - a joint effort of six companies in Japan to provide a free IPv6 and IPsec (for both IPv4 and IPv6) stack for BSD variants to the world www.kame.net
LIR

    Local Internet Registry
NIC

    Network Interface Card
RFC

    Request For Comments - set of technical and organizational notes about the Internet
USAGI

    UniverSAl playGround for Ipv6 Project - works to deliver the production quality IPv6 protocol stack for the Linux system.

|1.7.2. Document related
|1.7.2.1. Long code line wrapping signal char

The special character “¬” is used for signaling that this code line is wrapped for better viewing in PDF and PS files.
|1.7.2.2. Placeholders

In generic examples you will sometimes find the following:

<myipaddress>

For real use on your system command line or in scripts this has to be replaced with relevant content (removing the < and > of course), the result would be e.g.

|1.2.3.4

|1.7.2.3. Commands in the shell

Commands executable as non-root user begin with $, e.g.

$ whoami

Commands executable as root user begin with #, e.g.

# whoami

|1.8. Requirements for using this HOWTO
|1.8.1. Personal prerequisites
|1.8.1.1. Experience with Unix tools

You should be familiar with the major Unix tools e.g. grep, awk, find, ... , and know about their most commonly used command-line options.
|1.8.1.2. Experience with networking theory

You should know about layers, protocols, addresses, cables, plugs, etc. If you are new to this field, here is one good starting point for you: http://www.rigacci.org/docs/biblio/online/intro_to_networking/book1.htm
|1.8.1.3. Experience with IPv4 configuration

You should definitely have some experience in IPv4 configuration, otherwise it will be hard for you to understand what is really going on.
|1.8.1.4. Experience with the Domain Name System (DNS)

Also you should understand what the Domain Name System (DNS) is, what it provides and how to use it.
|1.8.1.5. Experience with network debugging strategies

You should at least understand how to use tcpdump and what it can show you. Otherwise, network debugging will very difficult for you.
|1.8.2. Linux operating system compatible hardware

Surely you wish to experiment with real hardware, and not only read this HOWTO to fall asleep here and there. ;-7)
Chapter 2. Basics
|2.1. What is IPv6?

IPv6 is a new layer 3 protocol which will supersede IPv4 (also known as IP). IPv4 was designed long time ago (RFC 760 / Internet Protocol from January 1980) and since its inception, there have been many requests for more addresses and enhanced capabilities. Latest RFC is RFC 2460 / Internet Protocol Version 6 Specification. Major changes in IPv6 are the redesign of the header, including the increase of address size from 32 bits to 128 bits. Because layer 3 is responsible for end-to-end packet transport using packet routing based on addresses, it must include the new IPv6 addresses (source and destination), like IPv4.

For more information about the IPv6 history take a look at older IPv6 related RFCs listed e.g. at SWITCH IPv6 Pilot / References.
|2.2. History of IPv6 in Linux

The years 1992, 1993 and 1994 of the IPv6 History (in general) are covered by following document: IPv6 or IPng (IP next generation).

To-do: better time-line, more content...
|2.2.1. Beginning

The first IPv6 related network code was added to the Linux kernel 2.1.8 in November 1996 by Pedro Roque. It was based on the BSD API:

diff -u --recursive --new-file v2.1.7/linux/include/linux/in6.h
¬ linux/include/linux/in6.h 
--- v2.1.7/linux/include/linux/in6.h Thu Jan 1 02:00:00 1970 
+++ linux/include/linux/in6.h Sun Nov 3 11:04:42 1996 
@@ -0,0 +1,99 @@ 
+/* 
+ * Types and definitions for AF_INET6 
+ * Linux INET6 implementation 
+ * + * Authors: 
+ * Pedro Roque <******> 
+ * 
+ * Source: 
+ * IPv6 Program Interfaces for BSD Systems 
+ * <draft-ietf-ipngwg-bsd-api-05.txt>

The shown lines were copied from patch-2.1.8 (e-mail address was blanked on copy&paste).
|2.2.2. In between

Because of lack of manpower, the IPv6 implementation in the kernel was unable to follow the discussed drafts or newly released RFCs. In October 2000, a project was started in Japan, called USAGI, whose aim was to implement all missing, or outdated IPv6 support in Linux. It tracks the current IPv6 implementation in FreeBSD made by the KAME project. From time to time they create snapshots against current vanilla Linux kernel sources.

Until kernel development series 2.5.x was started, the USAGI patch was so big, that Linux networking maintainers were unable to include it completly in the production source of the Linux kernel 2.4.x series.

During kernel development series 2.5.x, USAGI tried to insert all of their current extensions into this.
|2.2.3. Current

Many of the long-term developed IPv6 related patches by USAGI and others are integrated into vanilla kernel series 2.6.x.
|2.2.4. Future

USAGI and others are still working on implementation of newer features like mobililty and others. From time to time, new extension patches are released and also integration into vanilla kernel series is made.
|2.3. What do IPv6 addresses look like?

As previously mentioned, IPv6 addresses are 128 bits long. This number of bits generates very high decimal numbers with up to 39 digits:

|2^128-1: 340282366920938463463374607431768211455

Such numbers are not really addresses that can be memorized. Also the IPv6 address schema is bitwise orientated (just like IPv4, but that's not often recognized). Therefore a better notation of such big numbers is hexadecimal. In hexadecimal, 4 bits (also known as “nibble”) are represented by a digit or character from 0-9 and a-f (10-15). This format reduces the length of the IPv6 address to 32 characters.

|2^128-1: 0xffffffffffffffffffffffffffffffff

This representation is still not very convenient (possible mix-up or loss of single hexadecimal digits), so the designers of IPv6 chose a hexadecimal format with a colon as separator after each block of 16 bits. In addition, the leading "0x" (a signifier for hexadecimal values used in programming languages) is removed:

|2^128-1: ffff:ffff:ffff:ffff:ffff:ffff:ffff:ffff

A usable address (see address types later) is e.g.:

|2001:0db8:0100:f101:0210:a4ff:fee3:9566

For simplifications, leading zeros of each 16 bit block can be omitted:

|2001:0db8:0100:f101:0210:a4ff:fee3:9566  -> 
¬ 2001:db8:100:f101:210:a4ff:fee3:9566

One sequence of 16 bit blocks containing only zeroes can be replaced with “::“. But not more than one at a time, otherwise it is no longer a unique representation.

|2001:0db8:100:f101:0:0:0:1  ->  2001:db8:100:f101::1

The biggest reduction is seen by the IPv6 localhost address:

|0000:0000:0000:0000:0000:0000:0000:0001  ->  ::1

There is also a so-called compact (base85 coded) representation defined RFC 1924 / A Compact Representation of IPv6 Addresses (published on 1. April 1996), never seen in the wild, probably an April fool's joke, but here is an example:

# ipv6calc --addr_to_base85 2001:0db8:0100:f101:0210:a4ff:fee3:9566 
|9R}vSQZ1W=9A_Q74Lz&R

    Info: ipv6calc is an IPv6 address format calculator and converter program and can be found here: ipv6calc homepage (Mirror)

|2.4. FAQ (Basics)
|2.4.1. Why is the name IPv6 and not IPv5 as successor for IPv4?

On any IP header, the first 4 bits are reserved for protocol version. So theoretically a protocol number between 0 and 15 is possible:

|    4: is already used for IPv4

|    5: is reserved for the Stream Protocol (STP, RFC 1819 / Internet Stream Protocol Version 2) (which never really made it to the public)

The next free number was 6. Hence IPv6 was born!
|2.4.2. IPv6 addresses: why such a high number of bits?

During the design of IPv4, people thought that 32 bits were enough for the world. Looking back into the past, 32 bits were enough until now and will perhaps be enough for another few years. However, 32 bits are not enough to provide each network device with a global address in the future. Think about mobile phones, cars (including electronic devices on its CAN-bus), toasters, refrigerators, light switches, and so on...

So designers have chosen 128 bits, 4 times more in length than in IPv4 today.

The usable size is smaller than it may appear however. This is because in the currently defined address schema, 64 bits are used for interface identifiers. The other 64 bits are used for routing. Assuming the current strict levels of aggregation (/48, /32, ...), it is still possible to “run out” of space, but hopefully not in the near future.

See also for more information RFC 1715 / The H Ratio for Address Assignment Efficiency and RFC 3194 / The Host-Density Ratio for Address Assignment Efficiency.
|2.4.3. IPv6 addresses: why so small a number of bits on a new design?

While, there are (possibly) some people (only know about Jim Fleming...) on the Internet who are thinking about IPv8 and IPv16, their design is far away from acceptance and implementation. In the meantime 128 bits was the best choice regarding header overhead and data transport. Consider the minimum Maximum Transfer Unit (MTU) in IPv4 (576 octets) and in IPv6 (1280 octets), the header length in IPv4 is 20 octets (minimum, can increase to 60 octets with IPv4 options) and in IPv6 is 40 octets (fixed). This is 3.4 % of minimum MTU in IPv4 and 3.1 % of minimum MTU in IPv6. This means the header overhead is almost equal. More bits for addresses would require bigger headers and therefore more overhead. Also, consider the maximum MTU on normal links (like Ethernet today): it's 1500 octets (in special cases: 9k octets using Jumbo frames). Ultimately, it wouldn't be a proper design if 10 % or 20 % of transported data in a Layer-3 packet were used for addresses and not for payload.
Chapter 3. Address types

Like IPv4, IPv6 addresses can be split into network and host parts using subnet masks.

IPv4 has shown that sometimes it would be nice, if more than one IP address can be assigned to an interface, each for a different purpose (aliases, multi-cast). To remain extensible in the future, IPv6 is going further and allows more than one IPv6 address to be assigned to an interface. There is currently no limit defined by an RFC, only in the implementation of the IPv6 stack (to prevent DoS attacks).

Using this large number of bits for addresses, IPv6 defines address types based on some leading bits, which are hopefully never going to be broken in the future (unlike IPv4 today and the history of class A, B, and C).

Also the number of bits are separated into a network part (upper 64 bits) and a host part (lower 64 bits), to facilitate auto-configuration.
|3.1. Addresses without a special prefix
|3.1.1. Localhost address

This is a special address for the loopback interface, similiar to IPv4 with its “127.0.0.1”. With IPv6, the localhost address is:

|0000:0000:0000:0000:0000:0000:0000:0001 

or compressed:

::1

Packets with this address as source or destination should never leave the sending host.
|3.1.2. Unspecified address

This is a special address like “any” or “0.0.0.0” in IPv4 . For IPv6 it's:

|0000:0000:0000:0000:0000:0000:0000:0000 

or:

::

These addresses are mostly used/seen in socket binding (to any IPv6 address) or routing tables.

Note: the unspecified address cannot be used as destination address.
|3.1.3. IPv6 address with embedded IPv4 address

There are two addresses which contain an IPv4 address.
|3.1.3.1. IPv4-mapped IPv6 address

IPv4-only IPv6-compatible addresses are sometimes used/shown for sockets created by an IPv6-enabled daemon, but only binding to an IPv4 address.

These addresses are defined with a special prefix of length 96 (a.b.c.d is the IPv4 address):

|0:0:0:0:0:ffff:a.b.c.d/96

or in compressed format

::ffff:a.b.c.d/96

For example, the IPv4 address 1.2.3.4 looks like this:

::ffff:1.2.3.4

|3.1.3.2. IPv4-compatible IPv6 address

Used for automatic tunneling (RFC 2893 / Transition Mechanisms for IPv6 Hosts and Routers), which is being replaced by 6to4 tunneling.

|0:0:0:0:0:0:a.b.c.d/96

or in compressed format

::a.b.c.d/96

|3.2. Network part, also known as prefix

Designers defined some address types and left a lot of scope for future definitions as currently unknown requirements arise. RFC 4291 / IP Version 6 Addressing Architecture defines the current addressing scheme.

Now lets take a look at the different types of prefixes (and therefore address types):
|3.2.1. Link local address type

These are special addresses which will only be valid on a link of an interface. Using this address as destination the packet would never pass through a router. It's used for link communications such as:

    anyone else here on this link?

    anyone here with a special address (e.g. looking for a router)?

They begin with ( where “x” is any hex character, normally “0”)

fe8x:  <- currently the only one in use
fe9x:
feax:
febx:

An address with this prefix is found on each IPv6-enabled interface after stateless auto-configuration (which is normally always the case).
|3.2.2. Site local address type

These are addresses similar to the RFC 1918 / Address Allocation for Private Internets in IPv4 today, with the added advantage that everyone who use this address type has the capability to use the given 16 bits for a maximum number of 65536 subnets. Comparable with the 10.0.0.0/8 in IPv4 today.

Another advantage: because it's possible to assign more than one address to an interface with IPv6, you can also assign such a site local address in addition to a global one.

It begins with:

fecx:  <- most commonly used
fedx:
feex:
fefx:

(where “x” is any hex character, normally “0”)

This address type is now deprecated RFC 3879 / Deprecating Site Local Addresses, but for a test in a lab, such addresses are still a good choice in my humble opinion.
|3.2.3. Unique Local IPv6 Unicast Addresses

Because the original defined site local addresses are not unique, this can lead to major problems, if two former independend networks would be connected later (overlapping of subnets). This and other issues lead to a new address type named RFC 4193 / Unique Local IPv6 Unicast Addresses.

It begins with:

fcxx:
fdxx:  <- currently the only one in use

A part of the prefix (40 bits) are generated using a pseudo-random algorithm and it's improbable, that two generated ones are equal.

Example for a prefix (generated using a web-based tool: Goebel Consult / createLULA):

fd0f:8b72:ac90::/48

|3.2.4. Global address type "(Aggregatable) global unicast"

Today, there is one global address type defined (the first design, called "provider based," was thrown away some years ago RFC 1884 / IP Version 6 Addressing Architecture [obsolete], you will find some remains in older Linux kernel sources).

It begins with (x are hex characters)

|2xxx: 
|3xxx:

Note: the prefix “aggregatable” is thrown away in current drafts. There are some further subtypes defined, see below:
|3.2.4.1. 6bone test addresses

These were the first global addresses which were defined and in use. They all start with

|3ffe:

Example:

|3ffe:ffff:100:f102::1

A special 6bone test address which will never be globally unique begins with

|3ffe:ffff: 

and is mostly shown in older examples. The reason for this is, if real addresses are are shown, it's possible for someone to do a copy & paste to their configuration files, thus inadvertently causing duplicates on a globally unique address. This would cause serious problems for the original host (e.g. getting answer packets for request that were never sent). Because IPv6 is now in production, this prefix is no longer be delegated and is removed from routing after 6.6.2006 (see RFC 3701 / 6bone Phaseout for more).
|3.2.4.2. 6to4 addresses

These addresses, designed for a special tunneling mechanism [RFC 3056 / Connection of IPv6 Domains via IPv4 Clouds and RFC 2893 / Transition Mechanisms for IPv6 Hosts and Routers], encode a given IPv4 address and a possible subnet and begin with

|2002:

For example, representing 192.168.1.1/5:

|2002:c0a8:0101:5::1

A small shell command line can help you generating such address out of a given IPv4 one:

ipv4="1.2.3.4"; sla="5"; printf "2002:%02x%02x:%02x%02x:%04x::1" `echo $ipv4
¬ | tr "." " "` $sla

See also tunneling using 6to4 and information about 6to4 relay routers.
|3.2.4.3. Assigned by provider for hierarchical routing

These addresses are delegated to Internet service providers (ISP) and begin currently with

|2001:

Prefixes to major (backbone owning) ISPs (also known as LIRs) are delegated by local registries and currently have a prefix with length 32 assigned.

Any ISP customer can get a prefix with length 48.
|3.2.4.4. Addresses reserved for examples and documentation

Currently, two address ranges are reserved for examples and documentation RFC 3849 / IPv6 Address Prefix Reserved for Documentation:

|3fff:ffff::/32
|2001:0DB8::/32   EXAMPLENET-WF

These address ranges should be filtered based on source addresses and should NOT be routed on border routers to the internet, if possible.
|3.2.5. Multicast addresses

Multicast addresses are used for related services.

They alway start with (xx is the scope value)

ffxy:

They are split into scopes and types:
|3.2.5.1. Multicast scopes

Multicast scope is a parameter to specify the maximum distance a multicast packet can travel from the sending entity.

Currently, the following regions (scopes) are defined:

    ffx1: node-local, packets never leave the node.

    ffx2: link-local, packets are never forwarded by routers, so they never leave the specified link.

    ffx5: site-local, packets never leave the site.

    ffx8: organization-local, packets never leave the organization (not so easy to implement, must be covered by routing protocol).

    ffxe: global scope.

    others are reserved

|3.2.5.2. Multicast types

There are many types already defined/reserved (see RFC 4291 / IP Version 6 Addressing Architecture for details). Some examples are:

    All Nodes Address: ID = 1h, addresses all hosts on the local node (ff01:0:0:0:0:0:0:1) or the connected link (ff02:0:0:0:0:0:0:1).

    All Routers Address: ID = 2h, addresses all routers on the local node (ff01:0:0:0:0:0:0:2), on the connected link (ff02:0:0:0:0:0:0:2), or on the local site (ff05:0:0:0:0:0:0:2)

|3.2.5.3. Solicited node link-local multicast address

Special multicast address used as destination address in neighborhood discovery, because unlike in IPv4, ARP no longer exists in IPv6.

An example of this address looks like

ff02::1:ff00:1234

Used prefix shows that this is a link-local multicast address. The suffix is generated from the destination address. In this example, a packet should be sent to address “fe80::1234”, but the network stack doesn't know the current layer 2 MAC address. It replaces the upper 104 bits with “ff02:0:0:0:0:1:ff00::/104” and leaves the lower 24 bits untouched. This address is now used `on-link' to find the corresponding node which has to send a reply containing its layer 2 MAC address.
|3.2.6. Anycast addresses

Anycast addresses are special addresses and are used to cover things like nearest DNS server, nearest DHCP server, or similar dynamic groups. Addresses are taken out of the unicast address space (aggregatable global or site-local at the moment). The anycast mechanism (client view) will be handled by dynamic routing protocols.

Note: Anycast addresses cannot be used as source addresses, they are only used as destination addresses.
|3.2.6.1. Subnet-router anycast address

A simple example for an anycast address is the subnet-router anycast address. Assuming that a node has the following global assigned IPv6 address:

|2001:db8:100:f101:210:a4ff:fee3:9566/64  <- Node's address

The subnet-router anycast address will be created blanking the suffix (least significant 64 bits) completely:

|2001:db8:100:f101::/64  <- subnet-router anycast address

|3.3. Address types (host part)

For auto-configuration and mobility issues, it was decided to use the lower 64 bits as the host part of the address in most of the current address types. Therefore each single subnet can hold a large amount of addresses.

This host part can be inspected differently:
|3.3.1. Automatically computed (also known as stateless)

With auto-configuration, the host part of the address is computed by converting the MAC address of an interface (if available), with the EUI-64 method, to a unique IPv6 address. If no MAC address is available for this device (happens e.g. on virtual devices), something else (like the IPv4 address or the MAC address of a physical interface) is used instead.

E.g. a NIC has following MAC address (48 bit):

|00:10:a4:01:23:45

This would be expanded according to theIEEE-Tutorial EUI-64 design for EUI-48 identifiers to the 64 bit interface identifier:

|0210:a4ff:fe01:2345

With a given prefix, the result is the IPv6 address shown in example above:

|2001:0db8:0100:f101:0210:a4ff:fe01:2345

|3.3.1.1. Privacy problem with automatically computed addresses and a solution

Because the "automatically computed" host part is globally unique (except when a vendor of a NIC uses the same MAC address on more than one NIC), client tracking is possible on the host when not using a proxy of any kind.

This is a known problem, and a solution was defined: privacy extension, defined in RFC 3041 / Privacy Extensions for Stateless Address Autoconfiguration in IPv6 (there is also already a newer draft available: draft-ietf-ipv6-privacy-addrs-v2-*). Using a random and a static value a new suffix is generated from time to time. Note: this is only reasonable for outgoing client connections and isn't really useful for well-known servers.
|3.3.2. Manually set

For servers, it's probably easier to remember simpler addresses, this can also be accommodated. It is possible to assign an additional IPv6 address to an interface, e.g.

|2001:0db8:100:f101::1

For manual suffixes like “::1” shown in the above example, it's required that the 7th most significant bit is set to 0 (the universal/local bit of the automatically generated identifier). Also some other (otherwise unchosen ) bit combinations are reserved for anycast addresses, too.
|3.4. Prefix lengths for routing

In the early design phase it was planned to use a fully hierarchical routing approach to reduce the size of the routing tables maximally. The reasons behind this approach were the number of current IPv4 routing entries in core routers (> 104 thousand in May 2001), reducing the need of memory in hardware routers (ASIC “Application Specified Integrated Circuit” driven) to hold the routing table and increase speed (fewer entries hopefully result in faster lookups).

Todays view is that routing will be mostly hierarchically designed for networks with only one service provider. With more than one ISP connections, this is not possible, and subject to an issue named multi-homing (infos on multi-homing: drafts-ietf-multi6-*,IPv6 Multihoming Solutions).
|3.4.1. Prefix lengths (also known as "netmasks")

Similar to IPv4, the routable network path for routing to take place. Because standard netmask notation for 128 bits doesn't look nice, designers employed the IPv4 Classless Inter Domain Routing (CIDR, RFC 1519 / Classless Inter-Domain Routing) scheme, which specifies the number of bits of the IP address to be used for routing. It is also called the "slash" notation.

An example:

|2001:0db8:100:1:2:3:4:5/48

This notation will be expanded:

    Network: 

|2001:0db8:0100:0000:0000:0000:0000:0000

    Netmask: 

ffff:ffff:ffff:0000:0000:0000:0000:0000

|3.4.2. Matching a route

Under normal circumstances (no QoS), a lookup in a routing table results in the route with the most significant number of address bits being selected. In other words, the route with the biggest prefix length matches first.

For example if a routing table shows following entries (list is not complete):

|2001:0db8:100::/48     ::            U  1 0 0 sit1 
|2000::/3               ::192.88.99.1 UG 1 0 0 tun6to4

Shown destination addresses of IPv6 packets will be routed through shown device

|2001:0db8:100:1:2:3:4:5/48  ->  routed through device sit1
|2001:0db8:200:1:2:3:4:5/48  ->  routed through device tun6to4

Chapter 4. IPv6-ready system check

Before you can start using IPv6 on a Linux host, you have to test, whether your system is IPv6-ready. You may have to do some work to enable it first.
|4.1. IPv6-ready kernel

Modern Linux distributions already contain IPv6-ready kernels, the IPv6 capability is generally compiled as a module, but it's possible that this module is not loaded automatically on startup.

Note: you shouldn't anymore use kernel series 2.2.x, because it's not IPv6-up-to-date anymore. Also the IPv6 support in series 2.4.x is no longer improved according to definitions in latest RFCs. It's recommend to use series 2.6.x now.
|4.1.1. Check for IPv6 support in the current running kernel

To check, whether your current running kernel supports IPv6, take a look into your /proc-file-system. Following entry must exists:

/proc/net/if_inet6

A short automatical test looks like:

# test -f /proc/net/if_inet6 && echo "Running kernel is IPv6 ready"

If this fails, it is quite likely, that the IPv6 module is not loaded.
|4.1.2. Try to load IPv6 module

You can try to load the IPv6 module executing

# modprobe ipv6

If this is successful, this module should be listed, testable with following auto-magically line:

# lsmod |grep -w 'ipv6' && echo "IPv6 module successfully loaded"

And the check shown above should now run successfully.

Note: unloading the module is currently not supported and can result, under some circumstances, in a kernel crash.
|4.1.2.1. Automatically loading of module

Its possible to automatically load the IPv6 module on demand. You only have to add following line in the configuration file of the kernel module loader (normally /etc/modules.conf or /etc/conf.modules):

alias net-pf-10 ipv6  # automatically load IPv6 module on demand

It's also possible to disable automatically loading of the IPv6 module using following line

alias net-pf-10 off   # disable automatically load of IPv6 module on demand

Additional note: in kernels series 2.6.x, the module loader mechanism was changed. The new configuration file has to be named /etc/modprobe.conf instead of /etc/modules.conf.
|4.1.3. Compile kernel with IPv6 capabilities

If both above shown results were negative and your kernel has no IP6 support, than you have the following options:

    Update your distribution to a current one which supports IPv6 out-of-the-box (recommended for newbies)

    Compile a new vanilla kernel (easy, if you know which options you needed)

    Recompile kernel sources given by your Linux distribution (sometimes not so easy)

    Compile a kernel with USAGI extensions

If you decide to compile a kernel, you should have previous experience in kernel compiling and read the Linux Kernel HOWTO.

A comparison between vanilla and USAGI extended kernels is available on IPv6+Linux-Status-Kernel.
|4.1.3.1. Compiling a vanilla kernel

More detailed hints about compiling an IPv6-enabled kernel can be found e.g. on IPv6-HOWTO-2#kernel.

Note: you should use whenever possible kernel series 2.6.x or above, because the IPv6 support in series 2.4.x only will no longer get backported features from 2.6.x and IPv6 support in series 2.2.x is hopeless outdated.
|4.1.3.2. Compiling a kernel with USAGI extensions

Same as for vanilla kernel, only recommend for advanced users, which are already familiar with IPv6 and kernel compilation. See also USAGI project / FAQ and Obtaining the best IPv6 support with Linux (Article) (Mirror).
|4.1.4. IPv6-ready network devices

Not all existing network devices have already (or ever) the capability to transport IPv6 packets. A current status can be found at IPv6+Linux-status-kernel.html#transport.

A major issue is that because of the network layer structure of kernel implementation an IPv6 packet isn't really recognized by it's IP header number (6 instead of 4). It's recognized by the protocol number of the Layer 2 transport protocol. Therefore any transport protocol which doesn't use such protocol number cannot dispatch IPv6 packets. Note: the packet is still transported over the link, but on receivers side, the dispatching won't work (you can see this e.g. using tcpdump).
|4.1.4.1. Currently known never “IPv6 capable links”

    Serial Line IP (SLIP, RFC 1055 / SLIP), should be better called now to SLIPv4, device named: slX

    Parallel Line IP (PLIP), same like SLIP, device names: plipX

    ISDN with encapsulation rawip, device names: isdnX

|4.1.4.2. Currently known “not supported IPv6 capable links”

    ISDN with encapsulation syncppp, device names: ipppX (design issue of the ipppd, will be merged into more general PPP layer in kernel series 2.5.x)

|4.2. IPv6-ready network configuration tools

You wont get very far, if you are running an IPv6-ready kernel, but have no tools to configure IPv6. There are several packages in existence which can configure IPv6.
|4.2.1. net-tools package

The net-tool package includes some tools like ifconfig and route, which helps you to configure IPv6 on an interface. Look at the output of ifconfig -? or route -?, if something is shown like IPv6 or inet6, then the tool is IPv6-ready.

Auto-magically check:

# /sbin/ifconfig -? 2>& 1|grep -qw 'inet6' && echo "utility 'ifconfig' is
¬ IPv6-ready"

Same check can be done for route:

# /sbin/route -? 2>& 1|grep -qw 'inet6' && echo "utility 'route' is IPv6-ready"

|4.2.2. iproute package

Alexey N. Kuznetsov (current a maintainer of the Linux networking code) created a tool-set which configures networks through the netlink device. Using this tool-set you have more functionality than net-tools provides, but its not very well documented and isn't for the faint of heart.

# /sbin/ip 2>&1 |grep -qw 'inet6' && echo "utility 'ip' is IPv6-ready"

If the program /sbin/ip isn't found, then I strongly recommend you install the iproute package.

    You can get it from your Linux distribution (if contained)

    You're able to look for a proper RPM package at RPMfind/iproute (sometimes rebuilding of a SRPMS package is recommended)

|4.3. IPv6-ready test/debug programs

After you have prepared your system for IPv6, you now want to use IPv6 for network communications. First you should learn how to examine IPv6 packets with a sniffer program. This is strongly recommended because for debugging/troubleshooting issues this can aide in providing a diagnosis very quickly.
|4.3.1. IPv6 ping

This program is normally included in package iputils. It is designed for simple transport tests sending ICMPv6 echo-request packets and wait for ICMPv6 echo-reply packets.

Usage

# ping6 <hostwithipv6address>
# ping6 <ipv6address>
# ping6 [-I <device>] <link-local-ipv6address>

Example

# ping6 -c 1 ::1 
PING ::1(::1) from ::1 : 56 data bytes 
|64 bytes from ::1: icmp_seq=0 hops=64 time=292 usec

--- ::1 ping statistics --- 
|1 packets transmitted, 1 packets received, 0% packet loss 
round-trip min/avg/max/mdev = 0.292/0.292/0.292/0.000 ms

Hint: ping6 needs raw access to socket and therefore root permissions. So if non-root users cannot use ping6 then there are two possible problems:

    ping6 is not in users path (probably, because ping6 is generally stored in /usr/sbin -> add path (not really recommended)

    ping6 doesn't execute properly, generally because of missing root permissions -> chmod u+s /usr/sbin/ping6

|4.3.1.1. Specifying interface for IPv6 ping

Using link-local addresses for an IPv6 ping, the kernel does not know through which (physically or virtual) device it must send the packet - each device has a link-local address. A try will result in following error message:

# ping6 fe80::212:34ff:fe12:3456 
connect: Invalid argument

In this case you have to specify the interface additionally like shown here:

# ping6 -I eth0 -c 1 fe80::2e0:18ff:fe90:9205 
PING fe80::212:23ff:fe12:3456(fe80::212:23ff:fe12:3456) from
¬ fe80::212:34ff:fe12:3478 eth0: 56 data bytes 
|64 bytes from fe80::212:23ff:fe12:3456: icmp_seq=0 hops=64 time=445 usec

--- fe80::2e0:18ff:fe90:9205 ping statistics --- 
|1 packets transmitted, 1 packets received, 0% packet loss round-trip
¬ min/avg/max/mdev = 0.445/0.445/0.445/0.000 ms

|4.3.1.2. Ping6 to multicast addresses

An interesting mechanism to detect IPv6-active hosts on a link is to ping6 to the link-local all-node multicast address:

# ping6 -I eth0 ff02::1
PING ff02::1(ff02::1) from fe80:::2ab:cdff:feef:0123 eth0: 56 data bytes
|64 bytes from ::1: icmp_seq=1 ttl=64 time=0.104 ms
|64 bytes from fe80::212:34ff:fe12:3450: icmp_seq=1 ttl=64 time=0.549 ms (DUP!) 

Unlike in IPv4, where replies to a ping on the broadcast address can be disabled, in IPv6 currently this behavior cannot be disable except by local IPv6 firewalling.
|4.3.2. IPv6 traceroute6

This program is normally included in package iputils. It's a program similar to IPv4 traceroute. Below you will see an example:

# traceroute6 www.6bone.net 
traceroute to 6bone.net (3ffe:b00:c18:1::10) from 2001:0db8:0000:f101::2, 30
¬ hops max, 16 byte packets 
| 1 localipv6gateway (2001:0db8:0000:f101::1) 1.354 ms 1.566 ms 0.407 ms 
| 2 swi6T1-T0.ipv6.switch.ch (3ffe:2000:0:400::1) 90.431 ms 91.956 ms 92.377 ms 
| 3 3ffe:2000:0:1::132 (3ffe:2000:0:1::132) 118.945 ms 107.982 ms 114.557 ms 
| 4 3ffe:c00:8023:2b::2 (3ffe:c00:8023:2b::2) 968.468 ms 993.392 ms 973.441 ms 
| 5 3ffe:2e00:e:c::3 (3ffe:2e00:e:c::3) 507.784 ms 505.549 ms 508.928 ms 
| 6 www.6bone.net (3ffe:b00:c18:1::10) 1265.85 ms * 1304.74 ms

Note: unlike some modern versions of IPv4 traceroute, which can use ICMPv4 echo-request packets as well as UDP packets (default), current IPv6-traceroute is only able to send UDP packets. As you perhaps already know, ICMP echo-request packets are more accepted by firewalls or ACLs on routers inbetween than UDP packets.
|4.3.3. IPv6 tracepath6

This program is normally included in package iputils. It's a program like traceroute6 and traces the path to a given destination discovering the MTU along this path. Below you will see an example:

# tracepath6 www.6bone.net 
| 1?: [LOCALHOST] pmtu 1480 
| 1: 3ffe:401::2c0:33ff:fe02:14 150.705ms 
| 2: 3ffe:b00:c18::5 267.864ms 
| 3: 3ffe:b00:c18::5 asymm 2 266.145ms pmtu 1280 
| 3: 3ffe:3900:5::2 asymm 4 346.632ms 
| 4: 3ffe:28ff:ffff:4::3 asymm 5 365.965ms 
| 5: 3ffe:1cff:0:ee::2 asymm 4 534.704ms 
| 6: 3ffe:3800::1:1 asymm 4 578.126ms !N 
Resume: pmtu 1280

|4.3.4. IPv6 tcpdump

On Linux, tcpdump is the major tool for packet capturing. Below you find some examples. IPv6 support is normally built-in in current releases of version 3.6.

tcpdump uses expressions for filtering packets to minimize the noise:

    icmp6: filters native ICMPv6 traffic

    ip6: filters native IPv6 traffic (including ICMPv6)

    proto ipv6: filters tunneled IPv6-in-IPv4 traffic

    not port ssh: to suppress displaying SSH packets for running tcpdump in a remote SSH session

Also some command line options are very useful to catch and print more information in a packet, mostly interesting for digging into ICMPv6 packets:

    “-s 512”: increase the snap length during capturing of a packet to 512 bytes

    “-vv”: really verbose output

    “-n”: don't resolve addresses to names, useful if reverse DNS resolving isn't working proper

|4.3.4.1. IPv6 ping to 2001:0db8:100:f101::1 native over a local link

# tcpdump -t -n -i eth0 -s 512 -vv ip6 or proto ipv6 
tcpdump: listening on eth0 
|2001:0db8:100:f101:2e0:18ff:fe90:9205 > 2001:0db8:100:f101::1: icmp6: echo
¬ request (len 64, hlim 64) 
|2001:0db8:100:f101::1 > 2001:0db8:100:f101:2e0:18ff:fe90:9205: icmp6: echo
¬ reply (len 64, hlim 64)

|4.3.4.2. IPv6 ping to 2001:0db8:100::1 routed through an IPv6-in-IPv4-tunnel

|1.2.3.4 and 5.6.7.8 are tunnel endpoints (all addresses are examples)

# tcpdump -t -n -i ppp0 -s 512 -vv ip6 or proto ipv6 
tcpdump: listening on ppp0 
|1.2.3.4 > 5.6.7.8: 2002:ffff:f5f8::1 > 2001:0db8:100::1: icmp6: echo request
¬ (len 64, hlim 64) (DF) (ttl 64, id 0, len 124) 
|5.6.7.8 > 1.2.3.4: 2001:0db8:100::1 > 2002:ffff:f5f8::1: icmp6: echo reply (len
¬ 64, hlim 61) (ttl 23, id 29887, len 124) 
|1.2.3.4 > 5.6.7.8: 2002:ffff:f5f8::1 > 2001:0db8:100::1: icmp6: echo request
¬ (len 64, hlim 64) (DF) (ttl 64, id 0, len 124) 
|5.6.7.8 > 1.2.3.4: 2001:0db8:100::1 > 2002:ffff:f5f8::1: icmp6: echo reply (len
¬ 64, hlim 61) (ttl 23, id 29919, len 124)

|4.4. IPv6-ready programs

Current distributions already contain the most needed IPv6 enabled client and servers. See first on IPv6+Linux-Status-Distribution. If still not included, you can check IPv6 & Linux - Current Status - Applications whether the program is already ported to IPv6 and usable with Linux. For common used programs there are some hints available at IPv6 & Linux - HowTo - Part 3 and IPv6 & Linux - HowTo - Part 4.
|4.5. IPv6-ready client programs (selection)

To run the following shown tests, it's required that your system is IPv6 enabled, and some examples show addresses which only can be reached if a connection to the 6bone is available.
|4.5.1. Checking DNS for resolving IPv6 addresses

Because of security updates in the last years every Domain Name System (DNS) server should run newer software which already understands the (intermediate) IPv6 address-type AAAA (the newer one named A6 isn't still common at the moment because only supported using BIND9 and newer and also the non-existent support of root domain IP6.ARPA). A simple test whether the used system can resolve IPv6 addresses is

# host -t AAAA www.join.uni-muenster.de

and should show something like following:

www.join.uni-muenster.de. is an alias for tolot.join.uni-muenster.de. 
tolot.join.uni-muenster.de. has AAAA address
¬ 2001:638:500:101:2e0:81ff:fe24:37c6

|4.5.2. IPv6-ready telnet clients

IPv6-ready telnet clients are available. A simple test can be done with

$ telnet 3ffe:400:100::1 80
Trying 3ffe:400:100::1... 
Connected to 3ffe:400:100::1. 
Escape character is '^]'. 
HEAD / HTTP/1.0

HTTP/1.1 200 OK 
Date: Sun, 16 Dec 2001 16:07:21 
GMT Server: Apache/2.0.28 (Unix) 
Last-Modified: Wed, 01 Aug 2001 21:34:42 GMT 
ETag: "3f02-a4d-b1b3e080" 
Accept-Ranges: bytes 
Content-Length: 2637 
Connection: close 
Content-Type: text/html; charset=ISO-8859-1

Connection closed by foreign host.

If the telnet client don't understand the IPv6 address and says something like “cannot resolve hostname”, then it's not IPv6-enabled.
|4.5.3. IPv6-ready ssh clients
|4.5.3.1. openssh

Current versions of openssh are IPv6-ready. Depending on configuring before compiling it has two behavior.

    --without-ipv4-default: the client tries an IPv6 connect first automatically and fall back to IPv4 if not working

    --with-ipv4-default: default connection is IPv4, IPv6 connection must be force like following example shows

$ ssh -6 ::1 
user@::1's password: ****** 
[user@ipv6host user]$

If your ssh client doesn't understand the option “-6” then it's not IPv6-enabled, like most ssh version 1 packages.
|4.5.3.2. ssh.com

SSH.com's SSH client and server is also IPv6 aware now and is free for all Linux and FreeBSD machine regardless if used for personal or commercial use.
|4.5.4. IPv6-ready web browsers

A current status of IPv6 enabled web browsers is available at IPv6+Linux-status-apps.html#HTTP.

Most of them have unresolved problems at the moment

    If using an IPv4 only proxy in the settings, IPv6 requests will be sent to the proxy, but the proxy will fail to understand the request and the request fails. Solution: update proxy software (see later).

    Automatic proxy settings (*.pac) cannot be extended to handle IPv6 requests differently (e.g. don't use proxy) because of their nature (written in Java-script and well hard coded in source like to be seen in Maxilla source code).

Also older versions don't understand an URL with IPv6 encoded addresses like http://[2001:a60:9002:1::186:6]/ (this given URL only works with an IPv6-enabled browser!).

A short test is to try shown URL with a given browser and using no proxy.
|4.5.4.1. URLs for testing

A good starting point for browsing using IPv6 is http://www.kame.net/. If the turtle on this page is animated, the connection is via IPv6, otherwise the turtle is static.
|4.6. IPv6-ready server programs

In this part of this HOWTO, more client specific issues are mentioned. Therefore hints for IPv6-ready servers like sshd, httpd, telnetd, etc. are shown below in Hints for IPv6-enabled daemons.
|4.7. FAQ (IPv6-ready system check)
|4.7.1. Using tools
|4.7.1.1. Q: Cannot ping6 to link-local addresses

Error message: "connect: Invalid argument"

Kernel doesn't know, which physical or virtual link you want to use to send such ICMPv6 packets. Therefore it displays this error message.

Solution: Specify interface like: “ping6 -I eth0 fe80::2e0:18ff:fe90:9205”, see also program ping6 usage.
|4.7.1.2. Q: Cannot ping6 or traceroute6 as normal user

Error message: “icmp socket: Operation not permitted”

These utilities create special ICMPv6 packets and send them out. This is done by using raw sockets in the kernel. But raw sockets can only be used by the “root” user. Therefore normal users get such error message.

Solution: If it's really needed that all users should be able to use these utilities, you can add the “suid” bit using ”chmod u+s /path/to/program”, see also program ping6 usage. If not all users should be able to, you can change the group of the program to e.g. “wheel”, add these power users to this group and remove the execution bit for other users using “chmod o-rwx /path/to/program”. Or configure “sudo” to enable your security policy.
Chapter 5. Configuring interfaces
|5.1. Different network devices

On a node, there exist different network devices. They can be collected in classes

    Physically bounded, like eth0, tr0

    Virtually existing, like ppp0, tun0, tap0, sit0, isdn0, ippp0

|5.1.1. Physically bounded

Physically bounded interfaces like Ethernet or Token-Ring are normal ones and need no special treatment.
|5.1.2. Virtually bounded

Virtually bounded interfaces always need special support
|5.1.2.1. IPv6-in-IPv4 tunnel interfaces

These interfaces are normally named sitx. The name sit is a shortcut for Simple Internet Transition. This device has the capability to encapsulate IPv6 packets into IPv4 ones and tunnel them to a foreign endpoint.

sit0 has a special meaning and cannot be used for dedicated tunnels.
|5.1.2.2. PPP interfaces

PPP interfaces get their IPv6 capability from an IPv6 enabled PPP daemon.
|5.1.2.3. ISDN HDLC interfaces

IPv6 capability for HDLC with encapsulation ip is already built-in in the kernel
|5.1.2.4. ISDN PPP interfaces

ISDN PPP interfaces (ippp) aren't IPv6 enabled by kernel. Also there are also no plans to do that because in kernel 2.5.+ they will be replaced by a more generic ppp interface layer.
|5.1.2.5. SLIP + PLIP

Like mentioned earlier, this interfaces don't support IPv6 transport (sending is OK, but dispatching on receiving don't work).
|5.1.2.6. Ether-tap device

Ether-tap devices are IPv6-enabled and also stateless configured. For use, the module “ethertap” has to be loaded before.
|5.1.2.7. tun devices

Currently not tested by me.
|5.1.2.8. ATM

|01/2002: Aren't currently supported by vanilla kernel, supported by USAGI extension
|5.1.2.9. Others

Did I forget an interface?...
|5.2. Bringing interfaces up/down

Two methods can be used to bring interfaces up or down.
|5.2.1. Using "ip"

Usage:

# ip link set dev <interface> up
# ip link set dev <interface> down

Example:

# ip link set dev eth0 up
# ip link set dev eth0 down

|5.2.2. Using "ifconfig"

Usage:

# /sbin/ifconfig <interface> up
# /sbin/ifconfig <interface> down

Example:

# /sbin/ifconfig eth0 up
# /sbin/ifconfig eth0 down

Chapter 6. Configuring IPv6 addresses

There are different ways to configure an IPv6 address on an interface. You can use use "ifconfig" or "ip".
|6.1. Displaying existing IPv6 addresses

First you should check, whether and which IPv6 addresses are already configured (perhaps auto-magically during stateless auto-configuration).
|6.1.1. Using "ip"

Usage:

# /sbin/ip -6 addr show dev <interface>

Example for a static configured host:

# /sbin/ip -6 addr show dev eth0
|2: eth0: <BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc pfifo_ fast qlen 100
inet6 fe80::210:a4ff:fee3:9566/10 scope link
inet6 2001:0db8:0:f101::1/64 scope global
inet6 fec0:0:0:f101::1/64 scope site 

Example for a host which is auto-configured

Here you see some auto-magically configured IPv6 addresses and their lifetime.

# /sbin/ip -6 addr show dev eth0 
|3: eth0: <BROADCAST,MULTICAST,PROMISC,UP&gt; mtu 1500 qdisc pfifo_fast qlen
¬ 100 
inet6 2002:d950:f5f8:f101:2e0:18ff:fe90:9205/64 scope global dynamic 
valid_lft 16sec preferred_lft 6sec 
inet6 3ffe:400:100:f101:2e0:18ff:fe90:9205/64 scope global dynamic 
valid_lft 2591997sec preferred_lft 604797sec inet6 fe80::2e0:18ff:fe90:9205/10
¬ scope link

|6.1.2. Using "ifconfig"

Usage:

# /sbin/ifconfig <interface>

Example (output filtered with grep to display only IPv6 addresses). Here you see different IPv6 addresses with different scopes.

# /sbin/ifconfig eth0 |grep "inet6 addr:"
inet6 addr: fe80::210:a4ff:fee3:9566/10 Scope:Link
inet6 addr: 2001:0db8:0:f101::1/64 Scope:Global
inet6 addr: fec0:0:0:f101::1/64 Scope:Site

|6.2. Add an IPv6 address

Adding an IPv6 address is similar to the mechanism of "IP ALIAS" addresses in Linux IPv4 addressed interfaces.
|6.2.1. Using "ip"

Usage:

# /sbin/ip -6 addr add <ipv6address>/<prefixlength> dev <interface> 

Example:

# /sbin/ip -6 addr add 2001:0db8:0:f101::1/64 dev eth0 

|6.2.2. Using "ifconfig"

Usage:

# /sbin/ifconfig <interface> inet6 add <ipv6address>/<prefixlength>

Example:

# /sbin/ifconfig eth0 inet6 add 2001:0db8:0:f101::1/64 

|6.3. Removing an IPv6 address

Not so often needed, be carefully with removing non existent IPv6 address, sometimes using older kernels it results in a crash.
|6.3.1. Using "ip"

Usage:

# /sbin/ip -6 addr del <ipv6address>/<prefixlength> dev <interface> 

Example:

# /sbin/ip -6 addr del 2001:0db8:0:f101::1/64 dev eth0 

|6.3.2. Using "ifconfig"

Usage:

# /sbin/ifconfig <interface> inet6 del <ipv6address>/<prefixlength>

Example:

# /sbin/ifconfig eth0 inet6 del 2001:0db8:0:f101::1/64

Chapter 7. Configuring normal IPv6 routes

If you want to leave your link and want to send packets in the world wide IPv6-Internet, you need routing. If there is already an IPv6 enabled router on your link, it's possible enough to add IPv6 routes.
|7.1. Displaying existing IPv6 routes

First you should check, whether and which IPv6 addresses are already configured (perhaps auto-magically during auto-configuration).
|7.1.1. Using "ip"

Usage:

# /sbin/ip -6 route show [dev <device>]

Example:

# /sbin/ip -6 route show dev eth0
|2001:0db8:0:f101::/64 proto kernel metric 256 mtu 1500 advmss 1440
fe80::/10             proto kernel metric 256 mtu 1500 advmss 1440
ff00::/8              proto kernel metric 256 mtu 1500 advmss 1440
default               proto kernel metric 256 mtu 1500 advmss 1440

|7.1.2. Using "route"

Usage:

# /sbin/route -A inet6 

Example (output is filtered for interface eth0). Here you see different IPv6 routes for different addresses on a single interface.

# /sbin/route -A inet6 |grep -w "eth0"
|2001:0db8:0:f101 ::/64 :: UA  256 0 0 eth0 <- Interface route for global
¬ address
fe80::/10        ::       UA  256 0 0 eth0 <- Interface route for link-local
¬ address
ff00::/8         ::       UA  256 0 0 eth0 <- Interface route for all multicast
¬ addresses
::/0             ::       UDA 256 0 0 eth0 <- Automatic default route

|7.2. Add an IPv6 route through a gateway

Mostly needed to reach the outside with IPv6 using an IPv6-enabled router on your link.
|7.2.1. Using "ip"

Usage:

# /sbin/ip -6 route add <ipv6network>/<prefixlength> via <ipv6address>
¬ [dev <device>]

Example:

# /sbin/ip -6 route add 2000::/3 via 2001:0db8:0:f101::1

|7.2.2. Using "route"

Usage:

# /sbin/route -A inet6 add <ipv6network>/<prefixlength> gw
¬ <ipv6address> [dev <device>] 

A device can be needed, too, if the IPv6 address of the gateway is a link local one.

Following shown example adds a route for all currently global addresses (2000::/3) through gateway 2001:0db8:0:f101::1

# /sbin/route -A inet6 add 2000::/3 gw 2001:0db8:0:f101::1

|7.3. Removing an IPv6 route through a gateway

Not so often needed manually, mostly done by network configure scripts on shutdown (full or per interface)
|7.3.1. Using "ip"

Usage:

# /sbin/ip -6 route del <ipv6network>/<prefixlength> via <ipv6address>
¬ [dev <device>]

Example:

# /sbin/ip -6 route del 2000::/3 via 2001:0db8:0:f101::1

|7.3.2. Using "route"

Usage:

# /sbin/route -A inet6 del <network>/<prefixlength> gw <ipv6address> [dev
¬ <device>]

Example for removing upper added route again:

# /sbin/route -A inet6 del 2000::/3 gw 2001:0db8:0:f101::1

|7.4. Add an IPv6 route through an interface

Not often needed, sometimes in cases of dedicated point-to-point links.
|7.4.1. Using "ip"

Usage:

# /sbin/ip -6 route add <ipv6network>/<prefixlength> dev <device>
¬ metric 1

Example:

# /sbin/ip -6 route add 2000::/3 dev eth0 metric 1

Metric “1” is used here to be compatible with the metric used by route, because the default metric on using “ip” is “1024”.
|7.4.2. Using "route"

Usage:

# /sbin/route -A inet6 add <ipv6network>/<prefixlength> dev <device>

Example:

# /sbin/route -A inet6 add 2000::/3 dev eth0 

|7.5. Removing an IPv6 route through an interface

Not so often needed to use by hand, configuration scripts will use such on shutdown.
|7.5.1. Using "ip"

Usage:

# /sbin/ip -6 route del <ipv6network>/<prefixlength> dev <device>

Example:

# /sbin/ip -6 route del 2000::/3 dev eth0 

|7.5.2. Using "route"

Usage:

# /sbin/route -A inet6 del <network>/<prefixlength> dev <device>

Example:

# /sbin/route -A inet6 del 2000::/3 dev eth0

|7.6. FAQ for IPv6 routes
|7.6.1. Support of an IPv6 default route

One idea of IPv6 was a hierachical routing, therefore only less routing entries are needed in routers.

There are some issues in current Linux kernels:
|7.6.1.1. Clients (not routing any packet!)

Client can setup a default route like prefix “::/0”, they also learn such route on autoconfiguration e.g. using radvd on the link like following example shows:

# ip -6 route show | grep ^default
default via fe80::212:34ff:fe12:3450 dev eth0 proto kernel metric 1024 expires
¬ 29sec mtu 1500 advmss 1440

|7.6.1.2. Routers in case of packet forwarding

Older Linux kernel (at least <= 2.4.17) don't support default routes. You can set them up, but the route lookup fails when a packet should be forwarded (normal intention of a router). If you're still using such older kernel, “default routing” can be setup using the currently used global address prefix “2000::/3”.

Note: take care about default routing without address filtering on edge routers. Otherwise unwanted multicast or site-local traffic can leave the edge.
Chapter 8. Neighbor Discovery

Neighbor discovery was the IPv6 successor for the ARP (Address Resolution Protocol) in IPv4. You can retrieve information about the current neighbors, in addition you can set and delete entries. The kernel keeps tracking of successful neighbor detection (like ARP in IPv4). You can dig into the learnt table using “ip”.
|8.1. Displaying neighbors using “ip”

With following command you can display the learnt or configured IPv6 neighbors

# ip -6 neigh show [dev <device>]

The following example shows one neighbor, which is a reachable router

# ip -6 neigh show
fe80::201:23ff:fe45:6789 dev eth0 lladdr 00:01:23:45:67:89 router nud reachable

|8.2. Manipulating neighbors table using “ip”
|8.2.1. Manually add an entry

With following command you are able to manually add an entry

# ip -6 neigh add <IPv6 address> lladdr <link-layer address> dev <device>

Example:

# ip -6 neigh add fec0::1 lladdr 02:01:02:03:04:05 dev eth0

|8.2.2. Manually delete an entry

Like adding also an entry can be deleted:

# ip -6 neigh del <IPv6 address> lladdr <link-layer address> dev <device>

Example:

# ip -6 neigh del fec0::1 lladdr 02:01:02:03:04:05 dev eth0

|8.2.3. More advanced settings

The tool “ip” is less documentated, but very strong. See online “help” for more:

# ip -6 neigh help
Usage: ip neigh { add | del | change | replace } { ADDR [ lladdr LLADDR ] 
          [ nud { permanent | noarp | stale | reachable } ] 
          | proxy ADDR } [ dev DEV ] 
       ip neigh {show|flush} [ to PREFIX ] [ dev DEV ] [ nud STATE ]

Looks like some options are only for IPv4...if you can contribute information about flags and advanced usage, pls. send.
Chapter 9. Configuring IPv6-in-IPv4 tunnels

If you want to leave your link and you have no IPv6 capable network around you, you need IPv6-in-IPv4 tunneling to reach the world wide IPv6-Internet.

There are some kind of tunnel mechanism and also some possibilities to setup tunnels.
|9.1. Types of tunnels

There are more than one possibility to tunnel IPv6 packets over IPv4-only links.
|9.1.1. Static point-to-point tunneling: 6bone

A point-to-point tunnel is a dedicated tunnel to an endpoint, which knows about your IPv6 network (for backward routing) and the IPv4 address of your tunnel endpoint and defined in RFC 2893 / Transition Mechanisms for IPv6 Hosts and Routers. Requirements:

    IPv4 address of your local tunnel endpoint must be static, global unique and reachable from the foreign tunnel endpoint

    A global IPv6 prefix assigned to you (see 6bone registry)

    A foreign tunnel endpoint which is capable to route your IPv6 prefix to your local tunnel endpoint (mostly remote manual configuration required)

|9.1.2. Automatically tunneling

Automatic tunneling occurs, when a node directly connects another node gotten the IPv4 address of the other node before.
|9.1.3. 6to4-Tunneling

|6to4 tunneling (RFC 3056 / Connection of IPv6 Domains via IPv4 Clouds) uses a simple mechanism to create automatic tunnels. Each node with a global unique IPv4 address is able to be a 6to4 tunnel endpoint (if no IPv4 firewall prohibits traffic). 6to4 tunneling is mostly not a one-to-one tunnel. This case of tunneling can be divided into upstream and downstream tunneling. Also, a special IPv6 address indicates that this node will use 6to4 tunneling for connecting the world-wide IPv6 network
|9.1.3.1. Generation of 6to4 prefix

The 6to4 address is defined like following (schema is taken from RFC 3056 / Connection of IPv6 Domains via IPv4 Clouds):

|   3+13   |    32     |    16  |            64 bits             | 
+---+------+-----------+--------+--------------------------------+ 
|  FP+TLA  |  V4ADDR   | SLA ID |           Interface ID         | 
|  0x2002  |           |        |                                | 
+---+------+-----------+--------+--------------------------------+

FP and TLA together (16 bits) have the value 0x2002. V4ADDR is the node's global unique IPv4 address (in hexadecimal notation). SLA is the subnet identifier (65536 local subnets possible) and are usable to represent your local network structure.

For gateways, such prefix is generated by normally using SLA “0000” and suffix “::1” (not a must, can be an arbitrary one with local-scope) and assigned to the 6to4 tunnel interface. Note that Microsoft Windows uses V4ADDR also for suffix.
|9.1.3.2. 6to4 upstream tunneling

The node has to know to which foreign tunnel endpoint its in IPv4 packed IPv6 packets should be send to. In “early” days of 6to4 tunneling, dedicated upstream accepting routers were defined. See NSayer's 6to4 information for a list of routers.

Nowadays, 6to4 upstream routers can be found auto-magically using the anycast address 192.88.99.1. In the background routing protocols handle this, see RFC 3068 / An Anycast Prefix for 6to4 Relay Routers for details.
|9.1.3.3. 6to4 downstream tunneling

The downstream (6bone -> your 6to4 enabled node) is not really fix and can vary from foreign host which originated packets were send to. There exist two possibilities:

    Foreign host uses 6to4 and sends packet direct back to your node (see below)

    Foreign host sends packets back to the world-wide IPv6 network and depending on the dynamic routing a relay router create a automatic tunnel back to your node.

|9.1.3.4. Possible 6to4 traffic

    from 6to4 to 6to4: is normally directly tunneled between the both 6to4 enabled hosts

    from 6to4 to non-6to4: is sent via upstream tunneling

    non-6to4 to 6to4: is sent via downstream tunneling

|9.2. Displaying existing tunnels
|9.2.1. Using "ip"

Usage:

# /sbin/ip -6 tunnel show [<device>]

Example:

# /sbin/ip -6 tunnel show 
sit0: ipv6/ip remote any local any ttl 64 nopmtudisc 
sit1: ipv6/ip remote 195.226.187.50 local any ttl 64

|9.2.2. Using "route"

Usage:

# /sbin/route -A inet6 

Example (output is filtered to display only tunnels through virtual interface sit0):

# /sbin/route -A inet6 | grep "\Wsit0\W*$" 
::/96      ::               U   256  2  0  sit0 
|2002::/16  ::               UA  256  0  0  sit0 
|2000::/3   ::193.113.58.75  UG    1  0  0  sit0 
fe80::/10  ::               UA  256  0  0  sit0 
ff00::/8   ::               UA  256  0  0  sit0

|9.3. Setup of point-to-point tunnel

There are 3 possibilities to add or remove point-to-point tunnels.

A good additional information about tunnel setup using “ip” is Configuring tunnels with iproute2 (article) (Mirror).
|9.3.1. Add point-to-point tunnels
|9.3.1.1. Using "ip"

Common method at the moment for a small amount of tunnels.

Usage for creating a tunnel device (but it's not up afterward, also a TTL must be specified because the default value is 0).

# /sbin/ip tunnel add <device> mode sit ttl <ttldefault> remote
¬ <ipv4addressofforeigntunnel> local <ipv4addresslocal>

Usage (generic example for three tunnels):

# /sbin/ip tunnel add sit1 mode sit ttl <ttldefault> remote
¬ <ipv4addressofforeigntunnel1> local <ipv4addresslocal>
# /sbin/ip link set dev sit1 up
# /sbin/ip -6 route add <prefixtoroute1> dev sit1 metric 1

# /sbin/ip tunnel add sit2 mode sit ttl <ttldefault>
¬ <ipv4addressofforeigntunnel2> local <ipv4addresslocal>
# /sbin/ip link set dev sit2 up
# /sbin/ip -6 route add <prefixtoroute2> dev sit2 metric 1

# /sbin/ip tunnel add sit3 mode sit ttl <ttldefault>
¬ <ipv4addressofforeigntunnel3> local <ipv4addresslocal>
# /sbin/ip link set dev sit3 up
# /sbin/ip -6 route add <prefixtoroute3> dev sit3 metric 1

|9.3.1.2. Using "ifconfig" and "route" (deprecated)

This not very recommended way to add a tunnel because it's a little bit strange. No problem if adding only one, but if you setup more than one, you cannot easy shutdown the first ones and leave the others running.

Usage (generic example for three tunnels):

# /sbin/ifconfig sit0 up

# /sbin/ifconfig sit0 tunnel <ipv4addressofforeigntunnel1>
# /sbin/ifconfig sit1 up
# /sbin/route -A inet6 add <prefixtoroute1> dev sit1

# /sbin/ifconfig sit0 tunnel <ipv4addressofforeigntunnel2>
# /sbin/ifconfig sit2 up
# /sbin/route -A inet6 add <prefixtoroute2> dev sit2

# /sbin/ifconfig sit0 tunnel <ipv4addressofforeigntunnel3>
# /sbin/ifconfig sit3 up
# /sbin/route -A inet6 add <prefixtoroute3> dev sit3

Important: DON'T USE THIS, because this setup implicit enable "automatic tunneling" from anywhere in the Internet, this is a risk, and it should not be advocated.
|9.3.1.3. Using "route" only

It's also possible to setup tunnels in Non Broadcast Multiple Access (NBMA) style, it's a easy way to add many tunnels at once. But none of the tunnel can be numbered (which is a not required feature).

Usage (generic example for three tunnels):

# /sbin/ifconfig sit0 up

# /sbin/route -A inet6 add <prefixtoroute1> gw
¬ ::<ipv4addressofforeigntunnel1> dev sit0
# /sbin/route -A inet6 add <prefixtoroute2> gw
¬ ::<ipv4addressofforeigntunnel2> dev sit0
# /sbin/route -A inet6 add <prefixtoroute3> gw
¬ ::<ipv4addressofforeigntunnel3> dev sit0

Important: DON'T USE THIS, because this setup implicit enable "automatic tunneling" from anywhere in the Internet, this is a risk, and it should not be advocated.
|9.3.2. Removing point-to-point tunnels

Manually not so often needed, but used by scripts for clean shutdown or restart of IPv6 configuration.
|9.3.2.1. Using "ip"

Usage for removing a tunnel device:

# /sbin/ip tunnel del <device>

Usage (generic example for three tunnels):

# /sbin/ip -6 route del <prefixtoroute1> dev sit1
# /sbin/ip link set sit1 down
# /sbin/ip tunnel del sit1

# /sbin/ip -6 route del <prefixtoroute2> dev sit2
# /sbin/ip link set sit2 down
# /sbin/ip tunnel del sit2

# /sbin/ip -6 route del <prefixtoroute3> dev sit3
# /sbin/ip link set sit3 down
# /sbin/ip tunnel del sit3

|9.3.2.2. Using "ifconfig" and "route" (deprecated because not very funny)

Not only the creation is strange, the shutdown also...you have to remove the tunnels in backorder, means the latest created must be removed first.

Usage (generic example for three tunnels):

# /sbin/route -A inet6 del <prefixtoroute3> dev sit3
# /sbin/ifconfig sit3 down

# /sbin/route -A inet6 del <prefixtoroute2> dev sit2
# /sbin/ifconfig sit2 down

# /sbin/route -A inet6 add <prefixtoroute1> dev sit1
# /sbin/ifconfig sit1 down

# /sbin/ifconfig sit0 down

|9.3.2.3. Using "route"

This is like removing normal IPv6 routes.

Usage (generic example for three tunnels):

# /sbin/route -A inet6 del <prefixtoroute1> gw
¬ ::<ipv4addressofforeigntunnel1> dev sit0
# /sbin/route -A inet6 del <prefixtoroute2> gw
¬ ::<ipv4addressofforeigntunnel2> dev sit0
# /sbin/route -A inet6 del <prefixtoroute3> gw
¬ ::<ipv4addressofforeigntunnel3> dev sit0

# /sbin/ifconfig sit0 down

|9.3.3. Numbered point-to-point tunnels

Sometimes it's needed to configure a point-to-point tunnel with IPv6 addresses like in IPv4 today. This is only possible with the first (ifconfig+route - deprecated) and third (ip+route) tunnel setup. In such cases, you can add the IPv6 address to the tunnel interface like shown on interface configuration.
|9.4. Setup of 6to4 tunnels

Pay attention that the support of 6to4 tunnels currently lacks on vanilla kernel series 2.2.x (see systemcheck/kernel for more information). Also note that that the prefix length for a 6to4 address is 16 because of from network point of view, all other 6to4 enabled hosts are on the same layer 2.
|9.4.1. Add a 6to4 tunnel

First, you have to calculate your 6to4 prefix using your local assigned global routable IPv4 address (if your host has no global routable IPv4 address, in special cases NAT on border gateways is possible):

Assuming your IPv4 address is

|1.2.3.4

the generated 6to4 prefix will be

|2002:0102:0304::

Local 6to4 gateways should (but it's not a must, you can choose an arbitrary suffix with local-scope, if you feel better) always assigned the suffix “::1”, therefore your local 6to4 address will be

|2002:0102:0304::1

Use e.g. following for automatic generation:

ipv4="1.2.3.4"; printf "2002:%02x%02x:%02x%02x::1" `echo $ipv4 | tr "." " "`

There are two ways possible to setup 6to4 tunneling now.
|9.4.1.1. Using "ip" and a dedicated tunnel device

This is now the recommended way (a TTL must be specified because the default value is 0).

Create a new tunnel device

# /sbin/ip tunnel add tun6to4 mode sit ttl <ttldefault> remote any local
¬ <localipv4address> 

Bring interface up

# /sbin/ip link set dev tun6to4 up 

Add local 6to4 address to interface (note: prefix length 16 is important!)

# /sbin/ip -6 addr add <local6to4address>/16 dev tun6to4 

Add (default) route to the global IPv6 network using the all-6to4-routers IPv4 anycast address

# /sbin/ip -6 route add 2000::/3 via ::192.88.99.1 dev tun6to4 metric 1

It was reported that some versions of “ip” (e.g. SuSE Linux 9.0) don't support IPv4-compatible IPv6 addresses for gateways, in this case the related IPv6 address has to be used:

# /sbin/ip -6 route add 2000::/3 via 2002:c058:6301::1 dev tun6to4 metric 1

|9.4.1.2. Using "ifconfig" and "route" and generic tunnel device “sit0” (deprecated)

This is now deprecated because using the generic tunnel device sit0 doesn't let specify filtering per device.

Bring generic tunnel interface sit0 up

# /sbin/ifconfig sit0 up 

Add local 6to4 address to interface

# /sbin/ifconfig sit0 add <local6to4address>/16

Add (default) route to the global IPv6 network using the all-6to4-relays IPv4 anycast address

# /sbin/route -A inet6 add 2000::/3 gw ::192.88.99.1 dev sit0

|9.4.2. Remove a 6to4 tunnel
|9.4.2.1. Using "ip" and a dedicated tunnel device

Remove all routes through this dedicated tunnel device

# /sbin/ip -6 route flush dev tun6to4

Shut down interface

# /sbin/ip link set dev tun6to4 down

Remove created tunnel device

# /sbin/ip tunnel del tun6to4 

|9.4.2.2. Using “ifconfig” and “route” and generic tunnel device “sit0” (deprecated)

Remove (default) route through the 6to4 tunnel interface

# /sbin/route -A inet6 del 2000::/3 gw ::192.88.99.1 dev sit0

Remove local 6to4 address to interface

# /sbin/ifconfig sit0 del <local6to4address>/16

Shut down generic tunnel device (take care about this, perhaps it's still in use...)

# /sbin/ifconfig sit0 down 

Chapter 10. Configuring IPv4-in-IPv6 tunnels

RFC 2473 / Generic Packet Tunneling in IPv6 Specification specifies mechanisms to tunnel several different packet types over IPv6 including IPv4.

NOTE: Support for IPv4-in-IPv6 tunnel is available only since kernel version 2.6.22.
|10.1. Displaying existing tunnels

Usage:

# /sbin/ip -6 tunnel show [<device>]

Example:

# /sbin/ip -6 tunnel show mode any
ip6tnl0: ipv6/ipv6 remote :: local :: encaplimit 0 hoplimit 0 tclass 0x00
¬ flowlabel 0x00000 (flowinfo 0x00000000)
ip6tnl1: ip/ipv6 remote fd00:0:0:2::a local fd00:0:0:2::1 dev eth1 encaplimit 4
¬ hoplimit 64 tclass 0x00 flowlabel 0x00000 (flowinfo 0x00000000)

NOTE: If you don't include "mode any", only IPv6-in-IPv6 tunnels are displayed.
|10.2. Setup of point-to-point tunnel

Usage for creating a 4over6 tunnel device (but it's not up afterward)

# /sbin/ip tunnel add <device> mode ip4ip6 remote <ipv6addressofforeigntunnel>
¬ local <ipv6addresslocal>

Usage (generic example for three tunnels):

# /sbin/ip -6 tunnel add ip6tnl1 mode ip4ip6 remote
¬ <ipv6addressofforeigntunnel1> local <ipv6addresslocal>
# /sbin/ip link set dev ip6tnl1 up 
# /sbin/ip -6 route add <prefixtoroute1> dev ip6tnl1 metric 1

# /sbin/ip -6 tunnel add ip6tnl2 mode ip4ip6 remote
¬ <ipv6addressofforeigntunnel2> local <ipv6addresslocal>
# /sbin/ip link set dev ip6tnl2 up
# /sbin/ip -6 route add <prefixtoroute2> dev ip6tnl2 metric 1

# /sbin/ip -6 tunnel add ip6tnl3 mode ip4ip6 remote
¬ <ipv6addressofforeigntunnel3> local <ipv6addresslocal>
# /sbin/ip link set dev ip6tnl3 up
# /sbin/ip -6 route add <prefixtoroute3> dev ip6tnl3 metric 1

|10.3. Removing point-to-point tunnels

Usage for removing a tunnel device:

# /sbin/ip -6 tunnel del <device>

Usage (generic example for three tunnels):

# /sbin/ip -6 route del <prefixtoroute1> dev ip6tnl1
# /sbin/ip link set ip6tnl1 down
# /sbin/ip -6 tunnel del ip6tnl1

# /sbin/ip -6 route del <prefixtoroute2> dev ip6tnl2
# /sbin/ip link set ip6tnl2 down
# /sbin/ip -6 tunnel del ip6tnl2

# /sbin/ip -6 route del <prefixtoroute3> dev ip6tnl3
# /sbin/ip link set ip6tnl3 down
# /sbin/ip -6 tunnel del ip6tnl3 

Chapter 11. Kernel settings in /proc-filesystem

Note: the source of this section is mostly the file “ip-sysctl.txt” which is included in current kernel sources in directory “Documentation/networking”. Credits to Pekka Savola for maintaining the IPv6-related part in this file. Also some text is more or less copied & pasted into this document.
|11.1. How to access the /proc-filesystem
|11.1.1. Using “cat” and “echo”

Using “cat” and “echo” is the simplest way to access the /proc filesystem, but some requirements are needed for that

    The /proc-filesystem had to be enabled in kernel, means on compiling following switch has to be set

CONFIG_PROC_FS=y

    The /proc-filesystem was mounted before, which can be tested using

# mount | grep "type proc"
none on /proc type proc (rw)

    You need read and sometimes also write access (normally root only) to the /proc-filesystem

Normally, only entries in /proc/sys/* are writable, the others are readonly and for information retrieving only.
|11.1.1.1. Retrieving a value

The value of an entry can be retrieved using “cat”:

# cat /proc/sys/net/ipv6/conf/all/forwarding
|0

|11.1.1.2. Setting a value

A new value can be set (if entry is writable) using “echo”:

# echo "1" >/proc/sys/net/ipv6/conf/all/forwarding

|11.1.2. Using “sysctl”

Using the “sysctl” program to access the kernel switches is a modern method today. You can use it also, if the /proc-filesystem isn't mounted. But you have only access to /proc/sys/*!

The program “sysctl” is included in package “procps” (on Red Hat Linux systems).

    The sysctl-interface had to be enabled in kernel, means on compiling following switch has to be set

CONFIG_SYSCTL=y

|11.1.2.1. Retrieving a value

The value of an entry can be retrieved now:

# sysctl net.ipv6.conf.all.forwarding
net.ipv6.conf.all.forwarding = 0

|11.1.2.2. Setting a value

A new value can be set (if entry is writable):

# sysctl -w net.ipv6.conf.all.forwarding=1
net.ipv6.conf.all.forwarding = 1

Note: Don't use spaces around the “=” on setting values. Also on multiple values per line, quote them like e.g.

# sysctl -w net.ipv4.ip_local_port_range="32768 61000"
net.ipv4.ip_local_port_range = 32768 61000

|11.1.2.3. Additionals

Note: There are sysctl versions in the wild which displaying “/” instead of the “.”

For more details take a look into sysctl's manpage.

Hint: for digging fast into the settings, use the option “-a” (display all entries) in conjunction with “grep”.
|11.1.3. Values found in /proc-filesystems

There are several formats seen in /proc-filesystem:

    BOOLEAN: simple a “0” (false) or a “1” (true)

    INTEGER: an integer value, can be unsigned, too

    more sophisticated lines with several values: sometimes a header line is displayed also, if not, have a look into the kernel source to retrieve information about the meaning of each value...

|11.2. Entries in /proc/sys/net/ipv6/
|11.2.1. conf/default/*

Change the interface-specific default settings.
|11.2.2. conf/all/*

Change all the interface-specific settings.

Exception: “conf/all/forwarding” has a different meaning here
|11.2.2.1. conf/all/forwarding

    Type: BOOLEAN

This enables global IPv6 forwarding between all interfaces.

In IPv6 you can't control forwarding per device, forwarding control has to be done using IPv6-netfilter (controlled with ip6tables) rulesets and specify input and output devices (see Firewalling/Netfilter6 for more). This is different to IPv4, where you are able to control forwarding per device (decision is made on interface where packet came in).

This also sets all interfaces' Host/Router setting 'forwarding' to the specified value. See below for details. This referred to as global forwarding.

If this value is 0, no IPv6 forwarding is enabled, packets never leave another interface, neither physical nor logical like e.g. tunnels.
|11.2.3. conf/interface/*

Change special settings per interface.

The functional behaviour for certain settings is different depending on whether local forwarding is enabled or not.
|11.2.3.1. accept_ra

    Type: BOOLEAN

    Functional default: enabled if local forwarding is disabled. disabled if local forwarding is enabled.

Accept Router Advertisements, and autoconfigure this interface with received data.
|11.2.3.2. accept_redirects

    Type: BOOLEAN

    Functional default: enabled if local forwarding is disabled. disabled if local forwarding is enabled.

Accept Redirects sent by an IPv6 router.
|11.2.3.3. autoconf

    Type: BOOLEAN

    Functional default: enabled if accept_ra_pinfo is enabled. disabled if accept_ra_pinfo is disabled.

Autoconfigure addresses using prefix information from router advertisements.
|11.2.3.4. dad_transmits

    Type: INTEGER

    Default: 1

The amount of Duplicate Address Detection probes to send.
|11.2.3.5. forwarding

    Type: BOOLEAN

    Default: FALSE if global forwarding is disabled (default), otherwise TRUE

Configure interface-specific Host/Router behaviour.

Note: It is recommended to have the same setting on all interfaces; mixed router/host scenarios are rather uncommon.

    Value FALSE: By default, Host behaviour is assumed. This means:

    IsRouter flag is not set in Neighbour Advertisements.

    Router Solicitations are being sent when necessary.

    If accept_ra is TRUE (default), accept Router Advertisements (and do autoconfiguration).

    If accept_redirects is TRUE (default), accept Redirects.

    Value TRUE: If local forwarding is enabled, Router behaviour is assumed. This means exactly the reverse from the above:

    IsRouter flag is set in Neighbour Advertisements.

    Router Solicitations are not sent.

    Router Advertisements are ignored.

    Redirects are ignored.

|11.2.3.6. hop_limit

    Type: INTEGER

    Default: 64

Default Hop Limit to set.
|11.2.3.7. mtu

    Type: INTEGER

    Default: 1280 (IPv6 required minimum)

Default Maximum Transfer Unit
|11.2.3.8. router_solicitation_delay

    Type: INTEGER

    Default: 1

Number of seconds to wait after interface is brought up before sending Router Solicitations.
|11.2.3.9. router_solicitation_interval

    Type: INTEGER

    Default: 4

Number of seconds to wait between Router Solicitations.
|11.2.3.10. router_solicitations

    Type: INTEGER

    Default: 3

Number of Router Solicitations to send until assuming no routers are present.
|11.2.4. neigh/default/*

Change default settings for neighbor detection and some special global interval and threshold values:
|11.2.4.1. gc_thresh1

    Type: INTEGER

    Default: 128

More to be filled.
|11.2.4.2. gc_thresh2

    Type: INTEGER

    Default: 512

More to be filled.
|11.2.4.3. gc_thresh3

    Type: INTEGER

    Default: 1024

Tuning parameter for neighbour table size.

Increase this value if you have a lot of interfaces and problem with routes start to act mysteriously and fail. Or if a running Zebra (routing daemon) reports:

ZEBRA: netlink-listen error: No buffer space available, type=RTM_NEWROUTE(24),
¬ seq=426, pid=0

|11.2.4.4. gc_interval

    Type: INTEGER

    Default: 30

More to be filled.
|11.2.5. neigh/interface/*

Change special settings per interface for neighbor detection.
|11.2.5.1. anycast_delay

    Type: INTEGER

    Default: 100

More to be filled.
|11.2.5.2. gc_stale_time

    Type: INTEGER

    Default: 60

More to be filled.
|11.2.5.3. proxy_qlen

    Type: INTEGER

    Default: 64

More to be filled.
|11.2.5.4. unres_qlen

    Type: INTEGER

    Default: 3

More to be filled.
|11.2.5.5. app_solicit

    Type: INTEGER

    Default: 0

More to be filled.
|11.2.5.6. locktime

    Type: INTEGER

    Default: 0

More to be filled.
|11.2.5.7. retrans_time

    Type: INTEGER

    Default: 100

More to be filled.
|11.2.5.8. base_reachable_time

    Type: INTEGER

    Default: 30

More to be filled.
|11.2.5.9. mcast_solicit

    Type: INTEGER

    Default: 3

More to be filled.
|11.2.5.10. ucast_solicit

    Type: INTEGER

    Default: 3

More to be filled
|11.2.5.11. delay_first_probe_time

    Type: INTEGER

    Default: 5

More to be filled.
|11.2.5.12. proxy_delay

    Type: INTEGER

    Default: 80

More to be filled.
|11.2.6. route/*

Change global settings for routing.
|11.2.6.1. flush

Removed in newer kernel releases - more to be filled.
|11.2.6.2. gc_interval

    Type: INTEGER

    Default: 30

More to be filled.
|11.2.6.3. gc_thresh

    Type: INTEGER

    Default: 1024

More to be filled.
|11.2.6.4. mtu_expires

    Type: INTEGER

    Default: 600

More to be filled.
|11.2.6.5. gc_elasticity

    Type: INTEGER

    Default: 0

More to be filled.
|11.2.6.6. gc_min_interval

    Type: INTEGER

    Default: 5

More to be filled.
|11.2.6.7. gc_timeout

    Type: INTEGER

    Default: 60

More to be filled.
|11.2.6.8. min_adv_mss

    Type: INTEGER

    Default: 12

More to be filled.
|11.2.6.9. max_size

    Type: INTEGER

    Default: 4096

More to be filled.
|11.3. IPv6-related entries in /proc/sys/net/ipv4/

At the moment (and this will be until IPv4 is completly converted to an independend kernel module) some switches are also used here for IPv6.
|11.3.1. ip_*
|11.3.1.1. ip_local_port_range

This control setting is used by IPv6 also.
|11.3.2. tcp_*

This control settings are used by IPv6 also.
|11.3.3. icmp_*

This control settings are not used by IPv6. To enable ICMPv6 rate limiting (which is very recommended because of the capability of ICMPv6 storms) netfilter-v6 rules must be used.
|11.3.4. others

Unknown, but probably not used by IPv6.
|11.4. IPv6-related entries in /proc/net/

In /proc/net there are several read-only entries available. You cannot retrieve information using “sysctl” here, so use e.g. “cat”.
|11.4.1. if_inet6

    Type: One line per addresss containing multiple values

Here all configured IPv6 addresses are shown in a special format. The example displays for loopback interface only. The meaning is shown below (see “net/ipv6/addrconf.c” for more).

# cat /proc/net/if_inet6
|00000000000000000000000000000001 01 80 10 80 lo
+------------------------------+ ++ ++ ++ ++ ++
|                                |  |  |  |  |
|1                                2  3  4  5  6

    IPv6 address displayed in 32 hexadecimal chars without colons as separator

    Netlink device number (interface index) in hexadecimal (see “ip addr” , too)

    Prefix length in hexadecimal

    Scope value (see kernel source “ include/net/ipv6.h” and “net/ipv6/addrconf.c” for more)

    Interface flags (see “include/linux/rtnetlink.h” and “net/ipv6/addrconf.c” for more)

    Device name

|11.4.2. ipv6_route

    Type: One line per route containing multiple values

Here all configured IPv6 routes are shown in a special format. The example displays for loopback interface only. The meaning is shown below (see “net/ipv6/route.c” for more).

# cat /proc/net/ipv6_route
|00000000000000000000000000000000 00 00000000000000000000000000000000 00
+------------------------------+ ++ +------------------------------+ ++
|                                |  |                                |
|1                                2  3                                4

¬ 00000000000000000000000000000000 ffffffff 00000001 00000001 00200200 lo
¬ +------------------------------+ +------+ +------+ +------+ +------+ ++
¬ |                                |        |        |        |        |
¬ 5                                6        7        8        9        10

    IPv6 destination network displayed in 32 hexadecimal chars without colons as separator

    IPv6 destination prefix length in hexadecimal

    IPv6 source network displayed in 32 hexadecimal chars without colons as separator

    IPv6 source prefix length in hexadecimal

    IPv6 next hop displayed in 32 hexadecimal chars without colons as separator

    Metric in hexadecimal

    Reference counter

    Use counter

    Flags

    Device name

|11.4.3. sockstat6

    Type: One line per protocol with description and value

Statistics about used IPv6 sockets. Example:

# cat /proc/net/sockstat6
TCP6: inuse 7 
UDP6: inuse 2 
RAW6: inuse 1 
FRAG6: inuse 0 memory 0

|11.4.4. tcp6

To be filled.
|11.4.5. udp6

To be filled.
|11.4.6. igmp6

To be filled.
|11.4.7. raw6

To be filled.
|11.4.8. ip6_flowlabel

To be filled.
|11.4.9. rt6_stats

To be filled.
|11.4.10. snmp6

    Type: One line per SNMP description and value

SNMP statistics, can be retrieved via SNMP server and related MIB table by network management software.
|11.4.11. ip6_tables_names

Available netfilter6 tables
Chapter 12. Netlink-Interface to kernel

To be filled...I have no experience with that...
Chapter 13. Address Resolver

Name to IPv4 or IPv6 address resolving is usually done using a libc resolver library. There are some issues known using the function getaddrinfo.

More info can be found at Linux & IPv6: getaddrinfo and search domains - Research and RFC 3484 on Linux.

More to be filled later...
Chapter 14. Network debugging
|14.1. Server socket binding
|14.1.1. Using “netstat” for server socket binding check

It's always interesting which server sockets are currently active on a node. Using “netstat” is a short way to get such information:

Used options: -nlptu

Example:

# netstat -nlptu
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State      
¬ PID/Program name
tcp        0      0 0.0.0.0:32768           0.0.0.0:*               LISTEN     
¬ 1258/rpc.statd
tcp        0      0 0.0.0.0:32769           0.0.0.0:*               LISTEN     
¬ 1502/rpc.mountd
tcp        0      0 0.0.0.0:515             0.0.0.0:*               LISTEN     
¬ 22433/lpd Waiting
tcp        0      0 1.2.3.1:139             0.0.0.0:*               LISTEN     
¬ 1746/smbd
tcp        0      0 0.0.0.0:111             0.0.0.0:*               LISTEN     
¬ 1230/portmap
tcp        0      0 0.0.0.0:6000            0.0.0.0:*               LISTEN     
¬ 3551/X
tcp        0      0 1.2.3.1:8081            0.0.0.0:*               LISTEN     
¬ 18735/junkbuster
tcp        0      0 1.2.3.1:3128            0.0.0.0:*               LISTEN     
¬ 18822/(squid)
tcp        0      0 127.0.0.1:953           0.0.0.0:*               LISTEN     
¬ 30734/named
tcp        0      0 ::ffff:1.2.3.1:993      :::*                    LISTEN     
¬ 6742/xinetd-ipv6
tcp        0      0 :::13                   :::*                    LISTEN     
¬ 6742/xinetd-ipv6
tcp        0      0 ::ffff:1.2.3.1:143      :::*                    LISTEN     
¬ 6742/xinetd-ipv6
tcp        0      0 :::53                   :::*                    LISTEN     
¬ 30734/named
tcp        0      0 :::22                   :::*                    LISTEN     
¬ 1410/sshd
tcp        0      0 :::6010                 :::*                    LISTEN     
¬ 13237/sshd
udp        0      0 0.0.0.0:32768           0.0.0.0:*                          
¬ 1258/rpc.statd
udp        0      0 0.0.0.0:2049            0.0.0.0:*                          
¬ -
udp        0      0 0.0.0.0:32770           0.0.0.0:*                          
¬ 1502/rpc.mountd
udp        0      0 0.0.0.0:32771           0.0.0.0:*                          
¬ -
udp        0      0 1.2.3.1:137             0.0.0.0:*                          
¬ 1751/nmbd
udp        0      0 0.0.0.0:137             0.0.0.0:*                          
¬ 1751/nmbd
udp        0      0 1.2.3.1:138             0.0.0.0:*                          
¬ 1751/nmbd
udp        0      0 0.0.0.0:138             0.0.0.0:*                          
¬ 1751/nmbd
udp        0      0 0.0.0.0:33044           0.0.0.0:*                          
¬ 30734/named
udp        0      0 1.2.3.1:53              0.0.0.0:*                          
¬ 30734/named
udp        0      0 127.0.0.1:53            0.0.0.0:*                          
¬ 30734/named
udp        0      0 0.0.0.0:67              0.0.0.0:*                          
¬ 1530/dhcpd
udp        0      0 0.0.0.0:67              0.0.0.0:*                          
¬ 1530/dhcpd
udp        0      0 0.0.0.0:32858           0.0.0.0:*                          
¬ 18822/(squid)
udp        0      0 0.0.0.0:4827            0.0.0.0:*                          
¬ 18822/(squid)
udp        0      0 0.0.0.0:111             0.0.0.0:*                          
¬ 1230/portmap
udp        0      0 :::53                   :::*                               
¬ 30734/named

|14.2. Examples for tcpdump packet dumps

Here some examples of captured packets are shown, perhaps useful for your own debugging...

...more coming next...
|14.2.1. Router discovery
|14.2.1.1. Router advertisement

|15:43:49.484751 fe80::212:34ff:fe12:3450 > ff02::1: icmp6: router
¬ advertisement(chlim=64, router_ltime=30, reachable_time=0,
¬ retrans_time=0)(prefix info: AR valid_ltime=30, preffered_ltime=20,
¬ prefix=2002:0102:0304:1::/64)(prefix info: LAR valid_ltime=2592000,
¬ preffered_ltime=604800, prefix=2001:0db8:0:1::/64)(src lladdr:
¬ 0:12:34:12:34:50) (len 88, hlim 255)

Router with link-local address “fe80::212:34ff:fe12:3450” send an advertisement to the all-node-on-link multicast address “ff02::1” containing two prefixes “2002:0102:0304:1::/64” (lifetime 30 s) and “2001:0db8:0:1::/64” (lifetime 2592000 s) including its own layer 2 MAC address “0:12:34:12:34:50”.
|14.2.1.2. Router solicitation

|15:44:21.152646 fe80::212:34ff:fe12:3456 > ff02::2: icmp6: router solicitation
¬ (src lladdr: 0:12:34:12:34:56) (len 16, hlim 255)

Node with link-local address “fe80::212:34ff:fe12:3456” and layer 2 MAC address “0:12:34:12:34:56” is looking for a router on-link, therefore sending this solicitation to the all-router-on-link multicast address “ff02::2”.
|14.2.2. Neighbor discovery
|14.2.2.1. Neighbor discovery solicitation for duplicate address detection

Following packets are sent by a node with layer 2 MAC address “0:12:34:12:34:56” during autoconfiguration to check whether a potential address is already used by another node on the link sending this to the solicited-node link-local multicast address.

    Node wants to configure its link-local address “fe80::212:34ff:fe12:3456”, checks for duplicate now

|15:44:17.712338 :: > ff02::1:ff12:3456: icmp6: neighbor sol: who has
¬ fe80::212:34ff:fe12:3456(src lladdr: 0:12:34:12:34:56) (len 32, hlim 255)

    Node wants to configure its global address “2002:0102:0304:1:212:34ff:fe12:3456” (after receiving advertisement shown above), checks for duplicate now

|15:44:21.905596 :: > ff02::1:ff12:3456: icmp6: neighbor sol: who has
¬ 2002:0102:0304:1:212:34ff:fe12:3456(src lladdr: 0:12:34:12:34:56) (len 32,
¬ hlim 255)

    Node wants to configure its global address “2001:0db8:0:1:212:34ff:fe12:3456” (after receiving advertisement shown above), checks for duplicate now

|15:44:22.304028 :: > ff02::1:ff12:3456: icmp6: neighbor sol: who has
¬ 2001:0db8:0:1:212:34ff:fe12:3456(src lladdr: 0:12:34:12:34:56) (len 32, hlim
¬ 255)

|14.2.2.2. Neighbor discovery solicitation for looking for host or gateway

    Node wants to send packages to “2001:0db8:0:1::10” but has no layer 2 MAC address to send packet, so send solicitation now

|13:07:47.664538 2002:0102:0304:1:2e0:18ff:fe90:9205 > ff02::1:ff00:10: icmp6:
¬ neighbor sol: who has 2001:0db8:0:1::10(src lladdr: 0:e0:18:90:92:5) (len 32,
¬ hlim 255)

    Node looks for “fe80::10” now

|13:11:20.870070 fe80::2e0:18ff:fe90:9205 > ff02::1:ff00:10: icmp6: neighbor
¬ sol: who has fe80::10(src lladdr: 0:e0:18:90:92:5) (len 32, hlim 255)

Chapter 15. Support for persistent IPv6 configuration in Linux distributions

Some Linux distribution contain already support of a persistent IPv6 configuration using existing or new configuration and script files and some hook in the IPv4 script files.
|15.1. Red Hat Linux and “clones”

Since starting writing the IPv6 & Linux - HowTo it was my intention to enable a persistent IPv6 configuration which catch most of the wished cases like host-only, router-only, dual-homed-host, router with second stub network, normal tunnels, 6to4 tunnels, and so on. Nowadays there exists a set of configuration and script files which do the job very well (never heard about real problems, but I don't know how many use the set). Because this configuration and script files are extended from time to time, they got their own homepage: initscripts-ipv6 homepage (Mirror). Because I began my IPv6 experience using a Red Hat Linux 5.0 clone, my IPv6 development systems are mostly Red Hat Linux based now, it's kind a logic that the scripts are developed for this kind of distribution (so called historic issue). Also it was very easy to extend some configuration files, create new ones and create some simple hook for calling IPv6 setup during IPv4 setup.

Fortunately, in Red Hat Linux since 7.1 a snapshot of my IPv6 scripts is included, this was and is still further on assisted by Pekka Savola.

Mandrake since version 8.0 also includes an IPv6-enabled initscript package, but a minor bug still prevents usage (“ifconfig” misses “inet6” before “add”).
|15.1.1. Test for IPv6 support of network configuration scripts

You can test, whether your Linux distribution contain support for persistent IPv6 configuration using my set. Following script library should exist:

/etc/sysconfig/network-scripts/network-functions-ipv6

Auto-magically test:

# test -f /etc/sysconfig/network-scripts/network-functions-ipv6 && echo "Main
¬ IPv6 script library exists"

The version of the library is important if you miss some features. You can get it executing following (or easier look at the top of the file):

# source /etc/sysconfig/network-scripts/network-functions-ipv6 &&
¬ getversion_ipv6_functions 
|20011124

In shown example, the used version is 20011124. Check this against latest information on initscripts-ipv6 homepage (Mirror) to see what has been changed. You will find there also a change-log.
|15.1.2. Short hint for enabling IPv6 on current RHL 7.1, 7.2, 7.3, ...

    Check whether running system has already IPv6 module loaded

# modprobe -c | grep net-pf-10
alias net-pf-10 off

    If result is “off”, then enable IPv6 networking by editing /etc/sysconfig/network, add following new line

NETWORKING_IPV6=yes

    Reboot or restart networking using

# service network restart

    Now IPv6 module should be loaded

# modprobe -c | grep ipv6
alias net-pf-10 ipv6

If your system is on a link which provides router advertisement, autoconfiguration will be done automatically. For more information which settings are supported see /usr/share/doc/initscripts-$version/sysconfig.txt.
|15.2. SuSE Linux

In newer 7.x versions there is a really rudimentary support available, see /etc/rc.config for details.

Because of the really different configuration and script file structure it is hard (or impossible) to use the set for Red Hat Linux and clones with this distribution. In versions 8.x they completly change their configuration setup.
|15.2.1. SuSE Linux 7.3

    How to setup 6to4 IPv6 with SuSE 7.3

|15.2.2. SuSE Linux 8.0
|15.2.2.1. IPv6 address configuration

Edit file /etc/sysconfig/network/ifcfg-<Interface-Name> and setup following value

IP6ADDR="<ipv6-address>/<prefix>"

|15.2.2.2. Additional information

See file /usr/share/doc/packages/sysconfig/README
|15.2.3. SuSE Linux 8.1
|15.2.3.1. IPv6 address configuration

Edit file /etc/sysconfig/network/ifcfg-<Interface-Name> and setup following value

IPADDR="<ipv6-address>/<prefix>"

|15.2.3.2. Additional information

See file /usr/share/doc/packages/sysconfig/Network
|15.3. Debian Linux

Following information was contributed by Stephane Bortzmeyer <bortzmeyer at nic dot fr>

    Be sure that IPv6 is loaded, either because it is compiled into the kernel or because the module is loaded. For the latest, three solutions, adding it to /etc/modules, using the pre-up trick shown later or using kmod (not detailed here).

    Configure your interface. Here we assume eth0 and address (2001:0db8:1234:5::1:1). Edit /etc/network/interfaces:

iface eth0 inet6 static
        pre-up modprobe ipv6
        address 2001:0db8:1234:5::1:1
        # To suppress completely autoconfiguration:
        # up echo 0 > /proc/sys/net/ipv6/conf/all/autoconf
        netmask 64
        # The router is autoconfigured and has no fixed address.
        # It is magically
        # found. (/proc/sys/net/ipv6/conf/all/accept_ra). Otherwise:
        #gateway 2001:0db8:1234:5::1

And you reboot or you just

# ifup --force eth0

and you have your static address.
|15.3.1. Further information

    IPv6 with Debian Linux

    Jean-Marc V. Liotier's HOWTO for Freenet6 & Debian Users (announced 24.12.2002 on mailinglist users@ipv6.org )

Chapter 16. Auto-configuration
|16.1. Stateless auto-configuration

Is supported and seen on the assigned link-local address after an IPv6-enabled interface is up.

Example:

# ip -6 addr show dev eth0 scope link
|2: eth0: <BROADCAST,MULTICAST,UP> mtu 1500 qlen1000
    inet6 fe80::211:d8ff:fe6b:f0f5/64 scope link
       valid_lft forever preferred_lft forever 

|16.2. Stateful auto-configuration using Router Advertisement Daemon (radvd)

to be filled. See radvd daemon autoconfiguration below.
|16.3. Dynamic Host Configuration Protocol v6 (DHCPv6)

After a long time discussing issues, finally RFC 3315 / Dynamic Host Configuration Protocol for IPv6 (DHCPv6) was finished. At time updating this part (10/2005) currently two implementations are available:

    Dibbler by Tomasz Mrugalski <thomson at klub dot com dot pl> (Hints for configuration)

    DHCPv6 on Sourceforge (Hints for configuration)

    ISC DHCP (Hints for configuration)

Chapter 17. Mobility
|17.1. Common information
|17.1.1. Node Mobility

Support for IPv6 mobility can be enabled in Linux by installing the MIPL2 implementation found at: http://www.mobile-ipv6.org/

This implementation is compliant with RFC 3775. It is composed of a kernel patch and a mobility daemon called mip6d. Version 2.0.1 applies on Linux kernel 2.6.15.

Installation and setup are described in the Linux Mobile IPv6 HOWTO.
|17.1.2. Network Mobility

There also exists an implementation of network mobility for Linux, it is called NEPL and is based on MIPL. It can also be downloaded from: http://www.mobile-ipv6.org/.

The HOWTO document describing setup and configuration is available at: http://www.nautilus6.org/doc/nepl-howto/.
|17.1.3. Links

    Mobile IPv6 for Linux (MIPL) project: http://www.mobile-ipv6.org/

    Nautilus6 working group: http://nautilus6.org/

    Fast Handovers for Mobile IPv6 for Linux project: http://www.fmipv6.org/

    USAGI-patched Mobile IPv6 for Linux (UMIP):http://umip.linux-ipv6.org/

    Deploying IPsec/IKE-protected MIPv6 under Linux:http://natisbad.org/MIPv6/

    RFC 3775 / Mobility Support in IPv6

    RFC 3776 / Using IPsec to Protect Mobile IPv6 Signaling Between Mobile Nodes and Home Agents

    RFC 3963 / Network Mobility (NEMO)

    RFC 4068 / Fast Handovers for Mobile IPv6

    RFC 4423 / Host Identity Protocol (HIP) Architecture

    RFC 5201 / Host Identity Protocol

    HIP implementations: http://infrahip.hiit.fi/, http://hip4inter.net/, http://www.openhip.org/

Chapter 18. Firewalling

IPv6 firewalling is important, especially if using IPv6 on internal networks with global IPv6 addresses. Because unlike at IPv4 networks where in common internal hosts are protected automatically using private IPv4 addresses like RFC 1918 / Address Allocation for Private Internets or Automatic Private IP Addressing (APIPA)Google search for Microsoft + APIPA, in IPv6 normally global addresses are used and someone with IPv6 connectivity can reach all internal IPv6 enabled nodes.
|18.1. Firewalling using netfilter6

Native IPv6 firewalling is only supported in kernel versions 2.4+. In older 2.2- you can only filter IPv6-in-IPv4 by protocol 41.

Attention: no warranty that described rules or examples can really protect your system!

Audit your ruleset after installation, see Section 19.3 for more.

Since kernel version 2.6.20 IPv6 connection tracking is fully working (and does not break IPv4 NAT anymore like versions before)
|18.1.1. More information

    Netfilter project

    maillist archive of netfilter users

    maillist archive of netfilter developers

    Unofficial status informations

|18.2. Preparation

This step is only needed if distributed kernel and netfilter doesn't fit your requirements and new features are available but still not built-in.
|18.2.1. Get sources

Get the latest kernel source: http://www.kernel.org/

Get the latest iptables package:

    Source tarball (for kernel patches): http://www.netfilter.org/

|18.2.2. Extract sources

Change to source directory:

# cd /path/to/src 

Unpack and rename kernel sources

# tar z|jxf kernel-version.tar.gz|bz2 
# mv linux linux-version-iptables-version+IPv6 

Unpack iptables sources

# tar z|jxf iptables-version.tar.gz|bz2 

|18.2.3. Apply latest iptables/IPv6-related patches to kernel source

Change to iptables directory

# cd iptables-version 

Apply pending patches

# make pending-patches KERNEL_DIR=/path/to/src/linux-version-iptables-version/ 

Apply additional IPv6 related patches (still not in the vanilla kernel included)

# make patch-o-matic KERNEL_DIR=/path/to/src/linux-version-iptables-version/ 

Say yes at following options (iptables-1.2.2)

    ah-esp.patch

    masq-dynaddr.patch (only needed for systems with dynamic IP assigned WAN connections like PPP or PPPoE)

    ipv6-agr.patch.ipv6

    ipv6-ports.patch.ipv6

    LOG.patch.ipv6

    REJECT.patch.ipv6 

Check IPv6 extensions

# make print-extensions 
Extensions found: IPv6:owner IPv6:limit IPv6:mac IPv6:multiport

|18.2.4. Configure, build and install new kernel

Change to kernel sources

# cd /path/to/src/linux-version-iptables-version/ 

Edit Makefile

- EXTRAVERSION = 
+ EXTRAVERSION = -iptables-version+IPv6-try 

Run configure, enable IPv6 related

            Code maturity level options 
                  Prompt for development and/or incomplete code/drivers : yes 
            Networking options 
                  Network packet filtering: yes 
                  The IPv6 protocol: module 
                       IPv6: Netfilter Configuration 
                             IP6 tables support: module 
                             All new options like following: 
                                   limit match support: module 
                                   MAC address match support: module 
                                   Multiple port match support: module 
                                   Owner match support: module 
                                   netfilter MARK match support: module 
                                   Aggregated address check: module 
                                   Packet filtering: module 
                                        REJECT target support: module 
                                        LOG target support: module 
                                   Packet mangling: module 
                                   MARK target support: module 

Configure other related to your system, too

Compilation and installing: see the kernel section here and other HOWTOs
|18.2.5. Rebuild and install binaries of iptables

Make sure, that upper kernel source tree is also available at /usr/src/linux/

Rename older directory

# mv /usr/src/linux /usr/src/linux.old 

Create a new softlink

# ln -s /path/to/src/linux-version-iptables-version /usr/src/linux 

Rebuild SRPMS

# rpm --rebuild /path/to/SRPMS/iptables-version-release.src.rpm 

Install new iptables packages (iptables + iptables-ipv6)

    On RH 7.1 systems, normally, already an older version is installed, therefore use "freshen" 

# rpm -Fhv /path/to/RPMS/cpu/iptables*-version-release.cpu.rpm 

    If not already installed, use "install" 

# rpm -ihv /path/to/RPMS/cpu/iptables*-version-release.cpu.rpm 

    On RH 6.2 systems, normally, no kernel 2.4.x is installed, therefore the requirements don't fit. Use "--nodeps" to install it 

# rpm -ihv --nodeps /path/to/RPMS/cpu/iptables*-version-release.cpu.rpm 

Perhaps it's necessary to create a softlink for iptables libraries where iptables looks for them

# ln -s /lib/iptables/ /usr/lib/iptables 

|18.3. Usage
|18.3.1. Check for support

Load module, if so compiled

# modprobe ip6_tables 

Check for capability

# [ ! -f /proc/net/ip6_tables_names ] && echo "Current kernel doesn't support
¬ 'ip6tables' firewalling (IPv6)!" 

|18.3.2. Learn how to use ip6tables
|18.3.2.1. List all IPv6 netfilter entries

    Short 

# ip6tables -L 

    Extended 

# ip6tables -n -v --line-numbers -L 

|18.3.2.2. List specified filter

# ip6tables -n -v --line-numbers -L INPUT 

|18.3.2.3. Insert a log rule at the input filter with options

# ip6tables --table filter --append INPUT  -j LOG --log-prefix "INPUT:"
¬ --log-level 7 

|18.3.2.4. Insert a drop rule at the input filter

# ip6tables --table filter --append INPUT  -j DROP 

|18.3.2.5. Delete a rule by number

# ip6tables --table filter --delete INPUT 1 

|18.3.2.6. Enable connection tracking

Since kernel version 2.6.20 IPv6 connection tracking is well supported and should be used instead of using stateless filter rules.

# ip6tables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT

|18.3.2.7. Allow ICMPv6

Using older kernels (unpatched kernel 2.4.5 and iptables-1.2.2) no type can be specified

    Accept incoming ICMPv6 through tunnels 

# ip6tables -A INPUT -i sit+ -p icmpv6 -j ACCEPT 

    Allow outgoing ICMPv6 through tunnels 

# ip6tables -A OUTPUT -o sit+ -p icmpv6 -j ACCEPT 

Newer kernels allow specifying of ICMPv6 types:

# ip6tables -A INPUT -p icmpv6 --icmpv6-type echo-request -j ACCEPT

|18.3.2.8. Rate-limiting

Because it can happen (author already saw it to times) that an ICMPv6 storm will raise up, you should use available rate limiting for at least ICMPv6 ruleset. In addition logging rules should also get rate limiting to prevent DoS attacks against syslog and storage of log file partition. An example for a rate limited ICMPv6 looks like:

# ip6tables -A INPUT --protocol icmpv6 --icmpv6-type echo-request
¬ -j ACCEPT --match limit --limit 30/minute

|18.3.2.9. Allow incoming SSH

Here an example is shown for a ruleset which allows incoming SSH connection from a specified IPv6 address

    Allow incoming SSH from 2001:0db8:100::1/128 

# ip6tables -A INPUT -i sit+ -p tcp -s 2001:0db8:100::1/128 --sport 512:65535
¬ --dport 22 -j ACCEPT 

    Allow response packets (no longer needed if connection tracking is used!) 

# ip6tables -A OUTPUT -o sit+ -p tcp -d 2001:0db8:100::1/128 --dport 512:65535
¬ --sport 22 ! --syn -j ACCEPT 

|18.3.2.10. Enable tunneled IPv6-in-IPv4

To accept tunneled IPv6-in-IPv4 packets, you have to insert rules in your IPv4 firewall setup relating to such packets, for example

    Accept incoming IPv6-in-IPv4 on interface ppp0 

# iptables -A INPUT -i ppp0 -p ipv6 -j ACCEPT 

    Allow outgoing IPv6-in-IPv4 to interface ppp0 

# iptables -A OUTPUT -o ppp0 -p ipv6 -j ACCEPT 

If you have only a static tunnel, you can specify the IPv4 addresses, too, like

    Accept incoming IPv6-in-IPv4 on interface ppp0 from tunnel endpoint 192.0.2.2

# iptables -A INPUT -i ppp0 -p ipv6 -s 192.0.2.2 -j ACCEPT 

    Allow outgoing IPv6-in-IPv4 to interface ppp0 to tunnel endpoint 1.2.3.4 

# iptables -A OUTPUT -o ppp0 -p ipv6 -d 192.0.2.2 -j ACCEPT 

|18.3.2.11. Protection against incoming TCP connection requests

VERY RECOMMENDED! For security issues you should really insert a rule which blocks incoming TCP connection requests. Adapt "-i" option, if other interface names are in use!

    Block incoming TCP connection requests to this host 

# ip6tables -I INPUT -i sit+ -p tcp --syn -j DROP 

    Block incoming TCP connection requests to hosts behind this router 

# ip6tables -I FORWARD -i sit+ -p tcp --syn -j DROP 

Perhaps the rules have to be placed below others, but that is work you have to think about it. Best way is to create a script and execute rules in a specified way.
|18.3.2.12. Protection against incoming UDP connection requests

ALSO RECOMMENDED! Like mentioned on my firewall information it's possible to control the ports on outgoing UDP/TCP sessions. So if all of your local IPv6 systems are using local ports e.g. from 32768 to 60999 you are able to filter UDP connections also (until connection tracking works) like:

    Block incoming UDP packets which cannot be responses of outgoing requests of this host 

# ip6tables -I INPUT -i sit+ -p udp ! --dport 32768:60999 -j DROP 

    Block incoming UDP packets which cannot be responses of forwarded requests of hosts behind this router 

# ip6tables -I FORWARD -i sit+ -p udp ! --dport 32768:60999 -j DROP 

|18.3.3. Examples
|18.3.3.1. Simple example for Fedora

Following lines show a simple firewall configuration for Fedora 6 (since kernel version 2.6.20). It was modfied from the default one (generated by system-config-firewall) for supporting connection tracking and return the proper ICMPv6 code for rejects. Incoming SSH (port 22) connections are allowed.

File: /etc/sysconfig/ip6tables

*filter :INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
:RH-Firewall-1-INPUT - [0:0]
-A INPUT -j RH-Firewall-1-INPUT
-A FORWARD -j RH-Firewall-1-INPUT
-A RH-Firewall-1-INPUT -i lo -j ACCEPT
-A RH-Firewall-1-INPUT -p icmpv6 -j ACCEPT
-A RH-Firewall-1-INPUT -p 50 -j ACCEPT
-A RH-Firewall-1-INPUT -p 51 -j ACCEPT
-A RH-Firewall-1-INPUT -p udp --dport 5353 -d ff02::fb -j ACCEPT
-A RH-Firewall-1-INPUT -p udp -m udp --dport 631 -j ACCEPT 
-A RH-Firewall-1-INPUT -p tcp -m tcp --dport 631 -j ACCEPT
-A RH-Firewall-1-INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
-A RH-Firewall-1-INPUT -m state --state NEW -p tcp --dport 22 -j ACCEPT
-A RH-Firewall-1-INPUT -j REJECT --reject-with icmp6-adm-prohibited
COMMIT 

For completeness also the IPv4 configuration is shown here:

File: /etc/sysconfig/iptables

*filter :INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
:RH-Firewall-1-INPUT - [0:0]
-A INPUT -j RH-Firewall-1-INPUT
-A FORWARD -j RH-Firewall-1-INPUT
-A RH-Firewall-1-INPUT -i lo -j ACCEPT
-A RH-Firewall-1-INPUT -p icmp --icmp-type any -j ACCEPT
-A RH-Firewall-1-INPUT -p 50 -j ACCEPT
-A RH-Firewall-1-INPUT -p 51 -j ACCEPT
-A RH-Firewall-1-INPUT -p udp --dport 5353 -d 224.0.0.251 -j ACCEPT
-A RH-Firewall-1-INPUT -p udp -m udp --dport 631 -j ACCEPT
-A RH-Firewall-1-INPUT -p tcp -m tcp --dport 631 -j ACCEPT
-A RH-Firewall-1-INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT
-A RH-Firewall-1-INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT
-A RH-Firewall-1-INPUT -j REJECT --reject-with icmp-host-prohibited
COMMIT 

Usage:

    Create/modify the configuration files

    Activate IPv4 & IPv6 firewalling

# service iptables start
# service ip6tables start

    Enable automatic start after reboot

# chkconfig iptables on
# chkconfig ip6tables on

|18.3.3.2. Sophisticated example

Following lines show a more sophisticated but still stateless filter setup as an example. Happy netfilter6 ruleset creation....

# ip6tables -n -v -L 
Chain INPUT (policy DROP 0 packets, 0 bytes) 
 pkts bytes target     prot opt in     out     source               destination
|    0     0 extIN      all      sit+   *       ::/0                 ::/0 
|    4   384 intIN      all      eth0   *       ::/0                 ::/0 
|    0     0 ACCEPT     all      *      *       ::1/128              ::1/128 
|    0     0 ACCEPT     all      lo     *       ::/0                 ::/0 
|    0     0 LOG        all      *      *       ::/0                 ::/0       
¬        LOG flags 0 level 7 prefix `INPUT-default:' 
|    0     0 DROP       all      *      *       ::/0                 ::/0 
 
Chain FORWARD (policy DROP 0 packets, 0 bytes) 
 pkts bytes target     prot opt in     out     source               destination
¬ 
|    0     0 int2ext    all      eth0   sit+    ::/0                 ::/0 
|    0     0 ext2int    all      sit+   eth0    ::/0                 ::/0 
|    0     0 LOG        all      *      *       ::/0                 ::/0       
¬        LOG flags 0 level 7 prefix `FORWARD-default:' 
|    0     0 DROP       all      *      *       ::/0                 ::/0 
 
Chain OUTPUT (policy DROP 0 packets, 0 bytes) 
 pkts bytes target     prot opt in     out     source               destination
¬ 
|    0     0 extOUT     all      *      sit+    ::/0                 ::/0 
|    4   384 intOUT     all      *      eth0    ::/0                 ::/0 
|    0     0 ACCEPT     all      *      *       ::1/128              ::1/128 
|    0     0 ACCEPT     all      *      lo      ::/0                 ::/0 
|    0     0 LOG        all      *      *       ::/0                 ::/0       
¬        LOG flags 0 level 7 prefix `OUTPUT-default:' 
|    0     0 DROP       all      *      *       ::/0                 ::/0 
 
Chain ext2int (1 references) 
 pkts bytes target     prot opt in     out     source               destination
¬ 
|    0     0 ACCEPT     icmpv6    *      *       ::/0                 ::/0 
|    0     0 ACCEPT     tcp      *      *       ::/0                 ::/0       
¬        tcp spts:1:65535 dpts:1024:65535 flags:!0x16/0x02 
|    0     0 LOG        all      *      *       ::/0                 ::/0       
¬        LOG flags 0 level 7 prefix `ext2int-default:' 
|    0     0 DROP       tcp      *      *       ::/0                 ::/0 
|    0     0 DROP       udp      *      *       ::/0                 ::/0 
|    0     0 DROP       all      *      *       ::/0                 ::/0 
 
Chain extIN (1 references) 
 pkts bytes target     prot opt in     out     source               destination
¬ 
|    0     0 ACCEPT     tcp      *      *       3ffe:400:100::1/128  ::/0       
¬        tcp spts:512:65535 dpt:22 
|    0     0 ACCEPT     tcp      *      *       3ffe:400:100::2/128  ::/0       
¬        tcp spts:512:65535 dpt:22 
|    0     0 ACCEPT     icmpv6    *      *       ::/0                 ::/0 
|    0     0 ACCEPT     tcp      *      *       ::/0                 ::/0       
¬        tcp spts:1:65535 dpts:1024:65535 flags:!0x16/0x02 
|    0     0 ACCEPT     udp      *      *       ::/0                 ::/0       
¬        udp spts:1:65535 dpts:1024:65535 
|    0     0 LOG        all      *      *       ::/0                 ::/0       
¬        limit: avg 5/min burst 5 LOG flags 0 level 7 prefix `extIN-default:' 
|    0     0 DROP       all      *      *       ::/0                 ::/0 
 
Chain extOUT (1 references) 
 pkts bytes target     prot opt in     out     source               destination
¬ 
|    0     0 ACCEPT     tcp      *      *       ::/0                
¬ 2001:0db8:100::1/128tcp spt:22 dpts:512:65535 flags:!0x16/0x02 
|    0     0 ACCEPT     tcp      *      *       ::/0                
¬ 2001:0db8:100::2/128tcp spt:22 dpts:512:65535 flags:!0x16/0x02 
|    0     0 ACCEPT     icmpv6    *      *       ::/0                 ::/0 
|    0     0 ACCEPT     tcp      *      *       ::/0                 ::/0       
¬        tcp spts:1024:65535 dpts:1:65535 
|    0     0 ACCEPT     udp      *      *       ::/0                 ::/0       
¬        udp spts:1024:65535 dpts:1:65535 
|    0     0 LOG        all      *      *       ::/0                 ::/0       
¬        LOG flags 0 level 7 prefix `extOUT-default:' 
|    0     0 DROP       all      *      *       ::/0                 ::/0 
 
Chain int2ext (1 references) 
 pkts bytes target     prot opt in     out     source               destination
¬ 
|    0     0 ACCEPT     icmpv6    *      *       ::/0                 ::/0 
|    0     0 ACCEPT     tcp      *      *       ::/0                 ::/0       
¬        tcp spts:1024:65535 dpts:1:65535 
|    0     0 LOG        all      *      *       ::/0                 ::/0       
¬        LOG flags 0 level 7 prefix `int2ext:' 
|    0     0 DROP       all      *      *       ::/0                 ::/0 
|    0     0 LOG        all      *      *       ::/0                 ::/0       
¬        LOG flags 0 level 7 prefix `int2ext-default:' 
|    0     0 DROP       tcp      *      *       ::/0                 ::/0 
|    0     0 DROP       udp      *      *       ::/0                 ::/0 
|    0     0 DROP       all      *      *       ::/0                 ::/0 
 
Chain intIN (1 references) 
 pkts bytes target     prot opt in     out     source               destination
¬ 
|    0     0 ACCEPT     all      *      *       ::/0                
¬ fe80::/ffc0:: 
|    4   384 ACCEPT     all      *      *       ::/0                 ff02::/16 
 
Chain intOUT (1 references) 
 pkts bytes target     prot opt in     out     source               destination
¬ 
|    0     0 ACCEPT     all      *      *       ::/0                
¬ fe80::/ffc0:: 
|    4   384 ACCEPT     all      *      *       ::/0                 ff02::/16 
|    0     0 LOG        all      *      *       ::/0                 ::/0       
¬        LOG flags 0 level 7 prefix `intOUT-default:' 
|    0     0 DROP       all      *      *       ::/0                 ::/0 

Chapter 19. Security
|19.1. Node security

It's very recommended to apply all available patches and disable all not necessary services. Also bind services to the needed IPv4/IPv6 addresses only and install local firewalling.

More to be filled...
|19.2. Access limitations

Many services uses the tcp_wrapper library for access control. Below is described the use of tcp_wrapper.

More to be filled...
|19.3. IPv6 security auditing

Currently there are no comfortable tools out which are able to check a system over network for IPv6 security issues. Neither Nessus nor any commercial security scanner is as far as I know able to scan IPv6 addresses.
|19.3.1. Legal issues

ATTENTION: always take care that you only scan your own systems or after receiving a written order, otherwise legal issues are able to come up to you. CHECK destination IPv6 addresses TWICE before starting a scan.
|19.3.2. Security auditing using IPv6-enabled netcat

With the IPv6-enabled netcat (see IPv6+Linux-status-apps/security-auditing for more) you can run a portscan by wrapping a script around which run through a port range, grab banners and so on. Usage example:

# nc6 ::1 daytime
|13 JUL 2002 11:22:22 CEST

|19.3.3. Security auditing using IPv6-enabled nmap

NMap, one of the best portscaner around the world, supports IPv6 since version 3.10ALPHA1. Usage example:

# nmap -6 -sT ::1
Starting nmap V. 3.10ALPHA3 ( www.insecure.org/nmap/ ) 
Interesting ports on localhost6 (::1): 
(The 1600 ports scanned but not shown below are in state: closed) 
Port       State       Service 
|22/tcp     open        ssh 
|53/tcp     open        domain 
|515/tcp    open        printer 
|2401/tcp   open        cvspserver
Nmap run completed -- 1 IP address (1 host up) scanned in 0.525 seconds

|19.3.4. Security auditing using IPv6-enabled strobe

Strobe is a (compared to NMap) more a low budget portscanner, but there is an IPv6-enabling patch available (see IPv6+Linux-status-apps/security-auditing for more). Usage example:

# ./strobe ::1 strobe 1.05 (c) 1995-1999 Julian Assange <proff@iq.org>.
::1 2401 unassigned unknown
::1 22 ssh Secure Shell - RSA encrypted rsh 
::1 515 printer spooler (lpd)
::1 6010 unassigned unknown 
::1 53 domain Domain Name Server

Note: strobe isn't really developed further on, the shown version number isn't the right one.
|19.3.5. Audit results

If the result of an audit mismatch your IPv6 security policy, use IPv6 firewalling to close the holes, e.g. using netfilter6 (see Firewalling/Netfilter6 for more).

Info: More detailed information concerning IPv6 Security can be found here:

    IETF drafts - IPv6 Operations (v6ops)

    RFC 3964 / Security Considerations for 6to4

Chapter 20. Encryption and Authentication

Unlike in IPv4, encryption and authentication is a mandatory feature of IPv6. Those features are normally implemented using IPsec (which can be also used by IPv4).
|20.1. Modes of using encryption and authentication

Two modes of encryption and authentication of a connection are possible:
|20.1.1. Transport mode

Transport mode is a real end-to-end connection mode. Here, only the payload (usually ICMP, TCP or UDP) is encrypted with their particular header, while the IP header is not encrypted (but usually included in authentication).

Using AES-128 for encryption and SHA1 for authentication, this mode decreases the MTU by 42 octets.
|20.1.2. Tunnel mode

Tunnel mode can be used either for end-to-end or for gateway-to-gateway connection modes. Here, the complete IP packet is being encrypted and gets a new IP header prepended, all together constituing a new IP packet (this mechanism is also known as "encapsulation")

This mode usually decreases the MTU by 40 octets from the MTU of transport mode. I.e. using AES-128 for encryption and SHA1 for authentication 82 octets less than the normal MTU.
|20.2. Support in kernel (ESP and AH)
|20.2.1. Support in vanilla Linux kernel 2.4.x

At the time of writing missing in vanilla up to 2.4.28. There was an issue about keeping the Linux kernel source free of export/import-control-laws regarding encryption code. This is also one case why FreeS/WAN project wasn't included in vanilla source. Perhaps a backport from 2.6.x will be done in the future.
|20.2.2. Support in vanilla Linux kernel 2.6.x

Current versions (as time of writing 2.6.9 and upper) support native IPsec for IPv4 and IPv6.

Implementation was helped by the USAGI project.
|20.3. Automatic key exchange (IKE)

IPsec requires a key exchange of a secret. This is mostly done automatically by so called IKE daemons. They also handle the authentication of the peers, either by a common known secret (so called “pre-shared secret”) or by RSA keys (which can also be used from X.509 certificates).

Currently, two different IKE daemons are available for Linux, which totally differ in configuration and usage.

I prefer “pluto” from the *S/WAN implementation because of the easier and one-config-only setup.
|20.3.1. IKE daemon “racoon”

The IKE daemon “racoon” is taken from the KAME project and ported to Linux. Modern Linux distributions contain this daemon in the package “ipsec-tools”. Two executables are required for a proper IPsec setup. Take a look on Linux Advanced Routing & Traffic Control HOWTO / IPSEC, too.
|20.3.1.1. Manipulation of the IPsec SA/SP database with the tool “setkey”

“setkey” is important to define the security policy (SP) for the kernel.

File: /etc/racoon/setkey.sh

    Example for an end-to-end encrypted connection in transport mode

#!/sbin/setkey -f
flush;
spdflush;
spdadd 2001:db8:1:1::1 2001:db8:2:2::2 any -P out ipsec esp/transport//require;
spdadd 2001:db8:2:2::2 2001:db8:1:1::1 any -P in  ipsec esp/transport//require;

    Example for a end-to-end encrypted connection in tunnel mode

#!/sbin/setkey -f
flush;
spdflush;
spdadd 2001:db8:1:1::1 2001:db8:2:2::2 any -P out ipsec
¬ esp/tunnel/2001:db8:1:1::1-2001:db8:2:2::2/require;
spdadd 2001:db8:2:2::2 2001:db8:1:1::1 any -P in  ipsec
¬ esp/tunnel/2001:db8:2:2::2-2001:db8:1:1::1/require;

For the other peer, you have to replace “in” with “out”.
|20.3.1.2. Configuration of the IKE daemon “racoon”

“racoon” requires a configuration file for proper execution. It includes the related settings to the security policy, which should be set up previously using “setkey”.

File: /etc/racoon/racoon.conf

# Racoon IKE daemon configuration file.
# See 'man racoon.conf' for a description of the format and entries.
path include "/etc/racoon";
path pre_shared_key "/etc/racoon/psk.txt";

listen
{
        isakmp 2001:db8:1:1::1;
}

remote 2001:db8:2:2::2
{
        exchange_mode main;
        lifetime time 24 hour;
        proposal
        {
                encryption_algorithm 3des;
                hash_algorithm md5;
                authentication_method pre_shared_key;
                dh_group 2;
        }
}

# gateway-to-gateway
sainfo address 2001:db8:1:1::1 any address 2001:db8:2:2::2 any
{
        lifetime time 1 hour;
        encryption_algorithm 3des;
        authentication_algorithm hmac_md5;
        compression_algorithm deflate;
}

sainfo address 2001:db8:2:2::2 any address 2001:db8:1:1::1 any
{
        lifetime time 1 hour;
        encryption_algorithm 3des;
        authentication_algorithm hmac_md5;
        compression_algorithm deflate;
} 

Also set up the pre-shared secret:

File: /etc/racoon/psk.txt

# file for pre-shared keys used for IKE authentication
# format is: 'identifier' 'key'

|2001:db8:2:2::2 verysecret 

|20.3.1.3. Running IPsec with IKE daemon “racoon”

At least the daemon needs to be started. For the first time, use debug and foreground mode. The following example shows a successful IKE phase 1 (ISAKMP-SA) and 2 (IPsec-SA) negotiation:

# racoon -F -v -f /etc/racoon/racoon.conf
Foreground mode. 
|2005-01-01 20:30:15: INFO: @(#)ipsec-tools 0.3.3
¬ (http://ipsec-tools.sourceforge.net)
|2005-01-01 20:30:15: INFO: @(#)This product linked
¬ OpenSSL 0.9.7a Feb 19 2003 (http://www.openssl.org/)
|2005-01-01 20:30:15: INFO: 2001:db8:1:1::1[500] used as isakmp port (fd=7)
|2005-01-01 20:31:06: INFO: IPsec-SA request for 2001:db8:2:2::2
¬ queued due to no phase1 found.
|2005-01-01 20:31:06: INFO: initiate new phase 1 negotiation:
¬ 2001:db8:1:1::1[500]<=>2001:db8:2:2::2[500]
|2005-01-01 20:31:06: INFO: begin Identity Protection mode.
|2005-01-01 20:31:09: INFO: ISAKMP-SA established
¬ 2001:db8:1:1::1[500]-2001:db8:2:2::2[500] spi:da3d3693289c9698:ac039a402b2db401
|2005-01-01 20:31:09: INFO: initiate new phase 2 negotiation:
¬ 2001:6f8:900:94::2[0]<=>2001:db8:2:2::2[0]
|2005-01-01 20:31:10: INFO: IPsec-SA established:
¬ ESP/Tunnel 2001:db8:2:2::2->2001:db8:1:1::1 spi=253935531(0xf22bfab) 
|2005-01-01 20:31:10: INFO: IPsec-SA established:
¬ ESP/Tunnel 2001:db8:1:1::1->2001:db8:2:2::2 spi=175002564(0xa6e53c4) 

Each direction got its own IPsec-SA (like defined in the IPsec standard). With “tcpdump” on the related interface, you will see as result of an IPv6 ping:

|20:35:55.305707 2001:db8:1:1::1 > 2001:db8:2:2::2: ESP(spi=0x0a6e53c4,seq=0x3)
|20:35:55.537522 2001:db8:2:2::2 > 2001:db8:1:1::1: ESP(spi=0x0f22bfab,seq=0x3)

As expected, the negotiated SPIs are being used here.

And using “setkey”, current active parameters are shown:

# setkey -D
|2001:db8:1:1::1 2001:db8:2:2::2
        esp mode=tunnel spi=175002564(0x0a6e53c4) reqid=0(0x00000000)
        E: 3des-cbc  bd26bc45 aea0d249 ef9c6b89 7056080f 5d9fa49c 924e2edd
        A: hmac-md5  60c2c505 517dd8b7 c9609128 a5efc2db
        seq=0x00000000 replay=4 flags=0x00000000 state=mature
        created: Jan  1 20:31:10 2005   current: Jan  1 20:40:47 2005
        diff: 577(s)    hard: 3600(s)   soft: 2880(s)
        last: Jan  1 20:35:05 2005      hard: 0(s)      soft: 0(s)
        current: 540(bytes)     hard: 0(bytes)  soft: 0(bytes)
        allocated: 3    hard: 0 soft: 0
        sadb_seq=1 pid=22358 refcnt=0
|2001:db8:2:2::2 2001:db8:1:1::1
        esp mode=tunnel spi=253935531(0x0f22bfab) reqid=0(0x00000000)
        E: 3des-cbc  c1ddba65 83debd62 3f6683c1 20e747ac 933d203f 4777a7ce
        A: hmac-md5  3f957db9 9adddc8c 44e5739d 3f53ca0e
        seq=0x00000000 replay=4 flags=0x00000000 state=mature
        created: Jan  1 20:31:10 2005   current: Jan  1 20:40:47 2005
        diff: 577(s)    hard: 3600(s)   soft: 2880(s)
        last: Jan  1 20:35:05 2005      hard: 0(s)      soft: 0(s)
        current: 312(bytes)     hard: 0(bytes)  soft: 0(bytes)
        allocated: 3    hard: 0 soft: 0
        sadb_seq=0 pid=22358 refcnt=0 

|20.3.2. IKE daemon “pluto”

The IKE daemon “pluto” is included in distributions of the *S/WAN projects. *S/WAN project starts at the beginning as FreeS/WAN. Unfortunately, the FreeS/WAN project stopped further development in 2004. Because of the slow pace of development in the past, two spin-offs started: strongSwan and Openswan. Today, readily installable packages are available for at least Openswan (included in Fedora Core 3).

A major difference to “racoon”, only one configuration file is required. Also, an initscript exists for automatic setup after booting.
|20.3.2.1. Configuration of the IKE daemon “pluto”

The configuration is very similar to the IPv4 one, only one important option is necessary.

File: /etc/ipsec.conf

# /etc/ipsec.conf - Openswan IPsec configuration file
#
# Manual:     ipsec.conf.5
version 2.0     # conforms to second version of ipsec.conf specification

# basic configuration
config setup
        # Debug-logging controls:  "none" for (almost) none, "all" for lots.
        # klipsdebug=none
        # plutodebug="control parsing"

#Disable Opportunistic Encryption
include /etc/ipsec.d/examples/no_oe.conf

conn ipv6-p1-p2
        connaddrfamily=ipv6       # Important for IPv6!
        left=2001:db8:1:1::1
        right=2001:db8:2:2::2
        authby=secret
        esp=aes128-sha1
        ike=aes128-sha-modp1024
        type=transport
        #type=tunnel
        compress=no
        #compress=yes
        auto=add
        #auto=start

Don't forget to define the pre-shared secret here also.

File: /etc/ipsec.secrets

|2001:db8:1:1::1 2001:db8:2:2::2 : PSK      "verysecret"

|20.3.2.2. Running IPsec with IKE daemon “pluto”

If installation of Openswan was successfully, an initscript should exist for starting IPsec, simply run (on each peer):

# /etc/rc.d/init.d/ipsec start

Afterwards, start this connection on one peer. If you saw the line “IPsec SA established”, all worked fine.

# ipsec auto --up ipv6-peer1-peer2
|104 "ipv6-p1-p2" #1: STATE_MAIN_I1: initiate
|106 "ipv6-p1-p2" #1: STATE_MAIN_I2: sent MI2, expecting MR2
|108 "ipv6-p1-p2" #1: STATE_MAIN_I3: sent MI3, expecting MR3
|004 "ipv6-p1-p2" #1: STATE_MAIN_I4: ISAKMP SA established
|112 "ipv6-p1-p2" #2: STATE_QUICK_I1: initiate
|004 "ipv6-p1-p2" #2: STATE_QUICK_I2: sent QI2,
¬ IPsec SA established {ESP=>0xa98b7710 <0xa51e1f22} 

Because *S/WAN and setkey/racoon do use the same IPsec implementation in Linux 2.6.x kernel, “setkey” can be used here too to show current active parameters:

# setkey -D
|2001:db8:1:1::1 2001:db8:2:2::2
        esp mode=transport spi=2844489488(0xa98b7710) reqid=16385(0x00004001)
        E: aes-cbc  082ee274 2744bae5 7451da37 1162b483
        A: hmac-sha1  b7803753 757417da 477b1c1a 64070455 ab79082c
        seq=0x00000000 replay=64 flags=0x00000000 state=mature
        created: Jan  1 21:16:32 2005   current: Jan  1 21:22:20 2005
        diff: 348(s)    hard: 0(s)      soft: 0(s)
        last:                           hard: 0(s)      soft: 0(s)
        current: 0(bytes)       hard: 0(bytes)  soft: 0(bytes)
        allocated: 0    hard: 0 soft: 0
        sadb_seq=1 pid=23825 refcnt=0
|2001:db8:2:2::2 2001:db8:1:1::1
        esp mode=transport spi=2770214690(0xa51e1f22) reqid=16385(0x00004001)
        E: aes-cbc  6f59cc30 8d856056 65e07b76 552cac18
        A: hmac-sha1  c7c7d82b abfca8b1 5440021f e0c3b335 975b508b
        seq=0x00000000 replay=64 flags=0x00000000 state=mature
        created: Jan  1 21:16:31 2005   current: Jan  1 21:22:20 2005
        diff: 349(s)    hard: 0(s)      soft: 0(s)
        last:                           hard: 0(s)      soft: 0(s)
        current: 0(bytes)       hard: 0(bytes)  soft: 0(bytes)
        allocated: 0    hard: 0 soft: 0
        sadb_seq=0 pid=23825 refcnt=0 

|20.4. Additional informations:

On Linux Kernel 2.6.x you can get the policy and status of IPsec also using “ip”:

# ip xfrm policy
...

# ip xfrm state
...

Chapter 21. Quality of Service (QoS)

IPv6 supports QoS with use of Flow Labels and Traffic Classes. This can be controlled using “tc” (contained in package “iproute”).

Additional infos:

    RFC 3697 / IPv6 Flow Label Specification

more to be filled...
Chapter 22. Hints for IPv6-enabled daemons

Here some hints are shown for IPv6-enabled daemons.
|22.1. Berkeley Internet Name Domain (BIND) daemon “named”

IPv6 is supported since version 9. Always use newest available version. At least version 9.1.3 must be used, older versions can contain remote exploitable security holes.
|22.1.1. Listening on IPv6 addresses

Note: unlike in IPv4 current versions doesn't allow to bind a server socket to dedicated IPv6 addresses, so only any or none are valid. Because this can be a security issue, check the Access Control List (ACL) section below, too!
|22.1.1.1. Enable BIND named for listening on IPv6 address

To enable IPv6 for listening, following options are requested to change

options {
        # sure other options here, too
        listen-on-v6 { any; };
};

This should result after restart in e.g.

# netstat -lnptu |grep "named\W*$" 
tcp 0 0 :::53         :::*      LISTEN 1234/named
¬ # incoming TCP requests
udp 0 0 1.2.3.4:53    0.0.0.0:*        1234/named
¬ # incoming UDP requests to IPv4 1.2.3.4
udp 0 0 127.0.0.1:53  0.0.0.0:*        1234/named
¬ # incoming UDP requests to IPv4 localhost
udp 0 0 0.0.0.0:32868 0.0.0.0:*        1234/named
¬ # dynamic chosen port for outgoing queries
udp 0 0 :::53         :::*             1234/named
¬ # incoming UDP request to any IPv6

And a simple test looks like

# dig localhost @::1

and should show you a result.
|22.1.1.2. Disable BIND named for listening on IPv6 address

To disable IPv6 for listening, following options are requested to change

options {
        # sure other options here, too
        listen-on-v6 { none; };
};

|22.1.2. IPv6 enabled Access Control Lists (ACL)

IPv6 enabled ACLs are possible and should be used whenever it's possible. An example looks like following:

acl internal-net { 
|        127.0.0.1; 
|        1.2.3.0/24;  
|        2001:0db8:100::/56; 
        ::1/128; 
        ::ffff:1.2.3.4/128; 
};
acl ns-internal-net { 
|        1.2.3.4;  
|        1.2.3.5;  
|        2001:0db8:100::4/128; 
|        2001:0db8:100::5/128; 
};

This ACLs can be used e.g. for queries of clients and transfer zones to secondary name-servers. This prevents also your caching name-server to be used from outside using IPv6.

options {
        # sure other options here, too
        listen-on-v6 { none; };
        allow-query { internal-net; }; 
        allow-transfer { ns-internal-net; }; 
};

It's also possible to set the allow-query and allow-transfer option for most of single zone definitions, too.
|22.1.3. Sending queries with dedicated IPv6 address

This option is not required, but perhaps needed:

query-source-v6 address <ipv6address|*> port <port|*>;

|22.1.4. Per zone defined dedicated IPv6 addresses

It's also possible to define per zone some IPv6 addresses.
|22.1.4.1. Transfer source address

Transfer source address is used for outgoing zone transfers:

transfer-source-v6 <ipv6addr|*> [port port];

|22.1.4.2. Notify source address

Notify source address is used for outgoing notify messages:

notify-source-v6 <ipv6addr|*> [port port];

|22.1.5. IPv6 DNS zone files examples

Some information can be also found at IPv6 DNS Setup Information (article). Perhaps also helpful is the IPv6 Reverse DNS zone builder for BIND 8/9 (webtool).
|22.1.6. Serving IPv6 related DNS data

For IPv6 new types and root zones for reverse lookups are defined:

    AAAA and reverse IP6.INT: specified in RFC 1886 / DNS Extensions to support IP version 6, usable since BIND version 4.9.6

    A6, DNAME (DEPRECATED NOW!) and reverse IP6.ARPA: specified in RFC 2874 / DNS Extensions to Support IPv6 Address Aggregation and Renumbering, usable since BIND 9, but see also an information about the current state at Domain Name System Extension (dnsext)

Perhaps filled later more content, for the meantime take a look at given RFCs and

    AAAA and reverse IP6.INT: IPv6 DNS Setup Information

    A6, DNAME (DEPRECATED NOW!) and reverse IP6.ARPA: take a look into chapter 4 and 6 of the BIND 9 Administrator Reference Manual (ARM) distributed with the bind-package or get this here: BIND manual version 9.3

Because IP6.INT is deprecated (but still in use), a DNS server which will support IPv6 information has to serve both reverse zones.
|22.1.6.1. Current best practice

Because there are some troubles around using the new formats, current best practice is:

Forward lookup support:

    AAAA

Reverse lookup support:

    Reverse nibble format for zone ip6.int (FOR BACKWARD COMPATIBILITY)

    Reverse nibble format for zone ip6.arpa (RECOMMENDED)

|22.1.7. Checking IPv6-enabled connect

To check, whether BIND named is listening on an IPv6 socket and serving data see following examples.
|22.1.7.1. IPv6 connect, but denied by ACL

Specifying a dedicated server for the query, an IPv6 connect can be forced:

$ host -t aaaa www.6bone.net 2001:0db8:200:f101::1 
Using domain server: 
Name: 2001:0db8:200:f101::1 
Address: 2001:0db8:200:f101::1#53 
Aliases:

Host www.6bone.net. not found: 5(REFUSED)

Related log entry looks like following:

Jan 3 12:43:32 gate named[12347]: client
¬ 2001:0db8:200:f101:212:34ff:fe12:3456#32770: 
 query denied

If you see such entries in the log, check whether requests from this client should be allowed and perhaps review your ACL configuration.
|22.1.7.2. Successful IPv6 connect

A successful IPv6 connect looks like following:

$ host -t aaaa www.6bone.net 2001:0db8:200:f101::1 
Using domain server: 
Name: 2001:0db8:200:f101::1 
Address: 2001:0db8:200:f101::1#53 
Aliases:

www.6bone.net. is an alias for 6bone.net. 
|6bone.net. has AAAA address 3ffe:b00:c18:1::10

|22.2. Internet super daemon (xinetd)

IPv6 is supported since xinetd version around 1.8.9. Always use newest available version. At least version 2.3.3 must be used, older versions can contain remote exploitable security holes.

Some Linux distribution contain an extra package for the IPv6 enabled xinetd, some others start the IPv6-enabled xinetd if following variable is set: NETWORKING_IPV6="yes", mostly done by /etc/sysconfig/network (only valid for Red Hat like distributions). In newer releases, one binary supports IPv4 and IPv6.

If you enable a built-in service like e.g. daytime by modifying the configuration file in /etc/xinetd.d/daytime like

# diff -u /etc/xinetd.d/daytime.orig /etc/xinetd.d/daytime 
--- /etc/xinetd.d/daytime.orig Sun Dec 16 19:00:14 2001 
+++ /etc/xinetd.d/daytime Sun Dec 16 19:00:22 2001 
@@ -10,5 +10,5 @@ 
        protocol = tcp 
        user = root 
        wait = no 
-       disable = yes 
+       disable = no 
 }

After restarting the xinetd you should get a positive result like:

# netstat -lnptu -A inet6 |grep "xinetd*" 
tcp 0 0 ::ffff:192.168.1.1:993  :::*  LISTEN  12345/xinetd-ipv6 
tcp 0 0 :::13                   :::*  LISTEN  12345/xinetd-ipv6 <- service
¬ daytime/tcp
tcp 0 0 ::ffff:192.168.1.1:143  :::*  LISTEN  12345/xinetd-ipv6

Shown example also displays an IMAP and IMAP-SSL IPv4-only listening xinetd.

Note: earlier versions had a problem that an IPv4-only xinetd won't start on an IPv6-enabled node and also the IPv6-enabled xinetd won't start on an IPv4-only node. This is known to be fixed in later versions, at least version 2.3.11.
|22.3. Webserver Apache2 (httpd2)

Apache web server supports IPv6 native by maintainers since 2.0.14. Available patches for the older 1.3.x series are not current and shouldn't be used in public environment, but available at KAME / Misc.
|22.3.1. Listening on IPv6 addresses

Note: virtual hosts on IPv6 addresses are broken in versions until 2.0.28 (a patch is available for 2.0.28). But always try latest available version first because earlier versions had some security issues.
|22.3.1.1. Virtual host listen on an IPv6 address only

Listen [2001:0db8:100::1]:80
<VirtualHost [2001:0db8:100::1]:80>
        ServerName ipv6only.yourdomain.yourtopleveldomain
        # ...sure more config lines
</VirtualHost>

|22.3.1.2. Virtual host listen on an IPv6 and on an IPv4 address

Listen [2001:0db8:100::2]:80 
Listen 1.2.3.4:80
<VirtualHost [2001:0db8:100::2]:80 1.2.3.4:80>
        ServerName ipv6andipv4.yourdomain.yourtopleveldomain
        # ...sure more config lines
</VirtualHost>

This should result after restart in e.g.

# netstat -lnptu |grep "httpd2\W*$" 
tcp 0 0 1.2.3.4:80          0.0.0.0:* LISTEN 12345/httpd2 
tcp 0 0 2001:0db8:100::1:80 :::*      LISTEN 12345/httpd2 
tcp 0 0 2001:0db8:100::2:80 :::*      LISTEN 12345/httpd2

For simple tests use the telnet example already shown.
|22.3.1.3. Additional notes

    Apache2 supports a method called “sendfile” to speedup serving data. Some NIC drivers also support offline checksumming. In some cases, this can lead to connection problems and invalid TCP checksums. In this cases, disable “sendfile” either by recompiling using configure option “--without-sendfile” or by using the "EnableSendfile off" directive in configuration file.

|22.4. Router Advertisement Daemon (radvd)

The router advertisement daemon is very useful on a LAN, if clients should be auto-configured. The daemon itself should run on the Linux default IPv6 gateway router (it's not required that this is also the default IPv4 gateway, so pay attention who on your LAN is sending router advertisements).

You can specify some information and flags which should be contained in the advertisement. Common used are

    Prefix (needed)

    Lifetime of the prefix

    Frequency of sending advertisements (optional)

After a proper configuration, the daemon sends advertisements through specified interfaces and clients are hopefully receive them and auto-magically configure addresses with received prefix and the default route.
|22.4.1. Configuring radvd
|22.4.1.1. Simple configuration

Radvd's config file is normally /etc/radvd.conf. An simple example looks like following:

interface eth0 { 
        AdvSendAdvert on;
        MinRtrAdvInterval 3; 
        MaxRtrAdvInterval 10;
        prefix 2001:0db8:0100:f101::/64 { 
                AdvOnLink on; 
                AdvAutonomous on; 
                AdvRouterAddr on; 
        };
};

This results on client side in

# ip -6 addr show eth0 
|3: eth0: <BROADCAST,MULTICAST,UP> mtu 1500 qdisc pfifo_fast qlen 100 
    inet6 2001:0db8:100:f101:2e0:12ff:fe34:1234/64 scope global dynamic 
       valid_lft 2591992sec preferred_lft 604792sec 
    inet6 fe80::2e0:12ff:fe34:1234/10 scope link

Because no lifetime was defined, a very high value was used.
|22.4.1.2. Special 6to4 configuration

Version since 0.6.2pl3 support the automatic (re)-generation of the prefix depending on an IPv4 address of a specified interface. This can be used to distribute advertisements in a LAN after the 6to4 tunneling has changed. Mostly used behind a dynamic dial-on-demand Linux router. Because of the sure shorter lifetime of such prefix (after each dial-up, another prefix is valid), the lifetime configured to minimal values:

interface eth0 { 
        AdvSendAdvert on;
        MinRtrAdvInterval 3; 
        MaxRtrAdvInterval 10;
        prefix 0:0:0:f101::/64 { 
                AdvOnLink off; 
                AdvAutonomous on; 
                AdvRouterAddr on; 
                Base6to4Interface ppp0;
                AdvPreferredLifetime 20; 
                AdvValidLifetime 30;
        };
};

This results on client side in (assuming, ppp0 has currently 1.2.3.4 as local IPv4 address):

# /sbin/ip -6 addr show eth0 
|3: eth0: <BROADCAST,MULTICAST,UP> mtu 1500 qdisc pfifo_fast qlen 100 
   inet6 2002:0102:0304:f101:2e0:12ff:fe34:1234/64 scope global dynamic 
      valid_lft 22sec preferred_lft 12sec
   inet6 fe80::2e0:12ff:fe34:1234/10 scope link

Because a small lifetime was defined, such prefix will be thrown away quickly, if no related advertisement was received.

Additional note: if you do not used special 6to4 support in initscripts, you have to setup a special route on the internal interface on the router, otherwise you will get some backrouting problems. for the example showh here:

# /sbin/ip -6 route add 2002:0102:0304:f101::/64 dev eth0 metric 1

This route needs to be replaced every time the prefix changes, which is the case after a new IPv4 address was assigned to the dial-up interface.
|22.4.2. Debugging

A program called “radvdump” can help you looking into sent or received advertisements. Simple to use:

# radvdump 
Router advertisement from fe80::280:c8ff:feb9:cef9 (hoplimit 255) 
        AdvCurHopLimit: 64 
        AdvManagedFlag: off 
        AdvOtherConfigFlag: off 
        AdvHomeAgentFlag: off 
        AdvReachableTime: 0 
        AdvRetransTimer: 0 
        Prefix 2002:0102:0304:f101::/64 
                AdvValidLifetime: 30 
                AdvPreferredLifetime: 20 
                AdvOnLink: off 
                AdvAutonomous: on 
                AdvRouterAddr: on 
        Prefix 2001:0db8:100:f101::/64 
                AdvValidLifetime: 2592000 
                AdvPreferredLifetime: 604800 
                AdvOnLink: on 
                AdvAutonomous: on 
                AdvRouterAddr: on 
        AdvSourceLLAddress: 00 80 12 34 56 78

Output shows you each advertisement package in readable format. You should see your configured values here again, if not, perhaps it's not your radvd which sends the advertisement...look for another router on the link (and take the LLAddress, which is the MAC address for tracing).
|22.5. Dynamic Host Configuration v6 Server (dhcp6s)

DHCPv6 can be used for stateful configurations. The daemon itself need not necessary run on the Linux default IPv6 gateway router.

You can specify more information than by using radvd. The are most similar to IPv4 DHCP server.

After a proper configuration, the daemon reacts on received ICMPv6 multicast packets sent by a client to address ff02::1:2
|22.5.1. Configuration of the DHCPv6 server (dhcp6s)
|22.5.1.1. Simple configuration

dhcp6s's config file is normally /etc/dhcp6s.conf. An simple example looks like following:

interface eth0 {
        server-preference 255;
        renew-time 60;
        rebind-time 90;
        prefer-life-time 130;
        valid-life-time 200;
        allow rapid-commit;
        option dns_servers 2001:db8:0:f101::1 sub.domain.example;
        link AAA {
                range 2001:db8:0:f101::1000 to 2001:db8:0:f101::ffff/64;
                prefix 2001:db8:0:f101::/64;
        };
};

|22.5.2. Configuration of the DHCPv6 client (dhcp6c)
|22.5.2.1. Simple configuration

dhcp6c's config file is normally /etc/dhcp6c.conf. An simple example looks like following:

interface eth0 {
        send rapid-commit;
        request domain-name-servers;
}; 

|22.5.3. Usage
|22.5.3.1. dhcpv6_server

Start server, e.g.

# service dhcp6s start

|22.5.3.2. dhcpv6_client

Start client in foreground, e.g.

# dhcp6c -f eth0

|22.5.4. Debugging
|22.5.4.1. dhcpv6_server

The server has one foreground and two debug toggles (both should be used for debugging), here is an example:

# dhcp6s -d -D -f eth0

|22.5.4.2. dhcpv6_client

As general debugging for test whether the IPv6 DHCP server is reable on the link use an IPv6 ping to the DHCP multicast address:

# ping6 -I eth0 ff02::1:2

The client has one foreground and two debug toggles, here is an example:

# dhcp6c -d -f eth0
Oct/03/2005 17:18:16 dhcpv6 doesn't support hardware type 776 
Oct/03/2005 17:18:16 doesn't support sit0 address family 0 
Oct/03/2005 17:18:16 netlink_recv_rtgenmsg error 
Oct/03/2005 17:18:16 netlink_recv_rtgenmsg error 
Oct/03/2005 17:18:17 status code for this address is: success 
Oct/03/2005 17:18:17 status code: success 
Oct/03/2005 17:18:17 netlink_recv_rtgenmsg error 
Oct/03/2005 17:18:17 netlink_recv_rtgenmsg error 
Oct/03/2005 17:18:17 assigned address 2001:db8:0:f101::1002 prefix len is not
¬ in any RAs prefix length using 64 bit instead 
Oct/03/2005 17:18:17 renew time 60, rebind time 9

Note that the netlink error messages have no impact.
|22.6. ISC Dynamic Host Configuration Server (dhcpd)

ISC DHCP supports IPv6 since version 4.x.
|22.6.1. Configuration of the ISC DHCP server for IPv6 (dhcpd)

Note that currently, the ISC DHCP server can only serve IPv4 or IPv6, means you have to start the daemon twice (for IPv6 with option “-6”) to support both protocols.
|22.6.1.1. Simple configuration

Create a dedicated configuration file /etc/dhcp/dhcpd6.conf for the IPv6 part of the dhcpd. Note, that the router requires to have a interface configured with an IPv6 address out of the defined subnet.

default-lease-time 600;
max-lease-time 7200; 
log-facility local7; 
subnet6 2001:db8:0:1::/64 {
        # Range for clients
        range6 2001:db8:0:1::129 2001:db8:0:1::254;
        # Additional options
        option dhcp6.name-servers fec0:0:0:1::1;
        option dhcp6.domain-search "domain.example";
        # Prefix range for delegation to sub-routers
        prefix6 2001:db8:0:100:: 2001:db8:0:f00:: /56;
        # Example for a fixed host address
        host specialclient {
    		host-identifier option dhcp6.client-id 00:01:00:01:4a:1f:ba:e3:60:b9:1f:01:23:45;
        	fixed-address6 2001:db8:0:1::127;
    	} 
} 

Note that the “dhcp.client-id” no longer belongs to a MAC address, an unique ID is used instead! “dhcp6c” (see above) uses the file /var/lib/dhcpv6/dhcp6c_duid (would be created during first start, if not existing) as unique identity. It's a 14 byte long identifier, starting with a 2 byte length information (usually “0x000e”):

# hexdump -e '"%07.7_ax " 1/2 "%04x" " " 14/1 "%02x:" "\n"' /var/lib/dhcpv6/dhcp6c_duid 0000000 000e 00:01:00:01:4a:1f:ba:e3:60:b9:1f:01:23:45:

|22.6.2. Usage
|22.6.2.1. dhcpd

Start server in foreground:

# /usr/sbin/dhcpd -6 -f -cf /etc/dhcp/dhcpd.conf eth1 
Internet Systems Consortium DHCP Server 4.1.0 
Copyright 2004-2008 Internet Systems Consortium. 
All rights reserved. 
For info, please visit http://www.isc.org/sw/dhcp/ 
Not searching LDAP since ldap-server, ldap-port and ldap-base-dn were not specified in the config file 
Wrote 0 leases to leases file. 
Bound to *:547 
Listening on Socket/5/eth1/2001:db8:0:1::/64 
Sending on   Socket/5/eth1/2001:db8:0:1::/64

|22.7. DHCP Server Dibbler

Dibbler is also a DHCP server
|22.7.1. Configuration of the Dibbler DHCP server for IPv6
|22.7.1.1. Simple configuration

Create a dedicated configuration file /etc/dibbler/server.conf . Note, that the router requires to have a interface configured with an IPv6 address out of the defined subnet.

log-level 8 
log-mode short 
preference 0 
iface "eth1" {
 // also ranges can be defines, instead of exact values  t1 1800-2000  t2 2700-3000
  prefered-lifetime 3600
  valid-lifetime 7200
  class {
    pool 2001:6f8:12d8:1::/64
  }
  option dns-server fec0:0:0:1::1
  option domain domain.example
} 

|22.7.2. Usage
|22.7.2.1. dibbler-server

Start server in foreground:

# dibbler-server run 
| Dibbler - a portable DHCPv6, version 0.7.3 (SERVER, Linux port) 
| Authors : Tomasz Mrugalski<thomson(at)klub.com.pl>,Marek Senderski<msend(at)o2.pl> 
| Licence : GNU GPL v2 only. Developed at Gdansk University of Technology. 
| Homepage: http://klub.com.pl/dhcpv6/ 
|2009.05.28 10:18:48 Server Notice    My pid (1789) is stored in /var/lib/dibbler/server.pid 
|2009.05.28 10:18:48 Server Notice    Detected iface eth0/3, MAC=54:52:00:01:23:45. 
|2009.05.28 10:18:48 Server Notice    Detected iface eth1/2, MAC=54:52:00:67:89:ab. 
|2009.05.28 10:18:48 Server Notice    Detected iface lo/1, MAC=00:00:00:00:00:00. 
|2009.05.28 10:18:48 Server Debug     Skipping database loading. 
|2009.05.28 10:18:48 Server Debug     Cache:server-cache.xml file: parsing started, expecting 0 entries. 
|2009.05.28 10:18:48 Server Notice    Parsing /etc/dibbler/server.conf config file... 
|18:48 Server Debug     Setting 0 generic option(s). 
|18:48 Server Debug     0 per-client configurations (exceptions) added. 
|18:48 Server Debug     Parsing /etc/dibbler/server.conf done. 
|18:48 Server Info      0 client class(es) defined. 
|18:48 Server Debug     1 interface(s) specified in /etc/dibbler/server.conf 
|18:48 Server Info      Mapping allow, deny list to class 0:0 allow/deny entries in total. 
|18:48 Server Info      Interface eth1/2 configuration has been loaded. 
|18:48 Server Notice    Running in stateful mode. 
|18:48 Server Info      My DUID is 00:01:00:01:11:aa:6d:a7:54:52:00:67:89:ab. 
|18:48 Server Notice    Creating multicast (ff02::1:2) socket on eth1/2 (eth1/2) interface. 
|18:48 Server Debug     Cache: size set to 1048576 bytes, 1 cache entry size is 87 bytes, so maximum 12052 address-client pair(s) may be cached. 
|18:48 Server Notice    Accepting connections. Next event in 4294967295 second(s).

|22.8. tcp_wrapper

tcp_wrapper is a library which can help you to protect service against misuse.
|22.8.1. Filtering capabilities

You can use tcp_wrapper for

    Filtering against source addresses (IPv4 or IPv6)

    Filtering against users (requires a running ident daemon on the client)

|22.8.2. Which program uses tcp_wrapper

Following are known:

    Each service which is called by xinetd (if xinetd is compiled using tcp_wrapper library)

    sshd (if compiled using tcp_wrapper)

|22.8.3. Usage

tcp_wrapper is controlled by two files name /etc/hosts.allow and /etc/hosts.deny. For more information see

$ man hosts.allow

|22.8.3.1. Example for /etc/hosts.allow

In this file, each service which should be positive filtered (means connects are accepted) need a line.

sshd:           1.2.3. [2001:0db8:100:200::]/64
daytime-stream: 1.2.3. [2001:0db8:100:200::]/64

Note: there are broken implementations around, which uses following broken IPv6 network description: [2001:0db8:100:200::/64]. Hopefully, such versions will be fixed soon.
|22.8.3.2. Example for /etc/hosts.deny

This file contains all negative filter entries and should normally deny the rest using

ALL: ALL

If this node is a more sensible one you can replace the standard line above with this one, but this can cause a DoS attack (load of mailer and spool directory), if too many connects were made in short time. Perhaps a logwatch is better for such issues.

ALL: ALL: spawn (echo "Attempt from %h %a to %d at `date`" 
 | tee -a /var/log/tcp.deny.log | mail root@localhost)

|22.8.4. Logging

Depending on the entry in the syslog daemon configuration file /etc/syslog.conf the tcp_wrapper logs normally into /var/log/secure.
|22.8.4.1. Refused connection

A refused connection via IPv4 to an xinetd covered daytime service produces a line like following example

Jan 2 20:40:44 gate xinetd-ipv6[12346]: FAIL: daytime-stream libwrap
¬ from=::ffff:1.2.3.4
Jan 2 20:32:06 gate xinetd-ipv6[12346]: FAIL: daytime-stream libwrap 
 from=2001:0db8:100:200::212:34ff:fe12:3456

A refused connection via IPv4 to an dual-listen sshd produces a line like following example

Jan 2 20:24:17 gate sshd[12345]: refused connect from ::ffff:1.2.3.4
¬ (::ffff:1.2.3.4)
Jan 2 20:39:33 gate sshd[12345]: refused connect 
 from 2001:0db8:100:200::212:34ff:fe12:3456
¬ (2001:0db8:100:200::212:34ff:fe12:3456)

|22.8.4.2. Permitted connection

A permitted connection via IPv4 to an xinetd covered daytime service produces a line like following example

Jan 2 20:37:50 gate xinetd-ipv6[12346]: START: daytime-stream pid=0
¬ from=::ffff:1.2.3.4 
Jan 2 20:37:56 gate xinetd-ipv6[12346]: START: daytime-stream pid=0 
 from=2001:0db8:100:200::212:34ff:fe12:3456

A permitted connection via IPv4 to an dual-listen sshd produces a line like following example

Jan 2 20:43:10 gate sshd[21975]: Accepted password for user from ::ffff:1.2.3.4
¬ port 33381 ssh2
Jan 2 20:42:19 gate sshd[12345]: Accepted password for user 
 from 2001:0db8:100:200::212:34ff:fe12:3456 port 33380 ssh2

|22.9. vsftpd
|22.9.1. Listening on IPv6 addresses

Edit the configuration file, ususally /etc/vsftpd/vsftpd.conf, and adjust the listen option like

listen_ipv6=yes

That's all.
|22.10. proftpd
|22.10.1. Listening on IPv6 addresses

Edit the configuration file, ususally /etc/proftpd.conf, but take care, not 100% logical in virtual host setup

<VirtualHost 192.0.2.1>
        ...
        Bind 2001:0DB8::1
        ...
</VirtualHost>

That's all.
|22.11. Other daemons

Nowadays it's mostly simple, look for either a command line option or a configuration value to enable IPv6 listening. See manual page of the daemon or check related FAQs. It can happen that you can bind a daemon only to the IPv6-“any”-address (::) and not to bind to a dedicated IPv6 address, because the lack of support (depends on that what the programmer has implemented so far...).
Chapter 23. Programming
|23.1. Programming using C-API

Related RFCs:

    RFC 3493 / Basic Socket Interface Extensions for IPv6

    RFC 3542 / Advanced Sockets Application Program Interface (API) for IPv6

Following contents of this section is contributed by John Wenker, Sr. Software Engineer Performance Technologies San Diego, CA USA http://www.pt.com/.

This section describes how to write IPv6 client-server applications under the Linux operating system. First thing's first, and credit must be given where it is due. The information contained in this section is derived from Chapters 2 through 4 of IPv6 Network Programming by Jun-ichiro itojun Hagino (ISBN 1-55558-318-0). The reader is encouraged to consult that book for more detailed information. It describes how to convert IPv4 applications to be IPv6 compatible in a protocol-independent way, and describes some of the common problems encountered during the conversion along with suggested solutions. At the time of this writing, this is the only book of which the author is aware that specifically addresses how to program IPv6 applications [since writing this section, the author has also become aware of the Porting applications to IPv6 HowTo by Eva M. Castro at http://jungla.dit.upm.es/~ecastro/IPv6-web/ipv6.html]. Unfortunately, of the almost 360 pages in the book, maybe 60 are actually useful (the chapters mentioned). Nevertheless, without the guidance of that book, the author would have been unable to perform his job duties or compose this HowTo. While most (but certainly not all) of the information in the Hagino book is available via the Linux 'man' pages, application programmers will save a significant amount of time and frustration by reading the indicated chapters of the book rather than searching through the 'man' pages and online documentation.

Other than the Hagino book, any other information presented in this HowTo was obtained through trial and error. Some items or explanations may not be entirely “correct” in the grand IPv6 scheme, but seem to work in practical application.

The discussion that follows assumes the reader is already experienced with the traditional TCP/IP socket API. For more information on traditional socket programming, the Internetworking with TCP/IP series of textbooks by Comer & Stevens is hard to beat, specifically Volume III: Client-Server Programming and Applications, Linux/POSIX Sockets Version (ISBN 0-13-032071-4). This HowTo also assumes that the reader has had at least a bare basic introduction to IPv6 and in particular the addressing scheme for network addresses (see Section 2.3).
|23.1.1. Address Structures

This section provides a brief overview of the structures provided in the socket API to represent network addresses (or more specifically transport endpoints) when using the Internet protocols in a client-server application.
|23.1.1.1. IPv4 sockaddr_in

In IPv4, network addresses are 32 bits long and define a network node. Addresses are written in dotted decimal notation, such as 192.0.2.1, where each number represents eight bits of the address. Such an IPv4 address is represented by the struct sockaddr_in data type, which is defined in <netinet/in.h>.

struct sockaddr_in
{
   sa_family_t    sin_family;
   in_port_t      sin_port;
   struct in_addr sin_addr;
   /* Plus some padding for alignment */
};

The sin_family component indicates the address family. For IPv4 addresses, this is always set to AF_INET. The sin_addr field contains the 32-bit network address (in network byte order). Finally, the sin_port component represents the transport layer port number (in network byte order). Readers should already be familiar with this structure, as this is the standard IPv4 address structure.
|23.1.1.2. IPv6 sockaddr_in6

The biggest feature of IPv6 is its increased address space. Instead of 32-bit network addresses, IPv6 allots 128 bits to an address. Addresses are written in colon-hex notation of the form fe80::2c0:8cff:fe01:2345, where each hex number separated by colons represents 16 bits of the address. Two consecutive colons indicate a string of consecutive zeros for brevity, and at most only one double-colon may appear in the address. IPv6 addresses are represented by the struct sockaddr_in6 data type, also defined in <netinet/in.h>.

struct sockaddr_in6
{
   sa_family_t     sin6_family;
   in_port_t       sin6_port;
   uint32_t        sin6_flowinfo;
   struct in6_addr sin6_addr;
   uint32_t        sin6_scope_id;
};

The sin6_family, sin6_port, and sin6_addr components of the structure have the same meaning as the corresponding fields in the sockaddr_in structure. However, the sin6_family member is set to AF_INET6 for IPv6 addresses, and the sin6_addr field holds a 128-bit address instead of only 32 bits.

The sin6_flowinfo field is used for flow control, but is not yet standardized and can be ignored.

The sin6_scope_id field has an odd use, and it seems (at least to this naïve author) that the IPv6 designers took a huge step backwards when devising this. Apparently, 128-bit IPv6 network addresses are not unique. For example, it is possible to have two hosts, on separate networks, with the same link-local address (see Figure 1). In order to pass information to a specific host, more than just the network address is required; the scope identifier must also be specified. In Linux, the network interface name is used for the scope identifier (e.g. “eth0”) [be warned that the scope identifier is implementation dependent!]. Use the ifconfig(1M) command to display a list of active network interfaces.

A colon-hex network address can be augmented with the scope identifier to produce a "scoped address”. The percent sign ('%') is used to delimit the network address from the scope identifier. For example, fe80::1%eth0 is a scoped IPv6 address where fe80::1 represents the 128-bit network address and eth0 is the network interface (i.e. the scope identifier). Thus, if a host resides on two networks, such as Host B in example below, the user now has to know which path to take in order to get to a particular host. In Figure 1, Host B addresses Host A using the scoped address fe80::1%eth0, while Host C is addressed with fe80::1%eth1.

Host A (fe80::1) ---- eth0 ---- Host B ---- eth1 ---- Host C (fe80::1)

Getting back to the sockaddr_in6 structure, its sin6_scope_id field contains the index of the network interface on which a host may be found. Server applications will have this field set automatically by the socket API when they accept a connection or receive a datagram. For client applications, if a scoped address is passed as the node parameter to getaddrinfo(3) (described later in this HowTo), then the sin6_scope_id field will be filled in correctly by the system upon return from the function; if a scoped address is not supplied, then the sin6_scope_id field must be explicitly set by the client software prior to attempting to communicate with the remote server. The if_nametoindex(3) function is used to translate a network interface name into its corresponding index. It is declared in <net/if.h>.
|23.1.1.3. Generic Addresses

As any programmer familiar with the traditional TCP/IP socket API knows, several socket functions deal with "generic" pointers. For example, a pointer to a generic struct sockaddr data type is passed as a parameter to some socket functions (such as connect(2) or bind(2)) rather than a pointer to a specific address type. Be careful... the sockaddr_in6 structure is larger than the generic sockaddr structure! Thus, if your program receives a generic address whose actual type is unknown (e.g. it could be an IPv4 address structure or an IPv6 address structure), you must supply sufficient storage to hold the entire address. The struct sockaddr_storage data type is defined in <bits/socket.h> for this purpose [do not #include this file directly within an application; use <sys/socket.h> as usual, and <bits/socket.h> will be implicitly included].

For example, consider the recvfrom(2) system call, which is used to receive a message from a remote peer. Its function prototype is:

ssize_t recvfrom( int              s,
                  void            *buf,
                  size_t           len,
                  int              flags,
                  struct sockaddr *from,
                  socklen_t       *fromlen );

The from parameter points to a generic sockaddr structure. If data can be received from an IPv6 peer on the socket referenced by s, then from should point to a data type of struct sockaddr_storage, as in the following dummy example:

/*
** Read a message from a remote peer, and return a buffer pointer to
** the caller.
**
** 's' is the file descriptor for the socket.
*/
char *rcvMsg( int s )
{
   static char             bfr[ 1025 ];  /* Where the msg is stored. */
   ssize_t                 count;
   struct sockaddr_storage ss;           /* Where the peer adr goes. */
   socklen_t               sslen;
   sslen = sizeof( ss );
   count = recvfrom( s,
                     bfr,
                     sizeof( bfr ) - 1,
|                     0,
                     (struct sockaddr*) &ss,
                     &sslen );
   bfr[ count ] = '\0';   /* Null-terminates the message. */
   return bfr;
}  /* End rcvMsg() */

As seen in the above example, ss (a struct sockaddr_storage data object) is used to receive the peer address information, but it's address is typecast to a generic struct sockaddr* pointer in the call to recvfrom(2).
|23.1.2. Lookup Functions

Traditionally, hostname and service name resolution were performed by functions such as gethostbyname(3) and getservbyname(3). These traditional lookup functions are still available, but they are not forward compatible to IPv6. Instead, the IPv6 socket API provides new lookup functions that consolidate the functionality of several traditional functions. These new lookup functions are also backward compatible with IPv4, so a programmer can use the same translation algorithm in an application for both the IPv4 and IPv6 protocols. This is an important feature, because obviously a global IPv6 infrastructure isn't going to be put in place overnight. Thus, during the transition period from IPv4 to IPv6, client-server applications should be designed with the flexibility to handle both protocols simultaneously. The example programs at the end of this chapter do just that.

The primary lookup function in the new socket API is getaddrinfo(3). Its prototype is as follows.

int getaddrinfo( const char             *node,
                 const char             *service,
                 const struct addrinfo  *hints,
                 struct addrinfo       **res );

The node parameter is a pointer to the hostname or IP address being translated. The referenced string can be a hostname, IPv4 dotted decimal address, or IPv6 colon-hex address (possibly scoped). The service parameter is a pointer to the transport layer's service name or port number. It can be specified as a name found in /etc/services or a decimal number. getaddrinfo(3) resolves the host/service combination and returns a list of address records; a pointer to the list is placed in the location pointed at by res. For example, suppose a host can be identified by both an IPv4 and IPv6 address, and that the indicated service has both a TCP entry and UDP entry in /etc/services. In such a scenario, it is not inconceivable that four address records are returned; one for TCP/IPv6, one for UDP/IPv6, one for TCP/IPv4, and one for UDP/IPv4.

The definition for struct addrinfo is found in <netdb.h> (as is the declaration for getaddrinfo(3) and the other functions described in this section). The structure has the following format:

struct addrinfo
{
   int              ai_flags;
   int              ai_family;
   int              ai_socktype;
   int              ai_protocol;
   socklen_t        ai_addrlen;
   struct sockaddr *ai_addr;
   char            *ai_canonname;
   struct addrinfo *ai_next;
};

Consult the 'man' page for getaddrinfo(3) for detailed information about the various fields; this HowTo only describes a subset of them, and only to the extent necessary for normal IPv6 programming.

The ai_family, ai_socktype, and ai_protocol fields have the exact same meaning as the parameters to the socket(2) system call. The ai_family field indicates the protocol family (not the address family) associated with the record, and will be PF_INET6 for IPv6 or PF_INET for IPv4. The ai_socktype parameter indicates the type of socket to which the record corresponds; SOCK_STREAM for a reliable connection-oriented byte-stream or SOCK_DGRAM for connectionless communication. The ai_protocol field specifies the underlying transport protocol for the record.

The ai_addr field points to a generic struct sockaddr object. Depending on the value in the ai_family field, it will point to either a struct sockaddr_in (PF_INET) or a struct sockaddr_in6 (PF_INET6). The ai_addrlen field contains the size of the object pointed at by the ai_addr field.

As mentioned, getaddrinfo(3) returns a list of address records. The ai_next field points to the next record in the list.

The hints parameter to getaddrinfo(3) is also of type struct addrinfo and acts as a filter for the address records returned in res. If hints is NULL, all matching records are returned; but if hints is non-NULL, the referenced structure gives "hints" to getaddrinfo(3) about which records to return. Only the ai_flags, ai_family, ai_socktype, and ai_protocol fields are significant in the hints structure, and all other fields should be set to zero.

Programs can use hints->ai_family to specify the protocol family. For example, if it is set to PF_INET6, then only IPv6 address records are returned. Likewise, setting hints->ai_family to PF_INET results in only IPv4 address records being returned. If an application wants both IPv4 and IPv6 records, the field should be set to PF_UNSPEC.

The hints->socktype field can be set to SOCK_STREAM to return only records that correspond to connection-oriented byte streams, SOCK_DGRAM to return only records corresponding to connectionless communication, or 0 to return both.

For the Internet protocols, there is only one protocol associated with connection-oriented sockets (TCP) and one protocol associated with connectionless sockets (UDP), so setting hints->ai_socktype to SOCK_STREAM or SOCK_DGRAM is the same as saying, "Give me only TCP records," or "Give me only UDP records," respectively. With that in mind, the hints->ai_protocol field isn't really that important with the Internet protocols, and pretty much mirrors the hints->ai_socktype field. Nevertheless, hints->ai_protocol can be set to IPPROTO_TCP to return only TCP records, IPPROTO_UDP to return only UDP records, or 0 for both.

The node or service parameter to gethostbyname(3) can be NULL, but not both. If node is NULL, then the ai_flags field of the hints parameter specifies how the network address in a returned record is set (i.e. the sin_addr or sin6_addr field of the object pointed at by the ai_addr component in a returned record). If the AI_PASSIVE flag is set in hints, then the returned network addresses are left unresolved (all zeros). This is how server applications would use getaddrinfo(3). If the flag is not set, then the address is set to the local loopback address (::1 for IPv6 or 127.0.0.1 for IPv4). This is one way a client application can specify that the target server is running on the same machine as the client. If the service parameter is NULL, the port number in the returned address records remains unresolved.

The getaddrinfo(3) function returns zero on success, or an error code. In the case of an error, the gai_strerror(3) function is used to obtain a character pointer to an error message corresponding to the error code, just like strerror(3) does in the standard 'C' library.

Once the address list is no longer needed, it must be freed by the application. This is done with the freeaddrinfo(3) function.

The last function that will be mentioned in this section is getnameinfo(3). This function is the inverse of getaddrinfo(3); it is used to create a string representation of the hostname and service from a generic struct sockaddr data object. It has the following prototype.

int getnameinfo( const struct sockaddr *sa,
                 socklen_t              salen,
                 char                  *host,
                 size_t                 hostlen,
                 char                  *serv,
                 size_t                 servlen,
                 int                    flags );

The sa parameter points to the address structure in question, and salen contains its size. The host parameter points to a buffer where the null-terminated hostname string is placed, and the hostlen parameter is the size of that buffer. If there is no hostname that corresponds to the address, then the network address (dotted decimal or colon-hex) is placed in host. Likewise, the serv parameter points to a buffer where the null-terminated service name string (or port number) is placed, and the servlen parameter is the size of that buffer. The flags parameter modifies the function's behavior; in particular, the NI_NUMERICHOST flag indicates that the converted hostname should always be formatted in numeric form (i.e. dotted decimal or colon-hex), and the NI_NUMERICSERV flag indicates that the converted service should always be in numeric form (i.e. the port number).

The symbols NI_MAXHOST and NI_MAXSERV are available to applications and represent the maximum size of any converted hostname or service name, respectively. Use these when declaring output buffers for getnameinfo(3).
|23.1.3. Quirks Encountered

Before jumping into the programming examples, there are several quirks in IPv6 of which the reader should be aware. The more significant ones (in addition to the non-uniqueness of IPv6 network addresses already discussed) are described in the paragraphs below.
|23.1.3.1. IPv4 Mapped Addresses

For security reasons that this author won't pretend to understand, "IPv4 mapped addresses" should not be allowed in IPv6-capable server applications. To put it in terms that everyone can understand, this simply means that a server should not accept IPv4 traffic on an IPv6 socket (an otherwise legal operation). An IPv4 mapped address is a mixed-format address of the form:

::ffff:192.0.2.1

where the first portion is in IPv6 colon-hex format and the last portion is in IPv4 dotted decimal notation. The dotted decimal IPv4 address is the actual network address, but it is being mapped into an IPv6 compatible format.

To prevent IPv4 mapped addresses from being accepted on an IPv6 socket, server applications must explicitly set the IPV6_V6ONLY socket option on all IPv6 sockets created [the Hagino book implies that this is only a concern with server applications. However, it has been observed during testing that if a client application uses an IPv4 mapped address to specify the target server, and the target server has IPv4 mapped addresses disabled, the connection still completes regardless. On the server side, the connection endpoint is an IPv4 socket as desired; but on the client side, the connection endpoint is an IPv6 socket. Setting the IPV6_V6ONLY socket option on the client side as well as the server side prevents any connection from being established at all.]. There's only one problem. Apparently, IPV6_V6ONLY isn't defined on all systems [or at least it wasn't in 2005 when the Hagino book was written]. The server example at the end of this chapter provides a method for handling this problem.

If IPv4 traffic cannot be handled on IPv6 sockets, then that implies that server applications must open both an IPv4 and IPv6 socket for a particular network service if it wants to handle requests from either protocol. This goes back to the flexibility issue mentioned earlier. If getaddrinfo(3) returns multiple address records, then server applications should traverse the list and open a passive socket for each address provided.
|23.1.3.2. Cannot Specify the Scope Identifier in /etc/hosts

It is possible to assign a hostname to an IPv6 network address in /etc/hosts. For example, the following is an excerpt from the /etc/hosts file on the author's development system.

        ::1                        localhost
|        127.0.0.1                  localhost
        fe80::2c0:8cff:fe01:2345   pt141
|        192.0.2.1                  pt141

The "localhost" and "pt141" hostnames can be translated to either an IPv4 or IPv6 network address. So, for example, if "pt141" is passed as the node parameter to getaddrinfo(3), the function returns both an IPv4 and IPv6 address record for the host (assuming the behavior hasn't been modified by the hints parameter). Unfortunately, a scoped address cannot be used in /etc/hosts. Doing so results in getaddrinfo(3) returning only the IPv4 record.
|23.1.3.3. Client & Server Residing on the Same Machine

Suppose a machine has the IPv4 address 192.0.2.1. A client application running on that machine can connect to a server application on the same machine by using either the local loopback address (127.0.0.1) or the network address (192.0.2.1) as the target server. Much to this author's surprise (and dismay), it turns out that an IPv6 client application cannot connect to a server application on the same machine if it uses the network address of that machine as the target; it must use the local loopback address (::1).
|23.1.4. Putting It All Together (A Client-Server Programming Example)

Now it's time to put everything discussed thus far together into a sample client-server application. The remainder of this section is devoted to a remote time-of-day application (the 'daytime' Internet service) [I noticed that Ms. Castro used a 'daytime' example in her Porting applications to IPv6 HowTo. For the record, the source code presented here is original, developed from scratch, and any similarity between it and any other publicly available 'daytime' example is purely coincidental.]. The source code presented in this section was developed and tested on a RedHat Linux release using the 2.6 kernel (2.6.9 to be specific). Readers may use the source code freely, so long as proper credit is attributed; but of course the standard disclaimer must be given first:

    Although the sample source code is believed to be free of errors, the author makes no guarantees as to its reliability, especially considering that some error paths were intentionally omitted for brevity. Use it at your own risk!

When you get right down to it, there really aren't that many differences between IPv4 and IPv6 applications. The trick is to code IPv6 applications in a protocol-independent manner, such that they can handle both IPv4 and IPv6 simultaneously and transparently. This sample application does just that. The only protocol-dependent code in the example occurs when printing network addresses in verbose mode; but only after the ai_family field in the addrinfo structure has been checked, so the programs know exactly what type of address they're handling at the time.
|23.1.4.1. 'Daytime' Server Code

The server code is found in file tod6d.c (time-of-day IPv6 daemon). Once built, the server may be started using the following command syntax (assuming tod6d is the executable file):

tod6d [-v] [service]

ARGUMENTS:

service

    The service (or well-known port) on which to listen. Default is "daytime".

OPTIONS:

-v

    Turn on verbose mode.

The server handles both TCP and UDP requests on the network. The server source code contained in tod6d.c follows:

/******************************************************************************
* File: tod6d.c
* Description: Contains source code for an IPv6-capable 'daytime' server.
* Author: John Wenker, Sr. Software Engineer,
*         Performance Technologies, San Diego, USA
******************************************************************************/
/*
** System header files.
*/
#include <errno.h>        /* errno declaration & error codes.            */
#include <netdb.h>        /* getaddrinfo(3) et al.                       */
#include <netinet/in.h>   /* sockaddr_in & sockaddr_in6 definition.      */
#include <stdio.h>        /* printf(3) et al.                            */
#include <stdlib.h>       /* exit(2).                                    */
#include <string.h>       /* String manipulation & memory functions.     */
#include <sys/poll.h>     /* poll(2) and related definitions.            */
#include <sys/socket.h>   /* Socket functions (socket(2), bind(2), etc). */
#include <time.h>         /* time(2) & ctime(3).                         */
#include <unistd.h>       /* getopt(3), read(2), etc.                    */
/*
** Constants.
*/
#define DFLT_SERVICE "daytime"   /* Default service name.                    */
#define INVALID_DESC -1          /* Invalid file descriptor.                 */
#define MAXCONNQLEN  3           /* Max nbr of connection requests to queue. */
#define MAXTCPSCKTS  2           /* One TCP socket for IPv4 & one for IPv6.  */
#define MAXUDPSCKTS  2           /* One UDP socket for IPv4 & one for IPv6.  */
#define VALIDOPTS    "v"         /* Valid command options.                   */
/*
** Simple boolean type definition.
*/
typedef enum { false = 0, true } boolean;
/*
** Prototypes for internal helper functions.
*/
static int  openSckt( const char *service,
                      const char *protocol,
                      int         desc[ ],
                      size_t     *descSize );
static void tod( int    tSckt[ ],
                 size_t tScktSize,
                 int    uSckt[ ],
                 size_t uScktSize );
/*
** Global (within this file only) data objects.
*/
static char        hostBfr[ NI_MAXHOST ];   /* For use w/getnameinfo(3).    */
static const char *pgmName;                 /* Program name w/o dir prefix. */
static char        servBfr[ NI_MAXSERV ];   /* For use w/getnameinfo(3).    */
static boolean     verbose = false;         /* Verbose mode indication.     */
/*
** Usage macro for command syntax violations.
*/
#define USAGE                                       \
        {                                           \
           fprintf( stderr,                         \
                    "Usage: %s [-v] [service]\n",   \
                    pgmName );                      \
           exit( 127 );                             \
        }  /* End USAGE macro. */
/*
** Macro to terminate the program if a system call error occurs.  The system
** call must be one of the usual type that returns -1 on error.  This macro is
** a modified version of a macro authored by Dr. V. Vinge, SDSU Dept. of
** Computer Science (retired)... best professor I ever had.  I hear he writes
** great science fiction in addition to robust code, too.
*/
#define CHK(expr)                                                   \
        do                                                          \
        {                                                           \
           if ( (expr) == -1 )                                      \
           {                                                        \
              fprintf( stderr,                                      \
                       "%s (line %d): System call ERROR - %s.\n",   \
                       pgmName,                                     \
                       __LINE__,                                    \
                       strerror( errno ) );                         \
              exit( 1 );                                            \
           }   /* End IF system call failed. */                     \
        } while ( false )
/******************************************************************************
* Function: main
*
* Description:
*    Set up a time-of-day server and handle network requests.  This server
*    handles both TCP and UDP requests.
*
* Parameters:
*    The usual argc and argv parameters to a main() function.
*
* Return Value:
*    This is a daemon program and never returns.  However, in the degenerate
*    case where no sockets are created, the function returns zero.
******************************************************************************/
int main( int   argc,
          char *argv[ ] )
{
   int         opt;
   const char *service   = DFLT_SERVICE;
   int         tSckt[ MAXTCPSCKTS ];     /* Array of TCP socket descriptors. */
   size_t      tScktSize = MAXTCPSCKTS;  /* Size of uSckt (# of elements).   */
   int         uSckt[ MAXUDPSCKTS ];     /* Array of UDP socket descriptors. */
   size_t      uScktSize = MAXUDPSCKTS;  /* Size of uSckt (# of elements).   */
   /*
   ** Set the program name (w/o directory prefix).
   */
   pgmName = strrchr( argv[ 0 ], '/' );
   pgmName = pgmName == NULL  ?  argv[ 0 ]  :  pgmName + 1;
   /*
   ** Process command options.
   */
   opterr = 0;   /* Turns off "invalid option" error messages. */
   while ( ( opt = getopt( argc, argv, VALIDOPTS ) ) >= 0 )
   {
      switch ( opt )
      {
         case 'v':   /* Verbose mode. */
         {
            verbose = true;
            break;
         }
         default:
         {
            USAGE;
         }
      }  /* End SWITCH on command option. */
   }  /* End WHILE processing options. */
   /*
   ** Process command line arguments.
   */
   switch ( argc - optind )
   {
      case 0:  break;
      case 1:  service = argv[ optind ]; break;
      default: USAGE;
   }  /* End SWITCH on number of command line arguments. */
   /*
   ** Open both a TCP and UDP socket, for both IPv4 & IPv6, on which to receive
   ** service requests.
   */
   if ( ( openSckt( service, "tcp", tSckt, &tScktSize ) < 0 ) ||
        ( openSckt( service, "udp", uSckt, &uScktSize ) < 0 ) )
   {
      exit( 1 );
   }
   /*
   ** Run the time-of-day server.
   */
   if ( ( tScktSize > 0 ) || ( uScktSize > 0 ) )
   {
      tod( tSckt,         /* tod() never returns. */
           tScktSize,
           uSckt,
           uScktSize );
   }
   /*
   ** Since tod() never returns, execution only gets here if no sockets were
   ** created.
   */
   if ( verbose )
   {
      fprintf( stderr,
               "%s: No sockets opened... terminating.\n",
               pgmName );
   }
   return 0;
}  /* End main() */
/******************************************************************************
* Function: openSckt
*
* Description:
*    Open passive (server) sockets for the indicated inet service & protocol.
*    Notice in the last sentence that "sockets" is plural.  During the interim
*    transition period while everyone is switching over to IPv6, the server
*    application has to open two sockets on which to listen for connections...
*    one for IPv4 traffic and one for IPv6 traffic.
*
* Parameters:
*    service  - Pointer to a character string representing the well-known port
*               on which to listen (can be a service name or a decimal number).
*    protocol - Pointer to a character string representing the transport layer
*               protocol (only "tcp" or "udp" are valid).
*    desc     - Pointer to an array into which the socket descriptors are
*               placed when opened.
*    descSize - This is a value-result parameter.  On input, it contains the
*               max number of descriptors that can be put into 'desc' (i.e. the
*               number of elements in the array).  Upon return, it will contain
*               the number of descriptors actually opened.  Any unused slots in
*               'desc' are set to INVALID_DESC.
*
* Return Value:
*    0 on success, -1 on error.
******************************************************************************/
static int openSckt( const char *service,
                     const char *protocol,
                     int         desc[ ],
                     size_t     *descSize )
{
   struct addrinfo *ai;
   int              aiErr;
   struct addrinfo *aiHead;
   struct addrinfo  hints    = { .ai_flags  = AI_PASSIVE,    /* Server mode. 
¬ */
                                 .ai_family = PF_UNSPEC };   /* IPv4 or IPv6.
¬ */
   size_t           maxDescs = *descSize;
   /*
   ** Initialize output parameters.  When the loop completes, *descSize is 0.
   */
   while ( *descSize > 0 )
   {
      desc[ --( *descSize ) ] = INVALID_DESC;
   }
   /*
   ** Check which protocol is selected (only TCP and UDP are valid).
   */
   if ( strcmp( protocol, "tcp" ) == 0 )        /* TCP protocol.     */
   {
      hints.ai_socktype = SOCK_STREAM;
      hints.ai_protocol = IPPROTO_TCP;
   }
   else if ( strcmp( protocol, "udp" ) == 0 )   /* UDP protocol.     */
   {
      hints.ai_socktype = SOCK_DGRAM;
      hints.ai_protocol = IPPROTO_UDP;
   }
   else                                         /* Invalid protocol. */
   {
      fprintf( stderr,
               "%s (line %d): ERROR - Unknown transport "
               "layer protocol \"%s\".\n",
               pgmName,
               __LINE__,
               protocol );
      return -1;
   }
   /*
   ** Look up the service's well-known port number.  Notice that NULL is being
   ** passed for the 'node' parameter, and that the AI_PASSIVE flag is set in
   ** 'hints'.  Thus, the program is requesting passive address information.
   ** The network address is initialized to :: (all zeros) for IPv6 records, or
   ** 0.0.0.0 for IPv4 records.
   */
   if ( ( aiErr = getaddrinfo( NULL,
                               service,
                               &hints,
                               &aiHead ) ) != 0 )
   {
      fprintf( stderr,
               "%s (line %d): ERROR - %s.\n",
               pgmName,
               __LINE__,
               gai_strerror( aiErr ) );
      return -1;
   }
   /*
   ** For each of the address records returned, attempt to set up a passive
   ** socket.
   */
   for ( ai = aiHead;
         ( ai != NULL ) && ( *descSize < maxDescs );
         ai = ai->ai_next )
   {
      if ( verbose )
      {
         /*
         ** Display the current address info.   Start with the protocol-
         ** independent fields first.
         */
         fprintf( stderr,
                  "Setting up a passive socket based on the "
                  "following address info:\n"
                  "   ai_flags     = 0x%02X\n"
                  "   ai_family    = %d (PF_INET = %d, PF_INET6 = %d)\n"
                  "   ai_socktype  = %d (SOCK_STREAM = %d, SOCK_DGRAM = %d)\n"
                  "   ai_protocol  = %d (IPPROTO_TCP = %d, IPPROTO_UDP = %d)\n"
                  "   ai_addrlen   = %d (sockaddr_in = %d, "
                  "sockaddr_in6 = %d)\n",
                  ai->ai_flags,
                  ai->ai_family,
                  PF_INET,
                  PF_INET6,
                  ai->ai_socktype,
                  SOCK_STREAM,
                  SOCK_DGRAM,
                  ai->ai_protocol,
                  IPPROTO_TCP,
                  IPPROTO_UDP,
                  ai->ai_addrlen,
                  sizeof( struct sockaddr_in ),
                  sizeof( struct sockaddr_in6 ) );
         /*
         ** Now display the protocol-specific formatted socket address.  Note
         ** that the program is requesting that getnameinfo(3) convert the
         ** host & service into numeric strings.
         */
         getnameinfo( ai->ai_addr,
                      ai->ai_addrlen,
                      hostBfr,
                      sizeof( hostBfr ),
                      servBfr,
                      sizeof( servBfr ),
                      NI_NUMERICHOST | NI_NUMERICSERV );
         switch ( ai->ai_family )
         {
            case PF_INET:   /* IPv4 address record. */
            {
               struct sockaddr_in *p = (struct sockaddr_in*) ai->ai_addr;
               fprintf( stderr,
                        "   ai_addr      = sin_family:   %d (AF_INET = %d, "
                        "AF_INET6 = %d)\n"
                        "                  sin_addr:     %s\n"
                        "                  sin_port:     %s\n",
                        p->sin_family,
                        AF_INET,
                        AF_INET6,
                        hostBfr,
                        servBfr );
               break;
            }  /* End CASE of IPv4. */
            case PF_INET6:   /* IPv6 address record. */
            {
               struct sockaddr_in6 *p = (struct sockaddr_in6*) ai->ai_addr;
               fprintf( stderr,
                        "   ai_addr      = sin6_family:   %d (AF_INET = %d, "
                        "AF_INET6 = %d)\n"
                        "                  sin6_addr:     %s\n"
                        "                  sin6_port:     %s\n"
                        "                  sin6_flowinfo: %d\n"
                        "                  sin6_scope_id: %d\n",
                        p->sin6_family,
                        AF_INET,
                        AF_INET6,
                        hostBfr,
                        servBfr,
                        p->sin6_flowinfo,
                        p->sin6_scope_id );
               break;
            }  /* End CASE of IPv6. */
            default:   /* Can never get here, but just for completeness. */
            {
               fprintf( stderr,
                        "%s (line %d): ERROR - Unknown protocol family (%d).\n",
                        pgmName,
                        __LINE__,
                        ai->ai_family );
               freeaddrinfo( aiHead );
               return -1;
            }  /* End DEFAULT case (unknown protocol family). */
         }  /* End SWITCH on protocol family. */
      }  /* End IF verbose mode. */
      /*
      ** Create a socket using the info in the addrinfo structure.
      */
      CHK( desc[ *descSize ] = socket( ai->ai_family,
                                       ai->ai_socktype,
                                       ai->ai_protocol ) );
      /*
      ** Here is the code that prevents "IPv4 mapped addresses", as discussed
      ** in Section 22.1.3.1.  If an IPv6 socket was just created, then set the
      ** IPV6_V6ONLY socket option.
      */
      if ( ai->ai_family == PF_INET6 )
      {
#if defined( IPV6_V6ONLY )
         /*
         ** Disable IPv4 mapped addresses.
         */
         int v6Only = 1;
         CHK( setsockopt( desc[ *descSize ],
                          IPPROTO_IPV6,
                          IPV6_V6ONLY,
                          &v6Only,
                          sizeof( v6Only ) ) );
#else
         /*
         ** IPV6_V6ONLY is not defined, so the socket option can't be set and
         ** thus IPv4 mapped addresses can't be disabled.  Print a warning
         ** message and close the socket.  Design note: If the
         ** #if...#else...#endif construct were removed, then this program
         ** would not compile (because IPV6_V6ONLY isn't defined).  That's an
         ** acceptable approach; IPv4 mapped addresses are certainly disabled
         ** if the program can't build!  However, since this program is also
         ** designed to work for IPv4 sockets as well as IPv6, I decided to
         ** allow the program to compile when IPV6_V6ONLY is not defined, and
         ** turn it into a run-time warning rather than a compile-time error.
         ** IPv4 mapped addresses are still disabled because _all_ IPv6 traffic
         ** is disabled (all IPv6 sockets are closed here), but at least this
         ** way the server can still service IPv4 network traffic.
         */
         fprintf( stderr,
                  "%s (line %d): WARNING - Cannot set IPV6_V6ONLY socket "
                  "option.  Closing IPv6 %s socket.\n",
                  pgmName,
                  __LINE__,
                  ai->ai_protocol == IPPROTO_TCP  ?  "TCP"  :  "UDP" );
         CHK( close( desc[ *descSize ] ) );
         continue;   /* Go to top of FOR loop w/o updating *descSize! */
#endif /* IPV6_V6ONLY */
      }  /* End IF this is an IPv6 socket. */
      /*
      ** Bind the socket.  Again, the info from the addrinfo structure is used.
      */
      CHK( bind( desc[ *descSize ],
                 ai->ai_addr,
                 ai->ai_addrlen ) );
      /*
      ** If this is a TCP socket, put the socket into passive listening mode
      ** (listen is only valid on connection-oriented sockets).
      */
      if ( ai->ai_socktype == SOCK_STREAM )
      {
         CHK( listen( desc[ *descSize ],
                      MAXCONNQLEN ) );
      }
      /*
      ** Socket set up okay.  Bump index to next descriptor array element.
      */
      *descSize += 1;
   }  /* End FOR each address info structure returned. */
   /*
   ** Dummy check for unused address records.
   */
   if ( verbose && ( ai != NULL ) )
   {
      fprintf( stderr,
               "%s (line %d): WARNING - Some address records were "
               "not processed due to insufficient array space.\n",
               pgmName,
               __LINE__ );
   }  /* End IF verbose and some address records remain unprocessed. */
   /*
   ** Clean up.
   */
   freeaddrinfo( aiHead );
   return 0;
}  /* End openSckt() */
/******************************************************************************
* Function: tod
*
* Description:
*    Listen on a set of sockets and send the current time-of-day to any
*    clients.  This function never returns.
*
* Parameters:
*    tSckt     - Array of TCP socket descriptors on which to listen.
*    tScktSize - Size of the tSckt array (nbr of elements).
*    uSckt     - Array of UDP socket descriptors on which to listen.
*    uScktSize - Size of the uSckt array (nbr of elements).
*
* Return Value: None.
******************************************************************************/
static void tod( int    tSckt[ ],
                 size_t tScktSize,
                 int    uSckt[ ],
                 size_t uScktSize )
{
   char                     bfr[ 256 ];
   ssize_t                  count;
   struct pollfd           *desc;
   size_t                   descSize = tScktSize + uScktSize;
   int                      idx;
   int                      newSckt;
   struct sockaddr         *sadr;
   socklen_t                sadrLen;
   struct sockaddr_storage  sockStor;
   int                      status;
   size_t                   timeLen;
   char                    *timeStr;
   time_t                   timeVal;
   ssize_t                  wBytes;
   /*
   ** Allocate memory for the poll(2) array.
   */
   desc = malloc( descSize * sizeof( struct pollfd ) );
   if ( desc == NULL )
   {
      fprintf( stderr,
               "%s (line %d): ERROR - %s.\n",
               pgmName,
               __LINE__,
               strerror( ENOMEM ) );
      exit( 1 );
   }
   /*
   ** Initialize the poll(2) array.
   */
   for ( idx = 0;     idx < descSize;     idx++ )
   {
      desc[ idx ].fd      = idx < tScktSize  ?  tSckt[ idx ]
                                             :  uSckt[ idx - tScktSize ];
      desc[ idx ].events  = POLLIN;
      desc[ idx ].revents = 0;
   }
   /*
   ** Main time-of-day server loop.  Handles both TCP & UDP requests.  This is
   ** an interative server, and all requests are handled directly within the
   ** main loop.
   */
   while ( true )   /* Do forever. */
   {
      /*
      ** Wait for activity on one of the sockets.  The DO..WHILE construct is
      ** used to restart the system call in the event the process is
      ** interrupted by a signal.
      */
      do
      {
         status = poll( desc,
                        descSize,
                        -1 /* Wait indefinitely for input. */ );
      } while ( ( status < 0 ) && ( errno == EINTR ) );
      CHK( status );   /* Check for a bona fide system call error. */
      /*
      ** Get the current time.
      */
      timeVal = time( NULL );
      timeStr = ctime( &timeVal );
      timeLen = strlen( timeStr );
      /*
      ** Indicate that there is new network activity.
      */
      if ( verbose )
      {
         char *s = malloc( timeLen+1 );
         strcpy( s, timeStr );
         s[ timeLen-1 ] = '\0';   /* Overwrite '\n' in date string. */
         fprintf( stderr,
                  "%s: New network activity on %s.\n",
                  pgmName,
                  s );
         free( s );
      }  /* End IF verbose. */
      /*
      ** Process sockets with input available.
      */
      for ( idx = 0;     idx < descSize;     idx++ )
      {
         switch ( desc[ idx ].revents )
         {
            case 0:        /* No activity on this socket; try the next. */
               continue;
            case POLLIN:   /* Network activity.  Go process it.         */
               break;
            default:       /* Invalid poll events.                      */
            {
               fprintf( stderr,
                        "%s (line %d): ERROR - Invalid poll event (0x%02X).\n",
                        pgmName,
                        __LINE__,
                        desc[ idx ].revents );
               exit( 1 );
            }
         }  /* End SWITCH on returned poll events. */
         /*
         ** Determine if this is a TCP request or UDP request.
         */
         if ( idx < tScktSize )
         {
            /*
            ** TCP connection requested.  Accept it.  Notice the use of
            ** the sockaddr_storage data type.
            */
            sadrLen = sizeof( sockStor );
            sadr    = (struct sockaddr*) &sockStor;
            CHK( newSckt = accept( desc[ idx ].fd,
                                   sadr,
                                   &sadrLen ) );
            CHK( shutdown( newSckt,       /* Server never recv's anything. */
                           SHUT_RD ) );
            if ( verbose )
            {
               /*
               ** Display the socket address of the remote client.  Begin with
               ** the address-independent fields.
               */
               fprintf( stderr,
                        "Sockaddr info for new TCP client:\n"
                        "   sa_family = %d (AF_INET = %d, AF_INET6 = %d)\n"
                        "   addr len  = %d (sockaddr_in = %d, "
                        "sockaddr_in6 = %d)\n",
                        sadr->sa_family,
                        AF_INET,
                        AF_INET6,
                        sadrLen,
                        sizeof( struct sockaddr_in ),
                        sizeof( struct sockaddr_in6 ) );
               /*
               ** Display the address-specific fields.
               */
               getnameinfo( sadr,
                            sadrLen,
                            hostBfr,
                            sizeof( hostBfr ),
                            servBfr,
                            sizeof( servBfr ),
                            NI_NUMERICHOST | NI_NUMERICSERV );
               /*
               ** Notice that we're switching on an address family now, not a
               ** protocol family.
               */
               switch ( sadr->sa_family )
               {
                  case AF_INET:   /* IPv4 address. */
                  {
                     struct sockaddr_in *p = (struct sockaddr_in*) sadr;
                     fprintf( stderr,
                              "   sin_addr  = sin_family: %d\n"
                              "               sin_addr:   %s\n"
                              "               sin_port:   %s\n",
                              p->sin_family,
                              hostBfr,
                              servBfr );
                     break;
                  }  /* End CASE of IPv4. */
                  case AF_INET6:   /* IPv6 address. */
                  {
                     struct sockaddr_in6 *p = (struct sockaddr_in6*) sadr;
                     fprintf( stderr,
                              "   sin6_addr = sin6_family:   %d\n"
                              "               sin6_addr:     %s\n"
                              "               sin6_port:     %s\n"
                              "               sin6_flowinfo: %d\n"
                              "               sin6_scope_id: %d\n",
                              p->sin6_family,
                              hostBfr,
                              servBfr,
                              p->sin6_flowinfo,
                              p->sin6_scope_id );
                     break;
                  }  /* End CASE of IPv6. */
                  default:   /* Can never get here, but for completeness. */
                  {
                     fprintf( stderr,
                              "%s (line %d): ERROR - Unknown address "
                              "family (%d).\n",
                              pgmName,
                              __LINE__,
                              sadr->sa_family );
                     break;
                  }  /* End DEFAULT case (unknown address family). */
               }  /* End SWITCH on address family. */
            }  /* End IF verbose mode. */
            /*
            ** Send the TOD to the client.
            */
            wBytes = timeLen;
            while ( wBytes > 0 )
            {
               do
               {
                  count = write( newSckt,
                                 timeStr,
                                 wBytes );
               } while ( ( count < 0 ) && ( errno == EINTR ) );
               CHK( count );   /* Check for a bona fide error. */
               wBytes -= count;
            }  /* End WHILE there is data to send. */
            CHK( close( newSckt ) );
         }  /* End IF this was a TCP connection request. */
         else
         {
            /*
            ** This is a UDP socket, and a datagram is available.  The funny
            ** thing about UDP requests is that this server doesn't require any
            ** client input; but it can't send the TOD unless it knows a client
            ** wants the data, and the only way that can occur with UDP is if
            ** the server receives a datagram from the client.  Thus, the
            ** server must receive _something_, but the content of the datagram
            ** is irrelevant.  Read in the datagram.  Again note the use of
            ** sockaddr_storage to receive the address.
            */
            sadrLen = sizeof( sockStor );
            sadr    = (struct sockaddr*) &sockStor;
            CHK( count = recvfrom( desc[ idx ].fd,
                                   bfr,
                                   sizeof( bfr ),
|                                   0,
                                   sadr,
                                   &sadrLen ) );
            /*
            ** Display whatever was received on stdout.
            */
            if ( verbose )
            {
               ssize_t rBytes = count;
               fprintf( stderr,
                        "%s: UDP datagram received (%d bytes).\n",
                        pgmName,
                        count );
               while ( count > 0 )
               {
                  fputc( bfr[ rBytes - count-- ],
                         stdout );
               }
               if ( bfr[ rBytes-1 ] != '\n' )
                  fputc( '\n', stdout );   /* Newline also flushes stdout. */
               /*
               ** Display the socket address of the remote client.  Address-
               ** independent fields first.
               */
               fprintf( stderr,
                        "Remote client's sockaddr info:\n"
                        "   sa_family = %d (AF_INET = %d, AF_INET6 = %d)\n"
                        "   addr len  = %d (sockaddr_in = %d, "
                        "sockaddr_in6 = %d)\n",
                        sadr->sa_family,
                        AF_INET,
                        AF_INET6,
                        sadrLen,
                        sizeof( struct sockaddr_in ),
                        sizeof( struct sockaddr_in6 ) );
               /*
               ** Display the address-specific information.
               */
               getnameinfo( sadr,
                            sadrLen,
                            hostBfr,
                            sizeof( hostBfr ),
                            servBfr,
                            sizeof( servBfr ),
                            NI_NUMERICHOST | NI_NUMERICSERV );
               switch ( sadr->sa_family )
               {
                  case AF_INET:   /* IPv4 address. */
                  {
                     struct sockaddr_in *p = (struct sockaddr_in*) sadr;
                     fprintf( stderr,
                              "   sin_addr  = sin_family: %d\n"
                              "               sin_addr:   %s\n"
                              "               sin_port:   %s\n",
                              p->sin_family,
                              hostBfr,
                              servBfr );
                     break;
                  }  /* End CASE of IPv4 address. */
                  case AF_INET6:   /* IPv6 address. */
                  {
                     struct sockaddr_in6 *p = (struct sockaddr_in6*) sadr;
                     fprintf( stderr,
                              "   sin6_addr = sin6_family:   %d\n"
                              "               sin6_addr:     %s\n"
                              "               sin6_port:     %s\n"
                              "               sin6_flowinfo: %d\n"
                              "               sin6_scope_id: %d\n",
                              p->sin6_family,
                              hostBfr,
                              servBfr,
                              p->sin6_flowinfo,
                              p->sin6_scope_id );
                     break;
                  }  /* End CASE of IPv6 address. */
                  default:   /* Can never get here, but for completeness. */
                  {
                     fprintf( stderr,
                              "%s (line %d): ERROR - Unknown address "
                              "family (%d).\n",
                              pgmName,
                              __LINE__,
                              sadr->sa_family );
                     break;
                  }  /* End DEFAULT case (unknown address family). */
               }  /* End SWITCH on address family. */
            }  /* End IF verbose mode. */
            /*
            ** Send the time-of-day to the client.
            */
            wBytes = timeLen;
            while ( wBytes > 0 )
            {
               do
               {
                  count = sendto( desc[ idx ].fd,
                                  timeStr,
                                  wBytes,
|                                  0,
                                  sadr,        /* Address & address length   */
                                  sadrLen );   /*    received in recvfrom(). */
               } while ( ( count < 0 ) && ( errno == EINTR ) );
               CHK( count );   /* Check for a bona fide error. */
               wBytes -= count;
            }  /* End WHILE there is data to send. */
         }  /* End ELSE a UDP datagram is available. */
         desc[ idx ].revents = 0;   /* Clear the returned poll events. */
      }  /* End FOR each socket descriptor. */
   }  /* End WHILE forever. */
}  /* End tod() */

|23.1.4.2. 'Daytime' TCP Client Code

The TCP client code is found in file tod6tc.c (time-of-day IPv6 TCP client). Once built, the TCP client may be started using the following command syntax (assuming tod6tc is the executable file):

tod6tc [-v] [-s scope_id] [host [service]]

ARGUMENTS:

host

    The hostname or IP address (dotted decimal or colon-hex) of the remote host providing the service. Default is "localhost".
service

    The TCP service (or well-known port number) to which a connection attempt is made. Default is "daytime".

OPTIONS:

-s

    This option is only meaningful for IPv6 addresses, and is used to set the scope identifier (i.e. the network interface on which to establish the connection). Default is "eth0". If host is a scoped address, this option is ignored.
-v

    Turn on verbose mode.

The TCP client source code contained in tod6tc.c follows:

/******************************************************************************
* File: tod6tc.c
* Description: Contains source code for an IPv6-capable 'daytime' TCP client.
* Author: John Wenker, Sr. Software Engineer
*         Performance Technologies, San Diego, USA
******************************************************************************/
/*
** System header files.
*/
#include <errno.h>        /* errno declaration and error codes.             */
#include <net/if.h>       /* if_nametoindex(3).                             */
#include <netdb.h>        /* getaddrinfo(3) and associated definitions.     */
#include <netinet/in.h>   /* sockaddr_in and sockaddr_in6 definitions.      */
#include <stdio.h>        /* printf(3) et al.                               */
#include <stdlib.h>       /* exit(2).                                       */
#include <string.h>       /* String manipulation and memory functions.      */
#include <sys/socket.h>   /* Socket functions (socket(2), connect(2), etc). */
#include <unistd.h>       /* getopt(3), read(2), etc.                       */
/*
** Constants & macros.
*/
#define DFLT_HOST      "localhost"   /* Default server name.              */
#define DFLT_SCOPE_ID  "eth0"        /* Default scope identifier.         */
#define DFLT_SERVICE   "daytime"     /* Default service name.             */
#define INVALID_DESC   -1            /* Invalid file (socket) descriptor. */
#define MAXBFRSIZE     256           /* Max bfr sz to read remote TOD.    */
#define VALIDOPTS      "s:v"         /* Valid command options.            */
/*
** Type definitions (for convenience).
*/
typedef enum { false = 0, true } boolean;
typedef struct sockaddr_in       sockaddr_in_t;
typedef struct sockaddr_in6      sockaddr_in6_t;
/*
** Prototypes for internal helper functions.
*/
static int  openSckt( const char   *host,
                      const char   *service,
                      unsigned int  scopeId );
static void tod( int sckt );
/*
** Global (within this file only) data objects.
*/
static const char *pgmName;           /* Program name (w/o directory). */
static boolean     verbose = false;   /* Verbose mode.                 */
/*
** Usage macro.
*/
#define USAGE                                                            \
        {                                                                \
           fprintf( stderr,                                              \
                    "Usage: %s [-v] [-s scope_id] [host [service]]\n",   \
                    pgmName );                                           \
           exit( 127 );                                                  \
        }  /* End USAGE macro. */
/*
** This "macro" (even though it's really a function) is loosely based on the
** CHK() macro by Dr. V. Vinge (see server code).  The status parameter is
** a boolean expression indicating the return code from one of the usual system
** calls that returns -1 on error.  If a system call error occurred, an alert
** is written to stderr.  It returns a boolean value indicating success/failure
** of the system call.
**
** Example: if ( !SYSCALL( "write",
**                         count = write( fd, bfr, size ) ) )
**          {
**             // Error processing... but SYSCALL() will have already taken
**             // care of dumping an error alert to stderr.
**          }
*/
static __inline boolean SYSCALL( const char *syscallName,
                                 int         lineNbr,
                                 int         status )
{
   if ( ( status == -1 ) && verbose )
   {
      fprintf( stderr,
               "%s (line %d): System call failed ('%s') - %s.\n",
               pgmName,
               lineNbr,
               syscallName,
               strerror( errno ) );
   }
   return status != -1;   /* True if the system call was successful. */
}  /* End SYSCALL() */
/******************************************************************************
* Function: main
*
* Description:
*    Connect to a remote time-of-day service and write the remote host's TOD to
*    stdout.
*
* Parameters:
*    The usual argc & argv parameters to a main() program.
*
* Return Value:
*    This function always returns zero.
******************************************************************************/
int main( int   argc,
          char *argv[ ] )
{
   const char   *host     = DFLT_HOST;
   int           opt;
   int           sckt;
   unsigned int  scopeId  = if_nametoindex( DFLT_SCOPE_ID );
   const char   *service  = DFLT_SERVICE;
   /*
   ** Determine the program name (w/o directory prefix).
   */
   pgmName = (const char*) strrchr( argv[ 0 ], '/' );
   pgmName = pgmName == NULL  ?  argv[ 0 ]  :  pgmName+1;
   /*
   ** Process command line options.
   */
   opterr = 0;   /* Turns off "invalid option" error messages. */
   while ( ( opt = getopt( argc, argv, VALIDOPTS ) ) != -1 )
   {
      switch ( opt )
      {
         case 's':   /* Scope identifier (IPv6 kluge). */
         {
            scopeId = if_nametoindex( optarg );
            if ( scopeId == 0 )
            {
               fprintf( stderr,
                        "%s: Unknown network interface (%s).\n",
                        pgmName,
                        optarg );
               USAGE;
            }
            break;
         }
         case 'v':   /* Verbose mode. */
         {
            verbose = true;
            break;
         }
         default:
         {
            USAGE;
         }
      }  /* End SWITCH on command option. */
   } /* End WHILE processing command options. */
   /*
   ** Process command arguments.  At the end of the above loop, optind is the
   ** index of the first NON-option argv element.
   */
   switch ( argc - optind )
   {
      case 2:   /* Both host & service are specified on the command line. */
      {
          service = argv[ optind + 1 ];
          /***** Fall through *****/
      }
      case 1:   /* Host is specified on the command line. */
      {
          host = argv[ optind ];
          /***** Fall through *****/
      }
      case 0:   /* Use default host & service. */
      {
          break;
      }
      default:
      {
         USAGE;
      }
   }  /* End SWITCH on number of command arguments. */
   /*
   ** Open a connection to the indicated host/service.
   **
   ** Note that if all three of the following conditions are met, then the
   ** scope identifier remains unresolved at this point.
   **    1) The default network interface is unknown for some reason.
   **    2) The -s option was not used on the command line.
   **    3) An IPv6 "scoped address" was not specified for the hostname on the
   **       command line.
   ** If the above three conditions are met, then only an IPv4 socket can be
   ** opened (connect(2) fails without the scope ID properly set for IPv6
   ** sockets).
   */
   if ( ( sckt = openSckt( host,
                           service,
                           scopeId ) ) == INVALID_DESC )
   {
      fprintf( stderr,
               "%s: Sorry... a connection could not be established.\n",
               pgmName );
      exit( 1 );
   }
   /*
   ** Get the remote time-of-day.
   */
   tod( sckt );
   /*
   ** Close the connection and terminate.
   */
   (void) SYSCALL( "close",
                   __LINE__,
                   close( sckt ) );
   return 0;
}  /* End main() */
/******************************************************************************
* Function: openSckt
*
* Description:
*    Sets up a TCP connection to a remote server.  Getaddrinfo(3) is used to
*    perform lookup functions and can return multiple address records (i.e. a
*    list of 'struct addrinfo' records).  This function traverses the list and
*    tries to establish a connection to the remote server.  The function ends
*    when either a connection has been established or all records in the list
*    have been processed.
*
* Parameters:
*    host    - A pointer to a character string representing the hostname or IP
*              address (IPv4 or IPv6) of the remote server.
*    service - A pointer to a character string representing the service name or
*              well-known port number.
*    scopeId - For IPv6 sockets only.  This is the index corresponding to the
*              network interface on which to set up the connection.  This
*              parameter is ignored for IPv4 sockets or when an IPv6 "scoped
*              address" is specified in 'host' (i.e. where the colon-hex
*              network address is augmented with the scope ID).
*
* Return Value:
*    Returns the socket descriptor for the connection, or INVALID_DESC if all
*    address records have been processed and a connection could not be
*    established.
******************************************************************************/
static int openSckt( const char   *host,
                     const char   *service,
                     unsigned int  scopeId )
{
   struct addrinfo *ai;
   int              aiErr;
   struct addrinfo *aiHead;
   struct addrinfo  hints;
   sockaddr_in6_t  *pSadrIn6;
   int              sckt;
   /*
   ** Initialize the 'hints' structure for getaddrinfo(3).
   **
   ** Notice that the 'ai_family' field is set to PF_UNSPEC, indicating to
   ** return both IPv4 and IPv6 address records for the host/service.  Most of
   ** the time, the user isn't going to care whether an IPv4 connection or an
   ** IPv6 connection is established; the user simply wants to exchange data
   ** with the remote host and doesn't care how it's done.  Sometimes, however,
   ** the user might want to explicitly specify the type of underlying socket.
   ** It is left as an exercise for the motivated reader to add a command line
   ** option allowing the user to specify the IP protocol, and then process the
   ** list of addresses accordingly (it's not that difficult).
   */
   memset( &hints, 0, sizeof( hints ) );
   hints.ai_family   = PF_UNSPEC;     /* IPv4 or IPv6 records (don't care). */
   hints.ai_socktype = SOCK_STREAM;   /* Connection-oriented byte stream.   */
   hints.ai_protocol = IPPROTO_TCP;   /* TCP transport layer protocol only. */
   /*
   ** Look up the host/service information.
   */
   if ( ( aiErr = getaddrinfo( host,
                               service,
                               &hints,
                               &aiHead ) ) != 0 )
   {
      fprintf( stderr,
               "%s (line %d): ERROR - %s.\n",
               pgmName,
               __LINE__,
               gai_strerror( aiErr ) );
      return INVALID_DESC;
   }
   /*
   ** Go through the list and try to open a connection.  Continue until either
   ** a connection is established or the entire list is exhausted.
   */
   for ( ai = aiHead,   sckt = INVALID_DESC;
         ( ai != NULL ) && ( sckt == INVALID_DESC );
         ai = ai->ai_next )
   {
      /*
      ** IPv6 kluge.  Make sure the scope ID is set.
      */
      if ( ai->ai_family == PF_INET6 )
      {
         pSadrIn6 = (sockaddr_in6_t*) ai->ai_addr;
         if ( pSadrIn6->sin6_scope_id == 0 )
         {
            pSadrIn6->sin6_scope_id = scopeId;
         }  /* End IF the scope ID wasn't set. */
      }  /* End IPv6 kluge. */
      /*
      ** Display the address info for the remote host.
      */
      if ( verbose )
      {
         /*
         ** Temporary character string buffers for host & service.
         */
         char hostBfr[ NI_MAXHOST ];
         char servBfr[ NI_MAXSERV ];
         /*
         ** Display the address information just fetched.  Start with the
         ** common (protocol-independent) stuff first.
         */
         fprintf( stderr,
                  "Address info:\n"
                  "   ai_flags     = 0x%02X\n"
                  "   ai_family    = %d (PF_INET = %d, PF_INET6 = %d)\n"
                  "   ai_socktype  = %d (SOCK_STREAM = %d, SOCK_DGRAM = %d)\n"
                  "   ai_protocol  = %d (IPPROTO_TCP = %d, IPPROTO_UDP = %d)\n"
                  "   ai_addrlen   = %d (sockaddr_in = %d, "
                  "sockaddr_in6 = %d)\n",
                  ai->ai_flags,
                  ai->ai_family,
                  PF_INET,
                  PF_INET6,
                  ai->ai_socktype,
                  SOCK_STREAM,
                  SOCK_DGRAM,
                  ai->ai_protocol,
                  IPPROTO_TCP,
                  IPPROTO_UDP,
                  ai->ai_addrlen,
                  sizeof( struct sockaddr_in ),
                  sizeof( struct sockaddr_in6 ) );
         /*
         ** Display the protocol-specific formatted address.
         */
         getnameinfo( ai->ai_addr,
                      ai->ai_addrlen,
                      hostBfr,
                      sizeof( hostBfr ),
                      servBfr,
                      sizeof( servBfr ),
                      NI_NUMERICHOST | NI_NUMERICSERV );
         switch ( ai->ai_family )
         {
            case PF_INET:   /* IPv4 address record. */
            {
               sockaddr_in_t *pSadrIn = (sockaddr_in_t*) ai->ai_addr;
               fprintf( stderr,
                        "   ai_addr      = sin_family: %d (AF_INET = %d, "
                        "AF_INET6 = %d)\n"
                        "                  sin_addr:   %s\n"
                        "                  sin_port:   %s\n",
                        pSadrIn->sin_family,
                        AF_INET,
                        AF_INET6,
                        hostBfr,
                        servBfr );
               break;
            }  /* End CASE of IPv4 record. */
            case PF_INET6:   /* IPv6 address record. */
            {
               pSadrIn6 = (sockaddr_in6_t*) ai->ai_addr;
               fprintf( stderr,
                        "   ai_addr      = sin6_family:   %d (AF_INET = %d, "
                        "AF_INET6 = %d)\n"
                        "                  sin6_addr:     %s\n"
                        "                  sin6_port:     %s\n"
                        "                  sin6_flowinfo: %d\n"
                        "                  sin6_scope_id: %d\n",
                        pSadrIn6->sin6_family,
                        AF_INET,
                        AF_INET6,
                        hostBfr,
                        servBfr,
                        pSadrIn6->sin6_flowinfo,
                        pSadrIn6->sin6_scope_id );
               break;
            }  /* End CASE of IPv6 record. */
            default:   /* Can never get here, but just for completeness. */
            {
               fprintf( stderr,
                        "%s (line %d): ERROR - Unknown protocol family (%d).\n",
                        pgmName,
                        __LINE__,
                        ai->ai_family );
               break;
            }  /* End DEFAULT case (unknown protocol family). */
         }  /* End SWITCH on protocol family. */
      }  /* End IF verbose mode. */
      /*
      ** Create a socket.
      */
      if ( !SYSCALL( "socket",
                     __LINE__,
                     sckt = socket( ai->ai_family,
                                    ai->ai_socktype,
                                    ai->ai_protocol ) ) )
      {
         sckt = INVALID_DESC;
         continue;   /* Try the next address record in the list. */
      }
      /*
      ** Connect to the remote host.
      */
      if ( !SYSCALL( "connect",
                     __LINE__,
                     connect( sckt,
                              ai->ai_addr,
                              ai->ai_addrlen ) ) )
      {
         (void) close( sckt );   /* Could use SYSCALL() again here, but why? */
         sckt = INVALID_DESC;
         continue;   /* Try the next address record in the list. */
      }
   }  /* End FOR each address record returned by getaddrinfo(3). */
   /*
   ** Clean up & return.
   */
   freeaddrinfo( aiHead );
   return sckt;
}  /* End openSckt() */
/******************************************************************************
* Function: tod
*
* Description:
*    Receive the time-of-day from the remote server and write it to stdout.
*
* Parameters:
*    sckt - The socket descriptor for the connection.
*
* Return Value: None.
******************************************************************************/
static void tod( int sckt )
{
   char bfr[ MAXBFRSIZE+1 ];
   int  inBytes;
   /*
   ** The client never sends anything, so shut down the write side of the
   ** connection.
   */
   if ( !SYSCALL( "shutdown",
                  __LINE__,
                  shutdown( sckt, SHUT_WR ) ) )
   {
      return;
   }
   /*
   ** Read the time-of-day from the remote host.
   */
   do
   {
      if ( !SYSCALL( "read",
                     __LINE__,
                     inBytes = read( sckt,
                                     bfr,
                                     MAXBFRSIZE ) ) )
      {
         return;
      }
      bfr[ inBytes ] = '\0';   /* Null-terminate the received string. */
      fputs( bfr, stdout );    /* Null string if EOF (inBytes == 0).  */
   } while ( inBytes > 0 );
   fflush( stdout );
}  /* End tod() */

|23.1.4.3. 'Daytime' UDP Client Code

The UDP client code is found in file tod6uc.c (time-of-day IPv6 UDP client). It is almost an exact duplicate of the TCP client (and in fact was derived from it), but is included in this HowTo for completeness. Once built, the UDP client may be started using the following command syntax (assuming tod6uc is the executable file):

tod6uc [-v] [-s scope_id] [host [service]]

ARGUMENTS:

host

    The hostname or IP address (dotted decimal or colon-hex) of the remote host providing the service. Default is "localhost".
service

    The UDP service (or well-known port number) to which datagrams are sent. Default is "daytime".

OPTIONS:

-s

    This option is only meaningful for IPv6 addresses, and is used to set the scope identifier (i.e. the network interface on which to exchange datagrams). Default is "eth0". If host is a scoped address, this option is ignored.
-v

    Turn on verbose mode.

The UDP client source code contained in tod6uc.c follows:

/******************************************************************************
* File: tod6uc.c
* Description: Contains source code for an IPv6-capable 'daytime' UDP client.
* Author: John Wenker, Sr. Software Engineer
*         Performance Technologies, San Diego, USA
******************************************************************************/
/*
** System header files.
*/
#include <errno.h>        /* errno declaration and error codes.             */
#include <net/if.h>       /* if_nametoindex(3).                             */
#include <netdb.h>        /* getaddrinfo(3) and associated definitions.     */
#include <netinet/in.h>   /* sockaddr_in and sockaddr_in6 definitions.      */
#include <stdio.h>        /* printf(3) et al.                               */
#include <stdlib.h>       /* exit(2).                                       */
#include <string.h>       /* String manipulation and memory functions.      */
#include <sys/socket.h>   /* Socket functions (socket(2), connect(2), etc). */
#include <unistd.h>       /* getopt(3), recvfrom(2), sendto(2), etc.        */
/*
** Constants & macros.
*/
#define DFLT_HOST      "localhost"   /* Default server name.              */
#define DFLT_SCOPE_ID  "eth0"        /* Default scope identifier.         */
#define DFLT_SERVICE   "daytime"     /* Default service name.             */
#define INVALID_DESC   -1            /* Invalid file (socket) descriptor. */
#define MAXBFRSIZE     256           /* Max bfr sz to read remote TOD.    */
#define VALIDOPTS      "s:v"         /* Valid command options.            */
/*
** Type definitions (for convenience).
*/
typedef enum { false = 0, true } boolean;
typedef struct sockaddr_in       sockaddr_in_t;
typedef struct sockaddr_in6      sockaddr_in6_t;
/*
** Prototypes for internal helper functions.
*/
static int  openSckt( const char   *host,
                      const char   *service,
                      unsigned int  scopeId );
static void tod( int sckt );
/*
** Global (within this file only) data objects.
*/
static const char *pgmName;           /* Program name (w/o directory). */
static boolean     verbose = false;   /* Verbose mode.                 */
/*
** Usage macro.
*/
#define USAGE                                                            \
        {                                                                \
           fprintf( stderr,                                              \
                    "Usage: %s [-v] [-s scope_id] [host [service]]\n",   \
                    pgmName );                                           \
           exit( 127 );                                                  \
        }  /* End USAGE macro. */
/*
** This "macro" (even though it's really a function) is loosely based on the
** CHK() macro by Dr. V. Vinge (see server code).  The status parameter is
** a boolean expression indicating the return code from one of the usual system
** calls that returns -1 on error.  If a system call error occurred, an alert
** is written to stderr.  It returns a boolean value indicating success/failure
** of the system call.
**
** Example: if ( !SYSCALL( "write",
**                         count = write( fd, bfr, size ) ) )
**          {
**             // Error processing... but SYSCALL() will have already taken
**             // care of dumping an error alert to stderr.
**          }
*/
static __inline boolean SYSCALL( const char *syscallName,
                                 int         lineNbr,
                                 int         status )
{
   if ( ( status == -1 ) && verbose )
   {
      fprintf( stderr,
               "%s (line %d): System call failed ('%s') - %s.\n",
               pgmName,
               lineNbr,
               syscallName,
               strerror( errno ) );
   }
   return status != -1;   /* True if the system call was successful. */
}  /* End SYSCALL() */
/******************************************************************************
* Function: main
*
* Description:
*    Connect to a remote time-of-day service and write the remote host's TOD to
*    stdout.
*
* Parameters:
*    The usual argc & argv parameters to a main() program.
*
* Return Value:
*    This function always returns zero.
******************************************************************************/
int main( int   argc,
          char *argv[ ] )
{
   const char   *host     = DFLT_HOST;
   int           opt;
   int           sckt;
   unsigned int  scopeId  = if_nametoindex( DFLT_SCOPE_ID );
   const char   *service  = DFLT_SERVICE;
   /*
   ** Determine the program name (w/o directory prefix).
   */
   pgmName = (const char*) strrchr( argv[ 0 ], '/' );
   pgmName = pgmName == NULL  ?  argv[ 0 ]  :  pgmName+1;
   /*
   ** Process command line options.
   */
   opterr = 0;   /* Turns off "invalid option" error messages. */
   while ( ( opt = getopt( argc, argv, VALIDOPTS ) ) != -1 )
   {
      switch ( opt )
      {
         case 's':   /* Scope identifier (IPv6 kluge). */
         {
            scopeId = if_nametoindex( optarg );
            if ( scopeId == 0 )
            {
               fprintf( stderr,
                        "%s: Unknown network interface (%s).\n",
                        pgmName,
                        optarg );
               USAGE;
            }
            break;
         }
         case 'v':   /* Verbose mode. */
         {
            verbose = true;
            break;
         }
         default:
         {
            USAGE;
         }
      }  /* End SWITCH on command option. */
   } /* End WHILE processing command options. */
   /*
   ** Process command arguments.  At the end of the above loop, optind is the
   ** index of the first NON-option argv element.
   */
   switch ( argc - optind )
   {
      case 2:   /* Both host & service are specified on the command line. */
      {
          service = argv[ optind + 1 ];
          /***** Fall through *****/
      }
      case 1:   /* Host is specified on the command line. */
      {
          host = argv[ optind ];
          /***** Fall through *****/
      }
      case 0:   /* Use default host & service. */
      {
          break;
      }
      default:
      {
         USAGE;
      }
   }  /* End SWITCH on number of command arguments. */
   /*
   ** Open a connection to the indicated host/service.
   **
   ** Note that if all three of the following conditions are met, then the
   ** scope identifier remains unresolved at this point.
   **    1) The default network interface is unknown for some reason.
   **    2) The -s option was not used on the command line.
   **    3) An IPv6 "scoped address" was not specified for the hostname on the
   **       command line.
   ** If the above three conditions are met, then only an IPv4 socket can be
   ** opened (connect(2) fails without the scope ID properly set for IPv6
   ** sockets).
   */
   if ( ( sckt = openSckt( host,
                           service,
                           scopeId ) ) == INVALID_DESC )
   {
      fprintf( stderr,
               "%s: Sorry... a connectionless socket could "
               "not be set up.\n",
               pgmName );
      exit( 1 );
   }
   /*
   ** Get the remote time-of-day.
   */
   tod( sckt );
   /*
   ** Close the connection and terminate.
   */
   (void) SYSCALL( "close",
                   __LINE__,
                   close( sckt ) );
   return 0;
}  /* End main() */
/******************************************************************************
* Function: openSckt
*
* Description:
*    Sets up a UDP socket to a remote server.  Getaddrinfo(3) is used to
*    perform lookup functions and can return multiple address records (i.e. a
*    list of 'struct addrinfo' records).  This function traverses the list and
*    tries to establish a connection to the remote server.  The function ends
*    when either a connection has been established or all records in the list
*    have been processed.
*
* Parameters:
*    host    - A pointer to a character string representing the hostname or IP
*              address (IPv4 or IPv6) of the remote server.
*    service - A pointer to a character string representing the service name or
*              well-known port number.
*    scopeId - For IPv6 sockets only.  This is the index corresponding to the
*              network interface on which to exchange datagrams.  This
*              parameter is ignored for IPv4 sockets or when an IPv6 "scoped
*              address" is specified in 'host' (i.e. where the colon-hex
*              network address is augmented with the scope ID).
*
* Return Value:
*    Returns the socket descriptor for the connection, or INVALID_DESC if all
*    address records have been processed and a socket could not be initialized.
******************************************************************************/
static int openSckt( const char   *host,
                     const char   *service,
                     unsigned int  scopeId )
{
   struct addrinfo *ai;
   int              aiErr;
   struct addrinfo *aiHead;
   struct addrinfo  hints;
   sockaddr_in6_t  *pSadrIn6;
   int              sckt;
   /*
   ** Initialize the 'hints' structure for getaddrinfo(3).
   **
   ** Notice that the 'ai_family' field is set to PF_UNSPEC, indicating to
   ** return both IPv4 and IPv6 address records for the host/service.  Most of
   ** the time, the user isn't going to care whether an IPv4 connection or an
   ** IPv6 connection is established; the user simply wants to exchange data
   ** with the remote host and doesn't care how it's done.  Sometimes, however,
   ** the user might want to explicitly specify the type of underlying socket.
   ** It is left as an exercise for the motivated reader to add a command line
   ** option allowing the user to specify the IP protocol, and then process the
   ** list of addresses accordingly (it's not that difficult).
   */
   memset( &hints, 0, sizeof( hints ) );
   hints.ai_family   = PF_UNSPEC;     /* IPv4 or IPv6 records (don't care). */
   hints.ai_socktype = SOCK_DGRAM;    /* Connectionless communication.      */
   hints.ai_protocol = IPPROTO_UDP;   /* UDP transport layer protocol only. */
   /*
   ** Look up the host/service information.
   */
   if ( ( aiErr = getaddrinfo( host,
                               service,
                               &hints,
                               &aiHead ) ) != 0 )
   {
      fprintf( stderr,
               "%s (line %d): ERROR - %s.\n",
               pgmName,
               __LINE__,
               gai_strerror( aiErr ) );
      return INVALID_DESC;
   }
   /*
   ** Go through the list and try to open a connection.  Continue until either
   ** a connection is established or the entire list is exhausted.
   */
   for ( ai = aiHead,   sckt = INVALID_DESC;
         ( ai != NULL ) && ( sckt == INVALID_DESC );
         ai = ai->ai_next )
   {
      /*
      ** IPv6 kluge.  Make sure the scope ID is set.
      */
      if ( ai->ai_family == PF_INET6 )
      {
         pSadrIn6 = (sockaddr_in6_t*) ai->ai_addr;
         if ( pSadrIn6->sin6_scope_id == 0 )
         {
            pSadrIn6->sin6_scope_id = scopeId;
         }  /* End IF the scope ID wasn't set. */
      }  /* End IPv6 kluge. */
      /*
      ** Display the address info for the remote host.
      */
      if ( verbose )
      {
         /*
         ** Temporary character string buffers for host & service.
         */
         char hostBfr[ NI_MAXHOST ];
         char servBfr[ NI_MAXSERV ];
         /*
         ** Display the address information just fetched.  Start with the
         ** common (protocol-independent) stuff first.
         */
         fprintf( stderr,
                  "Address info:\n"
                  "   ai_flags     = 0x%02X\n"
                  "   ai_family    = %d (PF_INET = %d, PF_INET6 = %d)\n"
                  "   ai_socktype  = %d (SOCK_STREAM = %d, SOCK_DGRAM = %d)\n"
                  "   ai_protocol  = %d (IPPROTO_TCP = %d, IPPROTO_UDP = %d)\n"
                  "   ai_addrlen   = %d (sockaddr_in = %d, "
                  "sockaddr_in6 = %d)\n",
                  ai->ai_flags,
                  ai->ai_family,
                  PF_INET,
                  PF_INET6,
                  ai->ai_socktype,
                  SOCK_STREAM,
                  SOCK_DGRAM,
                  ai->ai_protocol,
                  IPPROTO_TCP,
                  IPPROTO_UDP,
                  ai->ai_addrlen,
                  sizeof( struct sockaddr_in ),
                  sizeof( struct sockaddr_in6 ) );
         /*
         ** Display the protocol-specific formatted address.
         */
         getnameinfo( ai->ai_addr,
                      ai->ai_addrlen,
                      hostBfr,
                      sizeof( hostBfr ),
                      servBfr,
                      sizeof( servBfr ),
                      NI_NUMERICHOST | NI_NUMERICSERV );
         switch ( ai->ai_family )
         {
            case PF_INET:   /* IPv4 address record. */
            {
               sockaddr_in_t *pSadrIn = (sockaddr_in_t*) ai->ai_addr;
               fprintf( stderr,
                        "   ai_addr      = sin_family: %d (AF_INET = %d, "
                        "AF_INET6 = %d)\n"
                        "                  sin_addr:   %s\n"
                        "                  sin_port:   %s\n",
                        pSadrIn->sin_family,
                        AF_INET,
                        AF_INET6,
                        hostBfr,
                        servBfr );
               break;
            }  /* End CASE of IPv4 record. */
            case PF_INET6:   /* IPv6 address record. */
            {
               pSadrIn6 = (sockaddr_in6_t*) ai->ai_addr;
               fprintf( stderr,
                        "   ai_addr      = sin6_family:   %d (AF_INET = %d, "
                        "AF_INET6 = %d)\n"
                        "                  sin6_addr:     %s\n"
                        "                  sin6_port:     %s\n"
                        "                  sin6_flowinfo: %d\n"
                        "                  sin6_scope_id: %d\n",
                        pSadrIn6->sin6_family,
                        AF_INET,
                        AF_INET6,
                        hostBfr,
                        servBfr,
                        pSadrIn6->sin6_flowinfo,
                        pSadrIn6->sin6_scope_id );
               break;
            }  /* End CASE of IPv6 record. */
            default:   /* Can never get here, but just for completeness. */
            {
               fprintf( stderr,
                        "%s (line %d): ERROR - Unknown protocol family (%d).\n",
                        pgmName,
                        __LINE__,
                        ai->ai_family );
               break;
            }  /* End DEFAULT case (unknown protocol family). */
         }  /* End SWITCH on protocol family. */
      }  /* End IF verbose mode. */
      /*
      ** Create a socket.
      */
      if ( !SYSCALL( "socket",
                     __LINE__,
                     sckt = socket( ai->ai_family,
                                    ai->ai_socktype,
                                    ai->ai_protocol ) ) )
      {
         sckt = INVALID_DESC;
         continue;   /* Try the next address record in the list. */
      }
      /*
      ** Set the target destination for the remote host on this socket.  That
      ** is, this socket only communicates with the specified host.
      */
      if ( !SYSCALL( "connect",
                     __LINE__,
                     connect( sckt,
                              ai->ai_addr,
                              ai->ai_addrlen ) ) )
      {
         (void) close( sckt );   /* Could use SYSCALL() again here, but why? */
         sckt = INVALID_DESC;
         continue;   /* Try the next address record in the list. */
      }
   }  /* End FOR each address record returned by getaddrinfo(3). */
   /*
   ** Clean up & return.
   */
   freeaddrinfo( aiHead );
   return sckt;
}  /* End openSckt() */
/******************************************************************************
* Function: tod
*
* Description:
*    Receive the time-of-day from the remote server and write it to stdout.
*
* Parameters:
*    sckt - The socket descriptor for the connection.
*
* Return Value: None.
******************************************************************************/
static void tod( int sckt )
{
   char bfr[ MAXBFRSIZE+1 ];
   int  inBytes;
   /*
   ** Send a datagram to the server to wake it up.  The content isn't
   ** important, but something must be sent to let it know we want the TOD.
   */
   if ( !SYSCALL( "write",
                  __LINE__,
                  write( sckt, "Are you there?", 14 ) ) )
   {
      return;
   }
   /*
   ** Read the time-of-day from the remote host.
   */
   if ( !SYSCALL( "read",
                  __LINE__,
                  inBytes = read( sckt,
                                  bfr,
                                  MAXBFRSIZE ) ) )
   {
      return;
   }
   bfr[ inBytes ] = '\0';   /* Null-terminate the received string. */
   fputs( bfr, stdout );    /* Null string if EOF (inBytes == 0).  */
   fflush( stdout );
}  /* End tod() */

|23.2. Other programming languages
|23.2.1. JAVA

Sun Java versions since 1.4 are IPv6 enabled, see e.g. Inet6Address (1.5/5.0) class. Hints are available in the Networking IPv6 User Guide for JDK/JRE 1.4 and 1.5 (5.0).
|23.2.2. Perl

As of May 2007 it's not known that the Perl core itself already supports IPv6. It can be added by using following modules:

    Socket6

Anyway, some other modules exist for/with IPv6 support (e.g. Net::IP), search for “IPv6” on http://search.cpan.org/.
Chapter 24. Interoperability

The TAHI Project checks the interoperability of different operating systems regarding the implementation of IPv6 features. Linux kernel already got the IPv6 Ready Logo Phase 1.
Chapter 25. Further information and URLs
|25.1. Paper printed books, articles, online reviews (mixed)
|25.1.1. Printed Books (English)
|25.1.1.1. Cisco

    Cisco Self-Study: Implementing IPv6 Networks (IPV6) by Regis Desmeules. Cisco Press; ISBN 1587050862; 500 pages; 1st edition (April 11, 2003). Note: This item will be published on April 11, 2003.

    Configuring IPv6 with Cisco IOS by Sam Brown, Sam Browne, Neal Chen, Robbie Harrell, Edgar, Jr. Parenti (Editor), Eric Knipp (Editor), Paul Fong (Editor)362 pages; Syngress Media Inc; ISBN 1928994849; (July 12, 2002).

|25.1.1.2. General

    IPv6 in Practice: A Unixer's Guide to the Next Generation Internet von Benedikt Stockebrand, November 2006; ISBN 3-540-24524-3

    IPv6 Essentials by Silvia Hagen, 2nd Edition, May 2006; ISBN 0-5961-0058-2 ToC, Index, Sample Chapter etc.; O'Reilly Pressrelease

    IPv6: The New Internet Protocol. By Christian Huitema; Published by Prentice-Hall; ISBN 0138505055. Description: This book, written by Christian Huitema - a member of the InternetArchitecture Board, gives an excellent description of IPv6, how it differs from IPv4, and the hows and whys of it's development. Source: http://www.cs.uu.nl/wais/html/na-dir/internet/tcp-ip/resource-list.html

    IPv6 Networks by Niles, Kitty; (ISBN 0070248079); 550 pages; Date Published 05/01/1998.

    Implementing IPV6. Supporting the Next Generation Internet Protocols by P. E. Miller, Mark A. Miller; Publisher: John Wiley & Sons; ISBN 0764545892; 2nd edition (March 15, 2000); 402 pages.

    Big Book of Ipv6 Addressing Rfcs by Peter H. Salus (Compiler), Morgan Kaufmann Publishers, April 2000, 450 pages ISBN 0126167702.

    Understanding IPV6 by Davies, Joseph; ISBN 0735612455; Date Published 05/01/2001; Number of Pages: 350.

    Migrating to IPv6 - IPv6 in Practice by Marc Blanchet Publisher: John Wiley & Sons; ISBN 0471498920; 1st edition (November 2002); 368 pages.

    Ipv6 Network Programming by Jun-ichiro Hagino; ISBN 1555583180

    Wireless boosting IPv6 by Carolyn Duffy Marsan, 10/23/2000.

    O'reilly Network search for keyword IPv6 results in 29 hits (28. January 2002)

|25.1.2. Articles, eBooks, Online Reviews (mixed)

    Getting Connected with 6to4 by Huber Feyrer, 06/01/2001

    Transient Addressing for Related Processes: Improved Firewalling by Using IPv6 and Multiple Addresses per Host; written by Peter M. Gleiz, Steven M. Bellovin (PC-PDF-Version; Palm-PDF-Version; PDB-Version)

    Internetworking IPv6 with Cisco Routers by Silvano Gai, McGrawHill Italia, 1997. The 13 chapters and appendix A-D are downloadable as PDF-documents.

    Migration and Co-existence of IPv4 and IPv6 in Residential Networks by Pekka Savola, CSC/FUNET, 2002

|25.1.3. Science Publications (abstracts, bibliographies, online resources)

See also: liinwww.ira.uka.de/ipv6 or Google / Scholar / IPv6

    GEANT IPv6 Workplan

    IPv6 Trials on UK Academic Networks: Bermuda Project Aug.2002: Participants - Getting connected - Project deliverables - Network topology - Address assignments - Wireless IPv6 access - IPv6 migration - Project presentations - Internet 2 - Other IPv6 projects - IPv6 fora and standards Bermuda 2...

    http://www.ipv6.ac.uk/

    IPv6 at the University of Southampton

    Microsoft Research IPv6 Implementation (MSRIPv6): MSRIPv6 Configuring 6to4 - Connectivity with MSR IPv6 - Our 6Bone Node... 

|25.1.4. Others

See following URL for more: SWITCH IPv6 Pilot / References
|25.2. Conferences, Meetings, Summits

Something missing? Suggestions are welcome!
|25.2.1. 2004

|    1st Global IPv6 Summit in Sao Paul, Brazil

|25.3. Online information
|25.3.1. Join the IPv6 backbone

More to be filled later...suggestions are welcome!
|25.3.1.1. Global registries

See regional registries.
|25.3.1.2. Major regional registries

    America: ARIN, ARIN / registration page, ARIN / IPv6 guidelines

    EMEA: Ripe NCC, Ripe NCC / registration page, Ripe NCC / IPv6 registration

    Asia/Pacific: APNIC, APNIC / IPv6 ressource guide

    Latin America and Caribbea: LACNIC, IPv6 Registration Services, IPv6 Allocation Policy

    Africa: AfriNIC

Also a list of major (prefix length 32) allocations per local registry is available here: Ripe NCC / IPv6 allocations.
|25.3.1.3. Tunnel brokers

Note: A list of available Tunnel broker can be found in the section Tunnel broker below.

    Former IPng. Tunnelbroker and IPv6 resources, now migrated to the SixXs System.

    Eckes' IPv6-with-Linux Page.

    tunnelc - a perl based tunnel client script: freshmeat.net: Project details for tunnel client SourceForge: Project Info - tunnelc (also here)

    Linux Advanced Routing & Traffic Control HOWTO, Chapter 6: IPv6 tunneling with Cisco and/or 6bone.

|25.3.1.4. 6to4

    NSayer's 6to4 information

    RFC 3068 / An Anycast Prefix for 6to4 Relay Routers

|25.3.1.5. ISATAP

    ISATAP (Intra-Site Automatic Tunnel Access Protocol) Information by JOIN

|25.3.2. Latest news and URLs to other documents

    Lot of URLs to others documents by Anil Edathara

    go6 - The IPv6 Portal: an IPv6 online portal with a wiki-based IPv6 knowledge center, an IPv6 discussion forum, an up-to-date collection of IPv6 Events and News, free IPv6 access and services, IPv6 software applications, and much more

|25.3.3. Protocol references
|25.3.3.1. IPv6-related Request For Comments (RFCs)

Publishing the list of IPv6-related RFCs is beyond the scope of this document, but given URLs will lead you to such lists:

    List sorted by IPng Standardization Status or IPng Current Specifications by Robert Hinden

    IPv6 Related Specifications on IPv6.org

|25.3.3.2. Current drafts of working groups

Current (also) IPv6-related drafts can be found here:

    IP Version 6 (ipv6)

    Next Generation Transition (ngtrans)

    Dynamic Host Configuration (dhc)

    Domain Name System Extension (dnsext)

    IPv6 Operations (v6ops)

    Mobile IP (mobileip)

    Get any information about IPv6, from overviews, through RFCs & drafts, to implementations (including availability of stacks on various platforms & source code for IPv6 stacks) 

|25.3.3.3. Others

    SWITCH IPv6 Pilot / References, big list of IPv6 references maintained by Simon Leinen

|25.3.4. More information

DeepSpace6 / more interesting links
|25.3.4.1. Linux related

    DeepSpace6 / (Not only) Linux IPv6 Portal - Italy (Mirror)

    IPv6-HowTo for Linux by Peter Bieringer - Germany, and his Bieringer / IPv6 - software archive

    Linux+IPv6 status by Peter Bieringer - Germany (going obsolete)

    DeepSpace6 / IPv6 Status Page - Italy (Mirror) (will superseed upper one)

    USAGI project - Japan, and their USAGI project - software archive

    Linux Optimized Link State Routing Protocol (OLSR) IPv6 HOWTO

    LinShim6

|25.3.4.2. Linux related per distribution

PLD

    PLD Linux Distribution (“market leader” in containing IPv6 enabled packages)
Red Hat

    Red Hat Enterprise Linux, Pekka Savola's IPv6 packages
Fedora

    Fedora Core Linux
Debian

    Debian Linux, IPv6 with Debian Linux
Novell/SuSE

    Novell/SuSE Linux
Mandriva

    Mandriva

For more see the IPv6+Linux Status Distributions page.
|25.3.4.3. General

    IPv6.org

|    6bone

    WIDE project - Japan

    SWITCH IPv6 Pilot - Switzerland

    IPv6 Corner of Hubert Feyrer - Germany

    IPv6 Forum - a world-wide consortium of leading Internet vendors, Research & Education Networks...

    Playground.sun.com / IPv6 Info Page - maintained by Robert Hinden, Nokia. Get any information about IPv6, from overviews, through RFCs & drafts, to implementations (including availability of stacks on various platforms & source code for IPv6 stacks).

|    6INIT - IPv6 Internet Initiative - an EU Fifth Framework Project under the IST Programme.

    IPv6 Task Force (European Union)

|    6init - IPv6 INternet IniTiative

    IPv6: The New Version of the Internet Protocol, by Steve Deering.

    IPv6: The Next Generation Internet Protocol, by Gary C. Kessler.

    IPv6: Next Generation Internet Protocol - 3Com

    internet || site and internet2 Working Group

    NetworkWorldFusion: Search / Doc Finder: searched for IPv6 (102 documents found 22.12.2002)

    The Register (Search for IPv6 will result in 30 documents, 22.12.2002)

    ZDNet Search for IPv6

    TechTarget Search for IPv6

    IPv6 & TCP Resources List

Something missing? Suggestions are welcome!
|25.3.4.4. Market Research

    A Tale of Two Wireless Technology Trends: Processor Development Outsourcing and IPv6Yankee Group - 4/1/2002 - 12 Pages - ID: YANL768881

    The World Atlas of the Internet: Americas; IDATE - 2/1/2002 - 242 PAges - ID: IDT803907. Countries covered: Central America, North America, South America; List: Price: $ 3,500.00; excerpt: Panorama of Internet access markets across the globe. Market assessment and forecasts up to 2006 for 34 countries: market structure: main ISPs and market shares; number of subscribers, of ISPs.

    Early Interest Rising for IPv6 by IDC (Author); List Price: $1,500.00; Edition: e-book (Acrobat Reader); Publisher: IDC; ISBN B000065T8E; (March 1, 2002) 

|25.3.4.5. Patents

    Delphion Research: Patent Search Page. Basic (free) registration needed. Examples found 21.12.2002 searching for IPv6: Communicating method between IPv4 terminal and IPv6 terminal and IPv4-IPv6 converting apparatus Translator for IP networks, network system using the translator, and IP network coupling method therefor

|25.3.5. By countries
|25.3.5.1. Europe

    www.ist-ipv6.org: IST IPv6 Cluster, European IPv6 Research and Development Projects

    Euro6IX: European IPv6 Internet Exchanges Backbone

|25.3.5.2. Austria

    IPv6@IKNnet and MIPv6 Research Group: TU Vienna, Austria (IPv6: project, publications, diploma / doctor thesis, Conference Proceedings etc.)

|25.3.5.3. Australia

    Carl's Australian IPv6 Pages (old content)

|25.3.5.4. Belgium

Suggestions are welcome!
|25.3.5.5. Brasil

    IPv6 do Brasil

|25.3.5.6. China

Suggestions are welcome!
|25.3.5.7. Czech

Suggestions are welcome!
|25.3.5.8. Germany

    Xing / IPv6

|25.3.5.9. France

    Renater: Renater IPv6 Project Page

    IPv6 - RSVP - ATM at INRIA

    NetBSD IPv6 Documentation

|25.3.5.10. Italy

    Project6: IPv6 networking with Linux

|25.3.5.11. Japan

    Yamaha IPv6 (sorry, all in japanese native ...)

|25.3.5.12. Korea

    ETRI: Electronics and Telecommunications Research Institut

    IPv6 Forum Korea: Korean IPv6 Deployment Project

|25.3.5.13. Mexico

    IPv6 Mexico (spain & english version): IPv6 Project Hompeage of The National Autonomous University of Mexico (UNAM)

|25.3.5.14. Netherland

    SURFnet: SURFnet IPv6 Backbone

    STACK, STACK (IPv6): Students' computer association of the Eindhoven University of Technology, Netherland

    IPng.nl: collaboration between WiseGuys and Intouch

|25.3.5.15. Portugal

Suggestions are welcome!
|25.3.5.16. Russia

    IPv6 Forum for Russia: Yaroslavl State University Internet Center

|25.3.5.17. Switzerland

Suggestions are welcome!
|25.3.5.18. United Kingdom

    British Telecom IPv6 Home: BT's ISP IPv6 Trial, UK's first IPv6 Internet Exchange etc.

|25.3.6. By operating systems
|25.3.6.1. *BSD

    KAME project (*BSD)

    NetBSD's IPv6 Networking FAQ

    FreeBSD Ports: Ipv6

|25.3.6.2. Cisco IOS

    Cisco IOS IPv6 Entry Page

    IPv6 for Cisco IOS Software, File 2 of 3: Aug 2002 -- Table of Contents: IPv6 for Cisco IOS Software; Configuring Documentation Specifics; Enabling IPv6 Routing and Configuring; IPv6 Addressing; Enabling IPv6 Processing Globally.

    Cisco Internet Networking Handbook, Chapter IPv6

|25.3.6.3. HPUX

    comp.sys.hp.hpux FAQ

|25.3.6.4. IBM

    Now that IBM's announced the availability of z/OS V1.4, what's new in this release? This question was posed on 15 August 2002

|25.3.6.5. Microsoft

    Microsoft Windows 2000 IPv6

    MSRIPv6 - Microsoft Research Network - IPv6 Homepage

    Internet Connection Firewall Does Not Block Internet Protocol Version 6 Traffic (6.11.2001)

    Internet Protocol Numbers (8.10.2002)

    IPv6 Technology Preview Refresh (16.10.2002)

    HOW TO: Install and Configure IP Version 6 in Windows .NET Enterprise Server (26.10.2002)

    Windows .NET Server 6to4 Router Service Quits When You Advertise a 2002 Address on the Public Interface (28.10.2002)

    msdn - Microsoft Windows CE .NET - IPv6 commands

|25.3.6.6. Solaris

    Sun Microsystems Solaris

    Solaris 2 Frequently Asked Questions (FAQ) 1.73

|25.3.6.7. Sumitoma

    Sumitomo Electric has implemented IPv6 on Suminet 3700 family routers

|25.3.6.8. ZebOS

    IpInfusion's ZebOS Server Routing Software

|25.3.7. IPv6 Security

    Internet Security Systems: Security Center, X-Force Database Search (21.12.2002 - 6 topics found relating to IPv6)

    NIST IPsec Project ( National Institute of Standards and Technology, NIST)

    Information Security

    NewOrder.box.sk (search for IPv6) (Articles, exploits, files database etc.) 

|25.3.8. Application lists

    DeepSpace6 / IPv6 Status Page (Mirror)

    IPv6.org / IPv6 enabled applications

    Freshmeat / IPv6 search, currently (14 Dec 2002) 62 projects

    IPv6 Forum / Web Links

|25.3.8.1. Analyzer tools

    Wireshark (former known as Ethereal) is a free network protocol analyzer for Unix and Windows

    Radcom RC100-WL - Download Radcom RC100-WL protocol analyzer version 3.20

|25.3.8.2. IPv6 Products

|    6wind - solutions for IPv4/IPv6 Router, QoS, Multicast, Mobility, Security/VPN/Firewall.

    Fefe's patches for IPv6 with djbdnsAug 2002 -- What is djbdns and why does it need IPv6? djbdns is a full blown DNS server which outperforms BIND in nearly all respects.

    ZebOS Server Routing Suite

    SPA Mail Server 2.21

    Inframail (Advantage Server Edition) 6.0

    HTTrack Website Copier

    CommView 5.0

    Posadis 0.50.6

|25.3.8.3. SNMP

    comp.protocpols.snmp SNMP FAQ Part 1 of 2

|25.4. IPv6 Infrastructure
|25.4.1. Statistics

    IPv6 routing table history created by Gert Döring, Space.Net

    Official 6bone Webserver list Statisic

|25.4.2. Internet Exchanges

Another list of IPv6 Internet Exchanges can be found here: IPv6 status of IXPs in Europe
|25.4.2.1. Estonia

    TIX (tallinn interneti exchange with ipv6 support)

|25.4.2.2. Europe

    Euro6IX, European IPv6 Internet Exchange Backbone

|25.4.2.3. France

    French National Internet Exchange IPv6 (since 1.11.2002 active). FNIX6 provides a free and reliable high speed FastEthernet interconnection between ISP located in TeleCity Paris.

|25.4.2.4. Germany

    INXS: (Cable & Wireless) Munich and Hamburg

|25.4.2.5. Japan

    NSPIXP-6: IPv6-based Internet Exchange in Tokyo

    JPIX, Tokyo

|25.4.2.6. Korea

|    6NGIX

|25.4.2.7. Netherlands

    AMS-IX: Amsterdam Internet Exchange

|25.4.2.8. UK

    UK6X: London

    XchangePoint: London

|25.4.2.9. USA

|    6TAP: Chicago. Supports peerings around the globe.

    PAIX: Palo Alto

|25.4.3. Tunnel broker

See also: http://www.deepspace6.net/docs/tunnelbrokers.html
|25.4.3.1. Belgium

Something missing? Suggestions are welcome!
|25.4.3.2. Canada

    Freenet6 - /48 Delegation, Canada Getting IPv6 Using Freenet6 on Debian Freenet6 creater

|25.4.3.3. China

Something missing? Suggestions are welcome!
|25.4.3.4. Estonia

    Estpak

|25.4.3.5. Germany

|    6bone Knoten Leipzig Info bez. Hackangriff (2001)

|25.4.3.6. Italy

    Comv6

    Bersafe (Italian language)

|25.4.3.7. Japan

Something missing? Suggestions are welcome!
|25.4.3.8. Malaysia

Something missing? Suggestions are welcome!
|25.4.3.9. Netherlands

    IPng Netherland - Intouch, SurfNet, AMS-IX, UUNet, Cistron, RIPE NCC and AT&T are connected at the AMS-IX. It is possible (there are requirements...) to get an static tunnel.

    SURFnet Customers

|25.4.3.10. Norway

    UNINETT - Pilot IPv6 Service (for Customers): tunnelbroker & address allocation Uninett-Autoupdate-HOWTO

|25.4.3.11. Spain

    Consulintel

|25.4.3.12. Switzerland

Something missing? Suggestions are welcome!
|25.4.3.13. UK

    NTT, United Kingdom - IPv6 Trial. IPv4 Tunnel and native IPv6 leased Line connections. POPs are located in London, UK Dusseldorf, Germany New Jersey, USA (East Coast) Cupertino, USA (West Coast) Tokyo, Japan

|25.4.3.14. USA

    ESnet, USA - Energy Sciences Network: Tunnel Registry & Address Delegation for directly connected ESnet sites and ESnet collaborators.

    Hurricane Electric, US backbone; Hurrican Electric Tunnelbroker (also available under http://tunnelbroker.com/) Press Release: Hurricane Electric Upgrades IPv6 Tunnel Broker Tunnel Broker Endpoint Autoupdate, Perl Script

|25.4.3.15. Singapore

Something missing? Suggestions are welcome!
|25.4.3.16. More Tunnel brokers...

    Public 6to4 relay routers (MS IIE boycott!)

|25.4.4. Native IPv6 Services

Note: These services are mostly only available with a valid IPv6 connection!
|25.4.4.1. Net News (NNTP)

Something missing? Suggestions are welcome!
|25.4.4.2. Game Server

    Quake2 over IPv6

|25.4.4.3. IRC Server

Something missing? Suggestions are welcome!
|25.4.4.4. Radio Stations, Music Streams

Something missing? Suggestions are welcome!
|25.4.4.5. Webserver

    Peter Bieringer's Home of Linux IPv6 HOWTO 

Something missing? Suggestions are welcome!
|25.5. Maillists

Lists of maillists are available at:

    DeepSpace6 / Mailling Lists

Major Mailinglists are listed in following table:

Focus	Request e-mail address	What to subscribe	Maillist e-mail address	Language	Access through WWW
Linux kernel networking including IPv6	majordomo (at) vger.kernel.org	netdev	netdev (at) vger.kernel.org	English	Info, Archive
Mobile IP(v6) for Linux	Web-based, see URL	mipl	mipl (at) mobile-ipv6.org	English	Info, Archive
Linux IPv6 users using USAGI extension	usagi-users-ctl (at) linux-ipv6.org	 	usagi-users (at) linux-ipv6.org	English	Info / Search, Archive
IPv6 on Debian Linux	 	 	debian-ipv6 (at) lists.debian.org	English	Info/Subscription/Archive
|6bone	majordomo (at) isi.edu	6bone	6bone (at) isi.edu	English	Info, Archive
IPv6 users in general	majordomo (at) ipv6.org	users	users (at) ipv6.org	English	Info, Archive
Bugtracking of Internet applications (1)	bugtraq-subscribe (at) securityfocus.com	 	bugtraq (at) securityfocus.com (2)	English	Info, Archive

(1) very recommended if you provide server applications.

(2) list is moderated.

Something missing? Suggestions are welcome!

Following other maillinglists & newsgroups are available via web:

    student-ipv6 (India) Description: This is the group for the Student Awareness group of IPv6 in India

    sun-ipv6-users Description: Please report problems/suggestions regarding SUN Microsystems IPng implementation

    IPv6-BITS Description: This List will co-ordinate the working of Project Vertebrae.

    linux-bangalore-ipv6 Description: The IPv6 deployment list of the Bangalore Linux User Group

    packet-switching Description: This mailing list provides a forum for discussion of packet switching theory, technology, implementation and application in any relevant aspect including without limitation LAPB, X.25, SDLC, P802.1d, LLC, IP, IPv6, IPX, DECNET, APPLETALK, FR, PPP, IP Telephony, LAN PBX systems, management protocols like SNMP, e-mail, network transparent window systems, protocol implementation, protocol verification, conformance testing and tools used in maintaining or developing packet switching systems.

    de.comm.protocols.tcp-ip Description: Umstellung auf IPv6 Source: Chartas der Newsgruppen in de.*

    Google Group: comp.protocols.tcp-ip

    Google Group: linux.debian.maint.ipv6

    Google Group: microsoft.public.platformsdk.networking.ipv6

    Google Group: fa.openbsd.ipv6

|25.6. Online tools
|25.6.1. Testing tools

    ping, traceroute, tracepath, 6bone registry, DNS: JOIN / Testtools (German language only, but should be no problem for non German speakers)

    traceroute6, whois: IPng.nl

    AAAA Lookup Checker http://www.cnri.dit.ie/cgi-bin/check_aaaa.pl

|25.6.2. Information retrievement

    List of worldwide all IPv6-aggregated IP-Blocks

|25.6.3. IPv6 Looking Glasses

    DRENv6 Looking Glass

|25.6.4. Helper applications

    IPv6 Prefix Calculator by TDOI

    DNS record checker

|25.7. Trainings, Seminars

    CIW Internetworking Professional Training CBT CD

    Training Pages, U.K. - Search for IPv6 (13 Courses, 2006-08-21)

    Erion IPv6 Training, UK

Something missing? Suggestions are welcome!
|25.8. 'The Online Discovery' ...

IPv6: Addressing The Needs Of the Future by Yankee Group (Author) List Price: $595.00 Edition: e-book (Acrobat Reader) Pages: 3 (three) Publisher: MarketResearch.com; ISBN B00006334Y; (November 1, 2001)

;-) The number of copies would be interesting...
Chapter 26. Revision history / Credits / The End
|26.1. Revision history

Versions x.y are published on the Internet.

Versions x.y.z are work-in-progress and published as LyX and SGML file on CVS. Because Deep Space 6 mirrors these SGML files and generate independend from TLDP public versions, this versions will show up there and also on its mirrors.
|26.1.1. Releases 0.x

|0.65

|    2009-12-13/PB: minor fixes
|0.64

|    2009-06-11/PB: extend DHCP server examples (ISC DHCP, Dibbler)
|0.63

|    2009-02-14/PB: Fix FSF address, major update on 4in6 tunnels, add new section for address resolving, add some URLs, remove broken URLs
|0.62

|    2008-11-09/PB: Adjust URL to Turkish howto, add some HIP related URLs, remove broken URLs
|0.61.1

|    2007-11-11/PB: fix broken description of shortcut BIND
|0.61

|    2007-10-06/PB: fix broken URLs to TLDP-CVS, minor URL update.
|0.60.2

|    2007-10-03/PB: fix description of sysctl/autoconf (credits to Francois-Xavier Le Bail)
|0.60.1

|    2007-06-16/PB: speling fixes (credits to Larry W. Burton)
|0.60

|    2007-05-29/PB: import major contribution to Programming using C-API written by John Wenker, minor fixes
|0.52

|    2007-05-23/PB: update firewalling chapter, improve document for proper SGML validation, minor bugfixes
|0.51

|    2006-11-08/PB: remove broken URLs, add a new book (credits to Bryan Vukich)
|0.50.2

|    2006-10-25/PB: fix typo in dhcp6 section (credits to Michele Ferritto)
|0.50.1

|    2006-09-23/PB: add some URLs
|0.50

|    2006-08-24/PB: check RFC URLs, fix URL to Chinese translation, finalize for publishing
|0.49.5

|    2006-08-23/PB: fix/remove broken URLs
|0.49.4

|    2006-08-21/PB: some review, update and enhancement of the content, replace old 6bone example addresses with the current defined ones.
|0.49.3

|    2006-08-20/PB: fix bug in maillist entries, 'mobility' is now a separate chapter
|0.49.2

|    2006-08-20/PB: update and cleanup of maillist entries
|0.49.1

|    2006-06-13/PB: major update of mobility section (contributed by Benjamin Thery)
|0.49

|    2005-10-03/PB: add configuration hints for DHCPv6, major broken URL cleanup (credits to Necdet Yucel)
|0.48.1

|    2005-01-15/PB: minor fixes
|0.48

|    2005-01-11/PB: grammar check and minor review of IPv6 IPsec section
|0.47.1

|    2005-01-01/PB: add information and examples about IPv6 IPsec, add some URLs
|0.47

|    2004-08-30/PB: add some notes about proftpd, vsftpd and other daemons, add some URLs, minor fixes, update status of Spanish translation
|0.46.4

|    2004-07-19/PB: minor fixes
|0.46.3

|    2004-06-23/PB: add note about started Greek translation, replace Taiwanese with Chinese for related translation
|0.46.2

|    2004-05-22/PB: minor fixes
|0.46.1

|    2004-04-18/PB: minor fixes
|0.46

|    2004-03-04/PB: announce Italian translation, add information about DHCPv6, minor updates
|0.45.1

|    2004-01-12/PB: add note about the official example address space
|0.45

|    2004-01-11/PB: minor fixes, add/fix some URLs, some extensions
|0.44.2

|    2003-10-30/PB: fix some copy&paste text bugs
|0.44.1

|    2003-10-19/PB: add note about start of Italian translation
|0.44

|    2003-08-15/PB: fix URLs, add hint on tcp_wrappers (about broken notation in some versions) and Apache2
|0.43.4

|    2003-07-26/PB: fix URL, add archive URL for maillist users at ipv6.org, add some ds6 URLs
|0.43.3

|    2003-06-19/PB: fix typos
|0.43.2

|    2003-06-11/PB: fix URL
|0.43.1

|    2003-06-07/PB: fix some URLs, fix credits, add some notes at IPsec
|0.43

|    2003-06-05/PB: add some notes about configuration in SuSE Linux, add URL of French translation
|0.42

|    2003-05-09/PB: minor fixes, announce French translation
|0.41.4

|    2003-05-02/PB: Remove a broken URL, update some others.
|0.41.3

|    2003-04-23/PB: Minor fixes, remove a broken URL, fix URL to Taiwanese translation
|0.41.2

|    2003-04-13/PB: Fix some typos, add a note about a French translation is in progress
|0.41.1

|    2003-03-31/PB: Remove a broken URL, fix another
|0.41

|    2003-03-22/PB: Add URL of German translation
|0.40.2

|    2003-02-27/PB: Fix a misaddressed URL
|0.40.1

|    2003-02-12/PB: Add Debian-Linux-Configuration, add a minor note on translations
|0.40

|    2003-02-10/PB: Announcing available German version
|0.39.2

|    2003-02-10/GK: Minor syntax and spelling fixes
|0.39.1

|    2003-01-09/PB: fix an URL (draft adopted to an RFC)
|0.39

|    2003-01-13/PB: fix a bug (forgotten 'link” on “ip link set” (credits to Yaniv Kaul)
|0.38.1

|    2003-01-09/PB: a minor fix
|0.38

|    2003-01-06/PB: minor fixes
|0.37.1

|    2003-01-05/PB: minor updates
|0.37

|    2002-12-31/GK: 270 new links added (searched in 1232 SearchEngines) in existing and 53 new (sub)sections
|0.36.1

|    2002-12-20/PB: Minor fixes
|0.36

|    2002-12-16/PB: Check of and fix broken links (credits to Georg Käfer), some spelling fixes
|0.35

|    2002-12-11/PB: Some fixes and extensions
|0.34.1

|    2002-11-25/PB: Some fixes (e.g. broken linuxdoc URLs)
|0.34

|    2002-11-19/PB: Add information about German translation (work in progress), some fixes, create a small shortcut explanation list, extend “used terms” and add two German books
|0.33

|    2002-11-18/PB: Fix broken RFC-URLs, add parameter ttl on 6to4 tunnel setup example
|0.32

|    2002-11-03/PB: Add information about Taiwanese translation
|0.31.1

|    2002-10-06/PB: Add another maillist
|0.31

|    2002-09-29/PB: Extend information in proc-filesystem entries
|0.30

|    2002-09-27/PB: Add some maillists
|0.29

|    2002-09-18/PB: Update statement about nmap (triggered by Fyodor)
|0.28.1

|    2002-09-16/PB: Add note about ping6 to multicast addresses, add some labels
|0.28

|    2002-08-17/PB: Fix broken LDP/CVS links, add info about Polish translation, add URL of the IPv6 Address Oracle
|0.27

|    2002-08-10/PB: Some minor updates
|0.26.2

|    2002-07-15/PB: Add information neighbor discovery, split of firewalling (got some updates) and security into extra chapters
|0.26.1

|    2002-07-13/PB: Update nmap/IPv6 information
|0.26

|    2002-07-13/PB: Fill /proc-filesystem chapter, update DNS information about depricated A6/DNAME, change P-t-P tunnel setup to use of “ip” only
|0.25.2

|    2002-07-11/PB: Minor spelling fixes
|0.25.1

|    2002-06-23/PB: Minor spelling and other fixes
|0.25

|    2002-05-16/PB: Cosmetic fix for 2^128, thanks to José Abílio Oliveira Matos for help with LyX
|0.24

|    2002-05-02/PB: Add entries in URL list, minor spelling fixes
|0.23

|    2002-03-27/PB: Add entries in URL list and at maillists, add a label and minor information about IPv6 on RHL
|0.22

|    2002-03-04/PB: Add info about 6to4 support in kernel series 2.2.x and add an entry in URL list and at maillists
|0.21

|    2002-02-26/PB: Migrate next grammar checks submitted by John Ronan 
|0.20.4

|    2002-02-21/PB: Migrate more grammar checks submitted by John Ronan, add some additional hints at DNS section 
|0.20.3

|    2002-02-12/PB: Migrate a minor grammar check patch submitted by John Ronan
|0.20.2

|    2002-02-05/PB: Add mipl to maillist table
|0.20.1

|    2002-01-31/PB: Add a hint how to generate 6to4 addresses
|0.20

|    2002-01-30/PB: Add a hint about default route problem, some minor updates
|0.19.2

|    2002-01-29/PB: Add many new URLs
|0.19.1

|    2002-01-27/PB: Add some forgotten URLs
|0.19

|    2002-01-25/PB: Add two German books, fix quote entinities in exported SGML code
|0.18.2

|    2002-01-23/PB: Add a FAQ on the program chapter
|0.18.1

|    2002-01-23/PB: Move “the end” to the end, add USAGI to maillists
|0.18

|    2002-01-22/PB: Fix bugs in explanation of multicast address types
|0.17.2

|    2002-01-22/PB: Cosmetic fix double existing text in history (at 0.16), move all credits to the end of the document
|0.17.1

|    2002-01-20/PB: Add a reference, fix URL text in online-test-tools
|0.17

|    2002-01-19/PB: Add some forgotten information and URLs about global IPv6 addresses
|0.16

|    2002-01-19/PB: Minor fixes, remove “bold” and “emphasize” formats on code lines, fix “too long unwrapped code lines” using selfmade utility, extend list of URLs.
|0.15

|    2002-01-15/PB: Fix bug in addresstype/anycast, move content related credits to end of document
|0.14

|    2002-01-14/PB: Minor review at all, new chapter “debugging”, review “addresses”, spell checking, grammar checking (from beginning to 3.4.1) by Martin Krafft, add tcpdump examples, copy firewalling/netfilter6 from IPv6+Linux-HowTo, minor enhancements
|0.13

|    2002-01-05/PB: Add example BIND9/host, move revision history to end of document, minor extensions
|0.12

|    2002-01-03/PB: Merge review of David Ranch
|0.11

|    2002-01-02/PB: Spell checking and merge review of Pekka Savola
|0.10

|    2002-01-02/PB: First public release of chapter 1

|26.2. Credits

The quickest way to be added to this nice list is to send bug fixes, corrections, and/or updates to me ;-).

If you want to do a major review, you can use the native LyX file (see original source) and send diffs against it, because diffs against SGML don't help too much.
|26.2.1. Major credits

    David Ranch <dranch at trinnet dot net>: For encouraging me to write this HOWTO, his editorial comments on the first few revisions, and his contributions to various IPv6 testing results on my IPv6 web site. Also for his major reviews and suggestions.

    Pekka Savola <pekkas at netcore dot fi>: For major reviews, input and suggestions.

    Martin F. Krafft <madduck at madduck dot net>: For grammar checks and general reviewing of the document.

    John Ronan <j0n at tssg dot wit dot ie>: For grammar checks.

    Georg Käfer <gkaefer at gmx dot at>: For detection of no proper PDF creation (fixed now by LDP maintainer Greg Ferguson), input for German books, big list of URLs, checking all URLs, many more suggestions, corrections and contributions, and the German translation

    Michel Boucey <mboucey at free dot fr>: Finding typos and some broken URLs, contribute some suggestions and URLs, and the French translation

    Michele Ferritto <m dot ferritto at virgilio dot it>: Finding bugs and the Italian translation

    Daniel Roesen <dr at cluenet dot de>: For grammar checks

    Benjamin Thery <benjamin dot thery at bull dot net>: For contribution of updated mobility section

    John Wenker <jjw at pt dot com>: major contribution to Programming using C-API

    Srivats P. <Srivats dot P at conexant dot com>: major contribution for 4in6 tunnels

|26.2.2. Other credits
|26.2.2.1. Document technique related

Writing a LDP HOWTO as a newbie (in LyX and exporting this to DocBook to conform to SGML) isn't as easy as some people say. There are some strange pitfalls... Nevertheless, thanks to:

    Authors of the LDP Author Guide

    B. Guillon: For his DocBook with LyX HOWTO

|26.2.2.2. Content related credits

Credits for fixes and hints are listed here, will grow sure in the future

    S .P. Meenakshi <meena at cs dot iitm dot ernet dot in>: For a hint using a “send mail” shell program on tcp_wrapper/hosts.deny

    Frank Dinies <FrankDinies at web dot de>: For a bugfix on IPv6 address explanation

    John Freed <jfreed at linux-mag dot com>: For finding a bug in IPv6 multicast address explanation

    Craig Rodrigues <crodrigu at bbn dot com>: For suggestion about RHL IPv6 setup

    Fyodor <fyodor at insecure dot org>: Note me about outdated nmap information

    Mauro Tortonesi <mauro at deepspace6 dot net>: For some suggestions

    Tom Goodale <goodale at aei-potsdam dot mpg dot de>: For some suggestions

    Martin Luemkemann <mluemkem at techfak dot uni-bielefeld dot de>: For a suggestion

    Jean-Marc V. Liotier <jim at jipo dot com>: Finding a bug

    Yaniv Kaul <ykaul at checkpoint dot com>: Finding a bug

    Arnout Engelen <arnouten at bzzt dot net>: For sending note about a draft was adopted to RFC now

    Stephane Bortzmeyer <bortzmeyer at nic dot fr>: Contributing persistent configuration on Debian

    lithis von saturnsys <lithis at saturnsys dot com>: Reporting a misaddressed URL

    Guy Hulbert <gwhulbert at rogers dot com>: Send a note that RFC1924 is probably an April fool's joke

    Tero Pelander <tpeland at tkukoulu dot fi>: Reporting a broken URL

    Walter Jontofsohn <wjontof at gmx dot de>: Hints for SuSE Linux 8.0/8.1

    Benjamin Hofstetter <benjamin dot hofstetter at netlabs dot org>: Reporting a mispointing URL

    J.P. Larocque <piranha at ely dot ath dot cx>: Reporting archive URL for maillist users at ipv6 dot org

    Jorrit Kronjee <jorrit at wafel dot org>: Reporting broken URLs

    Colm MacCarthaigh <colm dot maccarthaigh at heanet dot ie>: Hint for sendfile issue on Apache2

    Tiago Camilo <tandre at ipg dot pt>: Contribute some URLs about Mobile IPv6

    Harald Geiger: Reporting a bug in how described the bit counting of the universal/global bit

    Bjoern Jacke <bjoern at j3e dot de>: Triggered me to fix some outdated information on xinetd

    Christoph Egger <cegger at chrrr dot com>: Sending note about “ip” has problems with IPv4-compatible addresses on SuSE Linux 9.0 and trigger to add a hint on 6to4-radvd example

    David Lee Haw Ling <hawling at singnet dot com dot sg>: Sending information about a tunnel broker

    Michael H. Warfield <mhw at iss dot net>: Sending note about suffix for 6to4 routers

    Tomasz Mrugalski <thomson at klub dot com dot pl>: Sending updates for DHCPv6 section

    Jan Minar <jjminar at fastmail dot fm>: Reporting minor bugs

    Kalin KOZHUHAROV <kalin at tar dot bz>: Fixing a not so well explanation

    Roel van Dijk <rdvdijk at planet dot nl>: Reporting broken URLs

    Catalin Muresan <catalin dot muresan at astral dot ro>: Reporting minor bugs

    Dennis van Dok <dvandok at quicknet dot nl>: Reporting minor bugs

    Necdet Yucel <nyucel at comu dot edu dot tr>: Reporting broken URLs

    Bryan Vukich: Reporting a broken URL

    Daniele Masini: reporting a broken iptables example

    Yao Zhao: reporting a bug in IPv6 route remove description

    Aaron Kunde: reporting a broken URL and a content related bug

    Larry W. Burton: speling fixes

    Justin Pryzby: reporting broken shortcut description of BIND

|26.3. The End

Thanks for reading. Hope it helps!

If you have any questions, subscribe to proper maillist and describe your problem providing as much as information as possible.

	7.5 nmap

		7.5.1 Linux / UNIX: Scanning network for open ports with nmap command

by Vivek Gite on July 5, 2005 · 5 comments· Last updated August 12, 2007

You can use nmap tool for this job. It is flexible in specifying targets. User can scan entire network or selected host or single server. Nmap is also useful to test your firewall rules. namp is metwork exploration tool and security / port scanner. According to nmap man page:
It is an open source tool for network exploration and security auditing. It was designed to rapidly scan large networks, although it works fine against single hosts. Nmap uses raw IP packets in novel ways to determine what hosts are available on the network, what services (application name and version) those hosts are offering, what operating systems (and OS versions) they are running, what type of packet filters/firewalls are in use, and dozens of other characteristics. While Nmap is commonly used for security audits, many systems and network administrators find it useful for routine tasks such as network inventory, managing service upgrade schedules, and monitoring host or service uptime.
nmap port scanning

TCP Connect scanning for localhost and network 192.168.0.0/24
# nmap -v -sT localhost
# nmap -v -sT 192.168.0.0/24
nmap TCP SYN (half-open) scanning

# nmap -v -sS localhost
# nmap -v -sS 192.168.0.0/24
nmap TCP FIN scanning

# nmap -v -sF localhost
# nmap -v -sF 192.168.0.0/24
nmap TCP Xmas tree scanning

Useful to see if firewall protecting against this kind of attack or not:
# nmap -v -sX localhost
# nmap -v -sX 192.168.0.0/24
nmap TCP Null scanning

Useful to see if firewall protecting against this kind attack or not:
# nmap -v -sN localhost
# nmap -v -sN 192.168.0.0/24
nmap TCP Windows scanning

# nmap -v -sW localhost
# nmap -v -sW 192.168.0.0/24
nmap TCP RPC scanning

Useful to find out RPC (such as portmap) services
# nmap -v -sR localhost
# nmap -v -sR 192.168.0.0/24
nmap UDP scanning

Useful to find out UDP ports
# nmap -v -O localhost
# nmap -v -O 192.168.0.0/24
nmap remote software version scanning

You can also find out what software version opening the port.
# nmap -v -sV localhost
# nmap -v -sV 192.168.0.0/24
A note about Windows XP / 2003 / Vista version

Windows user can find ipEye and IPSecScan utilities useful. Please note that Nmap also runes on Windows OS.

Read the man page of nmap for more information:
$ man nmap

	7.6
8. Time, Date 

	8.1 set time
		8.1.1 Basic

While logged in as root do the following:

   a. Type "date".
   b. You should see some variation of"

      "Wed Nov 24, 9:29:17 EST 1999"
   c. To change the time type(as an example):

      date -s 10:10
   d. The system response will be:

      "Wed Nov 24, 10:10:02 EST 1999"
   e. Then if you want to set the hardware(BIOS) clock so the system will keep the time when it reboots type:

      clock -w

      or

      setclock

	$ date -s 11:59 

		8.1.2 In-Depth
Introduction

This document explains how to set your computer's clock from Linux, how to set your timezone, and other stuff related to Linux and how it does its time-keeping.

Your computer has two timepieces; a battery-backed one that is always running (the ``hardware'', ``BIOS'', or ``CMOS'' clock), and another that is maintained by the operating system currently running on your computer (the ``system'' clock). The hardware clock is generally only used to set the system clock when your operating system boots, and then from that point until you reboot or turn off your system, the system clock is the one used to keep track of time.

On Linux systems, you have a choice of keeping the hardware clock in UTC/GMT time or local time. The preferred option is to keep it in UTC because then daylight savings can be automatically accounted for. The only disadvantage with keeping the hardware clock in UTC is that if you dual boot with an operating system (such as DOS) that expects the hardware clock to be set to local time, the time will always be wrong in that operating system.
Setting your timezone

The timezone under Linux is set by a symbolic link from /etc/localtime[1] to a file in the /usr/share/zoneinfo[2] directory that corresponds with what timezone you are in. For example, since I'm in South Australia, /etc/localtime is a symlink to /usr/share/zoneinfo/Australia/South. To set this link, type:

ln -sf ../usr/share/zoneinfo/your/zone /etc/localtime

Replace your/zone with something like Australia/NSW or Australia/Perth. Have a look in the directories under /usr/share/zoneinfo to see what timezones are available.

[1] This assumes that /usr/share/zoneinfo is linked to /etc/localtime as it is under Red Hat Linux.

[2] On older systems, you'll find that /usr/lib/zoneinfo is used instead of /usr/share/zoneinfo. See also the later section ``The time in some applications is wrong''.
Setting UTC or local time

When Linux boots, one of the initialisation scripts will run the /sbin/hwclock program to copy the current hardware clock time to the system clock. hwclock will assume the hardware clock is set to local time unless it is run with the --utc switch. Rather than editing the startup script, under Red Hat Linux you should edit the /etc/sysconfig/clock file and change the ``UTC'' line to either ``UTC=true'' or ``UTC=false'' as appropriate.
Setting the system clock

To set the system clock under Linux, use the date command. As an example, to set the current time and date to July 31, 11:16pm, type ``date 07312316'' (note that the time is given in 24 hour notation). If you wanted to change the year as well, you could type ``date 073123161998''. To set the seconds as well, type ``date 07312316.30'' or ``date 073123161998.30''. To see what Linux thinks the current local time is, run date with no arguments.
Setting the hardware clock

To set the hardware clock, my favourite way is to set the system clock first, and then set the hardware clock to the current system clock by typing ``/sbin/hwclock --systohc'' (or ``/sbin/hwclock --systohc --utc'' if you are keeping the hardware clock in UTC). To see what the hardware clock is currently set to, run hwclock with no arguments. If the hardware clock is in UTC and you want to see the local equivalent, type ``/sbin/hwclock --utc''
The time in some applications is wrong

If some applications (such as date) display the correct time, but others don't, and you are running Red Hat Linux 5.0 or 5.1, you most likely have run into a bug caused by a move of the timezone information from /usr/lib/zoneinfo to /usr/share/zoneinfo. The fix is to create a symbolic link from /usr/lib/zoneinfo to /usr/share/zoneinfo: ``ln -s ../share/zoneinfo /usr/lib/zoneinfo''.
Summary

    * /etc/sysconfig/clock sets whether the hardware clock is stored as UTC or local time.

    * Symlink /etc/localtime to /usr/share/zoneinfo/... to set your timezone.

    * Run ``date MMDDhhmm'' to set the current system date/time.

    * Type ``/sbin/hwclock --systohc [--utc]'' to set the hardware clock.

Other interesting notes

The Linux kernel always stores and calculates time as the number of seconds since midnight of the 1st of January 1970 UTC regardless of whether your hardware clock is stored as UTC or not. Conversions to your local time are done at run-time. One neat thing about this is that if someone is using your computer from a different timezone, they can set the TZ environment variable and all dates and times will appear correct for their timezone.

If the number of seconds since the 1st of January 1970 UTC is stored as an signed 32-bit integer (as it is on your Linux/Intel system), your clock will stop working sometime on the year 2038. Linux has no inherent Y2K problem, but it does have a year 2038 problem. Hopefully we'll all be running Linux on 64-bit systems by then. 64-bit integers will keep our clocks running quite well until aproximately the year 292271-million.
Other programs worth looking at

    * rdate - get the current time from a remote machine; can be used to set the system time.

    * xntpd - like rdate, but it's extremely accurate and you need a permanent 'net connection. xntpd runs continuously and accounts for things like network delay and clock drift, but there's also a program (ntpdate) included that just sets the current time like rdate does.

Further information

    * date(1)

    * hwclock(8)

    * /usr/doc/HOWTO/mini/Clock

		8.1.3 Examples
For example, set new data to 2 Oct 2006 18:00:00, type the following command as root user:
# date -s "2 OCT 2006 18:00:00"
OR
# date --set="2 OCT 2006 18:00:00"

You can also simplify format using following syntax:
# date +%Y%m%d -s "20081128"
Linux Set Time

To set time use the following syntax:
# date +%T -s "10:13:13"
Where,

    10: Hour (hh)
    13: Minute (mm)
    13: Second (ss)

Use %p locale’s equivalent of either AM or PM, enter:
# date +%T%p -s "6:10:30AM"
# date +%T%p -s "12:10:30PM"


		8.1.4
	8.2
9. Manage disk space, quotas

	9.1 Check for disk usage on my user account
$du -h --exclude=.snapshot > ./1.txt 
then use either sed or vim to leave only the big files reported, for ex. in vim do:
:g/\d\+\.\?\d\+K/d

	9.2 Get a list of files and dirs sorted according to used space
	du -sk * | sort -n

	9.3 Check quota 
	go to /ws/yizaq and run "sudo nquota -u yizaq"

some aliases:
alias du_1st_lvl='for dir in `ls_dir`; do du -sh $dir; done'
alias list_young_files='echo list files that are younger than 1 year; find . -mtime -365; '
alias quotas='echo user $USER quotas: ; sudo /usr/cisco/bin/nquota -u yizaq ; echo top size dirs:; echo in ws: ;  cd /ws/yizaq ; du_1st_lvl; echo in home dir: ; cd  /users/yizaq; du_1st_lvl;  
10. System admininstration

	10.1 File information


		10.1.1 which
which <name> 
Determine from where "name" is run, if executable give the path to its location

		10.1.2 alias
alias <name>
what is the content of alias name

		10.1.3 locate
locate <name>
locate file name. Need to create or update file DB, use command updatedb

		10.1.4 whatis
whatis <name>
information about file name. need to create DB first, makewhatis

	10.2 Hard disks and partitions

		10.2.1 list partitions
cat /etc/fstab

fdisk -l /dev/hda

			10.2.1.1 fstab(5) - Linux man page
Name
fstab - static information about the filesystems
Synopsis
#include <fstab.h>
Description
The file fstab contains descriptive information about the various file systems. It is the duty of the system administrator to properly create and maintain this file. fstab can be modified by special utils (e.g. fstab-sync(8)). Each filesystem is described on a separate line; fields on each line are separated by tabs or spaces. Lines starting with '#' are comments. The order of records in fstab is important because fsck(8), mount(8), and umount(8) sequentially iterate through fstab doing their thing.

The first field, (fs_spec), describes the block special device or remote filesystem to be mounted.

For ordinary mounts it will hold (a link to) a block special device node (as created by mknod(8)) for the device to be mounted, like '/dev/cdrom' or '/dev/sdb7'. For NFS mounts one will have <host>:<dir>, e.g., 'knuth.aeb.nl:/'. For procfs, use 'proc'.

Instead of giving the device explicitly, one may indicate the (ext2 or xfs) filesystem that is to be mounted by its UUID or volume label (cf. e2label(8) or xfs_admin(8)), writing LABEL=<label> or UUID=<uuid>, e.g., 'LABEL=Boot' or 'UUID=3e6be9de-8139-11d1-9106-a43f08d823a6'. This will make the system more robust: adding or removing a SCSI disk changes the disk device name but not the filesystem volume label.

The second field, (fs_file), describes the mount point for the filesystem. For swap partitions, this field should be specified as 'none'. If the name of the mount point contains spaces these can be escaped as '\040'.

The third field, (fs_vfstype), describes the type of the filesystem. Linux supports lots of filesystem types, such as adfs, affs, autofs, coda, coherent, cramfs, devpts, efs, ext2, ext3, hfs, hpfs, iso9660, jfs, minix, msdos, ncpfs, nfs, ntfs, proc, qnx4, reiserfs, romfs, smbfs, sysv, tmpfs, udf, ufs, umsdos, vfat, xenix, xfs, and possibly others. For more details, see mount(8). For the filesystems currently supported by the running kernel, see /proc/filesystems. An entry swap denotes a file or partition to be used for swapping, cf. swapon(8). An entry ignore causes the line to be ignored. This is useful to show disk partitions which are currently unused.

The fourth field, (fs_mntops), describes the mount options associated with the filesystem.

It is formatted as a comma separated list of options. It contains at least the type of mount plus any additional options appropriate to the filesystem type. For documentation on the available options for non-nfs file systems, see mount(8). For documentation on all nfs-specific options have a look at nfs(5). Common for all types of file system are the options ''noauto'' (do not mount when "mount -a" is given, e.g., at boot time), ''user'' (allow a user to mount), ''owner'' (allow device owner to mount), ''pamconsole'' (allow a user at the console to mount), and ''comment'' (e.g., for use by fstab-maintaining programs). The ''owner'', ''pamconsole'' and ''comment'' options are Linux-specific. For more details, see mount(8).

The fifth field, (fs_freq), is used for these filesystems by the dump(8) command to determine which filesystems need to be dumped. If the fifth field is not present, a value of zero is returned and dump will assume that the filesystem does not need to be dumped.

The sixth field, (fs_passno), is used by the fsck(8) program to determine the order in which filesystem checks are done at reboot time. The root filesystem should be specified with a fs_passno of 1, and other filesystems should have a fs_passno of 2. Filesystems within a drive will be checked sequentially, but filesystems on different drives will be checked at the same time to utilize parallelism available in the hardware. If the sixth field is not present or zero, a value of zero is returned and fsck will assume that the filesystem does not need to be checked.

The proper way to read records from fstab is to use the routines getmntent(3).
Files
/etc/fstab
See Also
getmntent(3), mount(8), swapon(8), fs(5) nfs(5) fstab-sync(8)
History
The ancestor of this fstab file format appeared in 4.0BSD.
Referenced By
davfs2.conf(5), fsck.xfs(8), gentoo(1), getfsent(3), gkrellm(1), gnome-mount(1), mkdumprd(8), mkinitrd(8), mount.davfs(8), proc(5), quota(1), quotacheck(8), quotaon(8), umount.cifs(8), umount.davfs(8), usermount(1), xfs_fsr(8)

	"
			10.2.1.2 fdisk(8) - Linux man page
Name
fdisk - Partition table manipulator for Linux
Synopsis
fdisk [-u] [-b sectorsize] [-C cyls] [-H heads] [-S sects] device

fdisk -l [-u] [device ...]

fdisk -s partition ...

fdisk -v
Description
Hard disks can be divided into one or more logical disks called partitions. This division is described in the partition table found in sector 0 of the disk.

In the BSD world one talks about 'disk slices' and a 'disklabel'.

Linux needs at least one partition, namely for its root file system. It can use swap files and/or swap partitions, but the latter are more efficient. So, usually one will want a second Linux partition dedicated as swap partition. On Intel compatible hardware, the BIOS that boots the system can often only access the first 1024 cylinders of the disk. For this reason people with large disks often create a third partition, just a few MB large, typically mounted on /boot, to store the kernel image and a few auxiliary files needed at boot time, so as to make sure that this stuff is accessible to the BIOS. There may be reasons of security, ease of administration and backup, or testing, to use more than the minimum number of partitions.

fdisk (in the first form of invocation) is a menu driven program for creation and manipulation of partition tables. It understands DOS type partition tables and BSD or SUN type disklabels.

fdisk doesn't understand GUID Partition Table (GPT) and it is not designed for large partitions. In particular case use more advanced GNU parted(8).

The device is usually one of the following:

/dev/hda
/dev/hdb
/dev/sda
/dev/sdb

(/dev/hd[a-h] for IDE disks, /dev/sd[a-p] for SCSI disks, /dev/ed[a-d] for ESDI disks, /dev/xd[ab] for XT disks). A device name refers to the entire disk.

The partition is a device name followed by a partition number. For example, /dev/hda1 is the first partition on the first IDE hard disk in the system. Disks can have up to 15 partitions. See also /usr/src/linux/Documentation/devices.txt.

A BSD/SUN type disklabel can describe 8 partitions, the third of which should be a 'whole disk' partition. Do not start a partition that actually uses its first sector (like a swap partition) at cylinder 0, since that will destroy the disklabel.

An IRIX/SGI type disklabel can describe 16 partitions, the eleventh of which should be an entire 'volume' partition, while the ninth should be labeled 'volume header'. The volume header will also cover the partition table, i.e., it starts at block zero and extends by default over five cylinders. The remaining space in the volume header may be used by header directory entries. No partitions may overlap with the volume header. Also do not change its type and make some file system on it, since you will lose the partition table. Use this type of label only when working with Linux on IRIX/SGI machines or IRIX/SGI disks under Linux.

A DOS type partition table can describe an unlimited number of partitions. In sector 0 there is room for the description of 4 partitions (called 'primary'). One of these may be an extended partition; this is a box holding logical partitions, with descriptors found in a linked list of sectors, each preceding the corresponding logical partitions. The four primary partitions, present or not, get numbers 1-4. Logical partitions start numbering from 5.

In a DOS type partition table the starting offset and the size of each partition is stored in two ways: as an absolute number of sectors (given in 32 bits) and as a Cylinders/Heads/Sectors triple (given in 10+8+6 bits). The former is OK - with 512-byte sectors this will work up to 2 TB. The latter has two different problems. First of all, these C/H/S fields can be filled only when the number of heads and the number of sectors per track are known. Secondly, even if we know what these numbers should be, the 24 bits that are available do not suffice. DOS uses C/H/S only, Windows uses both, Linux never uses C/H/S.

If possible, fdisk will obtain the disk geometry automatically. This is not necessarily the physical disk geometry (indeed, modern disks do not really have anything like a physical geometry, certainly not something that can be described in simplistic Cylinders/Heads/Sectors form), but is the disk geometry that MS-DOS uses for the partition table.

Usually all goes well by default, and there are no problems if Linux is the only system on the disk. However, if the disk has to be shared with other operating systems, it is often a good idea to let an fdisk from another operating system make at least one partition. When Linux boots it looks at the partition table, and tries to deduce what (fake) geometry is required for good cooperation with other systems.

Whenever a partition table is printed out, a consistency check is performed on the partition table entries. This check verifies that the physical and logical start and end points are identical, and that the partition starts and ends on a cylinder boundary (except for the first partition).

Some versions of MS-DOS create a first partition which does not begin on a cylinder boundary, but on sector 2 of the first cylinder. Partitions beginning in cylinder 1 cannot begin on a cylinder boundary, but this is unlikely to cause difficulty unless you have OS/2 on your machine.

A sync() and a BLKRRPART ioctl() (reread partition table from disk) are performed before exiting when the partition table has been updated. Long ago it used to be necessary to reboot after the use of fdisk. I do not think this is the case anymore - indeed, rebooting too quickly might cause loss of not-yet-written data. Note that both the kernel and the disk hardware may buffer data.
DOS 6.x WARNING

The DOS 6.x FORMAT command looks for some information in the first sector of the data area of the partition, and treats this information as more reliable than the information in the partition table. DOS FORMAT expects DOS FDISK to clear the first 512 bytes of the data area of a partition whenever a size change occurs. DOS FORMAT will look at this extra information even if the /U flag is given -- we consider this a bug in DOS FORMAT and DOS FDISK.

The bottom line is that if you use fdisk to change the size of a DOS partition table entry, then you must also use dd to zero the first 512 bytes of that partition before using DOS FORMAT to format the partition. For example, if you were using disk to make a DOS partition table entry for /dev/hda1, then (after exiting fdisk and rebooting Linux so that the partition table information is valid) you would use the command "dd if=/dev/zero of=/dev/hda1 bs=512 count=1" to zero the first 512 bytes of the partition.

BE EXTREMELY CAREFUL if you use the dd command, since a small typo can make all of the data on your disk useless.

For best results, you should always use an OS-specific partition table program. For example, you should make DOS partitions with the DOS FDISK program and Linux partitions with the Linux fdisk program.
Options

-b sectorsize
    Specify the sector size of the disk. Valid values are 512, 1024, or 2048. (Recent kernels know the sector size. Use this only on old kernels or to override the kernel's ideas.) 
-C cyls
    Specify the number of cylinders of the disk. I have no idea why anybody would want to do so. 
-H heads
    Specify the number of heads of the disk. (Not the physical number, of course, but the number used for partition tables.) Reasonable values are 255 and 16. 
-S sects
    Specify the number of sectors per track of the disk. (Not the physical number, of course, but the number used for partition tables.) A reasonable value is 63. 
-l
    List the partition tables for the specified devices and then exit. If no devices are given, those mentioned in /proc/partitions (if that exists) are used. 
-u
    When listing partition tables, give sizes in sectors instead of cylinders. 
-s partition
    The size of the partition (in blocks) is printed on the standard output. 
-v
    Print version number of fdisk program and exit.

Bugs
There are several *fdisk programs around. Each has its problems and strengths. Try them in the order parted, fdisk, sfdisk.

The IRIX/SGI type disklabel is currently not supported by the kernel. Moreover, IRIX/SGI header directories are not fully supported yet.

The option 'dump partition table to file' is missing.
See Also
mkfs(8), parted(8), sfdisk(8)
Referenced By
addpart(8), cfdisk(8), delpart(8), ext2online(8), gpart(8), lphdisk(8), mkswap(8), ntfsresize(8), partx(8), pvcreate(8), pvresize(8), resize2fs(8), sg_dd(8), sgm_dd(8), sgp_dd(8), syslinux(1) 

			10.2.1.3 df(1) - Linux man page
Name
df - report file system disk space usage
Synopsis
df [OPTION]... [FILE]...
Description
This manual page documents the GNU version of df. df displays the amount of disk space available on the file system containing each file name argument. If no file name is given, the space available on all currently mounted file systems is shown. Disk space is shown in 1K blocks by default, unless the environment variable POSIXLY_CORRECT is set, in which case 512-byte blocks are used.

If an argument is the absolute file name of a disk device node containing a mounted file system, df shows the space available on that file system rather than on the file system containing the device node (which is always the root file system). This version of df cannot show the space available on unmounted file systems, because on most kinds of systems doing so requires very nonportable intimate knowledge of file system structures.
Options

Show information about the file system on which each FILE resides, or all file systems by default.

Mandatory arguments to long options are mandatory for short options too.

-a, --all
    include dummy file systems 
-B, --block-size=SIZE use SIZE-byte blocks
-h, --human-readable
    print sizes in human readable format (e.g., 1K 234M 2G) 
-H, --si
    likewise, but use powers of 1000 not 1024 
-i, --inodes
    list inode information instead of block usage 
-k
    like --block-size=1K 
-l, --local
    limit listing to local file systems 
--no-sync
    do not invoke sync before getting usage info (default) 
-P, --portability
    use the POSIX output format 
--sync
    invoke sync before getting usage info 
-t, --type=TYPE
    limit listing to file systems of type TYPE 
-T, --print-type
    print file system type 
-x, --exclude-type=TYPE
    limit listing to file systems not of type TYPE 
-v
    (ignored) 
--help
    display this help and exit 
--version
    output version information and exit

SIZE may be (or may be an integer optionally followed by) one of following: kB 1000, K 1024, MB 1000*1000, M 1024*1024, and so on for G, T, P, E, Z, Y.
Author
Written by Torbjorn Granlund, David MacKenzie, and Paul Eggert.
Reporting Bugs
Report bugs to <bug-coreutils@gnu.org>.
Copyright
Copyright � 2006 Free Software Foundation, Inc.
This is free software. You may redistribute copies of it under the terms of the GNU General Public License <http://www.gnu.org/licenses/gpl.html>. There is NO WARRANTY, to the extent permitted by law.
See Also
The full documentation for df is maintained as a Texinfo manual. If the info and df programs are properly installed at your site, the command

info df

should give you access to the complete manual.
Referenced By
cachefilesd.conf(5), inndf(8), virt-df(1), xfs_quota(8) 
		10.2.2 list hard disks
	ls /dev/hd*

		10.2.3 Converting Ext2 Filesystems to Ext3
Contents:

    * Executive Summary
    * Disclaimer
    * Converting from Ext2 to Ext3
    * Converting from Ext3 back to Ext2
    * Summary

Executive Summary

Until recently, the Ext2 filesystem has been the Linux default. Ext2 is a technological miracle. Low fragmentation, redundant enough to be reliably regenerated on error yet diskspace efficient, fast, and adaptable. But when the computer is rebooted or powered off without correctly shutting down, Ext2 filesystems are placed in an error state. When the computer comes back up, the user is confronted with some mildly confusing, and very intimidating, messages and choices. Should he let the filesystem correct itself? Warning, this can lose data!

Journalized filesystems are made to eliminate such error messages. The Ext3 filesystem is an Ext2 filesystem with a journal file and some filesystem driver additions making the filesystem journalized.

My research and those of others, as well as limited experimentation by myself, indicate that the tune2fs -j command, which is the primary command for converting from Ext2 to Ext3, is safe to run even on writeable mounted partitions. However, when possible, I run the command on unmounted or read-only mounted partitions. It might be superstitious, but I feel that is playing it safe. Nevertheless, when confronted with situations making unmounting difficult, I run the command on writeable mounted partitions.

Converting the root directory from Ext2 to Ext3 is a little more difficult. Converting back from Ext3 to Ext2 is a hairy and dangerous procedure. Instructions for both are given in this document, but think long and hard before converting the root directory.
Disclaimer

Obviously, you use this document at your own risk. I am not responsible for any damage or injury caused by your use of this document, or caused by errors and/or omissions in this document. If that's not acceptable to you, you may not use this document. By using this document you are accepting this disclaimer.
Converting from Ext2 to Ext3

The conversion procedure is simple enough. Imagine /dev/hda10 mounted as /test ¿ the procedure would be as follows:

    * Log in as root
    * Make sure /etc/fstab has /dev/hda10 mounted to /test as ext2, read write
    * umount /dev/hda10
          o If you can't unmount it, then remount it read only (mount -o remount,ro /dev/hda10)
    * tune2fs -j /dev/hda10
    * Edit /etc/fstab, and for /dev/hda10, change ext2 to ext3
    * mount /dev/hda10
    * /sbin/shutdown -h now
    * mount | grep /dev/hda10
          o If it's not shown as ext3, reboot, if still not, troubleshoot
          o Otherwise, you're done.

A few explanations are in order. The tunefs command creates the journal file, which is kept in a special inode on the device (by default). You then must change the /etc/fstab entry to reflect it's a journalling filesystem, and then mount it.
Converting the /usr directory
This applies only to those systems in which the /usr tree has its own partition. Converting the /usr directory presents a challenge because commands like tune2fs are located in the /usr tree, so it must be mounted. Mount it read only with the following command:

mount -o remount,ro /usr

Then run tune2fs -j, edit /etc/fstab, and then unmount and remount /usr.
Converting the / directory
First, think long and hard before deciding to convert the root directory. Ext3's primary purpose is shorter recovery from disaster rather than data loss prevention. Converting the root directory from Ext2 to Ext3 isn't difficult, but converting it back from Ext3 to Ext2 is a treacherous process fraught with problems. But, if you really must perform the Ext2 to Ext3 conversion on the root directory, here's how, assuming /dev/hda2 is mounted as the root directory and /dev/hda1 is mounted as /boot:

    * Log in as root
    * Edit /etc/fstab and change ext2 to  ext3 on the line referencing the root directory.
    * tune2fs -j /dev/hda2
    * cd /boot
    * mv initrd-2.4.18-26.8.0.img initrd-2.4.18-26.8.0.img.ext2
    * mkinitrd initrd-2.4.18-26.8.0.img 2.4.18-26.8.0
    * reboot

In the preceding, you MUST perform all the steps, including the mkinitrd, before rebooting. Failing to perform all the steps before rebooting produces a "buried shovel" where if only you could boot the machine, you could run the mkinitrd command, and if only you could run the mkinitrd command, you could boot the machine.
Converting from Ext3 back to Ext2
There may come a time when you want to convert back to Ext2. For directories other than the root directory or /usr, it's pretty easy. The following once again uses the example of /dev/hda10 mounted to directory /test:

    * umount /dev/hda10
    * tune2fs -O ^has_journal /dev/hda10
    * e2fsck /dev/hda10
    * Edit /etc/fstab to change /dev/hda10 to mount type ext2
    * mount /dev/hda10

The tune2fs command removes the journal inode, and the e2fsck command completes that removal.
Back-Converting the root directory

The root directory is a challenge for a number of reasons. First, it must be mounted for the system to run, but it must be unmounted to run the e2fsck command. Also, different distros behave different ways. The mkinitrd command varies widely between distros. The preceding works on a Red Hat 8.0 machine, but other machines might require other solutions. We'll assume that /dev/hda1 is /boot, while /dev/hda2 is the root directory (/).

    * Log in as root
    * Edit /etc/fstab to change the /dev/hda2 line from ext3 to ext2
    * reboot
    * Log in as root
    * mv initrd-2.4.18-26.8.0.img initrd-2.4.18-26.8.0.img.ext3
    * mkinitrd initrd-2.4.18-26.8.0.img 2.4.18-26.8.0
    * Place your Knoppix CD in the CD drive
    * reboot
    * Notice you are now booted to Knoppix
    * Ctrl+Alt+F2 to access a root prompt
    * umount /dev/hda1
    * umount /dev/hda2
    * tune2fs -O ^has_journal /dev/hda2
    * e2fsck /dev/hda2
    * mount -t ext3 /dev/hda2 /mnt/hda2
          o This should fail. If it does, it proves that the partition no longer has a journaling inode.
    * reboot
    * Remove the Knoppix CD and press Enter as prompted
    * Notice you are now booted to the original operating system (hopefully)
    * Log in as root
    * Execute a mount command to verify an Ext2 root partition.

Back Converting the /usr directory
Assume that /usr is mounted by /dev/hda4

    * Place your Knoppix CD in the CD drive
    * reboot
    * Notice you are now booted to Knoppix
    * Ctrl+Alt+F2 to access a root prompt
    * umount /dev/hda4
    * tune2fs -O ^has_journal /dev/hda4
    * e2fsck /dev/hda4
    * mount -t ext3 /dev/hda4 /mnt/hda4
          o This should fail. If it does, it proves that the partition no longer has a journaling inode.
    * reboot
    * Remove the Knoppix CD and press Enter as prompted
    * Notice you are now booted to the original operating system (hopefully)
    * Log in as root
    * Execute a mount command to verify an Ext2 /usr partition.

Summary
Converting from Ext2 to Ext3 is usually easy, although converting the root partition is usually undesirable. From what I hear, and from my limited testing, the tune2fs -j command is so safe that it can be performed on a partition mounted read-write. However, it's usually easy to unmount the partition or mount it read-only, and when that's easy, I've done it that way.

Changing the filesystem type of the root directory requires rebuilding the initrd image. Those procedures are explained in this document.


		"


		10.2.4 Increase partition size


			10.2.4.1 Example from ACS machine

Basically the root cause is that less than 500gb server will gave only 2gb of /storeddata partition size which is not sufficient for upgrading 5.4.

If it is for our purpose you could increase the size by running below commands,

  lvextend -L5G /dev/smosvg/storeddatavol
  umount /dev/smosvg/storeddatavol
  e2fsck -f /dev/smosvg/storeddatavol
  resize2fs /dev/smosvg/storeddatavol
  mount /dev/smosvg/storeddatavol

			10.2.4.2


		10.2.5 Linux Partition HOWTO
Anthony Lissot
Revision History
Revision 3.5	26 Dec 2005	
reorganized document page ordering. added page on setting up swap space. added page of partition labels. updated max swap size values in section 4. added instructions on making ext2/3 file systems. broken links identified by Richard Calmbach are fixed. created an XML version.
Revision 3.4.4	08 March 2004	
synchronized SGML version with HTML version. Updated lilo placement and swap size discussion.
Revision 3.3	04 April 2003	
synchronized SGML and HTML versions
Revision 3.3	10 July 2001	
Corrected Section 6, calculation of cylinder numbers
Revision 3.2	1 September 2000	
Dan Scott provides sgml conversion 2 Oct. 2000. Rewrote Introduction. Rewrote discussion on device names in Logical Devices. Reorganized Partition Types. Edited Partition Requirements. Added Recovering a deleted partition table.
Revision 3.1	12 June 2000	
Corrected swap size limitation in Partition Requirements, updated various links in Introduction, added submitted example in How to Partition with fdisk, added file system discussion in Partition Requirements.
Revision 3.0	1 May 2000	
First revision by Anthony Lissot based on Linux Partition HOWTO by Kristian Koehntopp.
Revision 2.4	3 November 1997	
Last revision by Kristian Koehntopp.

This Linux Mini-HOWTO teaches you how to plan and create partitions on IDE and SCSI hard drives. It discusses partitioning terminology and considers size and location issues. Use of the fdisk partitioning utility for creating and recovering of partition tables is covered. The most recent version of this document is here. The Turkish translation is here.

Table of Contents
| 1. Introduction
| 
|     1.1. What is a partition?
|     1.2. Other Partitioning Software:
|     1.3. Related HOWTOs
|     1.4. Additional information on your system:
| 
| 2. Devices
| 
|     2.1. Device names
|     2.2. Device numbers
| 
| 3. Partition Types
| 
|     3.1. Partition Types
|     3.2. Foreign Partition Types
|     3.3. Primary Partitions
|     3.4. Logical Partitions
|     3.5. Swap Partitions
| 
| 4. Partitioning requirements
| 
|     4.1. What Partitions do I need?
|     4.2. Discussion:
|     4.3. File Systems
|     4.4. Swap Partitions
| 
| 5. Partitioning with fdisk
| 
|     5.1. fdisk usage
|     5.2. Four primary partitions
|     5.3. Mixed primary and logical partitions
|     5.4. Submitted Examples
| 
| 6. Labels
| 
|     6.1. Volume Labels
|     6.2. Device Labels
| 
| 7. Formatting an ext2/3 partition
| 
|     .1. Simple Invocation
|     .2. Reserved blocks
| 
| 8. Recovering a Deleted Partition Table
| 9. Setting Up Swap Space
| 
|     9.1. Swap Files
|     9.2. Swap Files
|     9.3. Multiple Swap Areas
| 
| 10. Appendix
| 
|     10.1. Formating Partitions
|     10.2. Activating Swap Space
|     10.3. Mounting Partitions
|     10.4. Some facts about file systems and fragmentation

| 1. Introduction
| 1.1. What is a partition?

Partitioning is a means to divide a single hard drive into many logical drives. A partition is a contiguous set of blocks on a drive that are treated as an independant disk. A partition table (the creation of which is the topic of this HOWTO) is an index that relates sections of the hard drive to partitions.

Why have multiple partitions?

    Encapsulate your data. Since file system corruption is local to a partition, you stand to lose only some of your data if an accident occurs.

    Increase disk space efficiency. You can format partitions with varying block sizes, depending on your usage. If your data is in a large number of small files (less than 1k) and your partition uses 4k sized blocks, you are wasting 3k for every file. In general, you waste on average one half of a block for every file, so matching block size to the average size of your files is important if you have many files.

    Limit data growth. Runaway processes or maniacal users can consume so much disk space that the operating system no longer has room on the hard drive for its bookkeeping operations. This will lead to disaster. By segregating space, you ensure that things other than the operating system die when allocated disk space is exhausted. 

| 1.2. Other Partitioning Software:

    sfdisk: a command-line version of fdisk

    cfdisk: a curses-based version of fdisk

    parted: Gnu partition editor

    Partition Magic: a commercial utility to create, resize, merge and convert partitions, without destroying data.

    Disk Drake: a Perl/Gtk program to create, rsize, and delete partitions

| 1.3. Related HOWTOs

Table 1. Related HOWTOs
Title	Author	Description
Dual boot install strategies 	Gjoen Stein 	How to estimate the various size and speed requirements for different parts of the filesystem.
Linux Multiple Disk System Tuning	Gjoen Stein	How to estimate the various size and speed requirements for different parts of the filesystem.
Linux Large Disk 	Andries Brouwer 	Instructions and considerations regarding disks with more than 1024 cylinders
Linux Quota 	Ralf van Dooren 	Instructions on limiting disk space usage per user (quotas)
Partition-Rescue mini-HOWTO 	Jean-Daniel Dodin 	How to restore linux partitions after they have been deleted by a Windows install. Does not appear to preserve data.
Linux ADSM Backup 	Thomas Koenig 	Instructions on integrating Linux into an IBM ADSM backup environment.
Linux Backup with MSDOS 	Christopher Neufeld 	Information about MS-DOS driven Linux backups.
Linux HOWTO Index	Tim Bynum	Instructions on writing and submitting a HOWTO document

| 1.4. Additional information on your system:

    /usr/src/linux/Documentation

        ide.txt: Info about your IDE drivers

        scsi.txt: Info about your SCSI drivers 

| 2. Devices

There is a special nomenclature that linux uses to refer to hard drive partitions that must be understood in order to follow the discussion on the following pages.

In Linux, partitions are represented by device files. These are phoney files located in /dev. Here are a few entries:


brw-rw----    1 root     disk       3,   0 May  5  1998 hda
brw-rw----    1 root     disk       8,   0 May  5  1998 sda
crw-------    1 root     tty        4,  64 May  5  1998 ttyS0

A device file is a file with type c ( for "character" devices, devices that do not use the buffer cache) or b (for "block" devices, which go through the buffer cache). In Linux, all disks are represented as block devices only.
| 2.1. Device names
| 2.1.1. Naming Convention

By convention, IDE drives will be given device names /dev/hda to /dev/hdd. Hard Drive A (/dev/hda) is the first drive and Hard Drive C (/dev/hdc) is the third.

Table 2. IDE controller naming convention
drive name	drive controller	drive number
/dev/hda	1	1
/dev/hdb	1	2
/dev/hdc	2	1
/dev/hdd	2	2

A typical PC has two IDE controllers, each of which can have two drives connected to it. For example, /dev/hda is the first drive (master) on the first IDE controller and /dev/hdd is the second (slave) drive on the second controller (the fourth IDE drive in the computer).

You can write to these devices directly (using cat or dd). However, since these devices represent the entire disk, starting at the first block, you can mistakenly overwrite the master boot record and the partition table, which will render the drive unusable.

Table 3. partition names
drive name	drive controller	drive number	partition type	partition number
/dev/hda1	1	1	primary	1
/dev/hda2	1	1	primary	2
/dev/hda3	1	1	primary	3
/dev/hda4	1	1	swap	NA
/dev/hdb1	1	2	primary	1
/dev/hdb2	1	2	primary	2
/dev/hdb3	1	2	primary	3
/dev/hdb4	1	2	primary	4

Once a drive has been partitioned, the partitions will represented as numbers on the end of the names. For example, the second partition on the second drive will be /dev/hdb2. The partition type (primary) is listed in the table above for clarity, although the concept is not explained until Section 3.3.

Table 4. SCSI Drives
drive name	drive controller	drive number	partition type	partition number
/dev/sda1	1	6	primary	1
/dev/sda2	1	6	primary	2
/dev/sda3	1	6	primary	3

SCSI drives follow a similar pattern; They are represented by 'sd' instead of 'hd'. The first partition of the second SCSI drive would therefore be /dev/sdb1. In the table above, the drive number is arbitraily chosen to be 6 to introduce the idea that SCSI ID numbers do not map onto device names under linux.
| 2.1.2. Name Assignment

Under (Sun) Solaris and (SGI) IRIX, the device name given to a SCSI drive has some relationship to where you plug it in. Under linux, there is only wailing and gnashing of teeth.

Before


SCSI ID #2        SCSI ID #5       SCSI ID #7        SCSI ID #8
 /dev/sda          /dev/sdb         /dev/sdc          /dev/sdd

After


SCSI ID #2                         SCSI ID #7        SCSI ID #8
 /dev/sda                           /dev/sdb          /dev/sdc

SCSI drives have ID numbers which go from 1 through 15. Lower SCSI ID numbers are assigned lower-order letters. For example, if you have two drives numbered 2 and 5, then #2 will be /dev/sda and #5 will be /dev/sdb. If you remove either, all the higher numbered drives will be renamed the next time you boot up.

If you have two SCSI controllers in your linux box, you will need to examine the output of /bin/dmesg in order to see what name each drive was assigned. If you remove one of two controllers, the remaining controller might have all its drives renamed. Grrr...

There are two work-arounds; both involve using a program to put a label on each partition (see Section 6). The label is persistent even when the device is physically moved. You then refer to the partition directly or indirectly by label.
| 2.1.3. Logical Partitions

Table 5. Logical Partitions
drive name	drive controller	drive number	partition type	partition number
/dev/hdb1	1	2	primary	1
/dev/hdb2	1	2	extended	NA
/dev/hda5	1	2	logical	2
/dev/hdb6	1	2	logical	3

The table above illustrates a mysterious jump in the name assignments. This is due to the use of logical partitions (see Section 3.4, which always start with 5, for reasons explained later.

This is all you have to know to deal with linux disk devices. For the sake of completeness, see Kristian's discussion of device numbers below.
| 2.2. Device numbers

The only important thing with a device file are its major and minor device numbers, which are shown instead of the file size:


$ ls -l /dev/hda

Table 6. Device file attributes
brw-rw----	1	root	disk	3,	0	Jul 18 1994	/dev/hda
permissions	 	owner	group	major device number	minor device number	date	device name

When accessing a device file, the major number selects which device driver is being called to perform the input/output operation. This call is being done with the minor number as a parameter and it is entirely up to the driver how the minor number is being interpreted. The driver documentation usually describes how the driver uses minor numbers. For IDE disks, this documentation is in /usr/src/linux/Documentation/ide.txt. For SCSI disks, one would expect such documentation in /usr/src/linux/Documentation/scsi.txt, but it isn't there. One has to look at the driver source to be sure ( /usr/src/linux/driver/scsi/sd.c:184-196). Fortunately, there is Peter Anvin's list of device numbers and names in /usr/src/linux/Documentation/devices.txt; see the entries for block devices, major 3, 22, 33, 34 for IDE and major 8 for SCSI disks. The major and minor numbers are a byte each and that is why the number of partitions per disk is limited.
| 3. Partition Types
| 3.1. Partition Types

A partition is labeled to host a certain kind of file system (not to be confused with a volume label (see Section 6)). Such a file system could be the linux standard ext2 file system or linux swap space, or even foreign file systems like (Microsoft) NTFS or (Sun) UFS. There is a numerical code associated with each partition type. For example, the code for ext2 is 0x83 and linux swap is 0x82. To see a list of partition types and their codes, execute /sbin/sfdisk -T
| 3.2. Foreign Partition Types

The partition type codes have been arbitrarily chosen (you can't figure out what they should be) and they are particular to a given operating system. Therefore, it is theoretically possible that if you use two operating systems with the same hard drive, the same code might be used to designate two different partition types. OS/2 marks its partitions with a 0x07 type and so does Windows NT's NTFS. MS-DOS allocates several type codes for its various flavors of FAT file systems: 0x01, 0x04 and 0x06 are known. DR-DOS used 0x81 to indicate protected FAT partitions, creating a type clash with Linux/Minix at that time, but neither Linux/Minix nor DR-DOS are widely used any more.

OS/2 marks its partitions with a 0x07 type and so does Windows NT's NTFS. MS-DOS allocates several type codes for its various flavors of FAT file systems: 0x01, 0x04 and 0x06 are known. DR-DOS used 0x81 to indicate protected FAT partitions, creating a type clash with Linux/Minix at that time, but neither Linux/Minix nor DR-DOS are widely used any more.
| 3.3. Primary Partitions

The number of partitions on an Intel-based system was limited from the very beginning: The original partition table was installed as part of the boot sector and held space for only four partition entries. These partitions are now called primary partitions.
| 3.4. Logical Partitions

One primary partition of a hard drive may be subpartitioned. These are logical partitions. This effectively allows us to skirt the historical four partition limitation.

The primary partition used to house the logical partitions is called an extended partition and it has its own file system type (0x05). Unlike primary partitions, logical partitions must be contiguous. Each logical partition contains a pointer to the next logical partition, which implies that the number of logical partitions is unlimited. However, linux imposes limits on the total number of any type of partition on a drive, so this effectively limits the number of logical partitions. This is at most 15 partitions total on an SCSI disk and 63 total on an IDE disk.
| 3.5. Swap Partitions

Every process running on your computer is allocated a number of blocks of RAM. These blocks are called pages. The set of in-memory pages which will be referenced by the processor in the very near future is called a "working set." Linux tries to predict these memory accesses (assuming that recently used pages will be used again in the near future) and keeps these pages in RAM if possible.

If you have too many processes running on a machine, the kernel will try to free up RAM by writing pages to disk. This is what swap space is for. It effectively increases the amount of memory you have available. However, disk I/O is about a hundred times slower than reading from and writing to RAM. Consider this emergency memory and not extra memory.

If memory becomes so scarce that the kernel pages out from the working set of one process in order to page in for another, the machine is said to be thrashing. Some readers might have inadvertenly experienced this: the hard drive is grinding away like crazy, but the computer is slow to the point of being unusable. Swap space is something you need to have, but it is no substitute for sufficient RAM. See the discussion in Section 4.4 for tips on determining the size of swap space you need.
| 4. Partitioning requirements
| 4.1. What Partitions do I need?

For the Boot Drive: If you want to boot your operating system from the drive you are about to partition, you will need:

    A primary partition

    One or more swap partitions

    Zero or more primary/logical partitions

For any other drive:

    One or more primary/logical partitions

    Zero or more swap partitions

| 4.2. Discussion:

Boot Partition:

    Your boot partition ought to be a primary partition, not a logical partition. This will ease recovery in case of disaster, but it is not technically necessary. It must be of type 0x83 "Linux native". If you are using a version of lilo before 21-3 (ie, from the 1990s), your boot partition must be contained within the first 1024 cylinders of the drive. (Typically, the boot partition need only contain the kernel image.)

    If you have more than one boot partition (from other OSs, for example,) keep them all in the first 1024 cylinders (All DOS partitions must be within the first 1024). If you are using a modern version of lilo, or a means other than lilo to load your kernel (for example, a boot disk or the LOADLIN.EXE MS-DOS based Linux loader), the partition can be anywhere. See the Large-disk HOWTO for details. 
Swap Partition:

    Unless you swap to files (see Section 9.2) you will need a dedicated swap partition. It must be of type 0x82 "Linux swap". It may be positioned anywhere on the disk (but see Section 4.4.3). Either a primary or logical partition can be used for swap. More than one swap partition can exist on a drive. 8 total (across drives) are permitted. See notes on swap size below (Section 4.4). 
Logical Partition:

    A single primary partition must be used as a container (extended partition) for the logical partitions. The extended partition can go anywhere on the disk. The logical partitions must be contiguous, but needn't fill the extended partition. 

| 4.3. File Systems
| 4.3.1. Which file systems need their own partitions?

Everything in your linux file system can go in the same (single) partition. However, there are circumstances when you may want to restrict the growth of certain file systems. For example, if your mail spool was in the same partition as your root fs and it filled the remaining space in the partition, your computer would basically hang.

/var

    This fs contains spool directories such as those for mail and printing. In addition, it contains the error log directory. If your machine is a server and develops a chronic error, those msgs can fill the partition. Server computers ought to have /var in a different partition than /. 
/usr

    This is where most executable binaries go. In addition, the kernel source tree goes here, and much documentation. 
/tmp

    Some programs write temporary data files here. Usually, they are quite small. However, if you run computationally intensive jobs, like science or engineering applications, hundreds of megabytes could be required for brief periods of time. In this case, keep /tmp in a different partition than /. 
/home

    This is where users home directories go. If you do not impose quotas on your users, this ought to be in its own partition. 
/boot

    This is where your kernel images go. See discussion above for placement on old systems. 

| 4.3.2. File lifetimes and backup cycles as partitioning criteria

With ext2, partitioning decisions should be governed by backup considerations and to avoid external fragmentation Section 10.4 from different file lifetimes.

Files have lifetimes. After a file has been created, it will remain some time on the system and then be removed. File lifetime varies greatly throughout the system and is partly dependent on the pathname of the file. For example, files in /bin, /sbin, /usr/sbin, /usr/bin and similar directories are likely to have a very long lifetime: many months and above. Files in /home are likely to have a medium lifetime: several weeks or so. File in /var are usually short lived: Almost no file in /var/spool/news will remain longer than a few days, files in /var/spool/lpd measure their lifetime in minutes or less.

For backup it is useful if the amount of daily backup is smaller than the capacity of a single backup medium. A daily backup can be a complete backup or an incremental backup.

You can decide to keep your partition sizes small enough that they fit completely onto one backup medium (choose daily full backups). In any case a partition should be small enough that its daily delta (all modified files) fits onto one backup medium (choose incremental backup and expect to change backup media for the weekly/monthly full dump - no unattended operation possible).

Your backup strategy depends on that decision.

When planning and buying disk space, remember to set aside a sufficient amount of money for backup! Unbackuped data is worthless! Data reproduction costs are much higher than backup costs for virtually everyone!

For performance it is useful to keep files of different lifetimes on different partitions. This way the short lived files on the news partition may be fragmented very heavily. This has no impact on the performance of the / or /home partition.
| 4.4. Swap Partitions
| 4.4.1. How large should my swap space be?

Conventional wisdom creates swap space equal to the amount of RAM.

But keep in mind that this is just a rule of thumb. It is easily possible to create scenarios where programs have extremely large or extremely small working sets (see Section 3.5). For example, a simulation program with a large data set that is accessed in a very random fashion would have almost no noticeable locality of reference in its data segment, so its working set would be quite large.

On the other hand, a graphics program with many simultaneously opened JPEGs, all but one iconified, would have a very large data segment. But image transformations are all done on one single image, most of the memory occupied by the program is not accessed. The same is true for an editor with many editor windows where only one window is being modified at a time. These programs have - if they are designed properly - a very high locality of reference and large parts of them can be kept swapped out without too severe performance impact. A user who never never quits programs once launched would want a lot of swap space for the same reason.

Servers typically are configured with more swap space than their desktop counterparts. Even though a given amount of swap is sufficient for its operations, the server might come under transient heavy loads which cause it to page out at a high rate. Some administrators prefer this to the server crashing altogether. In these cases, swap might be several times the size of ram.
| 4.4.2. How large can my swap space be?

Currently, the maximum size of a swap partition is architecture-dependent. For i386, m68k, ARM and PowerPC, it is "officially" 2Gb. It is 128Gb on alpha, 1Gb on sparc, and 3Tb on sparc64. An opteron on the 2.6 kernel can write to a 16 Tb swap partition. For linux kernels 2.1 and earlier, the limit is 128Mb. The partition may be larger than 128 MB, but excess space is never used. If you want more than 128 MB of swap for a 2.1 and earlier kernel, you have to create multiple swap partitions (8 max). After 2.4, 32 swap areas are "officially" possible. See setting up swap for details.

footnote: "official" max swap size: With kernel 2.4, the limit is 64 swap spaces at a maximum of 64Gb each, although this is not reflected in the man page for mkswap. With the 64 bit opteron on the 2.6 kernel, 128 swap areas are permitted, each a whopping 16 Tb! (thanks to Peter Chubb for the calculation)
| 4.4.3. Where should I put my swap space?

The short answer is anywhere is fine. However, if you are interested in extracting as much speed as possible, there are two basic strategies (other than buying more RAM).

    Split the swap space across multiple drives, or at least on the drive you write to least.

    Put each swap partition on the outer tracks. 

Here are the considerations:

    If you have a disk with many heads and one with less heads and both are identical in other parameters, the disk with many heads will be faster. Reading data from different heads is fast, since it is purely electronic. Reading data from different tracks is slow, since it involves physically moving the head.

    It follows then that writing swap on a separate drive will be faster than moving the head back and forth on a single drive.

    Placement: Older disks have the same number of sectors on all tracks. With these disks it will be fastest to put your swap in the middle of the disks, assuming that your disk head will move from a random track towards the swap area.

    Newer disks use ZBR (zone bit recording). They have more sectors on the outer tracks. With a constant number of rpms, this yields a far greater performance on the outer tracks than on the inner ones. Put your swap on the fast tracks. (In general, low-numbered cylinders are associated low partition numbers. However, see Kristian's more recent comments on this issue. -Tony)

    Usage: Of course your disk head will not move randomly. If you have swap space in the middle of a disk between a constantly busy home partition and an almost unused archive partition, you would be better of if your swap were near the home partition for even shorter head movements. You would be even better off, if you had your swap on another otherwise unused disk, though.

    Striping: Speed can be increased by writing to multiple swap areas simultaneously. Swap spaces with the same priority will be written to like a RAID. See Section 9.3. 

Summary: Put your swap on a fast disk with many heads that is not busy doing other things. If you have multiple disks: Split swap and scatter it over all your disks or even different controllers.
| 5. Partitioning with fdisk

This section shows you how to actually partition your hard drive with the fdisk utility. Linux allows only 4 primary partitions. You can have a much larger number of logical partitions by sub-dividing one of the primary partitions. Only one of the primary partitions can be sub-divided.

Examples:

    Four primary partitions (see Section 5.2)

    Mixed primary and logical partitions (see Section 5.3) 

| 5.1. fdisk usage

fdisk is started by typing (as root) fdisk device at the command prompt. device might be something like /dev/hda or /dev/sda (see Section 2.1.1). The basic fdisk commands you need are:

p print the partition table

n create a new partition

d delete a partition

q quit without saving changes

w write the new partition table and exit

Changes you make to the partition table do not take effect until you issue the write (w) command. Here is a sample partition table:


Disk /dev/hdb: 64 heads, 63 sectors, 621 cylinders
Units = cylinders of 4032 * 512 bytes
 
   Device Boot    Start       End    Blocks   Id  System
/dev/hdb1   *         1       184    370912+  83  Linux
/dev/hdb2           185       368    370944   83  Linux
/dev/hdb3           369       552    370944   83  Linux
/dev/hdb4           553       621    139104   82  Linux swap

The first line shows the geometry of your hard drive. It may not be physically accurate, but you can accept it as though it were. The hard drive in this example is made of 32 double-sided platters with one head on each side (probably not true). Each platter has 621 concentric tracks. A 3-dimensional track (the same track on all disks) is called a cylinder. Each track is divided into 63 sectors. Each sector contains 512 bytes of data. Therefore the block size in the partition table is 64 heads * 63 sectors * 512 bytes er...divided by 1024. (See 4 for discussion on problems with this calculation.) The start and end values are cylinders.
| 5.2. Four primary partitions

The overview:

Decide on the size of your swap space (see Section 4.4) and where it ought to go (see Section 4.4.3). Divide up the remaining space for the three other partitions.

Example:

I start fdisk from the shell prompt:


# fdisk /dev/hdb 

which indicates that I am using the second drive on my IDE controller. (See Section 2.1.) When I print the (empty) partition table, I just get configuration information.


Command (m for help): p

Disk /dev/hdb: 64 heads, 63 sectors, 621 cylinders
Units = cylinders of 4032 * 512 bytes

I knew that I had a 1.2Gb drive, but now I really know: 64 * 63 * 512 * 621 = 1281982464 bytes. I decide to reserve 128Mb of that space for swap, leaving 1153982464. If I use one of my primary partitions for swap, that means I have three left for ext2 partitions. Divided equally, that makes for 384Mb per partition. Now I get to work.


Command (m for help): n
Command action
   e   extended
   p   primary partition (1-4)
p
Partition number (1-4): 1
First cylinder (1-621, default 1):<RETURN>
Using default value 1
Last cylinder or +size or +sizeM or +sizeK (1-621, default 621): +384M

Next, I set up the partition I want to use for swap:


Command (m for help): n
Command action
   e   extended
   p   primary partition (1-4)
p
Partition number (1-4): 2
First cylinder (197-621, default 197):<RETURN>
Using default value 197
Last cylinder or +size or +sizeM or +sizeK (197-621, default 621): +128M

Now the partition table looks like this:


   Device Boot    Start       End    Blocks   Id  System
/dev/hdb1             1       196    395104   83  Linux
/dev/hdb2           197       262    133056   83  Linux

I set up the remaining two partitions the same way I did the first. Finally, I make the first partition bootable:


Command (m for help): a
Partition number (1-4): 1

And I make the second partition of type swap:


Command (m for help): t
Partition number (1-4): 2
Hex code (type L to list codes): 82
Changed system type of partition 2 to 82 (Linux swap)      
Command (m for help): p

The end result:


Disk /dev/hdb: 64 heads, 63 sectors, 621 cylinders
Units = cylinders of 4032 * 512 bytes
 
   Device Boot    Start       End    Blocks   Id  System
/dev/hdb1   *         1       196    395104+  83  Linux
/dev/hdb2           197       262    133056   82  Linux swap
/dev/hdb3           263       458    395136   83  Linux
/dev/hdb4           459       621    328608   83  Linux          

Finally, I issue the write command (w) to write the table on the disk.

Side topics:

    Section 10.2

    Section 10.1

    Section 10.3

| 5.3. Mixed primary and logical partitions

The overview: create one use one of the primary partitions to house all the extra partitions. Then create logical partitions within it. Create the other primary partitions before or after creating the logical partitions.

Example:

I start fdisk from the shell prompt:


# fdisk /dev/sda

which indicates that I am using the first drive on my SCSI chain. (See Section 2.1.)

First I figure out how many partitions I want. I know my drive has a 183Gb capacity and I want 26Gb partitions (because I happen to have back-up tapes that are about that size).

183Gb / 26Gb = ~7

so I will need 7 partitions. Even though fdisk accepts partition sizes expressed in Mb and Kb, I decide to calculate the number of cylinders that will end up in each partition because fdisk reports start and stop points in cylinders. I see when I enter fdisk that I have 22800 cylinders.


> The number of cylinders for this disk is set to 22800.  There is
> nothing wrong with that, but this is larger than 1024, and could in
> certain setups cause problems with: 1) software that runs at boot
> time (e.g., LILO) 2) booting and partitioning software from other
> OSs  (e.g., DOS FDISK, OS/2 FDISK)

So, 22800 total cylinders divided by seven partitions is 3258 cylinders. Each partition will be about 3258 cylinders long. I ignore the warning msg because this is not my boot drive (Section 4).

Since I have 4 primary partitions, 3 of them can be 3258 long. The extended partition will have to be (4 * 3258), or 13032, cylinders long in order to contain the 4 logical partitions.

I enter the following commands to set up the first of the 3 primary partitions (stuff I type is bold ):


Command (m for help): n
Command action
   e   extended
   p   primary partition (1-4)
p
Partition number (1-4): 1
First cylinder (1-22800, default 1): <RETURN>
Using default value 1
Last cylinder or +size or +sizeM or +sizeK (1-22800, default 22800): 3258

The last partition is the extended partition:


Partition number (1-4): 4
First cylinder (9775-22800, default 9775): <RETURN>
Using default value 9775
Last cylinder or +size or +sizeM or +sizeK (9775-22800, default 22800): <RETURN>
Using default value 22800

The result, when I issue the print table command is:


/dev/sda1             1      3258  26169853+  83  Linux
/dev/sda2          3259      6516  26169885   83  Linux
/dev/sda3          6517      9774  26169885   83  Linux
/dev/sda4          9775     22800 104631345    5  Extended

Next I segment the extended partition into 4 logical partitions, starting with the first logical partition, into 3258-cylinder segments. The logical partitions automatically start from /dev/sda5.


Command (m for help):  n
First cylinder (9775-22800, default 9775): <RETURN>
Using default value 9775
Last cylinder or +size or +sizeM or +sizeK (9775-22800, default 22800): 13032

The end result is:


   Device Boot    Start       End    Blocks   Id  System
/dev/sda1             1      3258  26169853+  83  Linux
/dev/sda2          3259      6516  26169885   83  Linux
/dev/sda3          6517      9774  26169885   83  Linux
/dev/sda4          9775     22800 104631345    5  Extended
/dev/sda5          9775     13032  26169853+  83  Linux
/dev/sda6         13033     16290  26169853+  83  Linux
/dev/sda7         16291     19584  26459023+  83  Linux
/dev/sda8         19585     22800  25832488+  83  Linux

Finally, I issue the write command (w) to write the table on the disk. To make the partitions usable, I will have to format (Section 10.1) each partition and then mount (Section 10.3) it.
| 5.4. Submitted Examples

I'd like to submit my partition layout, because it works well with any distribution of Linux (even big RPM based ones). I have one hard drive that ... is 10 gigs, exactly. Windows can't see above 9.3 gigs of it, but Linux can see it all, and use it all. It also has much more than 1024 cylenders.

Table 7. Partition layout example
Partition	Mount point	Size
/dev/hda1	/boot	(15 megs)
/dev/hda2	windows 98 partition	(2 gigs)
/dev/hda3	extended	(N/A)
/dev/hda5	swap space	(64 megs)
/dev/hda6	/tmp	(50 megs)
/dev/hda7	/	(150 megs)
/dev/hda8	/usr	(1.5 gigs)
/dev/hda9	/home	(rest of drive)
I test new kernels for the USB mass storage, so that explains the large /boot partition. I install LILO into the MBR, and by default I boot windows (I'm not the only one to use this computer).

I also noticed that you don't have any REAL examples of partition tables, and for newbies I HIGHLY suggest putting quite a few up. I'm freshly out of the newbie stage, and partitioning was what messed me up the most.

Valkor
| 6. Labels

In linux, hard drives are referred to as devices, and devices are pseudo files in /dev. For example, the first partition of the second lowest numbered SCSI drive is /dev/sdb1. If the drive referred to as /dev/sda is removed from the chain, then the latter partition is automatically renamed /dev/sda1 at reboot.
| 6.1. Volume Labels

Volume labels make it possible for partitions to retain a consistent name regardless of where they are connected, and regardless of whatever else is connected. Labels are not mandatory for a linux volume. Each can be a maximum of 16 characters long.

There are three tools to make volume labels: mke2fs, tune2fs and e2label.
| 6.1.1. Simple Invocation

e2label /dev/hdb1 pubsw

tune2fs -L pubsw /dev/hdb1

Either of thse two commands will label the first partition of the second drive "pubsw". That label stays with that particular partition, even if the drive is moved to another controller or even another computer.

mke2fs	pubsw /dev/hdb1

mke2fs -L pubsw /dev/hdb1

will do the same thing as the first two commands - after they make the file system. This means that either of these last two commands will delete any existing data in the partition.
| 6.1.2. How to Use

Here is a sample fstab. This is a text file located in /etc, which is usually set up during the installation of the operating system. it describes where each partition wil be mounted, and how it will be mounted. It can be modified by you, either through a utility or manually, when you add/remove devices.


LABEL=/        /                    ext3    defaults        1 1
LABEL=/boot    /boot                ext2    defaults        1 2
none           /dev/pts             devpts  gid=5,mode=620  0 0
none           /dev/shm             tmpfs   defaults        0 0
LABEL=HOME     /home                ext3    defaults        1 2
none           /proc                proc    defaults        0 0
none           /sys                 sysfs   defaults        0 0
LABEL=/usr     /usr                 ext3    defaults        1 2
/dev/hdc1      /k-space             ext3    defaults        1 2
/dev/hda6      swap                 swap    defaults        0 0
/dev/hdd       /media/cdrecorder    auto    pamconsole,ro,exec,noauto,managed 0 0
/dev/fd0       /media/floppy        auto    pamconsole,exec,noauto,managed 0 0

The leftmost column lists devices and the second column lists mount points. This example contains a mixture of devices and labels. The master drive of the second controller is always mounted on /k-space. The partition labeled "HOME" is always mounted on /home, regardless of which drive it is on or which partition number it has. Notice that it is permissible to use mount points as labels, such as "/usr"
| 6.2. Device Labels

devlabel is a script which creates symbolic links to devices. For example,

devlabel -d /dev/hdb1 -s /dev/home

will create a link from /dev/hdb1 to /dev/home. Crucially, it stores a unique identifier for the hardware that was on /dev/hdb1 and stores that identifier along with the link name that you specified in /etc/sysconfig/devlabel. If the hardware is later moved to /dev/hdc1, its unique identifier will be queried (using /usr/bin/partition_uuid), matched to its entry in /etc/sysconfig/devlabel, and again linked to /dev/home.
| 7. Formatting an ext2/3 partition

When a hard drive is partitioned, it is mapped into sections, but the sections are empty. It is like a newly constructed library; shelves, signs, and a card catalogue system must be put in place before the books are put away.

The organizational structure inside a partition is called a file system. With Linux, the standard file system is ext2 and ext3. The ext3 file system is ext2, plus a log of disk writes called a journal. The journal allows the system to recover quickly from accidental power outages, among other things.

The principal tool for making an ext2/3 file system in a partition is mke2fs. It is usually found in /sbin. mkfs.ext2 and mkfs.ext3 are frontends which pass specific options to mke2fs.
| .1. Simple Invocation

mke2fs /dev/hdb1

mkfs.ext2 /dev/hdb1

both of which make an ext2 file system on the first partition of the second drive, and

mke2fs	-j /dev/hdb1

mkfs.ext3 /dev/hdb1

make an ext3 file system.
| .2. Reserved blocks

The -m option is probably the one of most use to non-experts. If the file system becomes filled and there is no more space to write, it is basically unusable because the operating system is constantly writing to disk. By default, five percent of the partition is reserved for use by the root user. This allows root to conduct administrative activities on the partition and perhaps move some data off. However, this is most critical when the partition contains / or home directories. For pure data partitions, this is just lost space. Five percent of a 250Gb partition is 12.5 Gb. Especially in the case of large partitions, it is safe to set the reserved space to the minimum, which is one percent.

mkfs.ext3 -m 1/dev/hdb1

creates a file system with only 1% of its space reserved for the root user. tune2fs -m can be used to adjust the reserved blocks after data is loaded on the partition.
| 8. Recovering a Deleted Partition Table

Below are instructions for manually recovering a deleted partition table. There are utilities such as gpart or TestDisk which can make this task considerably easier. If you are reading this, however, because you have run out of luck, this is what you will have to do:

    Make a partition that is at least as big as your first partition was. You can make it larger than the original partition by any amount. If you underestimate, there will be much wailing and gnashing of teeth.


    Command (m for help): n
    Command action
       e   extended
       p   primary partition (1-4)
    p
    Partition number (1-4): 1
    First cylinder (1-23361, default 1): <RETURN>
    Using default value 1
    Last cylinder or +size or +sizeM or +sizeK (1-22800, default 22800): 13032

    Command (m for help): w

    Run dumpe2fs on the first partition and grep out the block count.

    Example:


               % dumpe2fs /dev/sda1 | grep "Block count:"
               Block count:              41270953
          

    If you are uncertain about this value, repeat Step 1 with a bigger partition size. If the block count changes, then you underestimated the size of the original partition. Repeat Step 1 until you get a stable block count.

    Remove the partition you just created


             Command (m for help): d
             Partition number (1-4): 1
          

    Make a new partition with the exact size you got from the block count. Since you cannot enter block size in fdisk, you need to figure out how many cylinders to request. Here is the formula:


      (number of needed cylinders) = (number of blocks) / (block size)

      (block size) = (unit size) / 1024

      (unit size) = (number of heads) * (number of sectors/cylinder) * (number of bytes/sector)

    Consider the following example, where a hard drive has been partitioned into four primary partitions of 1, 2, 4, and 8 cylinders.


    disk /dev/sda: 16 heads, 63 sectors, 23361 cylinders
    Units = cylinders of 1008 * 512 bytes

       Device Boot    Start       End    Blocks   Id  System
    /dev/sda1             1         2       976+  83  Linux
    /dev/sda2             3         5      1512   83  Linux
    /dev/sda3             6        10      2520   83  Linux
    /dev/sda4            11        19      4536   83  Linux

    fdisk provides the configuration information I need in the head of the output. The unit size is 516096 ( 16 heads * 63 sectors/cyl * 512 bytes/sector ). The block size is 504 ( 516096 / 1024 ). The number of needed cylinders for the second partition is therefore 3 ( 1512 blocks / 504 ). The partition table shows that this is indeed the case: the first cylinder is 3, the second 4, and the last is 5, for a total of three cylinders. The number of needed cylinders for the third partition is calculated similarly: 2520 blocks / 504 = 5, which corresponds to blocks 6,7,8,9,10 . Notice that this calculation does not work for the first partition because the block count is wrong ( 976 instead of 1008 ). The plus sign indicates that not all the blocks are included in the fdisk value. When you try the calculation ( 976 / 504 ) you get 1.937. Knowing that the number of cylinders must be an integer, you can simply round up.

    Run e2fsck on it to verify that you can read the new partition.

    Repeat Steps 1-5 on remaining partitions. 

Remount your partitions. Amazingly, all of your data will be there.

Credit goes to: Mike Vevea, jedi sys admin, for providing the basic strategy.
| 9. Setting Up Swap Space
| 9.1. Swap Files

Normally, there are only two steps to setting up swap space, creating the partition and adding it to /etc/fstab. A typical fstab entry for a swap partition at /dev/hda6 would look like this:


/dev/hda6	swap	swap	defaults	0	0

The next time you reboot, the initialization scripts will activate it automatically and there's nothing more to be done.

However, if you want to make use of it right away, you'll need to activate it maually. As root, type:


mkswap -f /dev/hda6
swapon /dev/hda6

| 9.2. Swap Files

There might be times when you've run out of swap space and it is not practical to repartition a drive or add a new one. In this case, you can use a regular file in an ordinary partition. All you have to do is create a file of the size you want

dd if=/dev/zero of=/var/my_swap bs=1024 count=131072

and activate it


	mkswap -f /var/my_swap
	swapon /var/my_swap

This invocation creates a file called my_swap in /var. It is 128 Mb long (128 x 1024 = 131072). Initially, it is filled with zeros. However, mkswap marks it as swap space and swapon tells the kernel to start using it as swap space. When you are done with it,


swapoff /var/my_swap
rm /var/my_swap

| 9.3. Multiple Swap Areas

More than one swap partition can be used on the same system. Consider an example fstab where there is a single swap partition:


/dev/hda5   /        ext3   defaults        1	1
/dev/hda1   /boot    ext2   defaults        1	2
none        /dev/pts devpts gid=5,mode=620  0	0
none        /proc    proc   defaults        0	0
/dev/hda7   /usr     ext3   defaults        1	2
/dev/hda6   swap     swap   defaults        0	0

Imagine replacing the entry for the swap partition with these three lines:


/dev/hda6   none    swap    sw,pri=3    0	0
/dev/hdb2   none    swap    sw,pri=2    0	0
/dev/hdc2   none    swap    sw,pri=1    0	0

This configuration would cause the kernel to use /dev/hda6 first. it has the highest priority assigned to it (pri=3). The maximum priority can be 32767 and the lowest 0. If that space were to max out, the kernel would start using /dev/hdb2, and on to /dev/hdc2 after that. Why such a configuration? Imagine that the newest (fastest) drives are given the highest priority. This will minimize speed loss as swap space usage grows.

It is possible to write to all three simulataneously. If each has the same priority, the kernel will write to them much like a RAID, with commensurate speed increases.


/dev/hda6   none   swap   sw,pri=3   0   0
/dev/hdb2   none   swap   sw,pri=3   0   0
/dev/hdc2   none   swap   sw,pri=3   0   0

Notice that these three partitions are on separate drives, which is ideal in terms of speed enhancement.
| 10. Appendix
| 10.1. Formating Partitions

At the shell prompt, I begin making the file systems on my partitions. Continuing with the example in (see Section 5.3), this is:


# mke2fs /dev/sda1

I need to do this for each of my partitions, but not for /dev/sda4 (my extended partition). Linux supports types of file systems other than ext2. You can find out what kinds your kernel supports by looking in: /usr/src/linux/include/linux/fs.h

The most common file systems can be made with programs in /sbin that start with "mk" like mkfs.msdos and mke2fs.
| 10.2. Activating Swap Space

To set up a swap partition:


# mkswap -f /dev/hda5

To activate the swap area:


# swapon  /dev/hda5

Normally, the swap area is activated by the initialization scripts at boot time.
| 10.3. Mounting Partitions

Mounting a partition means attaching it to the linux file system. To mount a linux partition:


# mount -t ext2 /dev/sda1 /opt

-t ext2

    File system type. Other types you are likely to use are:

        ext3 (journaling sile system based on ext2)

        msdos (DOS)

        hfs (mac)

        iso9660 (CDROM)

        nfs (network file system)

/dev/sda1

    Device name. Other device names you are likely to use:

        /dev/hdb2 (second partition in second IDE drive)

        /dev/fd0 (floppy drive A)

        /dev/cdrom (CDROM)

/opt

    mount point. This is where you want to "see" your partition. When you type ls /opt, you can see what is in /dev/sda1. If there are already some directories and/or files under /opt, they will be invisible after this mount command. 

| 10.4. Some facts about file systems and fragmentation

Disk space is administered by the operating system in units of blocks and fragments of blocks. In ext2, fragments and blocks have to be of the same size, so we can limit our discussion to blocks.

Files come in any size. They don't end on block boundaries. So with every file a part of the last block of every file is wasted. Assuming that file sizes are random, there is approximately a half block of waste for each file on your disk. Tanenbaum calls this "internal fragmentation" in his book "Operating Systems".

You can guess the number of files on your disk by the number of allocated inodes on a disk. On my disk


# df -i
Filesystem           Inodes   IUsed   IFree  %IUsed Mounted on
/dev/hda3              64256   12234   52022    19%  /
/dev/hda5              96000   43058   52942    45%  /var

there are about 12000 files on / and about 44000 files on /var. At a block size of 1 KB, about 6+22 = 28 MB of disk space are lost in the tail blocks of files. Had I chosen a block size of 4 KB, I had lost 4 times this space.

Data transfer is faster for large contiguous chunks of data, though. That's why ext2 tries to preallocate space in units of 8 contigous blocks for growing files. Unused preallocation is released when the file is closed, so no space is wasted.

Noncontiguous placement of blocks in a file is bad for performance, since files are often accessed in a sequential manner. It forces the operating system to split a disk access and the disk to move the head. This is called "external fragmentation" or simply "fragmentation" and is a common problem with MS-DOS file systems. In conjunction with the abysmal buffer cache used by MS-DOS, the effects of file fragmentation on performance are very noticeable. DOS users are accustomed to defragging their disks every few weeks and some have even developed some ritualistic beliefs regarding defragmentation.

None of these habits should be carried over to Linux and ext2. Linux native file systems do not need defragmentation under normal use and this includes any condition with at least 5% of free space on a disk. There is a defragmentation tool for ext2 called defrag, but users are cautioned against casual use. A power outage during such an operation can trash your file system. Since you need to back up your data anyway, simply writing back from your copy will do the job.

The MS-DOS file system is also known to lose large amounts of disk space due to internal fragmentation. For partitions larger than 256 MB, DOS block sizes grow so large that they are no longer useful (This has been corrected to some extent with FAT32). Ext2 does not force you to choose large blocks for large file systems, except for very large file systems in the 0.5 TB range (that's terabytes with 1 TB equaling 1024 GB) and above, where small block sizes become inefficient. So unlike DOS there is no need to split up large disks into multiple partitions to keep block size down.

Use a 1Kb block size if you have many small files. For large partitions, 4Kb blocks are fine. 

		10.2.6 Linux Logical Volume Extend Size

You can extend a volume size in linux by just following the commands below

If you want to extend a size of volume /dev/mapper/volume-var to 400GB you need enter the following command.

lvextend -L400GB /dev/mapper/volume-var

O/P

Extending logical volume usr to 100.00 GiB
  Logical volume usr successfully resized

unmount the volume by

umount /dev/mapper/volume-var

Then resize the volume

resize2fs /dev/mapper/volume-var

O/P

resize2fs 1.41.12 (17-May-2010)
Filesystem at /dev/mapper/volume-var is mounted on /var; on-line resizing required
old desc_blocks = 1, new_desc_blocks = 7
Performing an on-line resize of /dev/mapper/volume-var to 26214400 (4k) blocks.
The filesystem on /dev/mapper/volume-var is now 26214400 blocks long.

Then mount the voulme file system

mount /dev/mapper/volume-var /var

You are done with lvm extending.
[Note: For /var and / partitions you will not be able to unmount them but you may resize the partition without unmounting and it is at your own risk of data loss].

		10.2.7
	10.3 cron

		10.3.1 cron(8) - Linux man page

Name

cron - daemon to execute scheduled commands (ISC Cron V4.1)

Synopsis

cron [-n | -p | -m<mailcommand>]
cron -x [ext,sch,proc,pars,load,misc,test,bit]

Description


 
Cron should be started from /etc/rc.d/init.d or /etc/init.d

Cron searches /var/spool/cron for crontab files which are named after accounts in crontabs found are loaded into memory. Cron also searches for /etc/crontab and the files in the directory, which are in a different format (see crontab(5) ). Cron then wakes up every minute, examining all stored crontabs, checking each command to see if it should be run in the current minute. When executing commands, any output is mailed to the owner of the crontab (or to the user named in the MAILTO environment variable in the crontab, if such exists).

Additionally, cron checks each minute to see if its spool directory's modtime (or the modtime on /etc/crontab) has changed, and if it has, cron will then examine the modtime on all crontabs and reload those which have changed. Thus cron need not be restarted whenever a crontab file is modified. Note that the crontab(1) command updates the modtime of the spool directory whenever it changes a crontab.

Daylight Saving Time and other time changes

Local time changes of less than three hours, such as those caused by the start or end of Daylight Saving Time, are handled specially. This only applies to jobs that run at a specific time and jobs that are run with a granularity greater than one hour. Jobs that run more frequently are scheduled normally.
If time has moved forward, those jobs that would have run in the interval that has been skipped will be run immediately. Conversely, if time has moved backward, care is taken to avoid running jobs twice.

Time changes of more than 3 hours are considered to be corrections to the clock or timezone, and the new time is used immediately.

PAM Access Control

On Red Hat systems, crond now supports access control with PAM - see pam(8). A PAM configuration file for crond is installed in /etc/pam.d/crond. crond loads the PAM environment from the pam_env module, but these can be overriden by settings in the crontab file.
Options

-m
This option allows you to specify a shell command string to use for sending cron mail output instead of sendmail(8). This command must accept a fully formatted mail message (with headers) on stdin and send it as a mail message to the recipients specified in the mail headers.

-n

This option changes default behavior causing it to run crond in the foreground. This can be useful when starting it out of init.

-p

Cron permit any crontab, which user set.

-x

With this option is possible to set debug flags.

Signals

On receipt of a SIGHUP , the cron daemon will close and reopen its log file. This is useful in scripts which rotate and age log files. Naturally this is not relevant if cron was built to use syslog(3).

Caveats

In this version of cron , without the -p option, /etc/crontab must not be writable by any user other than root, no crontab files may be links, or linked to by any other file, and no crontab files may be executable, or be writable by any user other than their owner.

See Also

crontab(1), crontab(5), pam(8)

Author

Paul Vixie <vixie@isc.org>
Referenced By

actsync(8), amdump(8), anacron(8), at.deny(5), atd(8), batcher(8), conflict(8), dbclean(8), faxrunq(1), hlfsd(8), in.hosts(5), incrond(8), innxbatch(8), innxmit(8), mirrordir(1), news.daily(8), nntpsend(8), pdumpfs(8), qsf(1), rdiff-backup(1), rnews(1), sge_bootstrap(5), sge_conf(5), tigercron(8), updatedb(8), warnquota(8), ypxfr(8)

		10.3.2 crontab(5) - Linux man page

Name

crontab - tables for driving cron (ISC Cron V4.1)

Description


 
A crontab file contains instructions to the cron(8) daemon of the general form: "run this command at this time on this date". Each user has their own crontab, and commands in any given crontab will be executed as the user who owns the crontab. Uucp and News will usually have their own crontabs, eliminating the need for explicitly running su(1) as part of a cron command.

Blank lines and leading spaces and tabs are ignored. Lines whose first non-space character is a pound-sign (#) are comments, and are ignored. Note that comments are not allowed on the same line as cron commands, since they will be taken to be part of the command. Similarly, comments are not allowed on the same line as environment variable settings.

An active line in a crontab will be either an environment setting or a cron command. An environment setting is of the form,

name = value

where the spaces around the equal-sign (=) are optional, and any subsequent non-leading spaces in value will be part of the value assigned to name. The value string may be placed in quotes (single or double, but matching) to preserve leading or trailing blanks.

Several environment variables are set up automatically by the cron(8) daemon. SHELL is set to /bin/sh, and LOGNAME and HOME are set from the /etc/passwd line of the crontab�s owner. HOME and SHELL may be overridden by settings in the crontab; LOGNAME may not.

(Another note: the LOGNAME variable is sometimes called USER on BSD systems... on these systems, USER will be set also.)

In addition to LOGNAME, HOME, and SHELL, cron(8) will look at MAILTO if it has any reason to send mail as a result of running commands in "this" crontab. If MAILTO is defined (and non-empty), mail is sent to the user so named. If MAILTO is defined but empty (MAILTO=""), no mail will be sent. Otherwise mail is sent to the owner of the crontab. This option is useful if you decide on /bin/mail instead of /usr/lib/sendmail as your mailer when you install cron -- /bin/mail doesn�t do aliasing, and UUCP usually doesn�t read its mail.

By default, cron will send mail using the mail 'Content-Type:' header of 'text/plain' with the 'charset=' parameter set to the charmap / codeset of the locale in which crond(8) is started up - ie. either the default system locale, if no LC_* environment variables are set, or the locale specified by the LC_* environment variables (see locale(7)). You can use different character encodings for mailed cron job output by setting the CONTENT_TYPE and CONTENT_TRANSFER_ENCODING variables in crontabs, to the correct values of the mail headers of those names.

The MLS_LEVEL environment variable provides support for multiple per-job SELinux security contexts in the same crontab. By default, cron jobs execute with the default SELinux security context of the user that created the crontab file. When using multiple security levels and roles, this may not be sufficient, because the same user may be running in a different role or at a different security level. For more about roles and SELinux MLS/MCS see selinux(8) and undermentioned crontab example. You can set MLS_LEVEL to the SELinux security context string specifying the SELinux security context in which you want the job to run, and crond will set the execution context of the or jobs to which the setting applies to the specified context. See also the crontab(1) -s option.

The format of a cron command is very much the V7 standard, with a number of upward-compatible extensions. Each line has five time and date fields, followed by a user name if this is the system crontab file, followed by a command. Commands are executed by cron(8) when the minute, hour, and month of year fields match the current time, and at least one of the two day fields (day of month, or day of week) match the current time (see "Note" below). Note that this means that non-existent times, such as "missing hours" during daylight savings conversion, will never match, causing jobs scheduled during the "missing times" not to be run. Similarly, times that occur more than once (again, during daylight savings conversion) will cause matching jobs to be run twice.

cron(8) examines cron entries once every minute.

The time and date fields are:

field allowed values
-----
--------------

minute

0-59

hour

0-23

day of month

1-31

month

1-12 (or names, see below)

day of week

0-7 (0 or 7 is Sun, or use names)

A field may be an asterisk (*), which always stands for "first-last".
Ranges of numbers are allowed. Ranges are two numbers separated with a hyphen. The specified range is inclusive. For example, 8-11 for an "hours" entry specifies execution at hours 8, 9, 10 and 11.

Lists are allowed. A list is a set of numbers (or ranges) separated by commas. Examples: "1,2,5,9", "0-4,8-12".

Step values can be used in conjunction with ranges. Following a range with "<number>" specifies skips of the number's value through the range. For example, "0-23/2" can be used in the hours field to specify command execution every other hour (the alternative in the V7 standard is "0,2,4,6,8,10,12,14,16,18,20,22"). Steps are also permitted after an asterisk, so if you want to say "every two hours", just use "*/2".

Names can also be used for the "month" and "day of week" fields. Use the first three letters of the particular day or month (case doesn't matter). Ranges or lists of names are not allowed.

The "sixth" field (the rest of the line) specifies the command to be run. The entire command portion of the line, up to a newline or % character, will be executed by /bin/sh or by the shell specified in the SHELL variable of the cronfile. Percent-signs (%) in the command, unless escaped with backslash (\), will be changed into newline characters, and all data after the first % will be sent to the command as standard input.

Note: The day of a command's execution can be specified by two fields - day of month, and day of week. If both fields are restricted (ie, aren't *), the command will be run when either field matches the current time. For example,
"30 4 1,15 * 5" would cause a command to be run at 4:30 am on the 1st and 15th of each month, plus every Friday.

Example Cron File

# use /bin/sh to run commands, no matter what /etc/passwd says
SHELL=/bin/sh
# mail any output to 'paul', no matter whose crontab this is
MAILTO=paul
#
# run five minutes after midnight, every day
5 0 * * *       $HOME/bin/daily.job >> $HOME/tmp/out 2>&1
# run at 2:15pm on the first of every month -- output mailed to paul
15 14 1 * *     $HOME/bin/monthly
# run at 10 pm on weekdays, annoy Joe
0 22 * * 1-5    mail -s "It's 10pm" joe%Joe,%%Where are your kids?%
23 0-23/2 * * * echo "run 23 minutes after midn, 2am, 4am ..., everyday"
5 4 * * sun     echo "run at 5 after 4 every sunday"
SELinux with multi level security (MLS)

In crontab is important specified security level by crontab -s or specifying the required level on the first line of the crontab. Each level is specified in /etc/selinux/targeted/seusers. For using crontab in MLS mode is really important:
- check/change actual role,
- set correct role for directory, which is used for input/output.

Example For Selinux Mls

# login as root
newrole -r sysadm_r
mkdir /tmp/SystemHigh
chcon -l SystemHigh /tmp/SystemHigh
crontab -e
# write in crontab file
MLS_LEVEL=SystemHigh
0-59 * * * * id -Z > /tmp/SystemHigh/crontest
Now if I log in as a normal user it can't work, because /tmp/SystemHigh is
higher than my level.
Files

/etc/crontab system crontab file

See Also

cron(8), crontab(1)

Extensions

When specifying day of week, both day 0 and day 7 will be considered Sunday. BSD and ATT seem to disagree about this.

Lists and ranges are allowed to co-exist in the same field. "1-3,7-9" would be rejected by ATT or BSD cron -- they want to see "1-3" or "7,8,9" ONLY.

Ranges can include "steps", so "1-9/2" is the same as "1,3,5,7,9".

Names of months or days of the week can be specified by name.

Environment variables can be set in the crontab. In BSD or ATT, the environment handed to child processes is basically the one from /etc/rc.

Command output is mailed to the crontab owner (BSD can't do this), can be mailed to a person other than the crontab owner (SysV can't do this), or the feature can be turned off and no mail will be sent at all (SysV can't do this either).

These special time specification "nicknames" are supported, which replace the 5 initial time and date fields, and are prefixed by the '@' character:

@reboot    :    Run once, at startup.
@yearly    :    Run once a year, ie.  "0 0 1 1 *".
@annually  :    Run once a year, ie.  "0 0 1 1 *".
@monthly   :    Run once a month, ie. "0 0 1 * *".
@weekly    :    Run once a week, ie.  "0 0 * * 0".
@daily     :    Run once a day, ie.   "0 0 * * *".
@hourly    :    Run once an hour, ie. "0 * * * *".
Caveats

In this version of cron , /etc/crontab must not be writable by any user other than root. No crontab files may be links, or linked to by any other file. No crontab files may be executable, or be writable by any user other than their owner.

Author

Paul Vixie <vixie@isc.org>
Referenced By

amdump(8), archivemail(1), dirvish.conf(5), ezmlm-cron(1), geoipupdate(1), miau(1), pdumpfs(8), perlpod(1), perlpod(3), perlpodspec(1), perlpodspec(3), snmpd.conf(5), tigercron(8), ypxfr(8)

		10.3.3 Linux Crontab: 15 Awesome Cron Job Examples
by SATHIYAMOORTHY on JUNE 11, 2009
 


An experienced Linux sysadmin knows the importance of running the routine maintenance jobs in the background automatically.

Linux Cron utility is an effective way to schedule a routine background job at a specific time and/or day on an on-going basis.

This article is part of the on-going Productivity Tips For Geeks series. In this article, let us review 15 awesome examples of crontab job scheduling.


Linux Crontab Format

MIN HOUR DOM MON DOW CMD
Table: Crontab Fields and Allowed Ranges (Linux Crontab Syntax)
Field	Description	Allowed Value
MIN	Minute field	0 to 59
HOUR	Hour field	0 to 23
DOM	Day of Month	1-31
MON	Month field	1-12
DOW	Day Of Week	0-6
CMD	Command	Any command to be executed.

			10.3.3.1 Scheduling a Job For a Specific Time Every Day

The basic usage of cron is to execute a job in a specific time as shown below. This will execute the Full backup shell script (full-backup) on 10th June 08:30 AM.

Please note that the time field uses 24 hours format. So, for 8 AM use 8, and for 8 PM use 20.

30 08 10 06 * /home/ramesh/full-backup
30 – 30th Minute
08 – 08 AM
10 – 10th Day
06 – 6th Month (June)
* – Every day of the week

			10.3.3.2 Schedule a Job For More Than One Instance (e.g. Twice a Day)

The following script take a incremental backup twice a day every day.

This example executes the specified incremental backup shell script (incremental-backup) at 11:00 and 16:00 on every day. The comma separated value in a field specifies that the command needs to be executed in all the mentioned time.

00 11,16 * * * /home/ramesh/bin/incremental-backup
00 – 0th Minute (Top of the hour)
11,16 – 11 AM and 4 PM
* – Every day
* – Every month
* – Every day of the week

			10.3.3.3 Schedule a Job for Specific Range of Time (e.g. Only on Weekdays)

If you wanted a job to be scheduled for every hour with in a specific range of time then use the following.

Cron Job everyday during working hours
This example checks the status of the database everyday (including weekends) during the working hours 9 a.m – 6 p.m

00 09-18 * * * /home/ramesh/bin/check-db-status
00 – 0th Minute (Top of the hour)
09-18 – 9 am, 10 am,11 am, 12 am, 1 pm, 2 pm, 3 pm, 4 pm, 5 pm, 6 pm
* – Every day
* – Every month
* – Every day of the week
Cron Job every weekday during working hours
This example checks the status of the database every weekday (i.e excluding Sat and Sun) during the working hours 9 a.m – 6 p.m.

00 09-18 * * 1-5 /home/ramesh/bin/check-db-status
00 – 0th Minute (Top of the hour)
09-18 – 9 am, 10 am,11 am, 12 am, 1 pm, 2 pm, 3 pm, 4 pm, 5 pm, 6 pm
* – Every day
* – Every month
1-5 -Mon, Tue, Wed, Thu and Fri (Every Weekday)

			10.3.3.4 How to View Crontab Entries?

View Current Logged-In User’s Crontab entries
To view your crontab entries type crontab -l from your unix account as shown below.


 
ramesh@dev-db$ crontab -l
@yearly /home/ramesh/annual-maintenance
*/10 * * * * /home/ramesh/check-disk-space

[Note: This displays crontab of the current logged in user]
View Root Crontab entries
Login as root user (su – root) and do crontab -l as shown below.

root@dev-db# crontab -l
no crontab for root
Crontab HowTo: View Other Linux User’s Crontabs entries
To view crontab entries of other Linux users, login to root and use -u {username} -l as shown below.

root@dev-db# crontab -u sathiya -l
@monthly /home/sathiya/monthly-backup
00 09-18 * * * /home/sathiya/check-db-status

			10.3.3.5 How to Edit Crontab Entries?

Edit Current Logged-In User’s Crontab entries
To edit a crontab entries, use crontab -e as shown below. By default this will edit the current logged-in users crontab.

ramesh@dev-db$ crontab -e
@yearly /home/ramesh/centos/bin/annual-maintenance
*/10 * * * * /home/ramesh/debian/bin/check-disk-space
~
"/tmp/crontab.XXXXyjWkHw" 2L, 83C

[Note: This will open the crontab file in Vim editor for editing.
Please note cron created a temporary /tmp/crontab.XX... ]
When you save the above temporary file with :wq, it will save the crontab and display the following message indicating the crontab is successfully modified.

~
"crontab.XXXXyjWkHw" 2L, 83C written
crontab: installing new crontab
Edit Root Crontab entries
Login as root user (su – root) and do crontab -e as shown below.

root@dev-db# crontab -e
Edit Other Linux User’s Crontab File entries
To edit crontab entries of other Linux users, login to root and use -u {username} -e as shown below.

root@dev-db# crontab -u sathiya -e
@monthly /home/sathiya/fedora/bin/monthly-backup
00 09-18 * * * /home/sathiya/ubuntu/bin/check-db-status
~
~
~
"/tmp/crontab.XXXXyjWkHw" 2L, 83C

			10.3.3.6 Schedule a Job for Every Minute Using Cron.

Ideally you may not have a requirement to schedule a job every minute. But understanding this example will will help you understand the other examples mentioned below in this article.

* * * * * CMD
The * means all the possible unit — i.e every minute of every hour through out the year. More than using this * directly, you will find it very useful in the following cases.

When you specify */5 in minute field means every 5 minutes.
When you specify 0-10/2 in minute field mean every 2 minutes in the first 10 minute.
Thus the above convention can be used for all the other 4 fields.

			10.3.3.7 Schedule a Background Cron Job For Every 10 Minutes.

Use the following, if you want to check the disk space every 10 minutes.

*/10 * * * * /home/ramesh/check-disk-space
It executes the specified command check-disk-space every 10 minutes through out the year. But you may have a requirement of executing the command only during office hours or vice versa. The above examples shows how to do those things.

Instead of specifying values in the 5 fields, we can specify it using a single keyword as mentioned below.

There are special cases in which instead of the above 5 fields you can use @ followed by a keyword — such as reboot, midnight, yearly, hourly.

Table: Cron special keywords and its meaning
Keyword	Equivalent
@yearly	0 0 1 1 *
@daily	0 0 * * *
@hourly	0 * * * *
@reboot	Run at startup.

			10.3.3.8 Schedule a Job For First Minute of Every Year using @yearly

If you want a job to be executed on the first minute of every year, then you can use the @yearly cron keyword as shown below.

This will execute the system annual maintenance using annual-maintenance shell script at 00:00 on Jan 1st for every year.

@yearly /home/ramesh/red-hat/bin/annual-maintenance

			10.3.3.9 Schedule a Cron Job Beginning of Every Month using @monthly

It is as similar as the @yearly as above. But executes the command monthly once using @monthly cron keyword.

This will execute the shell script tape-backup at 00:00 on 1st of every month.

@monthly /home/ramesh/suse/bin/tape-backup

			10.3.3.10 Schedule a Background Job Every Day using @daily

Using the @daily cron keyword, this will do a daily log file cleanup using cleanup-logs shell scriptat 00:00 on every day.

@daily /home/ramesh/arch-linux/bin/cleanup-logs "day started"

			10.3.3.11 How to Execute a Linux Command After Every Reboot using @reboot?

Using the @reboot cron keyword, this will execute the specified command once after the machine got booted every time.

@reboot CMD

			10.3.3.12 How to Disable/Redirect the Crontab Mail Output using MAIL keyword?

By default crontab sends the job output to the user who scheduled the job. If you want to redirect the output to a specific user, add or update the MAIL variable in the crontab as shown below.

ramesh@dev-db$ crontab -l
MAIL="ramesh"

@yearly /home/ramesh/annual-maintenance
*/10 * * * * /home/ramesh/check-disk-space

[Note: Crontab of the current logged in user with MAIL variable]

If you wanted the mail not to be sent to anywhere, i.e to stop the crontab output to be emailed, add or update the MAIL variable in the crontab as shown below.

MAIL=""

			10.3.3.13 How to Execute a Linux Cron Jobs Every Second Using Crontab.

You cannot schedule a every-second cronjob. Because in cron the minimum unit you can specify is minute. In a typical scenario, there is no reason for most of us to run any job every second in the system.


			10.3.3.14 Specify PATH Variable in the Crontab

All the above examples we specified absolute path of the Linux command or the shell-script that needs to be executed.

For example, instead of specifying /home/ramesh/tape-backup, if you want to just specify tape-backup, then add the path /home/ramesh to the PATH variable in the crontab as shown below.

ramesh@dev-db$ crontab -l

PATH=/bin:/sbin:/usr/bin:/usr/sbin:/home/ramesh

@yearly annual-maintenance
*/10 * * * * check-disk-space

[Note: Crontab of the current logged in user with PATH variable]

			10.3.3.15 Installing Crontab From a Cron File

Instead of directly editing the crontab file, you can also add all the entries to a cron-file first. Once you have all thoese entries in the file, you can upload or install them to the cron as shown below.

ramesh@dev-db$ crontab -l
no crontab for ramesh

$ cat cron-file.txt
@yearly /home/ramesh/annual-maintenance
*/10 * * * * /home/ramesh/check-disk-space

ramesh@dev-db$ crontab cron-file.txt

ramesh@dev-db$ crontab -l
@yearly /home/ramesh/annual-maintenance
*/10 * * * * /home/ramesh/check-disk-space
Note: This will install the cron-file.txt to your crontab, which will also remove your old cron entries. So, please be careful while uploading cron entries from a cron-file.txt.

		10.3.4 Run a cron job every minute

crontab -e 
*/1 * * * * echo "job every minute"

		10.3.5

	10.4 anacron


		10.4.1 anacron man page
anacron(8) - Linux man page

Name

anacron - runs commands periodically
Synopsis

anacron [-s] [-f] [-n] [-d] [-q] [-t anacrontab] [job] ...
anacron -u [-t anacrontab] [job] ...
anacron [-V|-h]
Description


 Anacron can be used to execute commands periodically, with a frequency specified in days. Unlike cron(8), it does not assume that the machine is running continuously. Hence, it can be used on machines that aren't running 24 hours a day, to control daily, weekly, and monthly jobs that are usually controlled by cron.
When executed, Anacron reads a list of jobs from a configuration file, normally /etc/anacrontab (see anacrontab(5)). This file contains the list of jobs that Anacron controls. Each job entry specifies a period in days, a delay in minutes, a unique job identifier, and a shell command.

For each job, Anacron checks whether this job has been executed in the last n days, where n is the period specified for that job. If not, Anacron runs the job's shell command, after waiting for the number of minutes specified as the delay parameter.

After the command exits, Anacron records the date in a special timestamp file for that job, so it can know when to execute it again. Only the date is used for the time calculations. The hour is not used.

When there are no more jobs to be run, Anacron exits.

Anacron only considers jobs whose identifier, as specified in the anacrontab matches any of the job command-line arguments. The job arguments can be shell wildcard patterns (be sure to protect them from your shell with adequate quoting). Specifying no job arguments, is equivalent to specifying "*" (That is, all jobs will be considered).

Unless the -d option is given (see below), Anacron forks to the background when it starts, and the parent process exits immediately.

Unless the -s or -n options are given, Anacron starts jobs immediately when their delay is over. The execution of different jobs is completely independent.

If a job generates any output on its standard output or standard error, the output is mailed to the user running Anacron (usually root).

Informative messages about what Anacron is doing are sent to syslogd(8) under facility cron, priority notice. Error messages are sent at priority error.

"Active" jobs (i.e. jobs that Anacron already decided to run and now wait for their delay to pass, and jobs that are currently being executed by Anacron), are "locked", so that other copies of Anacron won't run them at the same time.

Options

-f
Force execution of the jobs, ignoring the timestamps.
-u
Only update the timestamps of the jobs, to the current date, but don't run anything.
-s
Serialize execution of jobs. Anacron will not start a new job before the previous one finished.
-n
Run jobs now. Ignore the delay specifications in the /etc/anacrontab file. This options implies -s.
-d
Don't fork to the background. In this mode, Anacron will output informational messages to standard error, as well as to syslog. The output of jobs is mailed as usual.
-q
Suppress messages to standard error. Only applicable with -d.
-t anacrontab
Use specified anacrontab, rather than the default
-V
Print version information, and exit.
-h
Print short usage message, and exit.
Signals

After receiving a SIGUSR1 signal, Anacron waits for running jobs, if any, to finish and then exits. This can be used to stop Anacron cleanly.
Notes

Make sure that the time-zone is set correctly before Anacron is started. (The time-zone affects the date). This is usually accomplished by setting the TZ environment variable, or by installing a /usr/lib/zoneinfo/localtime file. See tzset(3) for more information.
Files

/etc/anacrontab
Contains specifications of jobs. See anacrontab(5) for a complete description.
/var/spool/anacron
This directory is used by Anacron for storing timestamp files.
See Also

anacrontab(5), cron(8), tzset(3)
The Anacron README file.

Bugs

Anacron never removes timestamp files. Remove unused files manually.
Anacron uses up to two file descriptors for each active job. It may run out of descriptors if there are more than about 125 active jobs (on normal kernels).

Mail comments, suggestions and bug reports to Sean 'Shaleh' Perry <shaleh@(debian.org|valinux.com)>.

Author

Anacron was originally conceived and implemented by Christian Schwarz <schwarz@monet.m.isar.de>.
The current implementation is a complete rewrite by Itai Tzur <itzur@actcom.co.il>.

The code base is currently maintained by Sean 'Shaleh' Perry <shaleh@(debian.org|valinux.com)>.

	10.5 NTP, NTPD


		10.5.1  Manual
ntpd - Network Time Protocol (NTP) daemon
giffrom Alice's Adventures in Wonderland, Lewis Carroll

The mushroom knows all the command line options.
Synopsis
ntpd [ -aAbdgLmNPqx ] [ -c conffile ] [ -f driftfile ] [ -g ] [ -k keyfile ] [ -l logfile ] [ -N high ] [ -p pidfile ] [ -r broadcastdelay ] [ -s statsdir ] [ -t key ] [ -v variable ] [ -V variable ] [ -x ]
Description
The ntpd program is an operating system daemon which sets and maintains the system time of day in synchronism with Internet standard time servers. It is a complete implementation of the Network Time Protocol (NTP) version 4, but also retains compatibility with version 3, as defined by RFC-1305, and version 1 and 2, as defined by RFC-1059 and RFC-1119, respectively. ntpd does most computations in 64-bit floating point arithmetic and does relatively clumsy 64-bit fixed point operations only when necessary to preserve the ultimate precision, about 232 picoseconds. While the ultimate precision, is not achievable with ordinary workstations and networks of today, it may be required with future gigahertz CPU clocks and gigabit LANs.
How NTP Operates

The ntpd program operates by exchanging messages with one or more configured servers at designated poll intervals. When started, whether for the first or subsequent times, the program requires several exahanges from the majority of these servers so the signal processing and mitigation algorithms can accumulate and groom the data and set the clock. In order to protect the network from bursts, the initial poll interval for each server is delayed an interval randomized over 0-16s. At the default initial poll interval of 64s, several minutes can elapse before the clock is set. The initial delay to set the clock can be reduced using the iburst keyword with the server configuration command, as described on the Configuration Options page.

Most operating systems and hardware of today incorporate a time-of-year (TOY) chip to maintain the time during periods when the power is off. When the machine is booted, the chip is used to initialize the operating system time. After the machine has synchronized to a NTP server, the operating system corrects the chip from time to time. In case there is no TOY chip or for some reason its time is more than 1000s from the server time, ntpd assumes something must be terribly wrong and the only reliable action is for the operator to intervene and set the clock by hand. This causes ntpd to exit with a panic message to the system log. The -g option overrides this check and the clock will be set to the server time regardless of the chip time. However, and to protect against broken hardware, such as when the CMOS battery fails or the clock counter becomes defective, once the clock has been set, an error greater than 1000s will cause ntpd to exit anyway.

Under ordinariy conditions, ntpd adjusts the clock in small steps so that the timescale is effectively continuous and without discontinuities. Under conditions of extreme network congestion, the roundtrip delay jitter can exceed three seconds and the synchronization distance, which is equal to one-half the roundtrip delay plus error budget terms, can become very large. The ntpd algorithms discard sample offsets exceeding 128 ms, unless the interval during which no sample offset is less than 128 ms exceeds 900s. The first sample after that, no matter what the offset, steps the clock to the indicated time. In practice this reduces the false alarm rate where the clock is stepped in error to a vanishingly low incidence.

As the result of this behavior, once the clock has been set, it very rarely strays more than 128 ms, even under extreme cases of network path congestion and jitter. Sometimes, in particular when ntpd is first started, the error might exceed 128 ms. This may on occasion cause the clock to be set backwards if the local clock time is more than 128 s in the future relative to the server. In some applications, this behavior may be unacceptable. If the -x option is included on the command line, the clock will never be stepped and only slew corrections will be used.

The issues should be carefully explored before deciding to use the -x option. The maximum slew rate possible is limited to 500 parts-per-million (PPM) as a consequence of the correctness principles on which the NTP protocol and algorithm design are based. As a result, the local clock can take a long time to converge to an acceptable offset, about 2,000 s for each second the clock is outside the acceptable range. During this interval the local clock will not be consistent with any other network clock and the system cannot be used for distributed applications that require correctly synchronized network time.

In spite of the above precautions, sometimes when large frequency errors are present the resulting time offsets stray outside the 128-ms range and an eventual step or slew time correction is required. If following such a correction the frequency error is so large that the first sample is outside the acceptable range, ntpd enters the same state as when the ntp.drift file is not present. The intent of this behavior is to quickly correct the frequency and restore operation to the normal tracking mode. In the most extreme cases (time.ien.it comes to mind), there may be occasional step/slew corrections and subsequent frequency corrections. It helps in these cases to use the burst keyword when configuring the server.
Frequency Discipline

The ntpd behavior at startup depends on whether the frequency file, usually ntp.drift, exists. This file contains the latest estimate of clock frequency error. When the ntpd is started and the file does not exist, the ntpd enters a special mode designed to quickly adapt to the particular system clock oscillator time and frequency error. This takes approximately 15 minutes, after which the time and frequency are set to nominal values and the ntpd enters normal mode, where the time and frequency are continuously tracked relative to the server. After one hour the frequency file is created and the current frequency offset written to it. When the ntpd is started and the file does exist, the ntpd frequency is initialized from the file and enters normal mode immediately. After that the current frequency offset is written to the file at hourly intervals.
Operating Modes

ntpd can operate in any of several modes, including symmetric active/passive, client/server broadcast/multicast and manycast, as described in the Association Management page. It normally operates continuously while monitoring for small changes in frequency and trimming the clock for the ultimate precision. However, it can operate in a one-time mode where the time is set from an external server and frequency is set from a previously recorded frequency file. A broadcast/multicast or manycast client can discover remote servers, compute server-client propagation delay correction factors and configure itself automatically. This makes it possible to deploy a fleet of workstations without specifying configuration details specific to the local environment.

By default, ntpd runs in continuous mode where each of possibly several external servers is polled at intervals determined by an intricate state machine. The state machine measures the incidental roundtrip delay jitter and oscillator frequency wander and determines the best poll interval using a heuristic algorithm. Ordinarily, and in most operating environments, the state machine will start with 64s intervals and eventually increase in steps to 1024s. A small amount of random variation is introduced in order to avoid bunching at the servers. In addition, should a server become unreachable for some time, the poll interval is increased in steps to 1024s in order to reduce network overhead.

In some cases it may not be practical for ntpd to run continuously. A common workaround has been to run the ntpdate program from a cron job at designated times. However, this program does not have the crafted signal processing, error checking and mitigation algorithms of ntpd. The -q option is intended for this purpose. Setting this option will cause ntpd to exit just after setting the clock for the first time. The procedure for initially setting the clock is the same as in continuous mode; most applications will probably want to specify the iburst keyword with the server configuration command. With this keyword a volley of messages are exchanged to groom the data and the clock is set in about a minute. If nothing is heard after a couple of minutes, the daemon times out and exits. After a suitable period of mourning, the ntpdate program may be retired.

When kernel support is available to discipline the clock frequency, which is the case for stock Solaris, Tru64, Linux and FreeBSD, a useful feature is available to discipline the clock frequency. First, ntpd is run in continuous mode with selected servers in order to measure and record the intrinsic clock frequency offset in the frequency file. It may take some hours for the frequency and offset to settle down. Then the ntpd is stopped and run in one-time mode as required. At each startup, the frequency is read from the file and initializes the kernel frequency.
Poll Interval Control

This version of NTP includes an intricate state machine to reduce the network load while maintaining a quality of synchronization consistent with the observed jitter and wander. There are a number of ways to tailor the operation in order enhance accuracy by reducing the interval or to reduce network overhead by increasing it. However, the user is advised to carefully consider the consequenses of changing the poll adjustment range from the default minimum of 64 s to the default maximum of 1,024 s. The default minimum can be changed with the tinker minpoll command to a value not less than 16 s. This value is used for all configured associations, unless overriden by the minpoll option on the configuration command. Note that most device drivers will not operate properly if the poll interval is less than 64 s and that the broadcast server and manycast client associations will also use the default, unless overriden.

In some cases involving dial up or toll services, it may be useful to increase the minimum interval to a few tens of minutes and maximum interval to a day or so. Under normal operation conditions, once the clock discipline loop has stabilized the interval will be increased in steps from the minumum to the maximum. However, this assumes the intrinsic clock frequency error is small enough for the discipline loop correct it. The capture range of the loop is 500 PPM at an interval of 64s decreasing by a factor of two for each doubling of interval. At a minimum of 1,024 s, for example, the capture range is only 31 PPM. If the intrinsic error is greater than this, the drift file ntp.drift will have to be specially tailored to reduce the residual error below this limit. Once this is done, the drift file is automatically updated once per hour and is available to initialize the frequency on subsequent daemon restarts.
The huff-n'-puff filter

In scenarios where a considerable amount of data are to be downloaded or uploaded over telephone modems, timekeeping quality can be seriously degraded. This occurs because the differential delays on the two directions of transmission can be quite large. In many cases the apparent time errors are so large as to exceed the step threshold and a step correction can occur during and after the data transfer is in progress.

The huff-n'-puff filter is designed to correct the apparent time offset in these cases. It depends on knowledge of the propagation delay when no other traffic is present. In common scenarios this occurs during other than work hours. The filter maintains a shift register that remembers the minimum delay over the most recent interval measured usually in hours. Under conditions of severe delay, the filter corrects the apparent offset using the sign of the offset and the difference between the apparent delay and minimum delay. The name of the filter reflects the negative (huff) and positive (puff) correction, which depends on the sign of the offset.

The filter is activated by the tinker command and huffpuff keyword, as described in the Miscellaneous Options page.
Notes

If NetInfo support is built into ntpd, then ntpd will attempt to read its configuration from the NetInfo if the default ntp.conf file cannot be read and no file is specified by the -c option.

Various internal ntpd variables can be displayed and configuration options altered while the ntpd is running using the ntpq and ntpdc utility programs.

When ntpd starts it looks at the value of umask, and if zero ntpd will set the umask to 022.
Command Line Options

-a
    Enable authentication mode (default).
-A
    Disable authentication mode.
-b
    Synchronize using NTP broadcast messages.
-c conffile
    Specify the name and path of the configuration file. (Disable netinfo?)
-d
    Specify debugging mode. This flag may occur multiple times, with each occurrence indicating greater detail of display.
-D level
    Specify debugging level directly.
-f driftfile
    Specify the name and path of the drift file.
-g
    Normally, ntpd exits if the offset exceeds the sanity limit, which is 1000 s by default. If the sanity limit is set to zero, no sanity checking is performed and any offset is acceptable. This option overrides the limit and allows the time to be set to any value without restriction; however, this can happen only once. After that, ntpd will exit if the limit is exceeded. This option can be used with the -q option.
-k keyfile
    Specify the name and path of the file containing the NTP authentication keys.
-l logfile
    Specify the name and path of the log file. The default is the system log facility.
-L
    Listen to virtual IPs.
-m
    Synchronize using NTP multicast messages on the IP multicast group address 224.0.1.1 (requires multicast kernel).
-n
    Don't fork.
-N priority
    To the extent permitted by the operating system, run the ntpd at a high priority.
-p pidfile
    Specify the name and path to record the ntpd's process ID.
-P
    Override the priority limit set by the operating system. Not recommended for sissies.
-q
    Exit the ntpd just after the first time the clock is set. This behavior mimics that of the ntpdate program, which is to be retired. The -g and -x options can be used with this option.
-r broadcastdelay
    Specify the default propagation delay from the broadcast/multicast server and this computer. This is necessary only if the delay cannot be computed automatically by the protocol.
-s statsdir
    Specify the directory path for files created by the statistics facility.
-t key
    Add a key number to the trusted key list.
-v variable
-V variable
    Add a system variable listed by default.
-x
    Normally, the time is slewed if the offset is less than the step threshold, which is 128 ms by default, and stepped if above the threshold. This option forces the time to be slewed in all cases. If the step threshold is set to zero, all offsets are stepped, regardless of value and regardless of the -x option. In general, this is not a good idea, as it bypasses the clock state machine which is designed to cope with large time and frequency errors Note: Since the slew rate is limited to 0.5 ms/s, each second of adjustment requires an amortization interval of 2000 s. Thus, an adjustment of many seconds can take hours or days to amortize. This option can be used with the -q option.

The Configuration File

Ordinarily, ntpd reads the ntp.conf configuration file at startup time in order to determine the synchronization sources and operating modes. It is also possible to specify a working, although limited, configuration entirely on the command line, obviating the need for a configuration file. This may be particularly useful when the local host is to be configured as a broadcast/multicast client, with all peers being determined by listening to broadcasts at run time.

Usually, the configuration file is installed in the /etc directory, but could be installed elsewhere (see the -c conffile command line option). The file format is similar to other Unix configuration files - comments begin with a # character and extend to the end of the line; blank lines are ignored.

Configuration commands consist of an initial keyword followed by a list of arguments, some of which may be optional, separated by whitespace. Commands may not be continued over multiple lines. Arguments may be host names, host addresses written in numeric, dotted-quad form, integers, floating point numbers (when specifying times in seconds) and text strings. Optional arguments are delimited by [ ] in the following descriptions, while alternatives are separated by |. The notation [ ... ] means an optional, indefinite repetition of the last item before the [ ... ].

Configuration Options
Authentication Options
Monitoring Options
Access Control Options
Reference Clock Options
Miscellaneous Options
Files
/etc/ntp.conf - the default name of the configuration file
/etc/ntp.drift - the default name of the drift file
/etc/ntp.keys - the default name of the key file 

		10.5.2 NTP Debugging Techniques
Once the NTP software distribution has been compiled and installed and the configuration file constructed, the next step is to verify correct operation and fix any bugs that may result. Usually, the command line that starts the daemon is included in the system startup file, so it is executed only at system boot time; however, the daemon can be stopped and restarted from root at any time. Usually, no command-line arguments are required, unless special actions described in the ntpd page are required. Once started, the daemon will begin sending and receiving messages, as specified in the configuration file.
Initial Startup

The best way to verify correct operation is using the ntpq and ntpdc utility programs, either on the server itself or from another machine elsewhere in the network. The ntpq program implements the management functions specified in the NTP specification RFC-1305, Appendix A. The ntpdc program implements additional functions not provided in the standard. Both programs can be used to inspect the state variables defined in the specification and, in the case of ntpdc, additional ones of interest. In addition, the ntpdc program can be used to selectively reconfigure and enable or disable some functions while the daemon is running.

In extreme cases with elusive bugs, the daemon can operate in two modes, depending on the presence of the -d command-line debug switch. If not present, the daemon detaches from the controlling terminal and proceeds autonomously. If one or more -d switches are present, the daemon does not detach and generates special output useful for debugging. In general, interpretation of this output requires reference to the sources. However, a single -d does produce only mildly cryptic output and can be very useful in finding problems with configuration and network troubles. With a little experience, the volume of output can be reduced by piping the output to grep and specifying the keyword of the trace you want to see.

Some problems are immediately apparent when the daemon first starts running. The most common of these are the lack of a UDP port for NTP (123) in the Unix /etc/services file (or equivalent in some systems). Note that NTP does not use TCP in any form. Other problems are apparent in the system log file. The log file should show the startup banner, some cryptic initialization data and the computed precision value. The next most common problem is incorrect DNS names. Check that each DNS name used in the configuration file exists and that the address responds to the Unix ping command.

When first started, the daemon normally polls the servers listed in the configuration file at 64-s intervals. In order to allow a sufficient number of samples for the NTP algorithms to reliably discriminate between correctly operating servers and possible intruders, at least four valid messages from the majority of servers and peers listed in the configuration file is required before the daemon can set the local clock. However, if the difference between the client time and server time is greater than the panic threshold, which defaults to 1000 s, the daemon will send a message to the system log and shut down without setting the clock. It is necessary to set the local clock to within the panic threshold first, either manually by eyeball and wristwatch and the Unix date command, or by the ntpdate or ntpd -q commands. The panic threshold can be changed by the tinker panic command discribed on the Miscellaneous Options page. The panic threshold can be disabled entirely by the -g command line option described on the ntpd - Network Time Protocol (NTP) daemon page.

If the difference between local time and server time is less than the panic threshold but greater than the step threshold, which defaults to 125 ms, the daemon will perform a step adjustment; otherwise, it will gradually slew the clock to the nominal time. The step threshold can be changed by the tinker step command discribed on the Miscellaneous Options page. The step threshold can be disabled entirely by the -x command line option described on the ntpd - Network Time Protocol (NTP) daemon page. In this case the clock will never be stepped; however, users should understand the implications for doing this in a distributed data network where all processing must be tightly synchronized. See the NTP Timescale and Leap Seconds page for further information. If a step adjustment is made, the clock discipline algorithm will start all over again, requiring another round of at least four messages as before. This is necessary so that all servers and peers operate on the same set of time values.

The clock discipline algorithm is designed to avoid large noise spikes that might occur on a congested network or access line. If an offset sample exceeds the step threshold, it is ignored and a timer started. If a later sample is below the step threshold, the counter is reset. However, if the counter is greater than the stepout interval, which defaults to 900 s, the next sample will step or slew the time as directed. The stepout threshold can be changed by the tinker stepout command discribed on the Miscellaneous Options page.

If, as discussed later on this page, for some reason the hardware clock oscillator frequency error is very large, the time errors upon first startup of the daemon may increase over time until exceeding the step threshold, which requires another step correction. However, due to provisions that reduce vulnerability to noise spikes, the second correction will not be done until after the stepout threshold. When the frequency error is very large, it may take a number of cycles like this until converging on the nominal frequency correction. After this, the correction is written to the ntp.drift file, which is read upon subsequent restarts, so the herky-jerky cycles should not recur.
Verifying Correct Operation

After starting the daemon, run the ntpq program using the -n switch, which will avoid possible distractions due to name resolution problems. Use the pe command to display a billboard showing the status of configured peers and possibly other clients poking the daemon. After operating for a few minutes, the display should be something like:

ntpq> pe
     remote      refid       st t when poll reach delay offset jitter
=====================================================================
-isipc6.cairn.ne .GPS1.        1 u  18  64  377  65.592 -5.891  0.044
+saicpc-isiepc2. pogo.udel.edu 2 u 241 128  370  10.477 -0.117  0.067
+uclpc.cairn.net pogo.udel.edu 2 u  37  64  177 212.111 -0.551  0.187
*pogo.udel.edu   .GPS1.        1 u  95 128  377   0.607  0.123  0.027

The host names or addresses shown in the remote column correspond to the server and peer entries listed in the configuration file; however, the DNS names might not agree if the names listed are not the canonical DNS names. The refid column shows the current source of synchronization, while the st column reveals the stratum, t the type (u = unicast, m = multicast, l = local, - = don't know), and poll the poll interval in seconds. The when column shows the time since the peer was last heard in seconds, while the reach column shows the status of the reachability register (see RFC-1305) in octal. The remaining entries show the latest delay, offset and jitter in milliseconds. Note that in NTP Version 4 what used to be the dispersion column has been replaced by the jitter column.

The tattletale symbol at the left margin displays the synchronization status of each peer. The currently selected peer is marked *, while additional peers designated acceptable for synchronization, but not currently selected, are marked +. Peers marked * and + are included in the weighted average computation to set the local clock; the data produced by peers marked with other symbols are discarded. See the ntpq page for the meaning of these symbols.

Additional details for each peer separately can be determined by the following procedure. First, use the as command to display an index of association identifiers, such as

ntpq> as
ind assID status  conf reach auth condition  last_event cnt
===========================================================
  1 50252  f314   yes   yes   ok    outlyer   reachable  1
  2 50253  f414   yes   yes   ok   candidat   reachable  1
  3 50254  f414   yes   yes   ok   candidat   reachable  1
  4 50255  f614   yes   yes   ok   sys.peer   reachable  1

Each line in this billboard is associated with the corresponding line in the pe billboard above. The assID shows the unique identifier for each mobilized association, while the status column shows the peer status word in hex, as defined in the NTP specification. Next, use the rv command and the respective assID identifier to display a detailed synopsis for the selected peer, such as

ntpq> rv 50253
status=f414 reach, conf, auth, sel_candidat, 1 event, event_reach,
srcadr=saicpc-isiepc2.cairn.net, srcport=123, dstadr=140.173.1.46,
dstport=123, keyid=3816249004, stratum=2, precision=-27,
rootdelay=10.925, rootdispersion=12.848, refid=pogo.udel.edu,
reftime=bd11b225.133e1437  Sat, Jul  8 2000 13:59:01.075, delay=10.550,
offset=-1.357, jitter=0.074, dispersion=1.444, reach=377, valid=7,
hmode=1, pmode=1, hpoll=6, ppoll=7, leap=00, flash=00 ok,
org=bd11b23c.01385836  Sat, Jul  8 2000 13:59:24.004,
rec=bd11b23c.02dc8fb8  Sat, Jul  8 2000 13:59:24.011,
xmt=bd11b21a.ac34c1a8  Sat, Jul  8 2000 13:58:50.672,
filtdelay=   10.45  10.50  10.63  10.40  10.48  10.43  10.49  11.26,
filtoffset=  -1.18  -1.26  -1.26  -1.35  -1.35  -1.42  -1.54  -1.81,
filtdisp=     0.51   1.47   2.46   3.45   4.40   5.34   6.33   7.28,
hostname="miro.time.saic.com", publickey=3171359012, pcookie=0x6629adb2,
hcookie=0x61f99cdb, initsequence=61, initkey=0x287b649c,
timestamp=3172053041

A detailed explanation of the fields in this billboard are beyond the scope of this discussion; however, most variables defined in the NTP Version 3 specification RFC-1305 are available along with others defined for NTP Version 4. This particular example was chosen to illustrate probably the most complex configuration involving symmetric modes and public-key cryptography. As the result of debugging experience, the names and values of these variables may change from time to time. An explanation of the current set is on the ntpq page.

A useful indicator of miscellaneous problems is the flash value, which reveals the state of the various sanity tests on incoming packets. There are currently eleven bits, one for each test, numbered from the right, which is for test 1. If the test fails, the corresponding bit is set to one and zero otherwise. If any bit is set following each processing step, the packet is discarded. The meaning of each test is described on the ntpq page.

The three lines identified as filtdelay, filtoffset and filtdisp reveal the roundtrip delay, clock offset and dispersion for each of the last eight measurement rounds, all in milliseconds. Note that the dispersion, which is an estimate of the error, increases as the age of the sample increases. From these data, it is usually possible to determine the incidence of severe packet loss, network congestion, and unstable local clock oscillators. There are no hard and fast rules here, since every case is unique; however, if one or more of the rounds show large values or change radically from one round to another, the network is probably congested or lossy.

Once the daemon has set the local clock, it will continuously track the discrepancy between local time and NTP time and adjust the local clock accordingly. There are two components of this adjustment, time and frequency. These adjustments are automatically determined by the clock discipline algorithm, which functions as a hybrid phase/frequency feedback loop. The behavior of this algorithm is carefully controlled to minimize residual errors due to network jitter and frequency variations of the local clock hardware oscillator that normally occur in practice. However, when started for the first time, the algorithm may take some time to converge on the intrinsic frequency error of the host machine.

The state of the local clock itself can be determined using the rv command (without the argument), such as

ntpq> rv
status=0644 leap_none, sync_ntp, 4 events, event_peer/strat_chg,
version="ntpd 4.0.99j4-r Fri Jul  7 23:38:17 GMT 2000 (1)",
processor="i386", system="FreeBSD3.4-RELEASE", leap=00, stratum=2,
precision=-27, rootdelay=0.552, rootdispersion=12.532, peer=50255,
refid=pogo.udel.edu,
reftime=bd11b220.ac89f40a  Sat, Jul  8 2000 13:58:56.673, poll=6,
clock=bd11b225.ee201472  Sat, Jul  8 2000 13:59:01.930, state=4,
phase=0.179, frequency=44.298, jitter=0.022, stability=0.001,
hostname="barnstable.udel.edu", publickey=3171372095, params=3171372095,
refresh=3172016539

An explanation about most of these variables is in the RFC-1305 specification. The most useful ones include clock, which shows when the clock was last adjusted, and reftime, which shows when the server clock of refid was last adjusted. The version, processor and system values are very helpful when included in bug reports. The mean millisecond time offset (phase) and deviation (jitter) monitor the clock quality, while the mean PPM frequency offset (frequency) and deviation (stability) monitor the clock stability and serve as a useful diagnostic tool. It has been the experience of NTP operators over the years that these data represent useful environment and hardware alarms. If the motherboard fan freezes up or some hardware bit sticks, the system clock is usually the first to notice it.

Among the new variables added for NTP Version 4 are the hostname, publickey, params and refresh, which are used for the Autokey public-key cryptography described on the Authentication Options page. The values show the filestamps, in NTP seconds, that the associated values were created. These are useful in diagnosing problems with cryptographic key consistency and ordering principles.

When nothing seems to happen in the pe billboard after some minutes, there may be a network problem. One common network problem is an access controlled router on the path to the selected peer or an access controlled server using methods described on the Access Control Options page. Another common problem is that the server is down or running in unsynchronized mode due to a local problem. Use the ntpq program to spy on the server variables in the same way you can spy on your own.

Normally, the daemon will adjust the local clock in small steps in such a way that system and user programs are unaware of its operation. The adjustment process operates continuously as long as the apparent clock error exceeds the step threshold for a period longer than the stepout threshold, which for most Internet paths is a very rare event. If the event is simply an outlyer due to an occasional network delay spike, the correction is simply discarded; however, if the apparent time error persists for longer than the stepout threshold of about 17 minutes, the local clock is stepped or slewed to the new value as directed. This behavior is designed to resist errors due to severely congested network paths, as well as errors due to confused radio clocks upon the epoch of a leap second.
Special Problems

The frequency tolerance of computer clock oscillators can vary widely, which can put a strain on the daemon's ability to compensate for the intrinsic frequency error. While the daemon can handle frequency errors up to 500 parts-per-million (PPM), or 43 seconds per day, values much above 100 PPM reduce the headroom and increase the time to learn the particular value and record it in the ntp.drift file. In extreme cases before the particular oscillator frequency error has been determined, the residual system time offsets can sweep from one extreme to the other of the 128-ms tracking window only for the behavior to repeat at 900-s intervals until the measurements have converged.

In order to determine if excessive frequency error is a problem, observe the nominal filtoffset values for a number of rounds and divide by the poll interval. If the result is something approaching 500 PPM, there is a good chance that NTP will not work properly until the frequency error is reduced by some means. A common cause is the hardware time-of-year (TOY) clock chip, which must be disabled when NTP disciplines the software clock. For some systems this can be done using the tickadj utility and the -s command line argument. For other systems this can be done using a command in the system startup file.

If the TOY chip is not the cause, the problem may be that the hardware clock frequency may simply be too slow or two fast. In some systems this might require tweaking a trimmer capacitor on the motherboard. For other systems the clock frequency can be adjusted in increments of 100 PPM using the tickadj utility and the -t command line argument. Note that the tickadj alters certain kernel variables and, while the utility attempts to figure out an acceptable way to do this, there are many cases where tickadj is incompatible with a running kernel.

Provisions are included in ntpd for access controls which deflect unwanted traffic from selected hosts or networks. The controls described on the Access Control Options include detailed packet filter operations based on source address and address mask. Normally, filtered packets are dropped without notice other than to increment tally counters. However, the server can configure to generate what is called a kiss-of-death (KOD) packet and send to the client. In case of outright access denied, the KOD is the response to the first client packet. In this case the client association is permanently disabled and the access denied bit (test 4) is set in the flash peer variable mentioned above and a message is sent to the system log.

The access control provisions include a limit on the packet rate from a host or network. If an incoming packet exceeds the limit, it is dropped and a KOD sent to the source. If this occurs after the client association has synchronized, the association is not disabled, but a message is sent to the system log. See the Access Control Options page for further informatin.

In some reported scenarios an access line may show low to moderate network delays during some period of the day and moderate to high delays during other periods. Often the delay on one direction of transmission dominates, which can result in large time offset errors, sometimes in the range up to a few seconds. It is not usually convenient to run ntpd throughout the day in such scenarios, since this could result in several time steps, especially if the condition persists for greater than the stepout threshold.

The recommended approach in such scenarios is first to calibrate the local clock frequency error by running ntpd in continuous mode during the quiet interval and let it write the frequency to the ntp.drift file. Then, run ntpd -q from a cron job each day at some time in the quiet interval. In systems with the nanokernel or microkernel performance enhancements, including Solaris, Tru64, Linux and FreeBSD, the kernel continuously disciplines the frequency so that the residual correction produced by ntpd is usually less than a few milliseconds.
Debugging Checklist
If the ntpq or ntpdc programs do not show that messages are being received by the daemon or that received messages do not result in correct synchronization, verify the following:

    Verify the /etc/services file host machine is configured to accept UDP packets on the NTP port 123. NTP is specifically designed to use UDP and does not respond to TCP.
    Check the system log for ntpd messages about configuration errors, name-lookup failures or initialization problems.
    Verify using ping or other utility that packets actually do make the round trip between the client and server. Verify using nslookup or other utility that the DNS server names do exist and resolve to valid Internet addresses.
    Using the ntpdc program, verify that the packets received and packets sent counters are incrementing. If the sent counter does not increment and the configuration file includes configured servers, something may be wrong in the host network or interface configuration. If this counter does increment, but the received counter does not increment, something may be wrong in the network or the server NTP daemon may not be running or the server itself may be down or not responding.
    If both the sent and received counters do increment, but the reach values in the pe billboard with ntpq continues to show zero, received packets are probably being discarded for some reason. If this is the case, the cause should be evident from the flash variable as discussed above and on the ntpq page.
    If the reach values in the pe billboard show the servers are alive and responding, note the tattletale symbols at the left margin, which indicate the status of each server resulting from the various grooming and mitigation algorithms. The interpretation of these symbols is discussed on the ntpq page. After a few minutes of operation, one or another of the reachable server candidates should show a * tattletale symbol. If this doesn't happen, the intersection algorithm, which classifies the servers as truechimers or falsetickers, may be unable to find a majority of truechimers among the server population.
    If all else fails, see the FAQ and/or the discussion and briefings at Network Time Synchronization Project.


		10.5.3 ntpdc - special NTP query program
giffrom Alice's Adventures in Wonderland, Lewis Carroll

This program is a big puppy.
Synopsis
ntpdc [ -ilnps ] [ -c command ] [ host ] [ ... ]
Description
ntpdc is used to query the ntpd daemon about its current state and to request changes in that state. The program may be run either in interactive mode or controlled using command line arguments. Extensive state and statistics information is available through the ntpdc interface. In addition, nearly all the configuration options which can be specified at startup using ntpd's configuration file may also be specified at run time using ntpdc.

If one or more request options are included on the command line when ntpdc is executed, each of the requests will be sent to the NTP servers running on each of the hosts given as command line arguments, or on localhost by default. If no request options are given, ntpdc will attempt to read commands from the standard input and execute these on the NTP server running on the first host given on the command line, again defaulting to localhost when no other host is specified. ntpdc will prompt for commands if the standard input is a terminal device.

ntpdc uses NTP mode 7 packets to communicate with the NTP server, and hence can be used to query any compatable server on the network which permits it. Note that since NTP is a UDP protocol this communication will be somewhat unreliable, especially over large distances in terms of network topology. ntpdc makes no attempt to retransmit requests, and will time requests out if the remote host is not heard from within a suitable timeout time.

The operation of ntpdc are specific to the particular implementation of the ntpd daemon and can be expected to work only with this and maybe some previous versions of the daemon. Requests from a remote ntpdc program which affect the state of the local server must be authenticated, which requires both the remote program and local server share a common key and key identifier.
Command Line Options
Specifying a command line option other than -i or -n will cause the specified query (queries) to be sent to the indicated host(s) immediately. Otherwise, ntpdc will attempt to read interactive format commands from the standard input.

-c command
    The following argument is interpreted as an interactive format command and is added to the list of commands to be executed on the specified host(s). Multiple -c options may be given.
-i
    Force ntpdc to operate in interactive mode. Prompts will be written to the standard output and commands read from the standard input.
-l
    Obtain a list of peers which are known to the server(s). This switch is equivalent to -c listpeers.
-n
    Output all host addresses in dotted-quad numeric format rather than converting to the canonical host names.
-p
    Print a list of the peers known to the server as well as a summary of their state. This is equivalent to -c peers.
-s
    Print a list of the peers known to the server as well as a summary of their state, but in a slightly different format than the -p switch. This is equivalent to -c dmpeers.

Interactive Commands
Interactive format commands consist of a keyword followed by zero to four arguments. Only enough characters of the full keyword to uniquely identify the command need be typed. The output of a command is normally sent to the standard output, but optionally the output of individual commands may be sent to a file by appending a <, followed by a file name, to the command line.

A number of interactive format commands are executed entirely within the ntpdc program itself and do not result in NTP mode 7 requests being sent to a server. These are described following.

? [ command_keyword ]
help [ command_keyword ]
    A ? by itself will print a list of all the command keywords known to this incarnation of ntpq. A ? followed by a command keyword will print funcation and usage information about the command. This command is probably a better source of information about ntpq than this manual page.
delay milliseconds
    Specify a time interval to be added to timestamps included in requests which require authentication. This is used to enable (unreliable) server reconfiguration over long delay network paths or between machines whose clocks are unsynchronized. Actually the server does not now require timestamps in authenticated requests, so this command may be obsolete.
host hostname
    Set the host to which future queries will be sent. Hostname may be either a host name or a numeric address.
hostnames [ yes | no ]
    If yes is specified, host names are printed in information displays. If no is specified, numeric addresses are printed instead. The default is yes, unless modified using the command line -n switch.
keyid keyid
    This command allows the specification of a key number to be used to authenticate configuration requests. This must correspond to a key number the server has been configured to use for this purpose.
quit
    Exit ntpdc.
passwd
    This command prompts you to type in a password (which will not be echoed) which will be used to authenticate configuration requests. The password must correspond to the key configured for use by the NTP server for this purpose if such requests are to be successful.
timeout millseconds
    Specify a timeout period for responses to server queries. The default is about 8000 milliseconds. Note that since ntpdc retries each query once after a timeout, the total waiting time for a timeout will be twice the timeout value set.

Control Message Commands
Query commands result in NTP mode 7 packets containing requests for information being sent to the server. These are read-only commands in that they make no modification of the server configuration state.

listpeers
    Obtains and prints a brief list of the peers for which the server is maintaining state. These should include all configured peer associations as well as those peers whose stratum is such that they are considered by the server to be possible future synchonization candidates.
peers
    Obtains a list of peers for which the server is maintaining state, along with a summary of that state. Summary information includes the address of the remote peer, the local interface address (0.0.0.0 if a local address has yet to be determined), the stratum of the remote peer (a stratum of 16 indicates the remote peer is unsynchronized), the polling interval, in seconds, the reachability register, in octal, and the current estimated delay, offset and dispersion of the peer, all in seconds.

    The character in the left margin indicates the mode this peer entry is operating in. A + denotes symmetric active, a - indicates symmetric passive, a = means the remote server is being polled in client mode, a ^ indicates that the server is broadcasting to this address, a ~ denotes that the remote peer is sending broadcasts and a * marks the peer the server is currently synchonizing to.

    The contents of the host field may be one of four forms. It may be a host name, an IP address, a reference clock implementation name with its parameter or REFCLK(implementation number, parameter). On hostnames no only IP-addresses will be displayed.
dmpeers
    A slightly different peer summary list. Identical to the output of the peers command, except for the character in the leftmost column. Characters only appear beside peers which were included in the final stage of the clock selection algorithm. A . indicates that this peer was cast off in the falseticker detection, while a + indicates that the peer made it through. A * denotes the peer the server is currently synchronizing with.
showpeer peer_address [...]
    Shows a detailed display of the current peer variables for one or more peers. Most of these values are described in the NTP Version 2 specification.
pstats peer_address [...]
    Show per-peer statistic counters associated with the specified peer(s).
clockinfo clock_peer_address [...]
    Obtain and print information concerning a peer clock. The values obtained provide information on the setting of fudge factors and other clock performance information.
kerninfo
    Obtain and print kernel phase-lock loop operating parameters. This information is available only if the kernel has been specially modified for a precision timekeeping function.
loopinfo [ oneline | multiline ]
    Print the values of selected loop filter variables. The loop filter is the part of NTP which deals with adjusting the local system clock. The offset is the last offset given to the loop filter by the packet processing code. The frequency is the frequency error of the local clock in parts-per-million (ppm). The time_const controls the stiffness of the phase-lock loop and thus the speed at which it can adapt to oscillator drift. The watchdog timer value is the number of seconds which have elapsed since the last sample offset was given to the loop filter. The oneline and multiline options specify the format in which this information is to be printed, with multiline as the default.
sysinfo
    Print a variety of system state variables, i.e., state related to the local server. All except the last four lines are described in the NTP Version 3 specification, RFC-1305.

    The system flags show various system flags, some of which can be set and cleared by the enable and disable configuration commands, respectively. These are the auth, bclient, monitor, pll, pps and stats flags. See the ntpd documentation for the meaning of these flags. There are two additional flags which are read only, the kernel_pll and kernel_pps. These flags indicate the synchronization status when the precision time kernel modifications are in use. The kernel_pll indicates that the local clock is being disciplined by the kernel, while the kernel_pps indicates the kernel discipline is provided by the PPS signal.

    The stability is the residual frequency error remaining afterthe system frequency correction is applied and is intended for maintenance and debugging. In most architectures, this value will initially decrease from as high as 500 ppm to a nominal value in the range .01 to 0.1 ppm. If it remains high for some time after starting the daemon, something may be wrong with the local clock, or the value of the kernel variable tick may be incorrect.

    The broadcastdelay shows the default broadcast delay, as set by the broadcastdelay configuration command.

    The authdelay shows the default authentication delay, as set by the authdelay configuration command.
sysstats
    Print statistics counters maintained in the protocol module.
memstats
    Print statistics counters related to memory allocation code.
iostats
    Print statistics counters maintained in the input-output module.
timerstats
    Print statistics counters maintained in the timer/event queue support code.
reslist
    Obtain and print the server's restriction list. This list is (usually) printed in sorted order and may help to understand how the restrictions are applied.
monlist [ version ]
    Obtain and print traffic counts collected and maintained by the monitor facility. The version number should not normally need to be specified.
clkbug clock_peer_address [...]
    Obtain debugging information for a reference clock driver. This information is provided only by some clock drivers and is mostly undecodable without a copy of the driver source in hand.

Runtime Configuration Requests
All requests which cause state changes in the server are authenticated by the server using a configured NTP key (the facility can also be disabled by the server by not configuring a key). The key number and the corresponding key must also be made known to xtnpdc. This can be done using the keyid and passwd commands, the latter of which will prompt at the terminal for a password to use as the encryption key. You will also be prompted automatically for both the key number and password the first time a command which would result in an authenticated request to the server is given. Authentication not only provides verification that the requester has permission to make such changes, but also gives an extra degree of protection again transmission errors.

Authenticated requests always include a timestamp in the packet data, which is included in the computation of the authentication code. This timestamp is compared by the server to its receive time stamp. If they differ by more than a small amount the request is rejected. This is done for two reasons. First, it makes simple replay attacks on the server, by someone who might be able to overhear traffic on your LAN, much more difficult. Second, it makes it more difficult to request configuration changes to your server from topologically remote hosts. While the reconfiguration facility will work well with a server on the local host, and may work adequately between time-synchronized hosts on the same LAN, it will work very poorly for more distant hosts. As such, if reasonable passwords are chosen, care is taken in the distribution and protection of keys and appropriate source address restrictions are applied, the run time reconfiguration facility should provide an adequate level of security.

The following commands all make authenticated requests.

addpeer peer_address [ keyid ] [ version ] [ prefer ]
    Add a configured peer association at the given address and operating in symmetric active mode. Note that an existing association with the same peer may be deleted when this command is executed, or may simply be converted to conform to the new configuration, as appropriate. If the optional keyid is a nonzero integer, all outgoing packets to the remote server will have an authentication field attached encrypted with this key. If the value is 0 (or not given) no authentication will be done. The version# can be 1, 2 or 3 and defaults to 3. The prefer keyword indicates a preferred peer (and thus will be used primarily for clock synchronisation if possible). The preferred peer also determines the validity of the PPS signal - if the preferred peer is suitable for synchronisation so is the PPS signal.
addserver peer_address [ keyid ] [ version ] [ prefer ]
    Identical to the addpeer command, except that the operating mode is client.
broadcast peer_address [ keyid ] [ version ] [ prefer ]
    Identical to the addpeer command, except that the operating mode is broadcast. In this case a valid key identifier and key are required. The peer_address parameter can be the broadcast address of the local network or a multicast group address assigned to NTP. If a multicast address, a multicast-capable kernel is required.
unconfig peer_address [...]
    This command causes the configured bit to be removed from the specified peer(s). In many cases this will cause the peer association to be deleted. When appropriate, however, the association may persist in an unconfigured mode if the remote peer is willing to continue on in this fashion.
fudge peer_address [ time1 ] [ time2 ] [ stratum ] [ refid ]
    This command provides a way to set certain data for a reference clock. See the source listing for further information.
enable [ flag ] [ ... ]
disable [ flag ] [ ... ]
    These commands operate in the same way as the enable and disable configuration file commands of ntpd. Following is a description of the flags. Note that only the auth, bclient, monitor, pll, pps and stats flags can be set by ntpdc; the pll_kernel and pps_kernel flags are read-only.

    auth
        Enables the server to synchronize with unconfigured peers only if the peer has been correctly authenticated using a trusted key and key identifier. The default for this flag is enable.
    bclient
        Enables the server to listen for a message from a broadcast or multicast server, as in the multicastclient command with default address. The default for this flag is disable.
    monitor
        Enables the monitoring facility. See the ntpdc program and the monlist command or further information. The default for this flag is enable.
    pll
        Enables the server to adjust its local clock by means of NTP. If disabled, the local clock free-runs at its intrinsic time and frequency offset. This flag is useful in case the local clock is controlled by some other device or protocol and NTP is used only to provide synchronization to other clients. In this case, the local clock driver is used. See the Reference Clock Drivers page for further information. The default for this flag is enable.
    pps
        Enables the pulse-per-second (PPS) signal when frequency and time is disciplined by the precision time kernel modifications. See the A Kernel Model for Precision Timekeeping page for further information. The default for this flag is disable.
    stats
        Enables the statistics facility. See the Monitoring Options page for further information. The default for this flag is enable.
    pll_kernel
        When the precision time kernel modifications are installed, this indicates the kernel controls the clock discipline; otherwise, the daemon controls the clock discipline.
    pps_kernel
        When the precision time kernel modifications are installed and a pulse-per-second (PPS) signal is available, this indicates the PPS signal controls the clock discipline; otherwise, the daemon or kernel controls the clock discipline, as indicated by the pll_kernel flag.

restrict address mask flag [ flag ]
    This command operates in the same way as the restrict configuration file commands of ntpd.
unrestrict address mask flag [ flag ]
    Unrestrict the matching entry from the restrict list.
delrestrict address mask [ ntpport ]
    Delete the matching entry from the restrict list.
readkeys
    Causes the current set of authentication keys to be purged and a new set to be obtained by rereading the keys file (which must have been specified in the ntpd configuration file). This allows encryption keys to be changed without restarting the server.
trustedkey keyid [...]
untrustedkey keyid [...]
    These commands operate in the same way as the trustedkey and untrustedkey configuration file commands of ntpd.
authinfo
    Returns information concerning the authentication module, including known keys and counts of encryptions and decryptions which have been done.
traps
    Display the traps set in the server. See the source listing for further information.
addtrap [ address [ port ] [ interface ]
    Set a trap for asynchronous messages. See the source listing for further information.
clrtrap [ address [ port ] [ interface]
    Clear a trap for asynchronous messages. See the source listing for further information.
reset
    Clear the statistics counters in various modules of the server. See the source listing for further information. 

		10.5.4 NAME ntp.conf -- Network Time Protocol (NTP) daemon configuration file


SYNOPSIS

     /etc/ntp.conf


DESCRIPTION

     The ntp.conf configuration file is read at initial startup by the ntpd(8)
     daemon in order to specify the synchronization sources, modes and other
     related information.  Usually, it is installed in the /etc directory, but
     could be installed elsewhere (see the daemon's -c command line option).

     The file format is similar to other UNIX configuration files.  Comments
     begin with a `#' character and extend to the end of the line; blank lines
     are ignored.  Configuration commands consist of an initial keyword fol-
     lowed by a list of arguments, some of which may be optional, separated by
     whitespace.  Commands may not be continued over multiple lines.  Argu-
     ments may be host names, host addresses written in numeric, dotted-quad
     form, integers, floating point numbers (when specifying times in seconds)
     and text strings.

     The rest of this page describes the configuration and control options.
     The "Notes on Configuring NTP and Setting up a NTP Subnet" page (avail-
     able as part of the HTML documentation provided in /usr/share/doc/ntp)
     contains an extended discussion of these options.  In addition to the
     discussion of general Configuration Support, there are sections describ-
     ing the following supported functionality and the options used to control
     it:

           o   Authentication Support

           o   Monitoring Support

           o   Access Control Support

           o   Reference Clock Support

     Following these is a section describing Miscellaneous Options.  While
     there is a rich set of options available, the only required option is one
     or more server, peer, broadcast or manycastclient commands.


Configuration Support

     Following is a description of the configuration commands in NTPv4.  These
     commands have the same basic functions as in NTPv3 and in some cases new
     functions and new arguments.  There are two classes of commands: configu-
     ration commands that configure a persistent association with a remote
     server or peer or reference clock, and auxiliary commands that specify
     environmental variables that control various related operations.

   Configuration Commands
     The various modes are determined by the command keyword and the type of
     the required IP address.  Addresses are classed by type as (s) a remote
     server or peer (IP class A, B and C), (b) the broadcast address of a
     local interface, (m) a multicast address (IP class D), or (r) a reference
     clock address (127.127.x.x).  Note that only those options applicable to
     each command are listed below.  Use of options not listed may not be
     caught as an error, but may result in some weird and even destructive
     behavior.

     server address [key key | autokey] [burst] [iburst] [version version]
             [prefer] [minpoll minpoll] [maxpoll maxpoll] [noselect] [preempt]

     peer address [key key | autokey] [version version] [prefer] [minpoll
             minpoll] [maxpoll maxpoll] [noselect]

     broadcast address [key key | autokey] [version version] [prefer] [minpoll
             minpoll] [ttl ttl]

     manycastclient address [key key | autokey] [version version] [prefer]
             [minpoll minpoll] [maxpoll maxpoll] [ttl ttl]

     These four commands specify the time server name or address to be used
     and the mode in which to operate.  The address can be either a DNS name
     or an IP address in dotted-quad notation.  Additional information on
     association behavior can be found in the "Association Management" page.

     server  For type s and r addresses, this command mobilizes a persistent
             client mode association with the specified remote server or local
             radio clock.  In this mode the local clock can synchronized to
             the remote server, but the remote server can never be synchro-
             nized to the local clock.  This command should not be used for
             type b or m addresses.

     peer    For type s addresses (only), this command mobilizes a persistent
             symmetric-active mode association with the specified remote peer.
             In this mode the local clock can be synchronized to the remote
             peer or the remote peer can be synchronized to the local clock.
             This is useful in a network of servers where, depending on vari-
             ous failure scenarios, either the local or remote peer may be the
             better source of time.  This command should NOT be used for type
             b, m or r addresses.

     broadcast
             For type b and m addresses (only), this command mobilizes a per-
             sistent broadcast mode association.  Multiple commands can be
             used to specify multiple local broadcast interfaces (subnets)
             and/or multiple multicast groups.  Note that local broadcast mes-
             sages go only to the interface associated with the subnet speci-
             fied, but multicast messages go to all interfaces.

             In broadcast mode the local server sends periodic broadcast mes-
             sages to a client population at the address specified, which is
             usually the broadcast address on (one of) the local network(s) or
             a multicast address assigned to NTP.  The IANA has assigned the
             multicast group address 224.0.1.1 exclusively to NTP, but other
             nonconflicting addresses can be used to contain the messages
             within administrative boundaries.  Ordinarily, this specification
             applies only to the local server operating as a sender; for oper-
             ation as a broadcast client, see the broadcastclient or
             multicastclient commands below.

     manycastclient
             For type m addresses (only), this command mobilizes a manycast
             client mode association for the multicast address specified.  In
             this case a specific address must be supplied which matches the
             address used on the manycastserver command for the designated
             manycast servers.  The NTP multicast address 224.0.1.1 assigned
             by the IANA should NOT be used, unless specific means are taken
             to avoid spraying large areas of the Internet with these messages
             and causing a possibly massive implosion of replies at the
             sender.

             The manycastserver command specifies that the local server is to
             operate in client mode with the remote servers that are discov-
             ered as the result of broadcast/multicast messages.  The client
             broadcasts a request message to the group address associated with
             the specified address and specifically enabled servers respond to
             these messages.  The client selects the servers providing the
             best time and continues as with the server command.  The remain-
             ing servers are discarded as if never heard.

     Options:

     autokey
             All packets sent to and received from the server or peer are to
             include authentication fields encrypted using the autokey scheme
             described in Authentication Support.

     burst   when the server is reachable and at each poll interval, send a
             burst of eight packets instead of the usual one packet.  The
             spacing between the first and the second packets is about 16s to
             allow a modem call to complete, while the spacing between the
             remaining packets is about 2s.  This is designed to improve time-
             keeping quality with the server command and s addresses.

     iburst  When the server is unreachable and at each poll interval, send a
             burst of eight packets instead of the usual one.  As long as the
             server is unreachable, the spacing between packets is about 16s
             to allow a modem call to complete.  Once the server is reachable,
             the spacing between packets is about 2s.  This is designed to
             speed the initial synchronization acquisition with the server
             command and s addresses and when ntpd(8) is started with the -q
             option.

     key key
             All packets sent to and received from the server or peer are to
             include authentication fields encrypted using the specified key
             identifier with values from 1 to 65534, inclusive.  The default
             is to include no encryption field.

     minpoll minpoll

     maxpoll maxpoll
             These options specify the minimum and maximum poll intervals for
             NTP messages, in seconds to the power of two.  The maximum poll
             interval defaults to 10 (1,024 s), but can be increased by the
             maxpoll option to an upper limit of 17 (36.4 h).  The minimum
             poll interval defaults to 6 (64 s), but can be decreased by the
             minpoll option to a lower limit of 4 (16 s).

     noselect
             Marks the server as unused, except for display purposes. The
             server is discarded by the selection algorithm. This option is
             valid only with the server and peer commands.

     preempt
             Specifies the association as preemptable rather than the default
             persistent. This option is valied only with the server command.

     prefer  Marks the server as preferred.  All other things being equal,
             this host will be chosen for synchronization among a set of cor-
             rectly operating hosts.

     true    Force the association to assume truechimer status; that is,
             always survive the selection and clustering algorithms. This
             option can be used with any association, but is most useful for
             reference clocks with large jitter on the serial port and preci-
             sion pulse-per-second (PPS) signals. Caution: this option defeats
             the algorithms designed to cast out falsetickers and can allow
             these sources to set the system clock. This option is valid only
             with the server and peer commands.  See the "Mitigation Rules and
             the prefer Keyword" page for further information.

     ttl ttl
             This option is used only with broadcast server and manycast
             client modes.  It specifies the time-to-live ttl to use on broad-
             cast server and multicast server and the maximum ttl for the
             expanding ring search with manycast client packets.  Selection of
             the proper value, which defaults to 127, is something of a black
             art and should be coordinated with the network administrator.

     version version
             Specifies the version number to be used for outgoing NTP packets.
             Versions 1-4 are the choices, with version 4 the default.

   Auxiliary Commands
     broadcastclient
             This command enables reception of broadcast server messages to
             any local interface (type b) address.  Upon receiving a message
             for the first time, the broadcast client measures the nominal
             server propagation delay using a brief client/server exchange
             with the server, then enters the broadcast client mode, in which
             it synchronizes to succeeding broadcast messages.  Note that, in
             order to avoid accidental or malicious disruption in this mode,
             both the server and client should operate using symmetric-key or
             public-key authentication as described in Authentication Support.

     manycastserver address ...
             This command enables reception of manycast client messages to the
             multicast group address(es) (type m) specified.  At least one
             address is required, but the NTP multicast address 224.0.1.1
             assigned by the IANA should NOT be used, unless specific means
             are taken to limit the span of the reply and avoid a possibly
             massive implosion at the original sender.  Note that, in order to
             avoid accidental or malicious disruption in this mode, both the
             server and client should operate using symmetric-key or public-
             key authentication as described in Authentication Support.

     multicastclient address ...
             This command enables reception of multicast server messages to
             the multicast group address(es) (type m) specified.  Upon receiv-
             ing a message for the first time, the multicast client measures
             the nominal server propagation delay using a brief client/server
             exchange with the server, then enters the broadcast client mode,
             in which it synchronizes to succeeding multicast messages.  Note
             that, in order to avoid accidental or malicious disruption in
             this mode, both the server and client should operate using sym-
             metric-key or public-key authentication as described in
             Authentication Support.


Authentication Support

     Authentication support allows the NTP client to verify that the server is
     in fact known and trusted and not an intruder intending accidentally or
     on purpose to masquerade as that server.  The NTPv3 specification
     RFC-1305 defines a scheme which provides cryptographic authentication of
     received NTP packets.  Originally, this was done using the Data Encryp-
     tion Standard (DES) algorithm operating in Cipher Block Chaining (CBC)
     mode, commonly called DES-CBC.  Subsequently, this was augmented by the
     RSA Message Digest 5 (MD5) algorithm using a private key, commonly called
     keyed-MD5.  Either algorithm computes a message digest, or one-way hash,
     which can be used to verify the server has the correct private key and
     key identifier.

     NTPv4 retains the NTPv3 schemes, properly described as symmetric-key
     cryptography and, in addition, provides a new Autokey scheme based on
     public-key cryptography.  Public-key cryptography is generally considered
     more secure than symmetric-key cryptography, since the security is based
     on a private value which is generated by each server and never revealed.
     With Autokey all key distribution and management functions involve only
     public values, which considerably simplifies key distribution and stor-
     age.

     Authentication is configured separately for each association using the
     key or autokey subcommands on the peer, server, broadcast and
     manycastclient commands as described in Configuration Options.  The
     authentication options described below specify the suite of keys, select
     the key for each configured association and manage the configuration
     operations.

     The auth flag controls whether new associations or remote configuration
     commands require cryptographic authentication.  This flag can be set or
     reset by the enable and disable configuration commands and also by remote
     configuration commands sent by a ntpdc(8) program running in another
     machine.  If this flag is enabled, which is the default case, new broad-
     cast client and symmetric passive associations and remote configuration
     commands must be cryptographically authenticated using either symmetric-
     key or public-key schemes.  If this flag is disabled, these operations
     are effective even if not cryptographic authenticated.  It should be
     understood that operating in the latter mode invites a significant vul-
     nerability where a rogue hacker can seriously disrupt client timekeeping.

     In networks with firewalls and large numbers of broadcast clients it may
     be acceptable to disable authentication, since that avoids key distribu-
     tion and simplifies network maintenance.  However, when the configuration
     file contains host names, or when a server or client is configured
     remotely, host names are resolved using the DNS and a separate name reso-
     lution process.  In order to protect against bogus name server messages,
     name resolution messages are authenticated using an internally generated
     key which is normally invisible to the user.  However, if cryptographic
     support is disabled, the name resolution process will fail.  This can be
     avoided either by specifying IP addresses instead of host names, which is
     generally inadvisable, or by enabling the flag for name resolution and
     disabled it once the name resolution process is complete.

     An attractive alternative where multicast support is available is many-
     cast mode, in which clients periodically troll for servers.  Crypto-
     graphic authentication in this mode uses public-key schemes as described
     below.  The principle advantage of this manycast mode is that potential
     servers need not be configured in advance, since the client finds them
     during regular operation, and the configuration files for all clients can
     be identical.

     In addition to the default symmetric-key cryptographic support, support
     for public-key cryptography is available if the requisite rsaref20 soft-
     ware distribution has been installed before building the distribution.
     Public-key cryptography provides secure authentication of servers without
     compromising accuracy and stability.  The security model and protocol
     schemes for both symmetric-key and public-key cryptography are described
     below.

   Symmetric-Key Scheme
     The original RFC-1305 specification allows any one of possibly 65,534
     keys, each distinguished by a 32-bit key identifier, to authenticate an
     association.  The servers and clients involved must agree on the key and
     key identifier to authenticate their messages.  Keys and related informa-
     tion are specified in a key file, usually called ntp.keys, which should
     be exchanged and stored using secure procedures beyond the scope of the
     NTP protocol itself.  Besides the keys used for ordinary NTP associa-
     tions, additional keys can be used as passwords for the ntpq(8) and
     ntpdc(8) utility programs.

     When ntpd(8) is first started, it reads the key file specified in the
     keys command and installs the keys in the key cache.  However, the keys
     must be activated with the trusted command before use.  This allows, for
     instance, the installation of possibly several batches of keys and then
     activating or deactivating each batch remotely using ntpdc(8).  This also
     provides a revocation capability that can be used if a key becomes com-
     promised.  The requestkey command selects the key used as the password
     for the ntpdc(8) utility, while the controlkey command selects the key
     used as the password for the ntpq(8) utility.

   Public-Key Scheme
     The original NTPv3 authentication scheme described in RFC-1305 continues
     to be supported; however, in NTPv4 an additional authentication scheme
     called Autokey is available.  It uses MD5 message digest, RSA public-key
     signature and Diffie-Hellman key agreement algorithms available from sev-
     eral sources, but not included in the NTPv4 software distribution.  In
     order to be effective, the rsaref20 package must be installed as
     described in the README.rsa file.  Once installed, the configure and
     build process automatically detects it and compiles the routines
     required.

     The Autokey scheme has several modes of operation corresponding to the
     various NTP modes supported.  RSA signatures with timestamps are used in
     all modes to verify the source of cryptographic values.  All modes use a
     special cookie which can be computed independently by the client and
     server.  In symmetric modes the cookie is constructed using the Diffie-
     Hellman key agreement algorithm.  In other modes the cookie is con-
     structed from the IP addresses and a private value known only to the
     server.  All modes use in addition a variant of the S-KEY scheme, in
     which a pseudo-random key list is generated and used in reverse order.
     These schemes are described along with an executive summary, current sta-
     tus, briefing slides and reading list, in the "Autonomous Authentication"
     page.

     The cryptographic values used by the Autokey scheme are incorporated as a
     set of files generated by the ntp-genkeys(8) program, including the sym-
     metric private keys, public/private key pair, and the agreement parame-
     ters.  See the ntp.keys(5) page for a description of the formats of these
     files.  They contain cryptographic values generated by the algorithms of
     the rsaref20 package and are in printable ASCII format.  All file names
     include the timestamp, in NTP seconds, following the default names given
     below.  Since the file data are derived from random values seeded by the
     system clock and the file name includes the timestamp, every generation
     produces a different file and different file name.

     The ntp.keys file contains the DES/MD5 private keys.  It must be distrib-
     uted by secure means to other servers and clients sharing the same secu-
     rity compartment and made visible only to root.  While this file is not
     used with the Autokey scheme, it is needed to authenticate some remote
     configuration commands used by the ntpdc(8), ntpq(8) utilities.  The
     ntpkey file contains the RSA private key.  It is useful only to the
     machine that generated it and never shared with any other daemon or
     application program, so must be made visible only to root.

     The ntp_dh file contains the agreement parameters, which are used only in
     symmetric (active and passive) modes.  It is necessary that both peers
     beginning a symmetric-mode association share the same parameters, but it
     does not matter which ntp_dh file generates them.  If one of the peers
     contains the parameters, the other peer obtains them using the Autokey
     protocol.  If both peers contain the parameters, the most recent copy is
     used by both peers.  If a peer does not have the parameters, they will be
     requested by all associations, either configured or not; but, none of the
     associations can proceed until one of them has received the parameters.
     Once loaded, the parameters can be provided on request to other clients
     and servers.  The ntp_dh file can be also be distributed using insecure
     means, since the data are public values.

     The ntpkey_host file contains the RSA public key, where host is the name
     of the host.  Each host must have its own ntpkey_host file, which is nor-
     mally provided to other hosts using the Autokey protocol.  Each server or
     peer association requires the public key associated with the particular
     server or peer to be loaded either directly from a local file or indi-
     rectly from the server using the Autokey protocol.  These files can be
     widely distributed and stored using insecure means, since the data are
     public values.

     The optional ntpkey_certif_host file contains the PKI certificate for the
     host.  This provides a binding between the host hame and RSA public key.
     In the current implementation the certificate is obtained by a client, if
     present, but the contents are ignored.

     Due to the widespread use of interface-specific naming, the host names
     used in configured and mobilized associations are determined by the UNIX
     gethostname(3) library routine.  Both the ntp-genkeys(8) program and the
     Autokey protocol derive the name of the public key file using the name
     returned by this routine.  While every server and client is required to
     load their own public and private keys, the public keys for each client
     or peer association can be obtained from the server or peer using the
     Autokey protocol.  Note however, that at the current stage of development
     the authenticity of the server or peer and the cryptographic binding of
     the server name, address and public key is not yet established by a cer-
     tificate authority or web of trust.

   Leapseconds Table
     The NIST provides a table showing the epoch for all historic occasions of
     leap second insertion since 1972.  The leapsecond table shows each epoch
     of insertion along with the offset of International Atomic Time (TAI)
     with respect to Coordinated Universal Time (UTC), as disseminated by NTP.
     The table can be obtained directly from NIST national time servers using
     FTP as the ASCII file pub/leap-seconds.

     While not strictly a security function, the Autokey scheme provides means
     to securely retrieve the leapsecond table from a server or peer.  Servers
     load the leapsecond table directly from the file specified in the crypto
     command, while clients can load the table indirectly from the servers
     using the Autokey protocol.  Once loaded, the table can be provided on
     request to other clients and servers.

   Key Management
     All key files are installed by default in /usr/local/etc, which is nor-
     mally in a shared file system in NFS-mounted networks and avoids
     installing them in each machine separately.  The default can be overrid-
     den by the keysdir configuration command.  However, this is not a good
     place to install the private key file, since each machine needs its own
     file.  A suitable place to install it is in /etc, which is normally not
     in a shared file system.

     The recommended practice is to keep the timestamp extensions when
     installing a file and to install a link from the default name (without
     the timestamp extension) to the actual file.  This allows new file gener-
     ations to be activated simply by changing the link.  However, ntpd(8)
     parses the link name when present to extract the extension value and
     sends it along with the public key and host name when requested.  This
     allows clients to verify that the file and generation time are always
     current.  However, the actual location of each file can be overridden by
     the crypto configuration command.

     All cryptographic keys and related parameters should be regenerated on a
     periodic and automatic basis, like once per month.  The ntp-genkeys(8)
     program uses the same timestamp extension for all files generated at one
     time, so each generation is distinct and can be readily recognized in
     monitoring data.  While a public/private key pair must be generated by
     every server and client, the public keys and agreement parameters do not
     need to be explicitly copied to all machines in the same security com-
     partment, since they can be obtained automatically using the Autokey pro-
     tocol.  However, it is necessary that all primary servers have the same
     agreement parameter file.  The recommended way to do this is for one of
     the primary servers to generate that file and then copy it to the other
     primary servers in the same compartment using the UNIX rdist(1) command.
     Future versions of the Autokey protocol are to contain provisions for an
     agreement protocol to do this automatically.

     Servers and clients can make a new generation in the following way.  All
     machines have loaded the old generation at startup and are operating nor-
     mally.  At designated intervals, each machine generates a new public/pri-
     vate key pair and makes links from the default file names to the new file
     names.  The ntpd(8) is then restarted and loads the new generation, with
     result clients no longer can authenticate correctly.  The Autokey proto-
     col is designed so that after a few minutes the clients time out and
     restart the protocol from the beginning, with result the new generation
     is loaded and operation continues as before.  A similar procedure can be
     used for the agreement parameter file, but in this case precautions must
     be take to be sure that all machines with this file have the same copy.

   Authentication Commands
     autokey [logsec]
             Specifies the interval between regenerations of the session key
             list used with the Autokey protocol.  Note that the size of the
             key list for each association depends on this interval and the
             current poll interval.  The default value is 12 (4096 s or about 1.1 hours).  For poll intervals above the specified interval, a
             session key list with a single entry will be regenerated for
             every message sent.

     controlkey key
             Specifies the key identifier to use with the ntpq(8) utility,
             which uses the standard protocol defined in RFC-1305.  The key
             argument is the key identifier for a trusted key, where the value
             can be in the range 1 to 65534, inclusive.

     crypto [flags flags] [privatekey file] [publickey file] [dhparms file]
             [leap file]
             This command requires the NTP daemon build process be configured
             with the RSA library.  This command activates public-key cryptog-
             raphy and loads the required RSA private and public key files and
             the optional Diffie-Hellman agreement parameter file, if present.
             If one or more files are left unspecified, the default names are
             used as described below.  Following are the subcommands:

             privatekey file
                     Specifies the location of the RSA private key file, which
                     otherwise defaults to /usr/local/etc/ntpkey.

             publickey file
                     Specifies the location of the RSA public key file, which
                     otherwise defaults to /usr/local/etc/ntpkey_host, where
                     host is the name of the generating machine.

             dhparms file
                     Specifies the location of the Diffie-Hellman parameters
                     file, which otherwise defaults to
                     /usr/local/etc/ntpkey_dh.

             leap file
                     Specifies the location of the leapsecond table file,
                     which otherwise defaults to /usr/local/etc/ntpkey_leap.

     keys keyfile
             Specifies the location of the DES/MD5 private key file containing
             the keys and key identifiers used by ntpd(8), ntpq(8) and
             ntpdc(8) when operating in symmetric-key mode.

     keysdir path
             This command requires the NTP daemon build process be configured
             with the RSA library.  It specifies the default directory path
             for the private key file, agreement parameters file and one or
             more public key files.  The default when this command does not
             appear in the configuration file is /usr/local/etc.

     requestkey key
             Specifies the key identifier to use with the ntpdc(8) utility
             program, which uses a proprietary protocol specific to this
             implementation of ntpd(8).  The key argument is a key identifier
             for the trusted key, where the value can be in the range 1 to
             65534, inclusive.

     revoke logsec
             Specifies the interval between re-randomization of certain cryp-
             tographic values used by the Autokey scheme, as a power of 2 in
             seconds.  These values need to be updated frequently in order to
             deflect brute-force attacks on the algorithms of the scheme; how-
             ever, updating some values is a relatively expensive operation.
             The default interval is 16 (65,536 s or about 18 hours).  For
             poll intervals above the specified interval, the values will be
             updated for every message sent.

     trustedkey key ...
             Specifies the key identifiers which are trusted for the purposes
             of authenticating peers with symmetric-key cryptography, as well
             as keys used by the ntpq(8) and ntpdc(8) programs.  The authenti-
             cation procedures require that both the local and remote servers
             share the same key and key identifier for this purpose, although
             different keys can be used with different servers.  The key argu-
             ments are 32-bit unsigned integers with values from 1 to 65,534.


Monitoring Support

     ntpd(8) includes a comprehensive monitoring facility suitable for contin-
     uous, long term recording of server and client timekeeping performance.
     See the statistics command below for a listing and example of each type
     of statistics currently supported.  Statistic files are managed using
     file generation sets and scripts in the ./scripts directory of this dis-
     tribution.  Using these facilities and UNIX cron(8) jobs, the data can be
     automatically summarized and archived for retrospective analysis.

   Monitoring Commands
     statistics name ...
             Enables writing of statistics records.  Currently, four kinds of
             name statistics are supported.

             loopstats
                     Enables recording of loop filter statistics information.
                     Each update of the local clock outputs a line of the fol-
                     lowing form to the file generation set named loopstats:

                     50935 75440.031 0.000006019 13.778190 0.000351733 0.013380 6

                     The first two fields show the date (Modified Julian Day)
                     and time (seconds and fraction past UTC midnight).  The
                     next five fields show time offset (seconds), frequency
                     offset (parts per million - PPM), RMS jitter (seconds),
                     Allan deviation (PPM) and clock discipline time constant.

             peerstats
                     Enables recording of peer statistics information.  This
                     includes statistics records of all peers of a NTP server
                     and of special signals, where present and configured.
                     Each valid update appends a line of the following form to
                     the current element of a file generation set named peer-
                     stats:

                     48773 10847.650 127.127.4.1 9714 -0.001605 0.00000 0.00142

                     The first two fields show the date (Modified Julian Day)
                     and time (seconds and fraction past UTC midnight).  The
                     next two fields show the peer address in dotted-quad
                     notation and status, respectively.  The status field is
                     encoded in hex in the format described in Appendix A of
                     the NTP specification RFC 1305.  The final three fields
                     show the offset, delay and RMS jitter, all in seconds.

             clockstats
                     Enables recording of clock driver statistics information.
                     Each update received from a clock driver appends a line
                     of the following form to the file generation set named
                     clockstats:

                     49213 525.624 127.127.4.1 93 226 00:08:29.606 D

                     The first two fields show the date (Modified Julian Day)
                     and time (seconds and fraction past UTC midnight).  The
                     next field shows the clock address in dotted-quad nota-
                     tion.  The final field shows the last timecode received
                     from the clock in decoded ASCII format, where meaningful.
                     In some clock drivers a good deal of additional informa-
                     tion can be gathered and displayed as well.  See informa-
                     tion specific to each clock for further details.

             rawstats
                     Enables recording of raw-timestamp statistics informa-
                     tion.  This includes statistics records of all peers of a
                     NTP server and of special signals, where present and con-
                     figured.  Each NTP message received from a peer or clock
                     driver appends a line of the following form to the file
                     generation set named rawstats:

                     50928 2132.543 128.4.1.1 128.4.1.20 3102453281.584327000 3102453281.58622800031 02453332.540806000 3102453332.541458000
                     The first two fields show the date (Modified Julian Day)
                     and time (seconds and fraction past UTC midnight).  The
                     next two fields show the remote peer or clock address
                     followed by the local address in dotted-quad notation.
                     The final four fields show the originate, receive, trans-
                     mit and final NTP timestamps in order.  The timestamp
                     values are as received and before processing by the vari-
                     ous data smoothing and mitigation algorithms.

     statsdir directory_path
             Indicates the full path of a directory where statistics files
             should be created (see below).  This keyword allows the (other-
             wise constant) filegen filename prefix to be modified for file
             generation sets, which is useful for handling statistics logs.

     filegen name [file filename] [type typename] [link | nolink] [enable |
             disable]
             Configures setting of generation file set name.  Generation file
             sets provide a means for handling files that are continuously
             growing during the lifetime of a server.  Server statistics are a
             typical example for such files.  Generation file sets provide
             access to a set of files used to store the actual data.  At any
             time at most one element of the set is being written to.  The
             type given specifies when and how data will be directed to a new
             element of the set.  This way, information stored in elements of
             a file set that are currently unused are available for adminis-
             trational operations without the risk of disturbing the operation
             of ntpd(8).  (Most important: they can be removed to free space
             for new data produced.)  Note that this command can be sent from
             the ntpdc(8) program running at a remote location.

             name    This is the type of the statistics records, as shown in
                     the statistics command.

             file filename
                     This is the file name for the statistics records.  File-
                     names of set members are built from three concatenated
                     elements prefix, filename and suffix:

                     prefix  This is a constant filename path.  It is not sub-
                             ject to modifications via the filegen option.  It
                             is defined by the server, usually specified as a
                             compile-time constant.  It may, however, be con-
                             figurable for individual file generation sets via
                             other commands.  For example, the prefix used
                             with loopstats and peerstats generation can be
                             configured using the statsdir option explained
                             above.

                     filename
                             This string is directly concatenated to the pre-
                             fix mentioned above (no intervening `/' (slash)).
                             This can be modified using the file argument to
                             the filegen statement.  No `..' elements are
                             allowed in this component to prevent filenames
                             referring to parts outside the file system hier-
                             archy denoted by prefix.

                     suffix  This part is reflects individual elements of a
                             file set.  It is generated according to the type
                             of a file set.

             type typename
                     A file generation set is characterized by its type.  The
                     following types are supported:

                     none    The file set is actually a single plain file.

                     pid     One element of file set is used per incarnation
                             of a ntpd(8) server.  This type does not perform
                             any changes to file set members during runtime,
                             however it provides an easy way of separating
                             files belonging to different ntpd(8) server
                             incarnations.  The set member filename is built
                             by appending a `.' (dot) to concatenated prefix
                             and filename strings, and appending the decimal
                             representation of the process ID of the ntpd(8)
                             server process.

                     day     One file generation set element is created per
                             day.  A day is defined as the period between
                             00:00 and 24:00 UTC.  The file set member suffix
                             consists of a `.' (dot) and a day specification
                             in the form YYYYMMdd.  YYYY is a 4-digit year
                             number (e.g., 1992).  MM is a two digit month
                             number.  dd is a two digit day number.  Thus, all
                             information written at 10 December 1992 would end
                             up in a file named ~prefix/filename/19921210.

                     week    Any file set member contains data related to a
                             certain week of a year.  The term week is defined
                             by computing day-of-year modulo 7.  Elements of
                             such a file generation set are distinguished by
                             appending the following suffix to the file set
                             filename base: A dot, a 4-digit year number, the
                             letter Ql W , and a 2-digit week number.  For
                             example, information from January, 10th 1992
                             would end up in a file with suffix .1992W1.

                     month   One generation file set element is generated per
                             month.  The file name suffix consists of a dot, a
                             4-digit year number, and a 2-digit month.

                     year    One generation file element is generated per
                             year.  The filename suffix consists of a dot and
                             a 4 digit year number.

                     age     This type of file generation sets changes to a
                             new element of the file set every 24 hours of
                             server operation.  The filename suffix consists
                             of a dot, the letter `a', and an 8-digit number.
                             This number is taken to be the number of seconds
                             the server is running at the start of the corre-
                             sponding 24-hour period.  Information is only
                             written to a file generation by specifying
                             enable; output is prevented by specifying
                             disable.

             link | nolink
                     It is convenient to be able to access the current element
                     of a file generation set by a fixed name.  This feature
                     is enabled by specifying link and disabled using nolink.
                     If link is specified, a hard link from the current file
                     set element to a file without suffix is created.  When
                     there is already a file with this name and the number of
                     links of this file is one, it is renamed appending a dot,
                     the letter `C', and the pid of the ntpd(8) server
                     process.  When the number of links is greater than one,
                     the file is unlinked.  This allows the current file to be
                     accessed by a constant name.

             enable | disable
                     Enables or disables the recording function.


Access Control Support

     ntpd(8) implements a general purpose address-and-mask based restriction
     list.  The list is sorted by address and by mask, and the list is
     searched in this order for matches, with the last match found defining
     the restriction flags associated with the incoming packets.  The source
     address of incoming packets is used for the match, with the 32- bit
     address being and'ed with the mask associated with the restriction entry
     and then compared with the entry's address (which has also been and'ed
     with the mask) to look for a match.  Additional information and examples
     can be found in the "Notes on Configuring NTP and Setting up a NTP
     Subnet" page.

     The restriction facility was implemented in conformance with the access
     policies for the original NSFnet backbone time servers.  While this
     facility may be otherwise useful for keeping unwanted or broken remote
     time servers from affecting your own, it should not be considered an
     alternative to the standard NTP authentication facility.  Source address
     based restrictions are easily circumvented by a determined cracker.

   The Kiss-of-Death Packet
     Ordinarily, packets denied service are simply dropped with no further
     action except incrementing statistics counters.  Sometimes a more proac-
     tive response is needed, such as a server message that explicitly
     requests the client to stop sending and leave a message for the system
     operator.  A special packet format has been created for this purpose
     called the kiss-of-death packet.  If the kod flag is set and either ser-
     vice is denied or the client limit is exceeded, the server returns the
     packet and sets the leap bits unsynchronized, stratum zero and the ASCII
     string "DENY" in the reference source identifier field.  If the kod flag
     is not set, the server simply drops the packet.

     A client or peer receiving a kiss-of-death packet performs a set of san-
     ity checks to minimize security exposure.  If this is the first packet
     received from the server, the client assumes an access denied condition
     at the server.  It updates the stratum and reference identifier peer
     variables and sets the access denied (test 4) bit in the peer flash vari-
     able.  If this bit is set, the client sends no packets to the server.  If
     this is not the first packet, the client assumes a client limit condition
     at the server, but does not update the peer variables.  In either case, a
     message is sent to the system log.

   Access Control Commands
     restrict numeric_address [mask numeric_mask] [flag ...]
             The numeric_address argument, expressed in dotted-quad form, is
             the address of a host or network.  The mask, also expressed in
             dotted-quad form, defaults to 255.255.255.255, meaning that the
             numeric_address is treated as the address of an individual host.
             A default entry (address 0.0.0.0, mask 0.0.0.0) is always
             included and, given the sort algorithm, is always the first entry
             in the list.  Note that, while numeric_address is normally given
             in dotted-quad format, the text string `default', with no mask
             option, may be used to indicate the default entry.  In the cur-
             rent implementation, flag always restricts access, i.e., an entry
             with no flags indicates that free access to the server is to be
             given.  The flags are not orthogonal, in that more restrictive
             flags will often make less restrictive ones redundant.  The flags
             can generally be classed into two categories, those which
             restrict time service and those which restrict informational
             queries and attempts to do run-time reconfiguration of the
             server.  One or more of the following flags may be specified:

             kod     If access is denied, send a kiss-of-death packet.

             ignore  Ignore all packets from hosts which match this entry.  If
                     this flag is specified neither queries nor time server
                     polls will be responded to.

             noquery
                     Ignore all NTP mode 6 and 7 packets (i.e., information
                     queries and configuration requests) from the source.
                     Time service is not affected.

             nomodify
                     Ignore all NTP mode 6 and 7 packets which attempt to mod-
                     ify the state of the server (i.e., run time reconfigura-
                     tion).  Queries which return information are permitted.

             notrap  Decline to provide mode 6 control message trap service to
                     matching hosts.  The trap service is a subsystem of the
                     mode 6 control message protocol which is intended for use
                     by remote event logging programs.

             lowpriotrap
                     Declare traps set by matching hosts to be low priority.
                     The number of traps a server can maintain is limited (the
                     current limit is 3).  Traps are usually assigned on a
                     first come, first served basis, with later trap
                     requestors being denied service.  This flag modifies the
                     assignment algorithm by allowing low priority traps to be
                     overridden by later requests for normal priority traps.

             noserve
                     Ignore NTP packets whose mode is other than 6 or 7.  In
                     effect, time service is denied, though queries may still
                     be permitted.

             nopeer  Provide stateless time service to polling hosts, but do
                     not allocate peer memory resources to these hosts even if
                     they otherwise might be considered useful as future syn-
                     chronization partners.

             notrust
                     Treat these hosts normally in other respects, but never
                     use them as synchronization sources.

             limited
                     These hosts are subject to limitation of number of
                     clients from the same net.  Net in this context refers to
                     the IP notion of net (class A, class B, class C, etc.).
                     Only the first client_limit hosts that have shown up at
                     the server and that have been active during the last
                     client_limit_period seconds are accepted.  Requests from
                     other clients from the same net are rejected.  Only time
                     request packets are taken into account.  Query packets
                     sent by the ntpq(8) and ntpdc(8) programs are not subject
                     to these limits.  A history of clients is kept using the
                     monitoring capability of ntpd(8).  Thus, monitoring is
                     always active as long as there is a restriction entry
                     with the limited flag.

             ntpport
                     This is actually a match algorithm modifier, rather than
                     a restriction flag.  Its presence causes the restriction
                     entry to be matched only if the source port in the packet
                     is the standard NTP UDP port (123).  Both ntpport and
                     non-ntpport may be specified.  The ntpport is considered
                     more specific and is sorted later in the list.

             version
                     Ignore these hosts if not the current NTP version.

             Default restriction list entries, with the flags ignore,
             interface, ntpport, for each of the local host's interface
             addresses are inserted into the table at startup to prevent the
             server from attempting to synchronize to its own time.  A default
             entry is also always present, though if it is otherwise unconfig-
             ured; no flags are associated with the default entry (i.e.,
             everything besides your own NTP server is unrestricted).

     clientlimit limit
             Set the client_limit variable, which limits the number of simul-
             taneous access-controlled clients.  The default value for this
             variable is 3.

     clientperiod period
             Set the client_limit_period variable, which specifies the number
             of seconds after which a client is considered inactive and thus
             no longer is counted for client limit restriction.  The default
             value for this variable is 3600 seconds.


Reference Clock Support

     The NTP Version 4 daemon supports some three dozen different radio,
     satellite and modem reference clocks plus a special pseudo-clock used for
     backup or when no other clock source is available.  Detailed descriptions
     of individual device drivers and options can be found in the "Reference
     Clock Drivers" page (available as part of the HTML documentation provided
     in /usr/share/doc/ntp).  Additional information can be found in the pages
     linked there, including the "Debugging Hints for Reference Clock Drivers"
     and "How To Write a Reference Clock Driver" pages.  In addition, support
     for a PPS signal is available as described in the "Pulse-per-second (PPS)
     Signal Interfacing" page.  Many drivers support special line disci-
     pline/streams modules which can significantly improve the accuracy using
     the driver.  These are described in the "Line Disciplines and Streams
     Drivers" page.

     A reference clock will generally (though not always) be a radio timecode
     receiver which is synchronized to a source of standard time such as the
     services offered by the NRC in Canada and NIST and USNO in the US.  The
     interface between the computer and the timecode receiver is device depen-
     dent, but is usually a serial port.  A device driver specific to each
     reference clock must be selected and compiled in the distribution; how-
     ever, most common radio, satellite and modem clocks are included by
     default.  Note that an attempt to configure a reference clock when the
     driver has not been compiled or the hardware port has not been appropri-
     ately configured results in a scalding remark to the system log file, but
     is otherwise non hazardous.

     For the purposes of configuration, ntpd(8) treats reference clocks in a
     manner analogous to normal NTP peers as much as possible.  Reference
     clocks are identified by a syntactically correct but invalid IP address,
     in order to distinguish them from normal NTP peers.  Reference clock
     addresses are of the form 127.127.t.u, where t is an integer denoting the
     clock type and u indicates the unit number in the range 0-3.  While it
     may seem overkill, it is in fact sometimes useful to configure multiple
     reference clocks of the same type, in which case the unit numbers must be
     unique.

     The server command is used to configure a reference clock, where the
     address argument in that command is the clock address.  The key, version
     and ttl options are not used for reference clock support.  The mode
     option is added for reference clock support, as described below.  The
     prefer option can be useful to persuade the server to cherish a reference
     clock with somewhat more enthusiasm than other reference clocks or peers.
     Further information on this option can be found in the "Mitigation Rules
     and the prefer Keyword" page.  The minpoll and maxpoll options have mean-
     ing only for selected clock drivers.  See the individual clock driver
     document pages for additional information.

     The fudge command is used to provide additional information for individ-
     ual clock drivers and normally follows immediately after the server com-
     mand.  The address argument specifies the clock address.  The refid and
     stratum options can be used to override the defaults for the device.
     There are two optional device-dependent time offsets and four flags that
     can be included in the fudge command as well.

     The stratum number of a reference clock is by default zero.  Since the
     ntpd(8) daemon adds one to the stratum of each peer, a primary server
     ordinarily displays an external stratum of one.  In order to provide
     engineered backups, it is often useful to specify the reference clock
     stratum as greater than zero.  The stratum option is used for this pur-
     pose.  Also, in cases involving both a reference clock and a pulse-per-
     second (PPS) discipline signal, it is useful to specify the reference
     clock identifier as other than the default, depending on the driver.  The
     refid option is used for this purpose.  Except where noted, these options
     apply to all clock drivers.

   Reference Clock Commands
     server 127.127.t.u [prefer] [mode int] [minpoll int] [maxpoll int]
             This command can be used to configure reference clocks in special
             ways.  The options are interpreted as follows:

             prefer  Marks the reference clock as preferred.  All other things
                     being equal, this host will be chosen for synchronization
                     among a set of correctly operating hosts.  See the
                     "Mitigation Rules and the prefer Keyword" page for fur-
                     ther information.

             mode int
                     Specifies a mode number which is interpreted in a device-
                     specific fashion.  For instance, it selects a dialing
                     protocol in the ACTS driver and a device subtype in the
                     parse drivers.

             minpoll int

             maxpoll int
                     These options specify the minimum and maximum polling
                     interval for reference clock messages, in seconds to the
                     power of two.  For most directly connected reference
                     clocks, both minpoll and maxpoll default to 6 (64 s).
                     For modem reference clocks, minpoll defaults to 10 (17.1
                     m) and maxpoll defaults to 14 (4.5 h).  The allowable
                     range is 4 (16 s) to 17 (36.4 h) inclusive.

     fudge 127.127.t.u [time1 sec] [time2 sec] [stratum int] [refid string]
             [mode int] [flag1 0 | 1] [flag2 0 | 1] [flag3 0 | 1] [flag4 0 |
             1]
             This command can be used to configure reference clocks in special
             ways.  It must immediately follow the server command which con-
             figures the driver.  Note that the same capability is possible at
             run time using the ntpdc(8) program.  The options are interpreted
             as follows:

             time1 sec
                     Specifies a constant to be added to the time offset pro-
                     duced by the driver, a fixed-point decimal number in sec-
                     onds.  This is used as a calibration constant to adjust
                     the nominal time offset of a particular clock to agree
                     with an external standard, such as a precision PPS sig-
                     nal.  It also provides a way to correct a systematic
                     error or bias due to serial port or operating system
                     latencies, different cable lengths or receiver internal
                     delay.  The specified offset is in addition to the propa-
                     gation delay provided by other means, such as internal
                     DIPswitches.  Where a calibration for an individual sys-
                     tem and driver is available, an approximate correction is
                     noted in the driver documentation pages.  Note: in order
                     to facilitate calibration when more than one radio clock
                     or PPS signal is supported, a special calibration feature
                     is available.  It takes the form of an argument to the
                     enable command described in Miscellaneous Options page
                     and operates as described in the "Reference Clock
                     Drivers" page.

             time2 secs
                     Specifies a fixed-point decimal number in seconds, which
                     is interpreted in a driver-dependent way.  See the
                     descriptions of specific drivers in the "reference clock
                     drivers" page.

             stratum int
                     Specifies the stratum number assigned to the driver, an
                     integer between 0 and 15.  This number overrides the
                     default stratum number ordinarily assigned by the driver
                     itself, usually zero.

             refid string
                     Specifies an ASCII string of from one to four characters
                     which defines the reference identifier used by the
                     driver.  This string overrides the default identifier
                     ordinarily assigned by the driver itself.

             mode int
                     Specifies a mode number which is interpreted in a device-
                     specific fashion.  For instance, it selects a dialing
                     protocol in the ACTS driver and a device subtype in the
                     parse drivers.

             flag1 0 | 1

             flag2 0 | 1

             flag3 0 | 1

             flag4 0 | 1
                     These four flags are used for customizing the clock
                     driver.  The interpretation of these values, and whether
                     they are used at all, is a function of the particular
                     clock driver.  However, by convention flag4 is used to
                     enable recording monitoring data to the clockstats file
                     configured with the filegen command.  Further information
                     on the filegen command can be found in Monitoring
                     Options.


Miscellaneous Options

     broadcastdelay seconds
             The broadcast and multicast modes require a special calibration
             to determine the network delay between the local and remote
             servers.  Ordinarily, this is done automatically by the initial
             protocol exchanges between the client and server.  In some cases,
             the calibration procedure may fail due to network or server
             access controls, for example.  This command specifies the default
             delay to be used under these circumstances.  Typically (for Eth-
             ernet), a number between 0.003 and 0.007 seconds is appropriate.
             The default when this command is not used is 0.004 seconds.

     driftfile driftfile
             This command specifies the name of the file used to record the
             frequency offset of the local clock oscillator.  If the file
             exists, it is read at startup in order to set the initial fre-
             quency offset and then updated once per hour with the current
             frequency offset computed by the daemon.  If the file does not
             exist or this command is not given, the initial frequency offset
             is assumed zero.  In this case, it may take some hours for the
             frequency to stabilize and the residual timing errors to subside.

             The file format consists of a single line containing a single
             floating point number, which records the frequency offset mea-
             sured in parts-per-million (PPM).  The file is updated by first
             writing the current drift value into a temporary file and then
             renaming this file to replace the old version.  This implies that
             ntpd(8) must have write permission for the directory the drift
             file is located in, and that file system links, symbolic or oth-
             erwise, should be avoided.

     enable [auth | bclient | calibrate | kernel | monitor | ntp | stats]

     disable [auth | bclient | calibrate | kernel | monitor | ntp | stats]
             Provides a way to enable or disable various server options.
             Flags not mentioned are unaffected.  Note that all of these flags
             can be controlled remotely using the ntpdc(8) utility program.

             bclient
                     When enabled, this is identical to the broadcastclient
                     command.  The default for this flag is disable.

             calibrate
                     Enables the calibration facility, which automatically
                     adjusts the time1 values for each clock driver to display
                     the same offset as the currently selected source or ker-
                     nel discipline signal.  See the "Reference Clock Drivers"
                     page for further information.  The default for this flag
                     is disable.

             kernel  Enables the precision-time kernel support for the
                     adjtime(2) system call, if implemented.  Ordinarily, sup-
                     port for this routine is detected automatically when the
                     NTP daemon is compiled, so it is not necessary for the
                     user to worry about this flag.  It is provided primarily
                     so that this support can be disabled during kernel devel-
                     opment.  The default for this flag is enable.

             monitor
                     Enables the monitoring facility.  See the ntpdc(8) pro-
                     gram and the monlist command or further information.  The
                     default for this flag is enable.

             ntp     Enables the server to adjust its local clock by means of
                     NTP.  If disabled, the local clock free-runs at its
                     intrinsic time and frequency offset.  This flag is useful
                     in case the local clock is controlled by some other
                     device or protocol and NTP is used only to provide syn-
                     chronization to other clients.  In this case, the local
                     clock driver can be used to provide this function and
                     also certain time variables for error estimates and leap-
                     indicators.  See the "Reference Clock Drivers" page for
                     further information.  The default for this flag is
                     enable.

             stats   Enables the statistics facility.  See the "Monitoring
                     Options" page for further information.  The default for
                     this flag is enable.

     logconfig configkeyword
             This command controls the amount and type of output written to
             the system syslog(3) facility or the alternate logfile log file.
             By default, all output is turned on.  All configkeyword keywords
             can be prefixed with `=', `+' and `-', where `=' sets the
             syslog(3) priority mask, `+' adds and `-' removes messages.
             syslog(3) messages can be controlled in four classes (clock,
             peer, sys and sync).  Within these classes four types of messages
             can be controlled.  Informational messages (info) control config-
             uration information.  Event messages (events) control logging of
             events (reachability, synchronization, alarm conditions).  Sta-
             tistical output is controlled with the statistics keyword.  The
             final message group is the status messages.  This describes
             mainly the synchronizations status.  Configuration keywords are
             formed by concatenating the message class with the event class.
             The all prefix can be used instead of a message class.  A message
             class may also be followed by the all keyword to enable/disable
             all messages of the respective message class.  Thus, a minimal
             log configuration could look like this:

             logconfig =syncstatus +sysevents

             This would just list the synchronizations state of ntpd(8) and
             the major system events.  For a simple reference server, the fol-
             lowing minimum message configuration could be useful:

             logconfig =syncall +clockall

             This configuration will list all clock information and synchro-
             nization information.  All other events and messages about peers,
             system events and so on is suppressed.

     logfile logfile
             This command specifies the location of an alternate log file to
             be used instead of the default system syslog(3) facility.

     setvar variable [default]
             This command adds an additional system variable.  These variables
             can be used to distribute additional information such as the
             access policy.  If the variable of the form name=value is fol-
             lowed by the default keyword, the variable will be listed as part
             of the default system variables (ntpq(8) rv command)).  These
             additional variables serve informational purposes only.  They are
             not related to the protocol other that they can be listed.  The
             known protocol variables will always override any variables
             defined via the setvar mechanism.  There are three special vari-
             ables that contain the names of all variable of the same group.
             The sys_var_list holds the names of all system variables.  The
             peer_var_list holds the names of all peer variables and the
             clock_var_list holds the names of the reference clock variables.

     tinker [step step | panic panic | dispersion dispersion | stepout stepout
             | minpoll minpoll | allan allan | huffpuff huffpuff]
             This command can be used to alter several system variables in
             very exceptional circumstances.  It should occur in the configu-
             ration file before any other configuration options.  The default
             values of these variables have been carefully optimized for a
             wide range of network speeds and reliability expectations.  In
             general, they interact in intricate ways that are hard to predict
             and some combinations can result in some very nasty behavior.
             Very rarely is it necessary to change the default values; but,
             some folks can't resist twisting the knobs anyway and this com-
             mand is for them.  Emphasis added: twisters are on their own and
             can expect no help from the support group.

             All arguments are in floating point seconds or seconds per sec-
             ond.  The minpoll argument is an integer in seconds to the power
             of two.  The variables operate as follows:

             step step
                     The argument becomes the new value for the step thresh-
                     old, normally 0.128 s.  If set to zero, step adjustments
                     will never occur.  In general, if the intent is only to
                     avoid step adjustments, the step threshold should be left
                     alone and the -x command line option be used instead.

             panic panic
                     The argument becomes the new value for the panic thresh-
                     old, normally 1000 s.  If set to zero, the panic sanity
                     check is disabled and a clock offset of any value will be
                     accepted.

             dispersion dispersion
                     The argument becomes the new value for the dispersion
                     increase rate, normally .000015.

             stepout stepout
                     The argument becomes the new value for the watchdog time-
                     out, normally 900 s.

             minpoll minpoll
                     The argument becomes the new value for the minimum poll
                     interval used when configuring multicast client, manycast
                     client and , symmetric passive mode association.  The
                     value defaults to 6 (64 s) and has a lower limit of 4 (16
                     s).

             allan allan
                     The argument becomes the new value for the minimum Allan
                     intercept, which is a parameter of the PLL/FLL clock dis-
                     cipline algorithm.  The value defaults to 1024 s, which
                     is also the lower limit.

             huffpuff huffpuff
                     The argument becomes the new value for the experimental
                     huff-n'-puff filter span, which determines the most
                     recent interval the algorithm will search for a minimum
                     delay.  The lower limit is 900 s (15 m), but a more rea-
                     sonable value is 7200 (2 hours).  There is no default,
                     since the filter is not enabled unless this command is
                     given.

     trap host_address [port port_number] [interface interface_address]
             This command configures a trap receiver at the given host address
             and port number for sending messages with the specified local
             interface address.  If the port number is unspecified, a value of
             18447 is used.  If the interface address is not specified, the
             message is sent with a source address of the local interface the
             message is sent through.  Note that on a multihomed host the
             interface used may vary from time to time with routing changes.

             The trap receiver will generally log event messages and other
             information from the server in a log file.  While such monitor
             programs may also request their own trap dynamically, configuring
             a trap receiver will ensure that no messages are lost when the
             server is started.


FILES

     /etc/ntp.conf   the default name of the configuration file
     ntp.keys        private MD5 keys
     ntpkey          RSA private key
     ntpkey_host     RSA public key
     ntp_dh          Diffie-Hellman agreement parameters


SEE ALSO

     ntpd(8), ntpdc(8), ntpq(8)

     In addition to the manual pages provided, comprehensive documentation is
     available on the world wide web at http://www.ntp.org/.  A snapshot of
     this documentation is available in HTML format in /usr/share/doc/ntp.

     David L. Mills, Network Time Protocol (Version 3), RFC1305.



		10.5.5 ntpq(8) - Linux man page
Name
ntpq - standard NTP query program

Synopsis
ntpq [-46dinp] [-c command] [host] [...]
Description

The ntpq utility program is used to monitor NTP daemon ntpd operations and determine performance. It uses the standard NTP mode 6 control message formats defined in Appendix B of the NTPv3 specification RFC1305. The same formats are used in NTPv4, although some of the variables have changed and new ones added. The description on this page is for the NTPv4 variables.

The program can be run either in interactive mode or controlled using command line arguments. Requests to read and write arbitrary variables can be assembled, with raw and pretty-printed output options being available. The ntpq can also obtain and print a list of peers in a common format by sending multiple queries to the server.

If one or more request options is included on the command line when ntpq is executed, each of the requests will be sent to the NTP servers running on each of the hosts given as command line arguments, or on localhost by default. If no request options are given, ntpq will attempt to read commands from the standard input and execute these on the NTP server running on the first host given on the command line, again defaulting to localhost when no other host is specified. ntpq will prompt for commands if the standard input is a terminal device.

ntpq uses NTP mode 6 packets to communicate with the NTP server, and hence can be used to query any compatible server on the network which permits it. Note that since NTP is a UDP protocol this communication will be somewhat unreliable, especially over large distances in terms of network topology. ntpq makes one attempt to retransmit requests, and will time requests out if the remote host is not heard from within a suitable timeout time.

Note that in contexts where a host name is expected, a -4 qualifier preceding the host name forces DNS resolution to the IPv4 namespace, while a -6 qualifier forces DNS resolution to the IPv6 namespace.

For examples and usage, see the NTP Debugging Techniques page.

Command line options are described following. Specifying a command line option other than -i or -n will cause the specified query (queries) to be sent to the indicated host(s) immediately. Otherwise, ntpq will attempt to read interactive format commands from the standard input.

    -4
        Force DNS resolution of following host names on the command line to the IPv4 namespace. 
    -6
        Force DNS resolution of following host names on the command line to the IPv6 namespace. 
    -c
        The following argument is interpreted as an interactive format command and is added to the list of commands to be executed on the specified host(s). Multiple -c options may be given. 
    -d
        Turn on debugging mode. 
    -i
        Force ntpq to operate in interactive mode. Prompts will be written to the standard output and commands read from the standard input. 
    -n
        Output all host addresses in dotted-quad numeric format rather than converting to the canonical host names. 
    -p
        Print a list of the peers known to the server as well as a summary of their state. This is equivalent to the peers interactive command.

Internal Commands

Interactive format commands consist of a keyword followed by zero to four arguments. Only enough characters of the full keyword to uniquely identify the command need be typed. The output of a command is normally sent to the standard output, but optionally the output of individual commands may be sent to a file by appending a >, followed by a file name, to the command line. A number of interactive format commands are executed entirely within the ntpq program itself and do not result in NTP mode 6 requests being sent to a server. These are described following.

    ? [command_keyword]
    helpl [command_keyword]
        A ? by itself will print a list of all the command keywords known to this incarnation of ntpq[char46] A ? followed by a command keyword will print function and usage information about the command. This command is probably a better source of information about ntpq than this manual page. 
    addvars variable_name [ = value] [...]
    rmvars variable_name [...]
    clearvars
        The data carried by NTP mode 6 messages consists of a list of items of the form variable_name = value, where the = value is ignored, and can be omitted, in requests to the server to read variables. ntpq maintains an internal list in which data to be included in control messages can be assembled, and sent using the readlist and writelist commands described below. The addvars command allows variables and their optional values to be added to the list. If more than one variable is to be added, the list should be comma-separated and not contain white space. The rmvars command can be used to remove individual variables from the list, while the clearlist command removes all variables from the list. 
    cooked
        Causes output from query commands to be "cooked", so that variables which are recognized by ntpq will have their values reformatted for human consumption. Variables which ntpq thinks should have a decodable value but didn't are marked with a trailing ?[char46] 
    debug more | less | off
        Turns internal query program debugging on and off. 
    delay milliseconds
        Specify a time interval to be added to timestamps included in requests which require authentication. This is used to enable (unreliable) server reconfiguration over long delay network paths or between machines whose clocks are unsynchronized. Actually the server does not now require timestamps in authenticated requests, so this command may be obsolete. 
    host hostname
        Set the host to which future queries will be sent. Hostname may be either a host name or a numeric address. 
    hostnames [yes | no]
        If yes is specified, host names are printed in information displays. If no is specified, numeric addresses are printed instead. The default is yes, unless modified using the command line -n switch. 
    keyid keyid
        This command specifies the key number to be used to authenticate configuration requests. This must correspond to a key number the server has been configured to use for this purpose. 
    ntpversion 1 | 2 | 3 | 4
        Sets the NTP version number which ntpq claims in packets. Defaults to 2, Note that mode 6 control messages (and modes, for that matter) didn't exist in NTP version 1. 
    passwd
        This command prompts for a password (which will not be echoed) which will be used to authenticate configuration requests. The password must correspond to the key configured for NTP server for this purpose. 
    quit
        Exit ntpq[char46] 
    raw
        Causes all output from query commands is printed as received from the remote server. The only formatting/interpretation done on the data is to transform non-ASCII data into a printable (but barely understandable) form. 
    timeout millseconds
        Specify a timeout period for responses to server queries. The default is about 5000 milliseconds. Note that since ntpq retries each query once after a timeout, the total waiting time for a timeout will be twice the timeout value set.

Control Message Commands

Each association known to an NTP server has a 16 bit integer association identifier. NTP control messages which carry peer variables must identify the peer the values correspond to by including its association ID. An association ID of 0 is special, and indicates the variables are system variables, whose names are drawn from a separate name space.

Control message commands result in one or more NTP mode 6 messages being sent to the server, and cause the data returned to be printed in some format. Most commands currently implemented send a single message and expect a single response. The current exceptions are the peers command, which will send a preprogrammed series of messages to obtain the data it needs, and the mreadlist and mreadvar commands, which will iterate over a range of associations.

    associations
        Obtains and prints a list of association identifiers and peer statuses for in-spec peers of the server being queried. The list is printed in columns. The first of these is an index numbering the associations from 1 for internal use, the second the actual association identifier returned by the server and the third the status word for the peer. This is followed by a number of columns containing data decoded from the status word. See the peers command for a decode of the condition field. Note that the data returned by the associations command is cached internally in ntpq[char46] The index is then of use when dealing with stupid servers which use association identifiers which are hard for humans to type, in that for any subsequent commands which require an association identifier as an argument, the form &index may be used as an alternative. 
    clockvar [assocID] [variable_name [ = value[...]] [...]
    cv [assocID] [variable_name [ = value [...] ][...]
        Requests that a list of the server's clock variables be sent. Servers which have a radio clock or other external synchronization will respond positively to this. If the association identifier is omitted or zero the request is for the variables of the system clock and will generally get a positive response from all servers with a clock. If the server treats clocks as pseudo-peers, and hence can possibly have more than one clock connected at once, referencing the appropriate peer association ID will show the variables of a particular clock. Omitting the variable list will cause the server to return a default variable display. 
    lassociations
        Obtains and prints a list of association identifiers and peer statuses for all associations for which the server is maintaining state. This command differs from the associations command only for servers which retain state for out-of-spec client associations (i.e., fuzzballs). Such associations are normally omitted from the display when the associations command is used, but are included in the output of lassociations[char46] 
    lpassociations
        Print data for all associations, including out-of-spec client associations, from the internally cached list of associations. This command differs from passociations only when dealing with fuzzballs. 
    lpeers
        Like R peers, except a summary of all associations for which the server is maintaining state is printed. This can produce a much longer list of peers from fuzzball servers. 
    mreadlist assocID assocID
    mrl assocID assocID
        Like the readlist command, except the query is done for each of a range of (nonzero) association IDs. This range is determined from the association list cached by the most recent associations command. 
    mreadvar assocID assocID [ variable_name [ = value[ ... ]
    mrv assocID assocID [ variable_name [ = value[ ... ]
        Like the readvar command, except the query is done for each of a range of (nonzero) association IDs. This range is determined from the association list cached by the most recent associations command. 
    opeers
        An old form of the peers command with the reference ID replaced by the local interface address. 
    passociations
        Displays association data concerning in-spec peers from the internally cached list of associations. This command performs identically to the associations except that it displays the internally stored data rather than making a new query. 
    peers
        Obtains a current list peers of the server, along with a summary of each peer's state. Summary information includes the address of the remote peer, the reference ID (0.0.0.0 if this is unknown), the stratum of the remote peer, the type of the peer (local, unicast, multicast or broadcast), when the last packet was received, the polling interval, in seconds, the reachability register, in octal, and the current estimated delay, offset and dispersion of the peer, all in milliseconds. The character at the left margin of each line shows the synchronization status of the association and is a valuable diagnostic tool. The encoding and meaning of this character, called the tally code, is given later in this page. 
    pstatus assocID
        Sends a read status request to the server for the given association. The names and values of the peer variables returned will be printed. Note that the status word from the header is displayed preceding the variables, both in hexadecimal and in pidgeon English. 
    readlist [ assocID]
    rl [ assocID ]
        Requests that the values of the variables in the internal variable list be returned by the server. If the association ID is omitted or is 0 the variables are assumed to be system variables. Otherwise they are treated as peer variables. If the internal variable list is empty a request is sent without data, which should induce the remote server to return a default display. 
    readvar assocID variable_name [ = value ] [ ...]
    rv assocID [ variable_name [ = value ] [...]
        Requests that the values of the specified variables be returned by the server by sending a read variables request. If the association ID is omitted or is given as zero the variables are system variables, otherwise they are peer variables and the values returned will be those of the corresponding peer. Omitting the variable list will send a request with no data which should induce the server to return a default display. The encoding and meaning of the variables derived from NTPv3 is given in RFC-1305; the encoding and meaning of the additional NTPv4 variables are given later in this page. 
    writevar assocID variable_name[ = value [ ...]
        Like the readvar request, except the specified variables are written instead of read. 
    writelist [ assocID ]
        Like the readlist request, except the internal list variables are written instead of read.

Tally Codes

The character in the left margin in the peers billboard, called the tally code, shows the fate of each association in the clock selection process. Following is a list of these characters, the pigeon used in the rv command, and a short explanation of the condition revealed.

    space reject
        The peer is discarded as unreachable, synchronized to this server (synch loop) or outrageous synchronization distance. 
    x falsetick
        The peer is discarded by the intersection algorithm as a falseticker. 
    [char46] excess
        The peer is discarded as not among the first ten peers sorted by synchronization distance and so is probably a poor candidate for further consideration. 
    - outlyer
        The peer is discarded by the clustering algorithm as an outlyer. 
    + candidat
        The peer is a survivor and a candidate for the combining algorithm. 
    # selected
        The peer is a survivor, but not among the first six peers sorted by synchronization distance. If the association is ephemeral, it may be demobilized to conserve resources. 
    * sys.peer
        The peer has been declared the system peer and lends its variables to the system variables. 
    o pps.peer
        The peer has been declared the system peer and lends its variables to the system variables. However, the actual system synchronization is derived from a pulse-per-second (PPS) signal, either indirectly via the PPS reference clock driver or directly via kernel interface.

System Variables

The status, leap, stratum, precision, rootdelay, rootdispersion, refid, reftime, poll, offset, and frequency variables are described in RFC-1305 specification. Additional NTPv4 system variables include the following.

    version
        Everything you might need to know about the software version and generation time. 
    processor
        The processor and kernel identification string. 
    system
        The operating system version and release identifier. 
    state
        The state of the clock discipline state machine. The values are described in the architecture briefing on the NTP Project page linked from www.ntp.org. 
    peer
        The internal integer used to identify the association currently designated the system peer. 
    jitter
        The estimated time error of the system clock measured as an exponential average of RMS time differences. 
    stability
        The estimated frequency stability of the system clock measured as an exponential average of RMS frequency differences.

When the NTPv4 daemon is compiled with the OpenSSL software library, additional system variables are displayed, including some or all of the following, depending on the particular dance:

    flags
        The current flags word bits and message digest algorithm identifier (NID) in hex format. The high order 16 bits of the four-byte word contain the NID from the OpenSSL ligrary, while the low-order bits are interpreted as follows: 
    0x01
        autokey enabled 
    0x02
        NIST leapseconds file loaded 
    0x10
        PC identity scheme 
    0x20
        IFF identity scheme 
    0x40
        GQ identity scheme

hostname
    The name of the host as returned by the Unix gethostname() library function. 
hostkey
    The NTP filestamp of the host key file. 
cert
    A list of certificates held by the host. Each entry includes the subject, issuer, flags and NTP filestamp in order. The bits are interpreted as follows: 
0x01
    certificate has been signed by the server 
0x02
    certificate is trusted 
0x04
    certificate is private 
0x08
    certificate contains errors and should not be trusted 
leapseconds
    The NTP filestamp of the NIST leapseconds file. 
refresh
    The NTP timestamp when the host public cryptographic values were refreshed and signed. 
signature
    The host digest/signature scheme name from the OpenSSL library. 
tai
    The TAI-UTC offset in seconds obtained from the NIST leapseconds table.

Peer Variables

The status, srcadr, srcport, dstadr, dstport, leap, stratum, precision, rootdelay, rootdispersion, readh, hmode, pmode, hpoll, ppoll, offset, delay, dspersion, reftime variables are described in the RFC-1305 specification, as are the timestamps org, rec and xmt[char46] Additional NTPv4 system variables include the following.

    flash
        The flash code for the most recent packet received. The encoding and meaning of these codes is given later in this page. 
    jitter
        The estimated time error of the peer clock measured as an exponential average of RMS time differences. 
    unreach
        The value of the counter which records the number of poll intervals since the last valid packet was received.

When the NTPv4 daemon is compiled with the OpenSSL software library, additional peer variables are displayed, including the following:

    flags
        The current flag bits. This word is the server host status word with additional bits used by the Autokey state machine. See the source code for the bit encoding. 
    hostname
        The server host name. 
    initkey key
        The initial key used by the key list generator in the Autokey protocol. 
    initsequence index
        The initial index used by the key list generator in the Autokey protocol. 
    signature
        The server message digest/signature scheme name from the OpenSSL software library. 
    timestamp time
        The NTP timestamp when the last Autokey key list was generated and signed.

Flash Codes

The flash code is a valuable debugging aid displayed in the peer variables list. It shows the results of the original sanity checks defined in the NTP specification RFC-1305 and additional ones added in NTPv4. There are 12 tests designated TEST1 through TEST12[char46] The tests are performed in a certain order designed to gain maximum diagnostic information while protecting against accidental or malicious errors. The flash variable is initialized to zero as each packet is received. If after each set of tests one or more bits are set, the packet is discarded.

Tests TEST1 through TEST3 check the packet timestamps from which the offset and delay are calculated. If any bits are set, the packet is discarded; otherwise, the packet header variables are saved. TEST4 and TEST5 are associated with access control and cryptographic authentication. If any bits are set, the packet is discarded immediately with nothing changed.

Tests TEST6 through TEST8 check the health of the server. If any bits are set, the packet is discarded; otherwise, the offset and delay relative to the server are calculated and saved. TEST9 checks the health of the association itself. If any bits are set, the packet is discarded; otherwise, the saved variables are passed to the clock filter and mitigation algorithms.

Tests TEST10 through TEST12 check the authentication state using Autokey public-key cryptography, as described in the Authentication Options page. If any bits are set and the association has previously been marked reachable, the packet is discarded; otherwise, the originate and receive timestamps are saved, as required by the NTP protocol, and processing continues.

The flash bits for each test are defined as follows.

    0x001 TEST1
        Duplicate packet. The packet is at best a casual retransmission and at worst a malicious replay. 
    0x002 TEST2
        Bogus packet. The packet is not a reply to a message previously sent. This can happen when the NTP daemon is restarted and before somebody else notices. 
    0x004 TEST3
        Unsynchronized. One or more timestamp fields are invalid. This normally happens when the first packet from a peer is received. 
    0x008 TEST4
        Access is denied. See the Access Control Options page. 
    0x010 TEST5
        Cryptographic authentication fails. See the Authentication Options page. 
    0x020 TEST6
        The server is unsynchronized. Wind up its clock first. 
    0x040 TEST7
        The server stratum is at the maximum than 15. It is probably unsynchronized and its clock needs to be wound up. 
    0x080 TEST8
        Either the root delay or dispersion is greater than one second, which is highly unlikely unless the peer is unsynchronized to Mars. 
    0x100 TEST9
        Either the peer delay or dispersion is greater than one second, which is highly unlikely unless the peer is on Mars. 
    0x200 TEST10
        The autokey protocol has detected an authentication failure. See the Authentication Options page. 
    0x400 TEST11
        The autokey protocol has not verified the server or peer is proventic and has valid public key credentials. See the Authentication Options page. 
    0x800 TEST12
        A protocol or configuration error has occurred in the public key algorithms or a possible intrusion event has been detected. See the Authentication Options page.

Bugs

The peers command is non-atomic and may occasionally result in spurious error messages about invalid associations occurring and terminating the command. The timeout time is a fixed constant, which means you wait a long time for timeouts since it assumes sort of a worst case. The program should improve the timeout estimate as it sends queries to a particular host, but doesn't.
See Also

ntpd(8), ntpdc(8) 

		10.5.6 logout user
 root or admin user can logout any user forcefully. If you are logged in as vivek and just wanted to logout or logoff, type logout command or hit CTRL+D:
$ logout

You will be logout of a login shell session or secure shell session.
Task: Linux logout user

If you would like to logout other users, you must login as root user. Next you need to use pkill command.
pkill command syntax

pkill -KILL -u {username}

To see list of logged in user type who or w command:
# who
OR
# w
To logout user called raj, enter:
# pkill -KILL -u raj
OR
$ sudo pkill -KILL -u raj

------------------------------------------------------------------------------------------------------------------------
There is a package called procps. It includes various useful (nifty) utilities. One of such utility is skill which is responsible to send a signal to users and process such as:

    Halt user terminal
    Kill user and logout


The procps package contains utilities to browse the /proc filesystem, which is not a real file system but a way for the kernel to provide information about the status of entries in its process table. Procps includes ps, free, skill, pkill, pgrep, snice, tload, top, uptime, vmstat, w, watch and pdwx commands.
Task: How To Halt/Stop a User Called vivek

Open a command-line terminal (select Applications > Accessories > Terminal), and then type the following commands. First, switch to the root user by typing su - and entering the root password, when prompted (you can also use sudo if configured). Type the skill command as follows:
# skill -STOP -u vivek
The skill command sends a terminate command (or another specified signal) to a specified set of processes.
Task: Resume Halted User Called vivek

Send CONT single to user vivek, type the following command:
# skill -CONT -u vivek
Task: Kill and Logout a User Called vivek

You can send KILL single, type the following command:
# skill -KILL -u vivek
Task: Kill and Logout All Users

The ultimate command to kill and logout all users is as follows:
# skill -KILL -v /dev/pts/*
[Warning examples may crash your computer] WARNING! These tools are obsolete, unportable and it is here due to historical reasons. Consider using the killall, pkill, and pgrep commands instead as follows.
pkill command

*/



To halt or stop a user called vivek, enter:
# pkill -STOP -u vivek
To resume a user called vivek, enter:
# pkill -CONT -u vivek
To kill all php-cgi process owned by vivek user, enter:
# pkill -KILL -u vivek php-cgi
------------------------------------------------------------------------------------------------------------------------


		10.5.7 Introduction

This page lists the status words, event messages and error codes used for ntpd reporting and monitoring. Status words are used to display the current status of the running program. There is one system status word and a peer status word for each association. There is a clock status word for each association that supports a reference clock. There is a flash code for each association which shows errors found in the last packet received (pkt) and during protocol processing (peer). These are commonly viewed using the ntpq program.

Significant changes in program state are reported as events. There is one set of system events and a set of peer events for each association. In addition, there is a set of clock events for each association that supports a reference clock. Events are normally reported to the protostats monitoring file and optionally to the system log. In addition, if the trap facility is configured, events can be reported to a remote program that can page an administrator.

This page also includes a description of the error messages produced by the Autokey protocol. These messages are normally sent to the cryptostats monitoring file.

In the following tables the Event Field is the status or event code assigned and the Message Field a short string used for display and event reporting. The Description field contains a longer explanation of the status or event. Some messages include additional information useful for error diagnosis and performance assessment.

System Status Word

The system status word consists of four fields LI (0-1), Source (2-7), Count (8-11) and Event (12-15). It is reported in the first line of the rv display produced by the ntpq program.

Leap
Source
Count
Event
The Leap Field displays the system leap indicator bits coded as follows:

Code	Message	Description
0	leap_none	normal synchronized state
1	leap_add_sec	insert second after 23:59:59 of the current day
2	leap_del_sec	delete second 23:59:59 of the current day
3	leap_alarm	never synchronized
The Source Field displays the current synchronization source coded as follows:

Code	Message	Description
0	sync_unspec	not yet synchronized
1	sync_pps	pulse-per-second signal (Cs, Ru, GPS, etc.)
2	sync_lf_radio	VLF/LF radio (WWVB, DCF77, etc.)
3	sync_hf_radio	MF/HF radio (WWV, etc.)
4	sync_uhf_radio	VHF/UHF radio/satellite (GPS, Galileo, etc.)
5	sync_local	local timecode (IRIG, LOCAL driver, etc.)
6	sync_ntp	NTP
7	sync_other	other (IEEE 1588, openntp, crony, etc.)
8	sync_wristwatch	eyeball and wristwatch
9	sync_telephone	telephone modem (ACTS, PTB, etc.)
The Count Field displays the number of events since the last time the code changed. Upon reaching 15, subsequent events with the same code are ignored.

The Event Field displays the most recent event message coded as follows:

Code	Message	Description
00	unspecified	unspecified
01	freq_not_set	frequency file not available
02	freq_set	frequency set from frequency file
03	spike_detect	spike detected
04	freq_mode	initial frequency training mode
05	clock_sync	clock synchronized
06	restart	program restart
07	panic_stop	clock error more than 600 s
08	no_system_peer	no system peer
09	leap_armed	leap second armed from file or Autokey
0a	leap_disarmed	leap second disarmed
0b	leap_event	leap event
0c	clock_step	clock stepped
0d	kern	kernel information message
0e	TAI...	leapsecond values update from file
0f	stale leapsecond values	new NIST leapseconds file needed
Peer Status Word

The peer status word consists of four fields: Status (0-4), Select (5-7), Count (8-11) and Code (12-15). It is reported in the first line of the rv associd display produced by the ntpq program.

Status
Select
Count
Code
The Status Field displays the peer status code bits in hexadecimal; each bit is an independent flag. (Note this field is 5 bits wide, and combines with the the 3-bit-wide Select Field to create the first full byte of the peer status word.) The meaning of each bit in the Status Field is listed in the following table:

Code	Message	Description
08	bcst	broadcast association
10	reach	host reachable
20	authenb	authentication enabled
40	auth	authentication ok
80	config	persistent association
The Select Field displays the current selection status. (The T Field in the following table gives the corresponding tally codes used in the ntpq peers display.) The values are coded as follows:

Code	Message	T	Description
0	sel_reject	 	discarded as not valid (TEST10-TEST13)
1	sel_falsetick	x	discarded by intersection algorithm
2	sel_excess	.	discarded by table overflow (not used)
3	sel_outlyer	-	discarded by the cluster algorithm
4	sel_candidate	+	included by the combine algorithm
5	sel_backup	#	backup (more than tos maxclock sources)
6	sel_sys.peer	*	system peer
7	sel_pps.peer	o	PPS peer (when the prefer peer is valid)
The Count Field displays the number of events since the last time the code changed. Upon reaching 15, subsequent events with the same code are ignored.

The Event Field displays the most recent event message coded as follows:

Code	Message	Description
01	mobilize	association mobilized
02	demobilize	association demobilized
03	unreachable	server unreachable
04	reachable	server reachable
05	restart	association restart
06	no_reply	no server found (ntpdate mode)
07	rate_exceeded	rate exceeded (kiss code RATE)
08	access_denied	access denied (kiss code DENY)
09	leap_armed	leap armed from server LI code
0a	sys_peer	become system peer
0b	clock_event	see clock status word
0c	bad_auth	authentication failure
0d	popcorn	popcorn spike suppressor
0e	interleave_mode	entering interleave mode
0f	interleave_error	interleave error (recovered)
Clock Status Word

The clock status word consists of four fields: Unused (0-7), Count (8-11) and Code (12-15). It is reported in the first line of the clockvar associd display produced by the ntpq program.

Unused
Count
Code
The Count Field displays the number of events since the last lockvar command, while the Event Field displays the most recent event message coded as follows:

Code	Message	Description
00	clk_unspe	nominal
01	clk_noreply	no reply to poll
02	clk_badformat	bad timecode format
03	clk_fault	hardware or software fault
04	clk_bad_signal	signal loss
05	clk_bad_date	bad date format
06	clk_bad_time	bad time format
When the clock driver sets the code to a new value, a clock_alarm (11) peer event is reported.

Flash Status Word

The flash status word is displayed by the ntpq program rv command. It consists of a number of bits coded in hexadecimal as follows:

Code	Tag	Message	Description
0001	TEST1	pkt_dup	duplicate packet
0002	TEST2	pkt_bogus	bogus packet
0004	TEST3	pkt_unsync	server not synchronized
0008	TEST4	pkt_denied	access denied
0010	TEST5	pkt_auth	 authentication failure
0020	TEST6	pkt_stratum	invalid leap or stratum
0040	TEST7	pkt_header	 header distance exceeded
0080	TEST8	pkt_autokey	Autokey sequence error
0100	TEST9	pkt_crypto	Autokey protocol error
0200	TEST10	peer_stratum	 invalid header or stratum
0400	TEST11	peer_dist	 distance threshold exceeded
0800	TEST12	peer_loop	 synchronization loop
1000	TEST13	peer_unreach	 unreachable or nonselect
Kiss Codes

Kiss codes are used in kiss-o'-death (koD) packets, billboard displays and log messages. They consist of a string of four zero-padded ASCII charactes. In practice they are informal and tend to change with time and implementation. Some of these codes can appear in the reference identifier field in ntpq billboards. Following is the current list:

Code	Description
ACST	manycast server
AUTH	authentication error
AUTO	Autokey sequence error
BCST	broadcast server
CRYPT	Autokey protocol error
DENY	access denied by server
INIT	association initialized
MCST	multicast server
RATE	rate exceeded
TIME	association timeout
STEP	step time change
Crypto Messages

These messages are sent to the cryptostats file when an error is detected in the Autokey protocol.

Code	Message	Description
01	bad_format	bad extension field format or length
02	bad_timestamp	bad timestamp
03	bad_filestamp	bad filestamp
04	bad_public_key	bad or missing public key
05	bad_digest	unsupported digest type
06	bad_identity	unsupported identity type
07	bad_siglength	bad signature length
08	bad signature	extension field signature not verified
09	cert_not_verified	certificate signature not verified
0a	cert_expired	host certificate expired
0b	bad_cookie	bad or missing cookie
0c	bad_leapseconds	bad or missing leapseconds values
0d	cert_missing	bad or missing certificate
0e	bad_group_key	bad or missing group key
0f	proto_error	protocol error

		10.5.8 The NTP FAQ and HOWTO: Understanding and using the Network Time Protocol (A first try on a non-technical Mini-HOWTO and FAQ on NTP)
		http://www.ntp.org/ntpfaq/NTP-s-algo.htm
Prev		Next
This is the NTP home page that some people like to see here
| 5. How does it work?

This section will try to explain how NTP will construct and maintain a working time synchronization network.

| 5.1. Basic Concepts

To help understanding the details of planning, configuring, and maintaining NTP, some basic concepts are presented here. The focus in this section is on theory.

| 1. Time References
| 5.1.1.1. What is a reference clock?
| 5.1.1.2. How will NTP use a reference clock?
| 5.1.1.3. How will NTP know about Time Sources?
| 5.1.1.4. What happens if the Reference Time changes?
| 5.1.1.5. What is a stratum 1 Server?
| 2. Time Exchange
| 5.1.2.1. How is Time synchronized?
| 5.1.2.2. Which Network Protocols are used by NTP?
| 5.1.2.3. How is Time encoded in NTP?
| 5.1.2.4. When are the Servers polled?
| 3. Performance
| 5.1.3.1. How accurate will my Clock be?
| 5.1.3.2. How frequently will the System Clock be updated?
| 5.1.3.3. How frequently are Correction Values updated?
| 5.1.3.4. How reliable are those Error-Estimates?
| 5.1.3.5. What is the Limit for the Number of Clients?
| 4. Robustness
| 5.1.4.1. What is the stratum?
| 5.1.4.2. How are Synchronization Loops avoided?
| 5. Tuning
| 5.1.5.1. What is the allowed range for minpoll and maxpoll?
| 5.1.5.2. What is the best polling Interval?
| 6. Operating System Clock Interface
| 5.1.6.1. How will NTP discipline my Clock?
| 1. Time References

| 5.1.1.1. What is a reference clock?

A reference clock is some device or machinery that spits out the current time. The special thing about these things is accuracy: Reference clocks must be accurately following some time standard.

Typical candidates for reference clocks are (very expensive) cesium clocks. Cheaper (and thus more popular) ones are receivers for some time signals broadcasted by national standard agencies. A typical example would be a GPS (Global Positioning System) receiver that gets the time from satellites. These satellites in turn have a cesium clock that is periodically corrected to provide maximum accuracy.

Less expensive (and accurate) reference clocks use one of the terrestrial broadcasts known as DCF77, MSF, and WWV.

In NTP these time references are also named stratum 0, the highest possible quality. (Each system that has its time synchronized to some reference clock can also be a time reference for other systems, but the stratum will increase for each synchronization.)

| 5.1.1.2. How will NTP use a reference clock?

A reference clock will provide the current time, that's for sure. NTP will compute some additional statistical values that describe the quality of time it sees. Among these values are: offset (or phase), jitter (or dispersion), frequency error, and stability (See also Section 3.3). Thus each NTP server will maintain an estimate of the quality of its reference clocks and of itself.

| 5.1.1.3. How will NTP know about Time Sources?

There are serveral ways how a NTP client will know about NTP servers to use:

Servers to be polled can be configured manually
Servers can send the time directly to a peer
Servers may send out the time using multicast or broadcast addresses
| 5.1.1.4. What happens if the Reference Time changes?

Ideally the reference time is the same everywhere in the world. Once synchronized, there should not be any unexpected changes between the clock of the operating system and the reference clock. Therefore, NTP has no special methods to handle the situation.

Instead, ntpd's reaction will depend on the offset between the local clock and the reference time. For a tiny offset ntpd will adjust the local clock as usual; for small and larger offsets, ntpd will reject the reference time for a while. In the latter case the operation system's clock will continue with the last corrections effective while the new reference time is being rejected. After some time, small offsets (significantly less than a second) will be slewed (adjusted slowly), while larger offsets will cause the clock to be stepped (set anew). Huge offsets are rejected, and ntpd will terminate itself, believing something very strange must have happened.

Naturally, the algorithm is also applied when ntpd is started for the first time or after reboot.

| 5.1.1.5. What is a stratum 1 Server?

A server operating at stratum 1 belongs to the class of best NTP servers available, because it has a reference clock (See What is a reference clock?) attached to it. As accurate reference clocks are expensive, only rather few of these servers are publically available.

A stratum 1 server should not only have a precise and well-maintained and calibrated reference clock, but also should be highly available as other systems may rely on its time service. Maybe that's the reason why not every NTP server with a reference clock is publically available.

| 2. Time Exchange

| 5.1.2.1. How is Time synchronized?

Time can be passed from one time source to another, typically starting from a reference clock connected to a stratum 1 server. Servers synchronized to a stratum 1 server will be stratum 2. Generally the stratum of a server will be one more than the stratum of its reference (See also Q: 5.1.4.1.).

Synchronizing a client to a network server consists of several packet exchanges where each exchange is a pair of request and reply. When sending out a request, the client stores its own time (originate timestamp) into the packet being sent. When a server receives such a packet, it will in turn store its own time (receive timestamp) into the packet, and the packet will be returned after putting a transmit timestamp into the packet. When receiving the reply, the receiver will once more log its own receipt time to estimate the travelling time of the packet. The travelling time (delay) is estimated to be half of "the total delay minus remote processing time", assuming symmetrical delays.

Those time differences can be used to estimate the time offset between both machines, as well as the dispersion (maximum offset error). The shorter and more symmetric the round-trip time, the more accurate the estimate of the current time.

Time is not believed until several packet exchanges have taken place, each passing a set of sanity checks. Only if the replies from a server satisfy the conditions defined in the protocol specification, the server is considered valid. Time cannot be synchronized from a server that is considered invalid by the protocol. Some essential values are put into multi-stage filters for statistical purposes to improve and estimate the quality of the samples from each server. All used servers are evaluated for a consistent time. In case of disagreements, the largest set of agreeing servers (truechimers) is used to produce a combined reference time, thereby declaring other servers as invalid (falsetickers).

Usually it takes about five minutes (five good samples) until a NTP server is accepted as synchronization source. Interestingly, this is also true for local reference clocks that have no delay at all by definition.

After initial synchronization, the quality estimate of the client usually improves over time. As a client becomes more accurate, one or more potential servers may be considered invalid after some time.

| 5.1.2.2. Which Network Protocols are used by NTP?

NTP uses UDP/IP packets for data transfer because of the fast connection setup and response times. The official port number for the NTP (that ntpd and ntpdate listen and talk to) is 123.

| 5.1.2.3. How is Time encoded in NTP?

There was a nice answer from Don Payette in news://comp.protocols.time.ntp, slightly adapted:

The NTP timestamp is a 64 bit binary value with an implied fraction point between the two 32 bit halves. If you take all the bits as a 64 bit unsigned integer, stick it in a floating point variable with at least 64 bits of mantissa (usually double) and do a floating point divide by 2^32, you'll get the right answer.

As an example the 64 bit binary value:

00000000000000000000000000000001 10000000000000000000000000000000
equals a decimal 1.5. The multipliers to the right of the point are 1/2, 1/4, 1/8, 1/16, etc.
To get the 200 picoseconds, take a one and divide it by 2^32 (4294967296), you get 0.00000000023283064365386962890625 or about 233E-12 seconds. A picosecond is 1E-12 seconds.

In addition one should know that the epoch for NTP starts in year 1900 while the epoch in UNIX starts in 1970. Therefore the following values both correspond to 2000-08-31_18:52:30.735861

UNIX: 39aea96e.000b3a75
        00111001 10101110 10101001 01101110.
        00000000 00001011 00111010 01110101
NTP:  bd5927ee.bc616000
        10111101 01011001 00100111 11101110.
        10111100 01100001 01100000 00000000
| 5.1.2.4. When are the Servers polled?

When polling servers, a similar algorithm as described in Q: 5.1.3.3. is used. Basically the jitter (white phase noise) should not exceed the wander (random walk frequency noise). The polling interval tries to be close to the point where the total noise is minimal, known as Allan intercept, and the interval is always a power of two. The minimum and maximum allowable exponents can be specified using minpoll and maxpoll respectively (See Q: 5.1.5.1.). If a local reference clock with low jitter is selected to synchronize the system clock, remote servers may be polled more frequently than without a local reference clock in recent version of ntpd. The intended purpose is to detect a faulty reference clock in time.[1]

| 3. Performance

| 5.1.3.1. How accurate will my Clock be?

For a general discussion see Section 3. Also keep in mind that corrections are applied gradually, so it may take up to three hours until the frequency error is compensated (see Figure 3).

Figure 3. Initial Run of NTP



Of course the final achievable accuracy depends on the time source being used. Basically, no client can be more accurate than its server. In addition the quality of network connection also influences the final accuracy. Slow and non predictable networks with varying delays are very bad for good time synchronization.

A time difference of less than 128ms between server and client is required to maintain NTP synchronization. The typical accuracy on the Internet ranges from about 5ms to 100ms, possibly varying with network delays. A recent survey[2] suggests that 90% of the NTP servers have network delays below 100ms, and about 99% are synchronized within one second to the synchronization peer.

With PPS synchronization an accuracy of 50µs and a stability below 0.1 PPM is achievable on a Pentium PC (running Linux for example). However, there are some hardware facts to consider. Judah Levine wrote:

In addition, the FreeBSD system I have been playing with has a clock oscillator with a temperature coefficient of about 2 PPM per degree C. This results in time dispersions on the order of lots of microseconds per hour (or lots of nanoseconds per second) due solely to the cycling of the room heating/cooling system. This is pretty good by PC standards. I have seen a lot worse.

Terje Mathisen wrote in reply to a question about the actual offsets achievable: "I found that 400 of the servers had offsets below 2ms, (...)"

David Dalton wrote about the same subject:

The true answer is: It All Depends.....

Mostly, it depends on your networking. Sure, you can get your machines within a few milliseconds of each other if they are connected to each other with normal 10-Base-T Ethernet connections and not too many routers hops in between. If all the machines are on the same quiet subnet, NTP can easily keep them within one millisecond all the time. But what happens if your network get congested? What happens if you have a broadcast storm (say 1,000 broadcast packets per second) that causes your CPU to go over 100% load average just examining and discarding the broadcast packets? What happens if one of your routers loses its mind? Your local system time could drift well outside the "few milliseconds" window in situations like these.

| 5.1.3.2. How frequently will the System Clock be updated?

As time should be a continuous and steady stream, ntpd updates the clock in small quantities. However, to keep up with clock errors, such corrections have to be applied frequently. If adjtime() is used, ntpd will update the system clock every second. If ntp_adjtime() is available, the operating system can compensate clock errors automatically, requiring only infrequent updates. See also Section 5.2 and Q: 5.1.6.1..

| 5.1.3.3. How frequently are Correction Values updated?

NTP maintains an internal clock quality indicator. If the clock seems stable, updates to the correction parameters happen less frequent. If the clock seems instable, more frequent updates are scheduled. Sometimes the update interval is also termed stiffness of the PLL, because only small changes are possible for long update intervals.

There's a decision value named poll adjust that can be queried with ntpdc's loopinfo command. A value of -30 means to decrease the polling interval, while a value of 30 means to increase it (within the bounds of minpoll and maxpoll). The value of watchdog timer is the time since the last update.

ntpdc> loopinfo
offset:               -0.000102 s
frequency:            16.795 ppm
poll adjust:          6
watchdog timer:       63 s
Recent versions of ntpd (like 4.1.0) seem to update the correction values less frequently, possibly causing problems. Even if the reference time sources are polled more frequently, the local system clock is adjusted less often.

| 5.1.3.4. How reliable are those Error-Estimates?

While in theory estimates of the clock error are maintained, there were practically some software bugs that made these numbers questionable. For example the new kernel clock model dealing with nanosecond resolution (nanokernel) produced overly optimistic estimates regarding the clock offset. The bug has been fixed in August 2000, but also different versions of the NTP daemon may produce different estimates for the same hardware.

| 5.1.3.5. What is the Limit for the Number of Clients?

The limit actually depends on several factors, like speed of the main processor and network bandwidth, but the limit is quite high. Terje Mathisen once presented a calculation:

2 packets/256 seconds * 500 K machines -> 4 K packets/second (half in each direction).

Packet size is close to minimum, definitely less than 128 bytes even with cryptographic authentication:

4 K * 128 -> 512 KB/s.

So, as long as you had a dedicated 100 Mbit/s full duplex leg from the central switch for each server, it should see average networks load of maximim 2-3%.

| 4. Robustness

| 5.1.4.1. What is the stratum?

The stratum is a measure for synchronization distance. Opposed to jitter or delay the stratum is a more static measure. Basically (and from the perspective from a client) it is the number of servers to a reference clock. So a reference clock itself appears at stratum 0, while the closest servers are at stratum 1. On the network there is no valid NTP message with stratum 0.

A server synchronized to a stratum n server will be running at stratum n + 1. The upper limit for stratum is 15. The purpose of stratum is to avoid synchronization loops by preferring servers with a lower stratum.

| 5.1.4.2. How are Synchronization Loops avoided?

In a synchonization loop, the time derived from one source along a specific path of servers is used as reference time again within such a path. This may cause an excessive accumulation of errors that is to be avoided. Therefore NTP uses different means to accomplish that:

The Internet address of a time source is used as reference identifier to avoid duplicates. The reference identifier is limited to 32 bits however.

The stratum as described in Q: 5.1.4.1. is used to form an acyclic synchronization network.

More precisely[2], the algorithm finds a shortest path spanning tree with metric based on synchronization distance dominated by hop count. The reference identifier provides additional information to avoid neighbor loops under conditions where the topology is changing rapidly. This is a very well known problem with algorithms such as this. See any textbook on computer network routing algorithms. Computer Networks by Bertsekas and Gallagher is a good one.

In IPv6 the reference ID field is a timestamp that can be used for the same purpose.

| 5. Tuning

| 5.1.5.1. What is the allowed range for minpoll and maxpoll?

The default polling value after restart of NTP is the value specified by minpoll. The default values for minpoll and maxpoll are 6 (64 seconds) and 10 (1024 seconds) respectively.

For xntp3-5.93e the smallest and largest allowable polling values are 4 (16 seconds) and 14 (4.5 hours) respectively. For actual polling intervals larger than 1024 seconds the kernel discipline is switched to FLL mode.

For ntp-4.0.99f the smallest and largest allowable polling values are 4 (16 seconds) and 17 (1.5 days) respectively. These values come from the include file ntp.h. The revised kernel discipline automatically switches to FLL mode if the update interval is longer than 2048 seconds. Below 256 seconds PLL mode is used, and in between these limits the mode can be selected using the STA_FLL bit.

| 5.1.5.2. What is the best polling Interval?

Actually there is none: Short polling intervals update the parameters frequently and are sensitive to jitter and random errors. Long intervals may require larger corrections with significant errors between the updates. However there seems to be an optimum between those two. For common operating system clocks this value happens to be close to the default maximum polling time, 1024s. See also Q: 5.1.3.1..

| 6. Operating System Clock Interface

| 5.1.6.1. How will NTP discipline my Clock?

In order to keep the right time, xntpd must make adjustments to the system clock. Different operating systems provide different means, but the most popular ones are listed below.

Basically there are four mechanisms (system calls) an NTP implementation can use to discipline the system clock (For details see the different RFCs found in Table 4):

settimeofday(2) to step (set) the time. This method is used if the time if off by more than 128ms.

adjtime(2) to slew (gradually change) the time. Slewing the time means to change the virtual frequency of the software clock to make the clock go faster or slower until the requested correction is achieved. Slewing the clock for a larger amount of time may require some time, too. For example standard Linux adjusts the time with a rate of 0.5ms per second.

ntp_adjtime(2) to control several parameters of the software clock (also known as kernel discipline). Among these parameters are:

Adjust the offset of the software clock, possibly correcting the virtual frequency as well

Adjust the virtual frequency of the software clock directly

Enable or disable PPS event processing

Control processing of leap seconds

Read and set some related characteristic values of the clock

hardpps() is a function that is only called from an interrupt service routine inside the operating system. If enabled, hardpps() will update the frequency and offset correction of the kernel clock in response to an external signal (See also Section 6.2.4).

Notes

[1]	
This statement was derived from a mail message by Professor David L. Mills in response to a suspected bug in version 4.1.0.

[2]	
...says Professor David L. Mills...

This is the NTP home page that some people like to see here
Prev	Home	Next
Implementations and Platforms	 	The Kernel Discipline


		10.5.10 Fixing NTP Refusing to Sync
April 26th, 2010Leave a commentGo to comments
I have just been confronted by NTP absolutely refusing to touch my system’s clock. The trouble with NTP is that it is absolute PITA to debug it at all since when it does not get in sync with its peers, it goes at great lengths to make its reasons as incomprehensible as possible.

For some reason, my system had absolutely massive drift – something in the order of half a second per minute, making the clock drift by several tens of minutes per day. So I installed NTP and hoped that it would magically fix up the issue, but it turns out that NTP by itself is absolutely unhelpful not only in cases of big offset, but also in cases of big drift – it will fix your clock when it is slightly inaccurate, but not when it is inaccurate a lot (…that is, when you would want to use it all the more).

First thing I did was check the hardware’s opinion. Comparing date and hwclock --show has shown that the hardware clock is doing fine, only kernel’s idea of time is drifting off. Next, it’s time to see what NTP thinks about its peers:

ntpq> peers
     remote           refid      st t when poll reach   delay   offset  jitter
==============================================================================
 tik.cesnet.cz   .GPS.            1 u   12   64  377    0.641  8494.05 2911.29
 tak.cesnet.cz   .GPS.            1 u    2   64  377    0.636  8594.86 2945.05
NTP polls each peer every “poll” seconds, “when” is relative time of last poll; “reach” keeps track of last successful polls, 377 is best. “Delay” is network delay, this is fine. “Offset” is the offset between local and peer clock, it’s at 8.5s now – not so good, but trouble is it gets bigger quickly. But what’s the real culprit is “jitter” – it’s huge! This means that the variance of offsets is huge – to put it simply, the offset is very different each time it is measured. Since no symbols are printed in the first column of the output, there is no peer synchronization going on.

So if we know a lot about NTP already, the high jitter should hint us that the offset measurements are unreliable. But the network connection of our server is very good, it would be nice to look at the actual measurements. Instead of peers, let’s look at their associations:

ntpq> as

ind assID status  conf reach auth condition  last_event cnt
===========================================================
  1 55713  9014   yes   yes  none    reject   reachable  1
  2 55714  9014   yes   yes  none    reject   reachable  1
NTP is not liking our peers. No surprise, with the big jitter. But what we are after are the assID numbers:

ntpq> rv 55713
assID=55713 status=9014 reach, conf, 1 event, event_reach,
srcadr=tik.cesnet.cz, srcport=123, dstadr=195.113.20.142, dstport=123,
leap=00, stratum=1, precision=-20, rootdelay=0.000,
rootdispersion=0.000, refid=GPS, reach=377, unreach=0, hmode=3, pmode=4,
hpoll=6, ppoll=6, flash=400 peer_dist, keyid=0, ttl=0, offset=13041.231,
delay=0.602, dispersion=0.944, jitter=2918.331,
reftime=cf803b51.ddd3e70e  Mon, Apr 26 2010 18:18:25.866,
org=cf803b83.e9b29181  Mon, Apr 26 2010 18:19:15.912,
rec=cf803b76.df382c7c  Mon, Apr 26 2010 18:19:02.871,
xmt=cf803b76.df0d40c7  Mon, Apr 26 2010 18:19:02.871,
filtdelay=     0.60    0.64    0.60    0.51    0.82    0.67    0.69    0.64,
filtoffset= 13041.2 12385.8 11720.4 11075.2 10409.6 9774.54 9129.22 8494.06,
filtdisp=      0.00    0.98    1.97    2.93    3.92    4.86    5.82    6.77
Looking at the last three lines, the reason for the huge jitter finally seems clear! Our clock drifts so fast that the offset will go up by several seconds through our few measurements.

Unfortunately, NTP does not seem to be giving us the actual estimated drift value between local clock and the peer. This would be very useful since that’s actually what makes NTP decide whether go ahead and sync or keep its hands away from the clock; it is said that 500ppm is the max. drift value for possible synchronization, but I don’t know how to connect that to any of the other numbers I see; when the clock is already in sync, it is probably the ‘frequency’ value in ‘rv’ (and it is stored in the drift file), but this value stays untouched before synchronization. Too bad.

So, now we know the issue is that kernel clock is going too slow and that NTP is not going to fix it for ourselves. So, we must resort to manual tinkering using adjtimex:

# adjtimex -p
         mode: 0
       offset: 0
    frequency: 0
     maxerror: 0
     esterror: 0
       status: 64
time_constant: 4
    precision: 1
    tolerance: 32768000
         tick: 9900
     raw time:  1272299204s 17444us = 1272299204.017444
 return value = 5
Wow, a lot of numbers. But the one that tells how fast the clock is going is the ‘tick’ value, and you can adjust it using adjtimex -t 10000 – that will make the clock go a lot faster, and is also sort-of canonical value. Let’s just do that and restart ntpd:

     remote           refid      st t when poll reach   delay   offset  jitter
==============================================================================
 tik.cesnet.cz   .GPS.            1 u    1   64    7    0.659  16852.5   1.840
 tak.cesnet.cz   .GPS.            1 u    2   64    7    0.665  16852.5   1.863
This is MUCH better! In fact, after few minutes NTP will decide to step the clock to compensate the offset, and after another while it will finally get in sync with the peers. If the jitter is still too big (but different), keep tweaking the tick value.

EDIT: It seems that alternatively, you can try to change your clock source – this might help especially in case of virtualization:

# cat /sys/devices/system/clocksource/clocksource0/available_clocksource
hpet acpi_pm jiffies tsc
# cat /sys/devices/system/clocksource/clocksource0/current_clocksource
hpet

Hope this helps if your NTP also refuses to fix your clock.

Open questions remain:

Why was my tick value so off? I guess I will never know. Maybe a reboot would fix it too, but I wasn’t keen to do that.
How to determine drift-per-peer value to see how much out of bounds it is?
How to make NTP automatically fix even huge drifts?
Why is NTP crafted to be so hard to debug without spending tens of minutes googling, staring at bunches of floats and decoding bitmasks manually?
Thanks to prema and otis for ideas and help.

Categories: linuxTags: clock, debian, ntp
Comments (8)Trackbacks (2)Leave a commentTrackback

Stefan Seyfried
April 28th, 2010 at 18:58	 | #1 Reply | Quote
Hi pasky,
I have seen similar stuff when /etc/adjtime was totally off. /etc/adjtime is used to record the time drift and to initialize adjtimex values.
I think it should not be used when NTP is in use, unless the kernel time is off grossly, which is seldom the case nowadays.
Unfortunately, many distributions still do a “hwclock –systohc” on shutdown – which updates /etc/adjtime, even nowadays, where even the CMOS clocks on PCs are pretty accurate. I think this is a bug, but I gave up, at least for SUSE to argue, and I just disable it manually in the shutdown scripts.
Hope this helps,
seife

Stefan Seyfried
April 28th, 2010 at 18:59	 | #2 Reply | Quote
Oh, I forgot: usually just removing /etc/adjtime and doing “hwclock –systohc –utc” once is enough to get it fixed for all time.

Larry McCarthy
May 29th, 2010 at 17:56	 | #3 Reply | Quote
You asked: “Why was my tick value so off? I guess I will never know. Maybe a reboot would fix it too, but I wasn’t keen to do that.”
I was having drift problems, so installed adjtime. When adjtime installed is spent 70 seconds “initializing” itself and set ticks to 9768. As you can imagine, this made the problem waaaay worse, to the point that NTP wouldn’t sync and I’d lost ~30 minutes over night.
So, your excellent adjtime vs. NTP analysis was just the thing I needed! Thanks!

Larry McCarthy
May 29th, 2010 at 18:20	 | #4 Reply | Quote
So, based on this really excellent article (I’m stoked; this helped *a lot*), here’s a little trick (sorry; don’t know how to tag “code” here):
# /etc/init.d/ntp stop ; ntpd -q ; sleep 100s ; ntpd -q; /etc/init.d/ntp start
This stops NTP, forces it to sync the clock (to “prime the pump”), sleeps for 100 seconds, forces a second clock sync, and restarts NTP. It produces output like this:
Stopping NTP server: ntpd.
ntpd: time set +12.262938s
ntpd: time set +2.623381s <— drift per 100s
Starting NTP server: ntpd.
The second "time set" – +2.623381s – is your 100s drift. Take that drift, (as a proportion of the current ticks), add (use the sign of the drift – if the sign on the drift is "-", you'd subtract ticks) it to the adjtime adjustment, and repeat 'til satisfied, like so:
# adjtimex -p
[...]
tick: 10000
[...]
# # add (+2.62s / 100s) * 10000 = 262 ticks
# adjtimex -t 10262
# /etc/init.d/ntp stop ; ntpd -q ; sleep 100s ; ntpd -q; /etc/init.d/ntp start
Stopping NTP server: ntpd.
ntpd: time set +3.044932s
ntpd: time set -0.259021s
Starting NTP server: ntpd
Now, -0.26s drift per 100s is probably "correctable" by NTP. If not, repeat this process some more…
- Larry

Larry McCarthy
May 29th, 2010 at 18:53	 | #5 Reply | Quote
Sorry to go on, but just one more thing, I promise…
So, once you get the drift below a second, you probably want to do a longer drift sample. To keep the arithmetic simple, continuing with the above example:
# /etc/init.d/ntp stop ; ntpd -q ; sleep 1026s ; ntpd -q; /etc/init.d/ntp start
Stopping NTP server: ntpd.
ntpd: time set -1.428262s <– Remember, ignore this. Take a 17m coffee break.
ntpd: time set -2.333034s
Starting NTP server: ntpd.
# # OK, remember, [tick] is currently 10262, so ((-2.33 / 1026) * 10262) ~ (-23)
# # and 10262 + (-23) = 10239, so…
# adjtimex -t 10239
To watch the results, I do:
# watch -n 8 ntpq -p
Yeah, 8s is kinda fast, but I'm the kind of guy who takes surface streets when the freeway's slow, just to have something to do…
Thanks again for this great post!
- Larry

Larry McCarthy
May 29th, 2010 at 19:04	 | #6 Reply | Quote
I know I promised, but this is important:
Once you’re happy with your jitter values – Single digits! Finally! – as Stefan said, you MUST do:
# rm /etc/adjtime
# hwclock –systohc –utc
*NOTE*: Windows dual-booters probably want to do:
# hwclock –systohc –localtime
And those are double-dashes; WordPress turns them into a single em-dash. Sorry.
OK. That’s it. I promise. :)

Deiva
May 10th, 2013 at 07:27	 | #7 Reply | Quote
Hi,
For me manually the ntp service syncing using ntpdate -u but when we start the service its giving the below output…please advice should i need to change the adtimex here am using redhat linux
[root@deiva ~]# ntpq -pn
remote refid st t when poll reach delay offset jitter
==============================================================================
*127.127.1.0 .LOCL. 10 l 7 64 37 0.000 0.000 0.001
|   172.16.8.4 .GPS. 1 u 8 64 37 5.462 -3029.9 1910.44
|   172.16.8.5 .GPS. 1 u 3 64 37 5.198 -3091.9 1946.43
[root@deiva ~]# service ntpd restart
Shutting down ntpd: [ OK ]
ntpd: Synchronizing with time server: [ OK ]
Syncing hardware clock to system time [ OK ]
Starting ntpd: [ OK ]
[root@deiva ~]# date
Sat May 4 17:36:25 CEST 2013
[root@deiva ~]# date
Sat May 4 17:39:56 CEST 2013
[root@deiva ~]# ntpq -pn
remote refid st t when poll reach delay offset jitter
==============================================================================
*127.127.1.0 .LOCL. 10 l 33 64 17 0.000 0.000 0.001
|   172.16.8.4 .GPS. 1 u 32 64 17 4.721 -56.787 2147.63
|   172.16.8.5 .GPS. 1 u 31 64 17 4.971 -72.407 2161.70
[root@deiva ~]# exit
== manuall service ==
[root@deiva ~]# ntpdate -u 172.16.8.4
30 Apr 20:12:28 ntpdate[96890]: step time server 172.16.8.4 offset -0.975695 sec
[root@deiva ~]# ntpq -pn
remote refid st t when poll reach delay offset jitter
==============================================================================
|   127.127.1.0 .LOCL. 10 l 21 64 3 0.000 0.000 0.001
|   172.16.8.4 .GPS. 1 u 21 64 3 5.175 -55.482 0.702
|   172.16.8.5 .GPS. 1 u 19 64 3 5.673 -71.212 15.585
ntpq> as
ind assID status conf reach auth condition last_event cnt
===========================================================
1 56510 9614 yes yes none sys.peer reachable 1
2 56511 9014 yes yes none reject reachable 1
3 56512 9014 yes yes none reject reachable 1
ntpq> rv 56511
assID=56511 status=9014 reach, conf, 1 event, event_reach,
srcadr=172.16.8.4, srcport=123, dstadr=10.69.23.2, dstport=123, leap=00,
stratum=1, precision=-9, rootdelay=0.000, rootdispersion=5.676,
refid=GPS, reach=377, unreach=0, hmode=3, pmode=4, hpoll=10, ppoll=10,
flash=400 peer_dist, keyid=0, ttl=0, offset=-23003.227, delay=4.651,
dispersion=12.835, jitter=10710.871,
reftime=d529fedf.146a7ef9 Tue, Apr 30 2013 10:27:11.079,
org=d529fef8.9c49ba5e Tue, Apr 30 2013 10:27:36.610,
rec=d529ff17.88f783ee Tue, Apr 30 2013 10:28:07.535,
xmt=d529ff17.8242ad4e Tue, Apr 30 2013 10:28:07.508,
filtdelay= 4.95 4.65 5.95 6.56 4.99 4.82 5.27 5.94,
filtoffset= -30922. -23003. -19036. -15038. -13023

		10.5.11 Timekeeping best practices for Linux guests (1006427) 
http://kb.vmware.com/selfservice/microsites/search.do?language=en_US&cmd=displayKC&externalId=1006427
Purpose
This article presents best practices for Linux timekeeping. These recommendations include specifics on the particular kernel command line options to use for the Linux operating system of interest. There is also a description of the recommended settings and usage for NTP time sync, configuration of VMware Tools time synchronization, and Virtual Hardware Clock configuration, to achieve best timekeeping results.

Resolution
Linux Timekeeping Best Practices

Notes:

    When both SMP and UP kernels are available, they must be applied to the appropriate SMP and UP virtual machines, otherwise a mismatch may cause time to drift.
    The recommended kernel parameters must be added to the kernel parameters already configured by the distribution.


32-bit Kernels

Linux Flavor 	Version 	Kernel Parameters 	Notes
RHEL 	RHEL 6.x 		No additional kernel parameters required.
RHEL 5.8 		No additional kernel parameters required. See the note below about the optional use of divider=10.
RHEL 5.7 		No additional kernel parameters required. See the note below about the optional use of divider=10.
RHEL 5.6 		No additional kernel parameters required. See the note below about the optional use of divider=10.
RHEL 5.5 		No additional kernel parameters required. See the note below about the optional use of divider=10.
RHEL 5.4 		No additional kernel parameters required. See the note below about the optional use of divider=10.
RHEL 5.3 	divider=10 clocksource=acpi_pm 	
RHEL 5.2 	divider=10 clocksource=acpi_pm 	
RHEL 5.1 	divider=10 clocksource=acpi_pm 	
RHEL 5.0 	clocksource=acpi_pm 	
RHEL 4.9 	clock=pmtmr
divider=10
hpet=disable 	
RHEL 4.8 	clock=pmtmr
divider=10
hpet=disable 	
RHEL 4.7 	clock=pmtmr
divider=10
hpet=disable 	
RHEL 4.6 	clock=pmtmr
hpet=disable 	
RHEL 4.5 	clock=pmtmr
hpet=disable 	
RHEL 4.4 	clock=pmtmr
hpet=disable 	
RHEL 4.3 	clock=pmtmr
hpet=disable 	
RHEL 4.2 	clock=pmtmr
hpet=disable 	
RHEL 4.1 	clock=pmtmr
hpet=disable 	
RHEL 4.0 	clock=pmtmr
hpet=disable 	
RHEL 3 (All updates) 		No additional kernel parameters required.
RHEL 2.1 		No additional kernel parameters required.
Redhat 	Redhat 9.0 		No additional kernel parameters required.
Redhat 8.0 		No additional kernel parameters required.
Redhat 7.3 		No additional kernel parameters required.
Redhat 7.2 		No additional kernel parameters required.
Redhat 7.1 		No additional kernel parameters required.
Redhat 7.0 		No additional kernel parameters required.
Redhat 6.2 		No additional kernel parameters required.
SLES 	SLES 11 (All updates) 		No additional kernel parameters required.
SLES 10 SP4 on ESX 5.0 and later 	clock=pmtmr
hpet=disable 	
SLES 10 SP4 on ESX 4.x 		Use a VMI enabled kernel.
SLES 10 SP3 on ESXi 5.0 	clock=pmtmr
hpet=disable 	
SLES 10 SP3 on ESX 3.5 and 4.x 		Use a VMI enabled kernel.
SLES 10 SP3 on ESX 3.0.x and earlier 	clock=pmtmr
hpet=disable 	
SLES 10 SP2 on ESXi 5.0 	clock=pmtmr
hpet=disable 	
SLES 10 SP2 on ESX 3.5 and 4.x 		Use a VMI enabled kernel.
SLES 10 SP2 on ESX 3.0.x and earlier 	clock=pmtmr
hpet=disable 	
SLES 10 SP1 	clock=pmtmr
hpet=disable 	
SLES 10 	clock=pmtmr
hpet=disable 	
SLES 9 (All updates) 	clock=pmtmr
hpet=disable 	
SLES 8 		No additional kernel parameters required.
SLES 7 		No additional kernel parameters required.
SLED 	SLED 11 		No additional kernel parameters required.
SLED 10 	clock=pmtmr
hpet=disable 	
SLED 9 	clock=pmtmr
hpet=disable 	
Suse Linux 	Suse Linux 10.3 	clocksource=acpi_pm 	
Suse Linux 10.2 	clocksource=acpi_pm 	
Suse Linux 10.1 	clock=pmtmr
hpet=disable 	
Suse Linux 10 	clock=pmtmr
hpet=disable 	
Suse Linux 9.3 	clock=pmtmr
hpet=disable 	
Suse Linux 9.2 	clock=pmtmr
hpet=disable 	
Suse Linux 9.1 	clock=pmtmr
hpet=disable 	
Suse Linux 9.0 		Remove desktop kernel command line parameter.
Suse Linux 8.2 		No additional kernel parameters required.
Suse Linux 8.1 		No additional kernel parameters required.
Suse Linux 8.0 		No additional kernel parameters required.
Suse Linux 7.3 		No additional kernel parameters required.
Ubuntu 	Ubuntu 12.04 		No additional kernel parameters required.
Ubuntu 11.10 		No additional kernel parameters required.
Ubuntu 11.04 		No additional kernel parameters required.
Ubuntu 10.10 		No additional kernel parameters required.
Ubuntu 10.04 		No additional kernel parameters required.
Ubuntu 9.10 		No additional kernel parameters required.
Ubuntu 9.04 with kernel 2.6.28-7.18 or later on ESX 4.0 or later 		No additional kernel parameters required.
Ubuntu 9.04 with kernel prior to 2.6.28-7.18 on ESX 4.0 or later 		Avoid using if possible. May cause guest operating system to stop responding.
Ubuntu 8.10 with kernel 2.6.27-12.28 or later on ESX 4.0 or later 		No additional kernel parameters required.
Ubuntu 8.10 with kernel prior to 2.6.27-12.28 on ESX 4.0 or later 		Avoid using if possible. May cause guest operating system to stop responding.
Ubuntu 8.10 	clocksource=acpi_pm 	
Ubuntu 8.04 on ESXi 5.0 	clocksource=acpi_pm 	
Ubuntu 8.04 on ESX 3.5 and 4.x 		Use a VMI enabled kernel.
Ubuntu 8.04 on ESX 3.0.x and earlier 	clocksource=acpi_pm 	
Ubuntu 7.10 	clocksource=acpi_pm 	
Ubuntu 7.04 	clocksource=acpi_pm 	
Ubuntu 6.10 	clock=pmtmr
hpet=disable 	
Ubuntu 6.06 	clock=pmtmr
hpet=disable 	
Ubuntu 5.10 	clock=pmtmr
hpet=disable 	
Ubuntu 5.04 	clock=pmtmr
hpet=disable 	
Mandriva 	Mandriva Corporate Desktop 4.0 	clock=pmtmr
hpet=disable 	
Mandriva Corporate Server 4 	clock=pmtmr
hpet=disable 	
Mandriva Linux 2008 	clocksource=acpi_pm 	
Mandriva Linux 2007 	clock=pmtmr
hpet=disable 	
Mandriva Linux 2006 	clock=pmtmr
hpet=disable 	
Mandrake 10.1 		Does not support pmtmr. Avoid using if possible.
Mandrake 10 		Does not support pmtmr. Avoid using if possible.
Mandrake 9.2 		No additional kernel parameters required.
Mandrake 9.1 		No additional kernel parameters required.
Mandrake 9.0 		No additional kernel parameters required.
Mandrake 8.2 		No additional kernel parameters required.
Mandrake 8.1 		No additional kernel parameters required.
Mandrake 8.0 		No additional kernel parameters required.
Turbolinux 	Turbolinux 10 Desktop 		Does not support pmtmr. Avoid using if possible.
Turbolinux 10 Server 	clock=pmtmr
hpet=disable 	
Turbolinux Enterprise 8 		No additional kernel parameters required.
Turbolinux Workstation 8 		No additional kernel parameters required.
Turbolinux 7.0 		No additional kernel parameters required.
Asianux 	Asianux Server 4 (all updates) 		No additional kernel parameters required.
Asianux 3.0 SP4 		No additional kernel parameters required.
Asianux 3.0 SP3 		No additional kernel parameters required.
Asianux 3.0 SP2 		No additional kernel parameters required.
Asianux 3.0 SP1 		No additional kernel parameters required. Use kernel 2.6.18-53.17AXS3 or later.
Asianux 3.0 	divider=10 clocksource=acpi_pm 	
CentOS 	CentOS 6.x 		No additional kernel parameters required.
CentOS 5.8 		No additional kernel parameters required. See the note below about the optional use of divider=10.
CentOS 5.7 		No additional kernel parameters required. See the note below about the optional use of divider=10.
CentOS 5.6 		No additional kernel parameters required. See the note below about the optional use of divider=10.
CentOS 5.5 		No additional kernel parameters required. See the note below about the optional use of divider=10.
CentOS 5.4 		No additional kernel parameters required. See the note below about the optional use of divider=10.
CentOS 5.3 	divider=10 clocksource=acpi_pm 	
CentOS 5.2 	divider=10 clocksource=acpi_pm 	
CentOS 5.1 	divider=10 clocksource=acpi_pm 	
CentOS 5.0 	clocksource=acpi_pm 	
CentOS 4.9 	clock=pmtmr
divider=10
hpet=disable 	
CentOS 4.8 	clock=pmtmr
divider=10
hpet=disable 	
CentOS 4.7 	clock=pmtmr
divider=10
hpet=disable 	
CentOS 4.6 	clock=pmtmr
hpet=disable 	
CentOS 4.5 	clock=pmtmr
hpet=disable 	
CentOS 4.4 	clock=pmtmr
hpet=disable 	
CentOS 4.3 	clock=pmtmr
hpet=disable 	
CentOS 4.2 	clock=pmtmr
hpet=disable 	
CentOS 4.1 	clock=pmtmr
hpet=disable 	
CentOS 4.0 	clock=pmtmr
hpet=disable 	
Oracle 	Oracle Linux 6.x 		No additional kernel parameters required.
Oracle Linux 5.8 		No additional kernel parameters required. See the note below about the optional use of divider=10.
Oracle Linux 5.7 		No additional kernel parameters required. See the note below about the optional use of divider=10.
Oracle Linux 5.6 		No additional kernel parameters required. See the note below about the optional use of divider=10.
OEL 5.5 		No additional kernel parameters required. See the note below about the optional use of divider=10.
OEL 5.4 		No additional kernel parameters required. See the note below about the optional use of divider=10.
OEL 5.3 	divider=10 clocksource=acpi_pm 	
OEL 5.2 	divider=10 clocksource=acpi_pm 	
OEL 5.1 	divider=10 clocksource=acpi_pm 	
OEL 5.0 	clocksource=acpi_pm 	
OEL 4.9 	clock=pmtmr
divider=10
hpet=disable 	
OEL 4.8 	clock=pmtmr
divider=10
hpet=disable 	
OEL 4.7 	clock=pmtmr
divider=10
hpet=disable 	
OEL 4.6 	clock=pmtmr
hpet=disable 	
OEL 4.5 	clock=pmtmr
hpet=disable 	
Debian 	Debian 6.0 		No additional kernel parameters required.
Debian 5.0 		No additional kernel parameters required.
Debian 4.x 	divider=10 clocksource=acpi_pm 	


64-bit Kernels
Linux Flavor 	Version 	Kernel Parameters 	Notes
RHEL 	RHEL 6.x 		No additional kernel parameters required.
RHEL 5.8 		No additional kernel parameters required. See the note below about the optional use of divider=10.
RHEL 5.7 		No additional kernel parameters required. See the note below about the optional use of divider=10.
RHEL 5.6 		No additional kernel parameters required. See the note below about the optional use of divider=10.
RHEL 5.5 		No additional kernel parameters required. See the note below about the optional use of divider=10.
RHEL 5.4 		No additional kernel parameters required. See the note below about the optional use of divider=10.
RHEL 5.3 	notsc divider=10
nohpet 	
RHEL 5.2 	notsc divider=10
nohpet 	
RHEL 5.1 with RHSA-2007:0993-13 	notsc divider=10
nohpet 	
RHEL 5.1 without RHSA-2007:0993-13 	notsc
nohpet 	
RHEL 5.0 		No additional kernel parameters required.
RHEL 4.9 	notsc divider=10
nohpet 	
RHEL 4.8 	notsc divider=10
nohpet 	
RHEL 4.7 	notsc divider=10
nohpet 	
RHEL 4.6 	notsc
nohpet 	
RHEL 4.5 	notsc
nohpet 	
RHEL 4.4 	notsc
nohpet 	
RHEL 4.3 	notsc
nohpet 	
RHEL 4.2 	notsc
nohpet 	
RHEL 4.1 	nohpet 	Does not support notsc. Avoid using if possible.
RHEL 4.0 	nohpet 	Does not support notsc. Avoid using if possible.
RHEL 3 Update 9 with RHSA-2008-0973 	disable_lost_ticks 	
RHEL 3 through Update 8 		Has no workaround for lost tick overcompensation. Avoid using if possible.
SLES 	SLES 11 (All updates) 		No additional kernel parameters required.
SLES 10 SP4 on ESX 4.0 and later 		No additional kernel parameters required.
SLES 10 SP3 on ESX 3.5 and later 		No additional kernel parameters required.
SLES 10 SP3 on ESX 3.0.x and earlier 	notsc
nohpet 	
SLES 10 SP2 on ESX 3.5 and later 		No additional kernel parameters required.
SLES 10 SP2 on ESX 3.0.x and earlier 	notsc
nohpet 	
SLES 10 SP1 	notsc
nohpet 	
SLES 10 	notsc
nohpet 	
SLES 9 with kernel version 2.6.5-7.312 or later 	ignore_lost_ticks
nohpet 	
SLES 9 with kernel version 2.6.5-7.311 or earlier 	nohpet 	Has no workaround for lost tick overcompensation. Avoid using if possible.
SLED 	SLED 11 		No additional kernel parameters required.
SLED 10 	clock=pmtmr
nohpet 	
Suse Linux 	Suse Linux 10.3 	clocksource=acpi_pm 	
Suse Linux 10.2 	notsc
nohpet 	
Suse Linux 10.1 	notsc
nohpet 	
Suse Linux 10 	notsc
nohpet 	
Suse Linux 9.3 	notsc
nohpet 	
Suse Linux 9.2 	nohpet 	Does not support notsc. Avoid using if possible.
Suse Linux 9.1 	nohpet 	Does not support notsc. Avoid using if possible.
Ubuntu 	Ubuntu 12.04 		No additional kernel parameters required.
Ubuntu 11.10 		No additional kernel parameters required.
Ubuntu 11.04 		No additional kernel parameters required.
Ubuntu 10.10 		No additional kernel parameters required.
Ubuntu 10.04 		No additional kernel parameters required.
Ubuntu 9.10 		No additional kernel parameters required.
Ubuntu 9.04 with kernel 2.6.28-7.18 or later on ESX 4.0 or later 		No additional kernel parameters required.
Ubuntu 9.04 with kernel prior to 2.6.28-7.18 on ESX 4.0 or later 		Avoid using if possible. May cause guest operating system to stop responding.
Ubuntu 8.10 with kernel 2.6.27-12.28 or later on ESX 4.0 or later 		No additional kernel parameters required.
Ubuntu 8.10 with kernel prior to 2.6.27-12.28 on ESX 4.0 or later 		Avoid using if possible. May cause guest operating system to stop responding.
Ubuntu 8.10 	clocksource=acpi_pm 	
Ubuntu 8.04 with kernel 2.6.24-24.52 or later on ESX 4.0 or later 		No additional kernel parameters required.
Ubuntu 8.04 with kernel prior to 2.6.24-24.52 on ESX 4.0 or later 		Avoid using if possible. May cause guest operating system to stop responding.
Ubuntu 8.04 	clocksource=acpi_pm 	
Ubuntu 7.10 	clocksource=acpi_pm 	
Ubuntu 7.04 		No additional kernel parameters required.
Ubuntu 6.10 	notsc
nohpet 	
Ubuntu 6.06 	notsc
nohpet 	
Ubuntu 5.10 	notsc
nohpet 	
Ubuntu 5.04 	nohpet 	Does not support notsc. Avoid using if possible.
Mandriva 	Mandriva Corporate Desktop 4.0 	notsc
nohpet 	
Mandriva Corporate Server 4 	notsc
nohpet 	
Mandriva Linux 2008 	clocksource=acpi_pm 	
Mandriva Linux 2007 	notsc
nohpet 	
Mandriva Linux 2006 	notsc
nohpet 	
Mandrake Linux 10.1 	nohpet 	Does not support notsc. Avoid using if possible.
Turbolinux 	Turbolinux 10 Desktop 	nohpet 	Does not support notsc. Avoid using if possible.
Turbolinux 10 Server 	nohpet 	Does not support notsc. Avoid using if possible.
Asianux 	Asianux Server 4 (all updates) 		No additional kernel parameters required.
Asianux 3.0 SP4 		No additional kernel parameters required.
Asianux 3.0 SP3 		No additional kernel parameters required.
Asianux 3.0 SP2 		No additional kernel parameters required.
Asianux 3.0 SP1 		No additional kernel parameters required. Use kernel 2.6.18-53.17AXS3 or later.
Asianux 3.0 	notsc
nohpet 	
CentOS 	CentOS 6.x 		No additional kernel parameters required.
CentOS 5.8 		No additional kernel parameters required. See the note below about the optional use of divider=10.
CentOS 5.7 		No additional kernel parameters required. See the note below about the optional use of divider=10.
CentOS 5.6 		No additional kernel parameters required. See the note below about the optional use of divider=10.
CentOS 5.5 		No additional kernel parameters required. See the note below about the optional use of divider=10.
CentOS 5.4 		No additional kernel parameters required. See the note below about the optional use of divider=10.
CentOS 5.3 	notsc divider=10
nohpet 	
CentOS 5.2 	notsc divider=10
nohpet 	
CentOS 5.1 	notsc
nohpet 	
CentOS 5.0 		No additional kernel parameters required.
CentOS 4.9 	notsc divider=10
nohpet
	
CentOS 4.8 	notsc divider=10
nohpet
	
CentOS 4.7 	notsc divider=10
nohpet
	
CentOS 4.6 	notsc
nohpet
	
CentOS 4.5 	notsc
nohpet
	
CentOS 4.4 	notsc
nohpet
	
CentOS 4.3 	notsc
nohpet
	
CentOS 4.2 	notsc
nohpet
	
CentOS 4.1 	nohpet 	Does not support notsc. Avoid using if possible.
CentOS 4.0 	nohpet 	Does not support notsc. Avoid using if possible.
Oracle 	Oracle Linux 6.x 		No additional kernel parameters required.
Oracle Linux 5.8 		No additional kernel parameters required. See the note below about the optional use of divider=10.
Oracle Linux 5.7 		No additional kernel parameters required. See the note below about the optional use of divider=10.
Oracle Linux 5.6 		No additional kernel parameters required. See the note below about the optional use of divider=10.
OEL 5.5 		No additional kernel parameters required. See the note below about the optional use of divider=10.
OEL 5.4 		No additional kernel parameters required. See the note below about the optional use of divider=10.
OEL 5.3 	notsc divider=10
nohpet 	
OEL 5.2 	notsc divider=10
nohpet 	
OEL 5.1 	notsc
nohpet 	
OEL 5.0 		No additional kernel parameters required.
OEL 4.9 	notsc divider=10
nohpet 	
OEL 4.8 	notsc divider=10
nohpet 	
OEL 4.7 	notsc divider=10
nohpet 	
OEL 4.6 	notsc
nohpet 	
OEL 4.5 	notsc
nohpet 	
Debian 	Debian 6.0 		No additional kernel parameters required.
Debian 5.0 		No additional kernel parameters required.
Debian 4.x 	notsc
nohpet 	

Recommended Configurations
These configurations are expected to have the best timekeeping behavior:

    RHEL 6 32-bit or 64-bit running on ESX 4.0 or later
    RHEL 5.4 or later 32-bit or 64-bit running on ESX 3.5 or later
    SLES 10 SP2 or later 64-bit running on ESX 3.5 or later
    SLES 10 SP2 or later 32-bit running on ESX 3.5 or ESX 4.x
    SLES 11 32-bit or 64-bit running on ESX 4.0 or later
    Ubuntu 8.04 32-bit running on ESX 3.5 or later
    Ubuntu 8.04 or later 64-bit running on ESX 4.0 or later
    Ubuntu 8.04 or later 32-bit running on ESX 3.5 or ESX 4.x

Among different versions of RHEL 5 and RHEL 4, RHEL 5.4 or later has the best timekeeping behavior. For older versions, those supporting divider=10 have better timekeeping behavior than those that do not.

VMI is supported in ESX 3.5 and ESX 4.x. Support for VMI is not present in ESX 5.0. For more information related to VMI enabled kernels, see:

    Enabling Virtual Machine Interface (VMI) in a Linux kernel and in ESX 3.5 (1003644)
    Enabling VMI with SLES10 SP2 32bit virtual machines on ESX (1005701)

If you are running Java inside a virtual machine, some of the above parameters may affect the performance of your virtual machine. For more information, see:

    Best practices for running Java in a virtual machine (1008480)
    Time runs too fast in a Windows virtual machine when the Multimedia Timer interface is used (1005953)


Editing Kernel Configuration
Kernel command line parameters are specified in the /etc/lilo.conf or /boot/grub/grub.conf file, depending on your choice of boot loader.

For LILO, put the kernel command line parameters at the end of the append line. For example, if the append line looks like:

    append="resume=/dev/hda6 splash=silent"

and you want to add clock=pmtmr divider=10, the updated text is:

    append="resume=/dev/hda6 splash=silent clock=pmtmr divider=10"

Remember to run /sbin/lilo after editing lilo.conf, so that your edits take effect.

For GRUB, put the kernel command line parameters at the end of the kernel line. For example if the kernel line looks like:

    kernel /vmlinuz-2.6.18 ro root=/dev/hda2

and you want to add clock=pmtmr divider=10, the updated text is:

    kernel /vmlinuz-2.6.18 ro root=/dev/hda2 clock=pmtmr divider=10

For additional information about working with boot loaders, see your Linux distribution's documentation.

NTP Recommendations
Note: VMware recommends you to use NTP instead of VMware Tools periodic time synchronization. NTP is an industry standard and ensures accurate timekeeping in your guest. You may have to open the firewall (UDP 123) to allow NTP traffic.

This is a sample /etc/ntp.conf:

tinker panic 0
restrict 127.0.0.1
restrict default kod nomodify notrap
server 0.vmware.pool.ntp.org
server 1.vmware.pool.ntp.org
server 2.vmware.pool.ntp.org
driftfile /var/lib/ntp/drift

This is a sample (RedHat specific) /etc/ntp/step-tickers:

0.vmware.pool.ntp.org
1.vmware.pool.ntp.org

The configuration directive tinker panic 0 instructs NTP not to give up if it sees a large jump in time. This is important for coping with large time drifts and also resuming virtual machines from their suspended state.

Note: The directive tinker panic 0 must be at the top of the ntp.conf file.

It is also important not to use the local clock as a time source, often referred to as the Undisciplined Local Clock. NTP has a tendency to fall back to this in preference to the remote servers when there is a large amount of time drift.

An example of such a configuration is:

server 127.127.1.0
fudge 127.127.1.0 stratum 10

Comment out both lines.

After making changes to NTP configuration, the NTP daemon must be restarted. Refer to your operating system vendor’s documentation.

VMware Tools time synchronization configuration
When using NTP in the guest, disable VMware Tools periodic time synchronization.

To disable VMware Tools periodic time sync, perform one of these options:

    Set tools.syncTime = "FALSE" in the configuration file (.vmx file) of the virtual machine.

    OR

    Deselect Time synchronization between the virtual machine and the host operating system in the VMware Tools toolbox GUI of the guest operating system.

    OR

    Run the vmware-guestd --cmd "vmx.set_option synctime 1 0" command in the guest operating system. To enable time syncing again, use the same command with "0 1" instead of "1 0".


For ESX 4.1 and later, use these parameters for Linux, Solaris, and FreeBSD:

    To display the current status of the service:

    vmware-toolbox-cmd timesync status

    To disable periodic time synchronization:

    vmware-toolbox-cmd timesync disable

These options do not disable one-time synchronizations done by VMware Tools for events such as tools startup, taking a snapshot, resuming from a snapshot, resuming from suspend, or VMotion. These events synchronize time in the guest operating system with time in the host operating system, so it is important to make sure that the host operating system's time is correct.

To do this for VMware ACE, VMware Fusion, VMware GSX Server, VMware Player, VMware Server, and VMware Workstation, run time synchronization software such as NTP or w32time in the host. For VMware ESX run NTP in the service console. For VMware ESXi, run NTP in the VMkernel.

Note: VMware Tools one-time synchronization events should not disabled.

Virtual Hardware clock configuration
When configuring the Linux guest operating system, if you are given a choice between keeping the “hardware” clock (that is, the virtual CMOS time of day clock) in UTC or local time, choose UTC. This avoids any confusion when your local time changes between standard and daylight saving time (in England, "summer time").

For additional information, see Timekeeping in VMware Virtual Machines.

Note for uses on divider=10
For some operating systems, divider=10 is a supported kernel configuration option, but might not be necessary for accurate timekeeping. Using it reduces the frequency of timer interrupts by 10x, which reduces the CPU overhead of processing timer interrupts. This overhead is especially noticeable for idle virtual machines. The only drawback of using divider=10 is that the granularity of wakeups provided by the kernel changes from 1 ms to 10 ms. The vast majority of applications are not affected by this, but using divider=10 may not be the right tradeoff for some time sensitive applications. For some operating systems, specifically older versions, divider=10 greatly improves timekeeping accuracy and is strongly recommended.

Additional Information
For translated versions of this article, see:

    Español: Las mejores prácticas de medición de tiempo para huéspedes Linux (2016603)
    Português: Melhores práticas de controle de horário para convidados Linux (2016014)
    日本語: Linux ゲストの時刻管理のベスト プラクティス (1033498)
    Mandarin Chinese: Linux系统时间同步最佳实践 (2020975)

Tags
install-vmware-tools-linux  linux-guest-os-time 

		10.5.12 Kernel Timer Systems
		http://elinux.org/Kernel_Timer_Systems
Jump to: navigation, search
Contents

|    1 Timer Wheel, Jiffies and HZ (or, the way it was)
|        1.1 Ingo Molnar's explanation of timer wheel performance
|    2 ktimers
|        2.1 Material needs rework
|        2.2 clock events
|        2.3 clock sources
|    3 Timer information
|        3.1 /proc/timer_list
|        3.2 /proc/timer_stats
|    4 Dynamic ticks
|        4.1 Testing
|        4.2 Powertop
|    5 timer API
|    6 time API
|    7 High Resolution Timers
|    8 Old timer wheel/jiffy replacement proposals
|        8.1 Jun Sun's "tock" proposal
|        8.2 John Stultz
|    9 Timer Tick Thread - LKML July 2005

Timer Wheel, Jiffies and HZ (or, the way it was)

The original kernel timer system (called the "timer wheel") was based on incrementing a kernel-internal value (jiffies) every timer interrupt. The timer interrupt becomes the default scheduling quamtum, and all other timers are based on jiffies. The timer interrupt rate (and jiffy increment rate) is defined by a compile-time constant called HZ. Different platforms use different values for HZ. Historically, the kernel used 100 as the value for HZ, yielding a jiffy interval of 10 ms. With 2.4, the HZ value for i386 was changed to 1000, yeilding a jiffy interval of 1 ms. Recently (2.6.13) the kernel changed HZ for i386 to 250. (1000 was deemed too high).
Ingo Molnar's explanation of timer wheel performance

Ingo Molnar did an in-depth explanation about the performance of the current "timer wheel" implementation of timers. This was part of a series of messages trying to justify the addition of ktimers (which have different characteristics).

It is possibly the best explanation of the timer wheel avaiable: See http://lkml.org/lkml/2005/10/19/46 and http://lwn.net/Articles/156329/
ktimers
Material needs rework

A bunch of material in this section needs to be created or expanded to take into account the new ktimer system by Thomas Gleixner.
clock events
clock sources
Timer information

There are two /proc files that are very useful for gathering information about timers on your system.
/proc/timer_list

/proc/timer_list has information about the currently configured clocks and timers on the system. This is useful for debugging the current status of the timer system (especially while you are developing clockevent and clocksource support for your platform.)

You can tell if high resolution is configured for you machine by looking at a few different things:

For standard resolution (at jiffy resolution), a clock will have a value for it's '.resolution' field equal to the period of a jiffy. For embedded machines, where HZ is typically 100, this will be 10 milliseconds, or 10000000 (ten million) nanoseconds.

Also for standard resolution, the Clock Event Device will have an event handler of "tick_handle_periodic".

For high resolution, the resolution of the clock will be listed as 1 nanosecond (which is ridiculous, but serves as an indicator of essentially arbitrary precision.) Also, the Clock Event Device will have an event handler of "hrtimer_interrupt".

[need more info here - and this should probably be written up and put in Documentation/filesystems/proc.txt]
/proc/timer_stats

/proc/timer_stats is a file in the /proc pseudo file system which allows you to see information about the routines that are requesting timers of the Linux kernel. By cat'ing this file, you can see which routines are using lots of timers, and how frequently they are requesting them. This can be of interest to see

To use /proc/timer_stats, configure the kernel with support for the feature. That is, set CONFIG_TIMER_STATS=y in your .config. This is on the Kernel Hacking menu, with the prompt: "Collect kernel timers statistics"

Compile and install your kernel, and reboot your machine.

To activate the collection of stats (and reset the counters), do "echo 1 >/proc/timer_stats"

To stop collecting stats, do "echo 0 >/proc/timer_stats"

You can dump the statistics either while the collection system is running or stopped. To dump the stats, use 'cat /proc/timer_stats'. This shows the average events/sec at the end as well so you get a rough idea of system activity.

/proc/timer_stats fields (for version 0.1 of the format) are:

<count>,  <pid> <command>   <start_func> (<expire_func>)

Dynamic ticks

Tickless kernel, dynamic ticks or NO_HZ is a config option that enables a kernel to run without a regular timer tick. The timer tick is a timer interrupt that is usually generated HZ times per second, with the value of HZ being set at compile time and varying between around 100 to 1500. Running without a timer tick means the kernel does less work when idle and can potentially save power because it does not have to wake up regularly just to service the timer. The configuration option is CONFIG_NO_HZ and is set by Tickless System (Dynamic Ticks), on the Kernel Features configuration menu.

    See the Clockevents and dyntick LNW.net article 

Testing

To tell if dynamic ticks is supported in your kernel you can:

Look in dmesg for a line like this one:

 # dmesg | grep -i nohz
 Switched to NOHz mode on CPU #0

Or look at the timer interrupts and compare to jiffies:

 # cat /proc/interrupts | grep -i time
 # sleep 10
 # cat /proc/interrupts | grep -i time

Powertop

Powertop is a tool that parses the /proc/timer_stats output and gives a picture of what is causing wakeups on your system. Minimizing these wakeups should allow you to decrease power consumption in your device. Powertop was originally written for the x86 architecture but also works for embedded processors. However, in order to get a clean display from it, you will need an ncurses lib with wide character support.

Here's a poor-man's version of powertop:

 # watch "cat /proc/timer_stats | sort -nr | head -n 20"

timer API

    interval timers
    posix timer API
    sleep, usleep and nanosleep 

time API

- do_gettimeofday

High Resolution Timers

See High Resolution Timers, which describe sub-jiffy timers. 

	10.6 Network settings

		10.6.1 Linux Static IP Address Configuration
How do I configure the Internet Protocol version 4 (IPv4) properties of a network connection with a static IP address for servers running Linux operating systems? How do I configure static IP address under Debian Linux or Redhat / RHEL / Fedora / Redhat Enterprise Linux server?

You need to update and/or edit the network configuration files. This tutorial provides procedures to configure a static IP address on a computer running the following operating systems:

RHEL / Red hat / Fedora / CentOS Linux eth0 config file - /etc/sysconfig/network-scripts/ifcfg-eth0
RHEL / Red hat / Fedora / CentOS Linux eth1 config file - /etc/sysconfig/network-scripts/ifcfg-eth1
Debian / Ubuntu Linux - /etc/network/interfaces
Sample Setup: Linux Static TCP/IP Settings

In this example you will use the following Internet Protocol Version 4 (TCP/IPv4) Properties including IP, default gateway, and preferred DNS servers:


IP address: 192.168.1.10
Netmask: 255.255.255.0
Hostname: server1.cyberciti.biz
Domain name: cyberciti.biz
Gateway IP: 192.168.1.254
DNS Server IP # 1: 192.168.1.254
DNS Server IP # 2: 8.8.8.8
DNS Server IP # 3: 202.54.2.5
RHEL / Red hat / Fedora / CentOS Linux Static IP Configuration


For static IP configuration you need to edit the following files using a text editor such as vi. Edit /etc/sysconfig/network as follows, enter:
# cat /etc/sysconfig/network

Sample static ip configuration:

 
NETWORKING=yes
HOSTNAME=server1.cyberciti.biz
GATEWAY=192.168.1.254
 
Edit /etc/sysconfig/network-scripts/ifcfg-eth0, enter:
# cat /etc/sysconfig/network-scripts/ifcfg-eth0

Sample static ip configuration:

 
# Intel Corporation 82573E Gigabit Ethernet Controller (Copper)
DEVICE=eth0
BOOTPROTO=static
DHCPCLASS=
HWADDR=00:30:48:56:A6:2E
IPADDR=192.168.1.10
NETMASK=255.255.255.0
ONBOOT=yes
 
Edit /etc/resolv.conf and setup DNS servers, enter:
# cat /etc/resolv.conf

Sample static IP configurations:

 
search cyberciti.biz
nameserver 192.168.1.254
nameserver 8.8.8.8
nameserver 202.54.2.5
 
Finally, you need to restart the networking service, enter:
# /etc/init.d/network restart

To verify new static ip configuration for eth0, enter:
# ifconfig eth0
# route -n
# ping 192.168.1.254
# ping google.com

Debian / Ubuntu Linux Static IP Configuration


Edit /etc/hostname, enter:
# cat /etc/hostname

Sample ip config:

 
server1.cyberciti.biz
 
Edit /etc/network/interfaces, enter:
# cat /etc/network/interfaces

Sample static ip config:

 
iface eth0 inet static
     address 192.168.1.10
     network 192.168.1.0
     netmask 255.255.255.0
     broadcast 192.168.1.255
     gateway 192.168.1.254
 
Edit /etc/resolv.conf and setup DNS servers, enter:
# cat /etc/resolv.conf

Sample dns static IP configurations:

 
search cyberciti.biz
nameserver 192.168.1.254
nameserver 8.8.8.8
nameserver 202.54.2.5
 
Finally, you need to restart the networking service under Debian / Ubuntu Linux, enter:
# /etc/init.d/networking restart

Type the following commands to verify your new setup, enter:
# ifconfig eth0
# route -n
# ping google.com

		10.6.2
	10.7
11. Installation from sources.

	11.1 General procedure.

		
		11.1.1 open archive
		bzip2 -d, gunzip and then tar xvf to /tmp/pkg_name

		11.1.2 configure and make
		./configure

		make

		11.1.3 Install
		change to root: either su or sudo su (in systems with sudo like Cisco RH developers servers).
		then: make install

12. Compression

	12.1 TAR

		12.1.1 Basics
		To create a GNU Tar file

			12.1.1.1 Creates a GZIP-compressed Tar file 
			named eglinux.tar.gz of all files with a .txt suffix:

tar -czvf eglinux.tar.gz *.txt

c for create
z for gzip compression
v is for verbose messages
f for adding all files

			12.1.1.2 To list files in a compressed Tar file

tar -tzf eglinux.tar.gz

			12.1.1.3 To extract files from a Tar file

Extracts all files from a compressed Tar file named eglinux.tar.gz.

tar -xvf eglinux.tar.gz

Extracts to a specific folder:

tar -xvf eglinux.tar.gz -C ~/des

Other versions of Tar may require the -z option to specify the compression type.

			12.1.1.4 Use bzip2 compression
Create archive:
$ tar cjf foo.tar.bz2 foo/

Extract archive:
$ tar xjf foo.tar.bz2



		12.1.2 Exclude or Include files

			12.1.2.1 How to exclude/include
On some systems, make creates filenames starting with a comma (,) to keep track of dependencies. Various editors create backup files whose names end with a percent sign (%) or a tilde (~). I often keep the original copy of a program with the .orig extension and old versions with a .old extension.
Click Here

I often don't want to save these files on my backups. There may be some binary files that I don't want to archive, but don't want to delete either.

A solution is to use the X flag to tar. [Check your tar manual page for the F and FF options, too. - JIK ] This flag specifies that the matching argument to tar is the name of a file that lists files to exclude from the archive. Here is an example:


% find project ! -type d -print | \
egrep '/,|%$|~$|\.old$|SCCS|/core$|\.o$|\.orig$' > Exclude
% tar cvfX project.tar Exclude project

In this example, find lists all files in the directories, but does not print the directory names explicitly. If you have a directory name in an excluded list, it will also exclude all the files inside the directory. egrep is then used as a filter to exclude certain files from the archive. Here, egrep is given several regular expressions to match certain files. This expression seems complex but is simple once you understand a few special characters:

A breakdown of the patterns and examples of the files that match these patterns is given here:

Instead of specifying which files are to be excluded, you can specify which files to archive using the - I option. As with the exclude flag, specifying a directory tells tar to include (or exclude) the entire directory. You should also note that the syntax of the - I option is different from the typical tar flag. The next example archives all C files and makefiles. It uses egrep's () grouping operators to make the $ anchor character apply to all patterns inside the parentheses:


% find project -type f -print | \
egrep '(\.[ch]|[Mm]akefile)$' > Include
% tar cvf project.tar -I Include

I suggest using find to create the include or exclude file. You can edit it afterward, if you wish. One caution: extra spaces at the end of any line will cause that file to be ignored.

One way to debug the output of the find command is to use /dev/null as the output file:


% tar cvfX /dev/null Exclude project

Including Other Directories

There are times when you want to make an archive of several directories. You may want to archive a source directory and another directory like /usr/local. The natural, but wrong, way to do this is to use the command:


% tar cvf /dev/rmt8 project /usr/local


			12.1.2.2 Example, create a backup of ACS project, exclude non source code files
find acs ! -type d -print |  egrep '/\.so.*$|\.a$|\.jar$|\.o$|\.d|lib' > Exclude

And

[yizaq@yizaq-lnx:Wed Sep 17:/view/yizaq__int.acs5_0.lx/vob/nm_acs]$  tar cvfzX acs.tar.bz2 Exclude acs/

Or a simpler way, create a read only view (no objects and lib files) and compress it:
$cd /cygdrive/c/views/snapshot/yizaq__int.acs5_0.ntss/nm_acs
$ tar cjf acs_5_0_FCS.tar.bz2 acs





13. Tools

	13.1 sort
Sort text files.
Sort, merge, or compare all the lines from the files given (or standard input.)

		13.1.1 Examples


	   * Sort in descending (reverse) numeric order.

          sort -nr

     Sort alphabetically, omitting the first and second fields.  This
     uses a single key composed of the characters beginning at the
     start of field three and extending to the end of each line.

          sort -k3

   * Sort numerically on the second field and resolve ties by sorting
     alphabetically on the third and fourth characters of field five.
     Use `:' as the field delimiter.

          sort -t : -k 2,2n -k 5.3,5.4

     Note that if you had written `-k 2' instead of `-k 2,2' `sort'
     would have used all characters beginning in the second field and
     extending to the end of the line as the primary _numeric_ key.
     For the large majority of applications, treating keys spanning
     more than one field as numeric will not do what you expect.

     Also note that the `n' modifier was applied to the field-end
     specifier for the first key.  It would have been equivalent to
     specify `-k 2n,2' or `-k 2n,2n'.  All modifiers except `b' apply
     to the associated _field_, regardless of whether the modifier
     character is attached to the field-start and/or the field-end part
     of the key specifier.

   * Sort the password file on the fifth field and ignore any leading
     white space.  Sort lines with equal values in field five on the
     numeric user ID in field three.

          sort -t : -k 5b,5 -k 3,3n /etc/passwd

     An alternative is to use the global numeric modifier `-n'.

          sort -t : -n -k 5b,5 -k 3,3 /etc/passwd

   * Generate a tags file in case insensitive sorted order.
          find src -type f -print0 | sort -t / -z -f | xargs -0 etags --append

     The use of `-print0', `-z', and `-0' in this case mean that
     pathnames that contain Line Feed characters will not get broken up
     by the sort operation.

     Finally, to ignore both leading and trailing white space, you
     could have applied the `b' modifier to the field-end specifier for
     the first key,

          sort -t : -n -k 5b,5b -k 3,3 /etc/passwd

     or by using the global `-b' modifier instead of `-n' and an
     explicit `n' with the second key specifier.

          sort -t : -b -k 5,5 -k 3,3n /etc/passwd

A file name of `-' means standard input.

By default, sort writes the results to the standard output.

		13.1.2 Using Sort to List Directories by Size, tags: Using Sort to List Directories by Size

		    13.1.2.1 with hidden files
            du -shc .??* * | sort +0n

GPT explanation:
The `du` command is used to estimate file and directory space usage in Unix-like operating systems, and the `sort` command is used to sort lines of text. Let's break down the `du` and `sort` command in your provided command:

```bash
du -shc .??* * | sort +0n
```

| 1. `du`: This is the command for disk usage. It is used to calculate the sizes of files and directories.
| 
| 2. `-s`: This option tells `du` to display only the total size for each argument (i.e., each directory or file).
| 
| 3. `-h`: This option makes the output human-readable by using units like "K" for kilobytes, "M" for megabytes, etc.
| 
| 4. `-c`: This option causes `du` to display a grand total at the end of the output, which represents the total size of all the arguments combined.
| 
| 5. `.??*`: This is a wildcard pattern that matches hidden files and directories (those starting with a dot). The `??` part ensures that it doesn't match `.` and `..`, which represent the current directory and parent directory.
| 
| 6. `*`: This wildcard matches all other files and directories in the current directory.
| 
| 7. `|`: This is the pipe symbol, which is used to redirect the output of the `du` command as input to the `sort` command.
| 
| 8. `sort`: This is the command for sorting lines of text.
| 
| 9. `+0n`: This is an argument to the `sort` command. It specifies the sorting criteria, which is column 0 (the first column) numerically. In other words, it sorts the lines based on the numerical values in the first column.

Putting it all together, this command calculates the disk usage (size) of files and directories in the current directory and its subdirectories, displaying the total size in human-readable format, including hidden files and directories. It then sorts the results numerically based on the total size in ascending order.

The `sort +0n` part can be improved for clarity and modern syntax using the `-k` option to specify the sorting column and `-n` to indicate numerical sorting. Here's the updated command:

```bash
du -shc .??* * | sort -k1,1n
```

This command achieves the same result but is more readable and uses the `-k` option to explicitly specify the sorting criteria.

		    13.1.2.2 without hidden files
du -sk * | sort +0nr , works but misses hidden files

This will display the size of all directories and sort them from largest to smallest. If you want to sort them from smallest to largest, simply remove the ¿r¿.

du -sk * | sort +0n

Crude way: $ du -h . | grep '[0-9]M' | sort



		13.1.3 Databases

			13.1.3.1 sqlite 


YIZAQ-M-D1BW:~ yizaq$ sqlite3
SQLite version 3.8.10.2 2015-05-20 18:17:19
Enter ".help" for usage hints.
Connected to a transient in-memory database.
Use ".open FILENAME" to reopen on a persistent database.
sqlite> ^D
YIZAQ-M-D1BW:~ yizaq$ sqlite3 /tmp/f.db
SQLite version 3.8.10.2 2015-05-20 18:17:19
Enter ".help" for usage hints.
sqlite> CREATE TABLE points(x INTEGER, y INTEGER)
   ...> ;
sqlite> INSERT INTO points (x,y) VALUES (1,2)
   ...> ;
sqlite> INSERT INTO points (x,y) VALUES (10,20);
sqlite> INSERT INTO points (x,y) VALUES (100,200);
sqlite> SELECT * FROM points WHERE (x>20)
   ...> ;
100|200
sqlite> 

		13.1.4 FZF
id=__fzf__linux__
Fzf


https://github.com/junegunn/fzf

Use:
find * -type f | fzf > selected

Without STDIN pipe, fzf will use find command to fetch the list of files excluding hidden ones. (You can override the default command with FZF_DEFAULT_COMMAND)
vim $(fzf)

Using the finder
* CTRL-J / CTRL-K (or CTRL-N / CTRL-P) to move cursor up and down
* Enter key to select the item, CTRL-C / CTRL-G / ESC to exit
* On multi-select mode (-m), TAB and Shift-TAB to mark multiple items
* Emacs style key bindings
* Mouse: scroll, click, double-click; shift-click and shift-scroll on multi-select mode


Layout
fzf by default starts in fullscreen mode, but you can make it start below the cursor with --height option.
vim $(fzf --height 40%)
Also check out --reverse and --layout options if you prefer "top-down" layout instead of the default "bottom-up" layout.
vim $(fzf --height 40% --reverse)
You can add these options to $FZF_DEFAULT_OPTS so that they're applied by default. For example,
export FZF_DEFAULT_OPTS='--height 40% --layout=reverse --border'

Search syntax
Unless otherwise specified, fzf starts in "extended-search mode" where you can type in multiple search terms delimited by spaces. e.g. ^music .mp3$ sbtrkt !fire
Token	Match type	Description
sbtrkt	fuzzy-match	Items that match sbtrkt
'wild	exact-match (quoted)	Items that include wild
^music	prefix-exact-match	Items that start with music
.mp3$	suffix-exact-match	Items that end with .mp3
!fire	inverse-exact-match	Items that do not include fire
!^music	inverse-prefix-exact-match	Items that do not start with music
!.mp3$	inverse-suffix-exact-match	Items that do not end with .mp3
If you don't prefer fuzzy matching and do not wish to "quote" every word, start fzf with -e or --exact option. Note that when --exact is set, '-prefix "unquotes" the term.
A single bar character term acts as an OR operator. For example, the following query matches entries that start with core and end with either go, rb, or py.
^core go$ | rb$ | py$

Environment variables
* FZF_DEFAULT_COMMAND
    * Default command to use when input is tty
    * e.g. export FZF_DEFAULT_COMMAND='fd --type f'
* FZF_DEFAULT_OPTS
    * Default options
    * e.g. export FZF_DEFAULT_OPTS="--layout=reverse --inline-info"



Install:

brew install fzf

# To install useful key bindings and fuzzy completion:
$(brew --prefix)/opt/fzf/install

Via git
git clone --depth 1 https://github.com/junegunn/fzf.git ~/.fzf
~/.fzf/install

Vim plugin
" If installed using Homebrew
set rtp+=/usr/local/opt/fzf

" If installed using git
set rtp+=~/.fzf

Upgrade
* git: cd ~/.fzf && git pull && ./install
* brew: brew update; brew reinstall fzf



		13.1.5
14. sudo

	14.1 Description
Execute single command as super user. requires entering a password

	14.2 Tips and tricks
-> To give someone else sudo access. Open a terminal connection and type sudo su. enter password. Now he can work with super user priveliges.


15. Command line power example

	15.1 The power of the for loop
Example:
[yizaq@yizaq-lnx:Sun May 10:/view/yizaq1__yizaq1.int.acs5_0.lx/vob/nm_acs/acs/runtime/prebuilt/lnx26/bin]$ for file in ` \ls /usr/share/centrifydc/libexec/* | awk -F/ '{print $6}'` ; do  cleartool ci -nc  $file ; done

Let's break it a part.
\ls is used in case ls is aliased to produce color codes and other annoying stuff that interrupts with all the piping.

produce list of files in dir with ls
| awk -F/ '{print $6}'   print only the file name not the full path
` ` backtick quote would be replaced with the result of the inner command as a set of paramaters (shell expansion)
for file in ``; do command $file; done			execute your commands

																		
Update a directory. Using locate for finding newer versions and updating the original files:
[yizaq@yizaq-lnx:Mon May 11:/view/yizaq1__yizaq1.int.acs5_0.lx/vob/nm_acs/acs/runtime/prebuilt/lnx26/lib/adagent]$ for file in `ls_file`  ; do echo "updating lib $file" ; cp `locate $file | sed -n '1p' ` $file; echo "comparing sums"; sha1sum $file ; sha1sum  `locate $file | sed -n '1p' ` ; done

This will ask before copying each lib. Use \cp to escape the cp alias and copy w/o prompt.
Breakdown:
sed -n '1p'				print only the first hit of locate
`locate $file | sed -n '1p' `		copy that first hit over
sha1sum $file ; sha1sum  `locate $file | sed -n '1p' ` ;		compare the checksums
for file in `ls_file`			run on all real files (not dirs, not links etc). ls_file is an alias I defined.
																			   */

	15.2 When `` and xargs are not enough
Example:
[yizaq@yizaq-lnx:Sun May 10:/view/yizaq1__yizaq1.int.acs5_0.lx/vob/nm_acs/acs/runtime/prebuilt/lnx26/bin]$ locate `ct ls * | grep -i checkedout | awk -F@ '{print $1}'` | grep -i centrify | grep 'bin/' | xargs -I {} cp {} .
xargs: invalid option -- I

In advanced xarg commands the -I is supported and allows to run the command for each result (instead of on all results at once). So what can we do?
We need a named file, so:
First:
[yizaq@yizaq-lnx:Sun May 10:/view/yizaq1__yizaq1.int.acs5_0.lx/vob/nm_acs/acs/runtime/prebuilt/lnx26/bin]$ locate `ct ls * | grep -i checkedout | awk -F@ '{print $1}'` | grep -i centrify | grep 'bin/'  > ~/temp/cp_bins

Second, add the cp file . to all lines:
[yizaq@yizaq-lnx:Sun May 10:/view/yizaq1__yizaq1.int.acs5_0.lx/vob/nm_acs/acs/runtime/prebuilt/lnx26/bin]$ sed -i 's_^.*$_cp & ._' ~/temp/cp_bins 

Result:
cp /usr/share/centrifydc/bin/adcheck .
cp /usr/share/centrifydc/bin/addebug .
cp /usr/share/centrifydc/bin/adenv .
cp /usr/share/centrifydc/bin/adinfo_extra.sh .
cp /usr/share/centrifydc/bin/cdcexec .


16.  SSH

	16.1  Disable session timeout
add to /etc/ssh/sshd_config:
TCPKeepAlive yes
ClientAliveInterval 30
ClientAliveCountMax 99999


	16.2 copy files, scp


		16.2.1  scp(1) - Linux man page
Name

scp - secure copy (remote file copy program)
Synopsis

scp [-1246BCpqrv] [-c cipher] [-F ssh_config] [-i identity_file] [-l limit] [-o ssh_option] [-P port] [-S program] [

         [user@]host1:]file1 ... [                                    [user@]host2:]file2

Description

scp copies files between hosts on a network. It uses ssh(1) for data transfer, and uses the same authentication and provides the same security as ssh(1). Unlike rcp(1), scp will ask for passwords or passphrases if they are needed for authentication.

File names may contain a user and host specification to indicate that the file is to be copied to/from that host. Local file names can be made explicit using absolute or relative pathnames to avoid scp treating file names containing ':' as host specifiers. Copies between two remote hosts are also permitted.

When copying a source file to a target file which already exists, scp will replace the contents of the target file (keeping the inode).

If the target file does not yet exist, an empty file with the target file name is created, then filled with the source file contents. No attempt is made at "near-atomic" transfer using temporary files.

The options are as follows:

      -1'        Forces scp to use protocol 1.

    -2' Forces scp to use protocol 2.

    -4' Forces scp to use IPv4 addresses only.

    -6' Forces scp to use IPv6 addresses only.

    -B' Selects batch mode (prevents asking for passwords or passphrases).

    -C' Compression enable. Passes the -C flag to ssh(1) to enable compression.

    -c cipher
    Selects the cipher to use for encrypting the data transfer. This option is directly passed to ssh(1).

    -F ssh_config
    Specifies an alternative per-user configuration file for ssh. This option is directly passed to ssh(1).

    -i identity_file
    Selects the file from which the identity (private key) for public key authentication is read. This option is directly passed to ssh(1).

    -l limit
    Limits the used bandwidth, specified in Kbit/s.

    -o ssh_option
    Can be used to pass options to ssh in the format used in ssh_config(5). This is useful for specifying options for which there is no separate scp command-line flag. For full details of the options listed below, and their possible values, see ssh_config(5).

    AddressFamily
    BatchMode
    BindAddress
    ChallengeResponseAuthentication
    CheckHostIP
    Cipher
    Ciphers
    Compression
    CompressionLevel
    ConnectionAttempts
    ConnectTimeout
    ControlMaster
    ControlPath
    GlobalKnownHostsFile
    GSSAPIAuthentication
    GSSAPIDelegateCredentials
    HashKnownHosts
    Host'
    HostbasedAuthentication
    HostKeyAlgorithms
    HostKeyAlias
    HostName
    IdentityFile
    IdentitiesOnly
    KbdInteractiveDevices
    LogLevel
    MACs'
    NoHostAuthenticationForLocalhost
    NumberOfPasswordPrompts
    PasswordAuthentication
    Port'
    PreferredAuthentications
    Protocol
    ProxyCommand
    PubkeyAuthentication
    RekeyLimit
    RhostsRSAAuthentication
    RSAAuthentication
    SendEnv
    ServerAliveInterval
    ServerAliveCountMax
    SmartcardDevice
    StrictHostKeyChecking
    TCPKeepAlive
    UsePrivilegedPort
    User'
    UserKnownHostsFile
    VerifyHostKeyDNS

    -P port
    Specifies the port to connect to on the remote host. Note that this option is written with a capital 'P', because -p is already reserved for preserving the times and modes of the file in rcp(1).

    -p' Preserves modification times, access times, and modes from the original file.

    -q' Quiet mode: disables the progress meter as well as warning and diagnostic messages from ssh(1).

    -r' Recursively copy entire directories. Note that scp follows symbolic links encountered in the tree traversal.

    -S program
    Name of program to use for the encrypted connection. The program must understand ssh(1) options.

    -v' Verbose mode. Causes scp and ssh(1) to print debugging messages about their progress. This is helpful in debugging connection, authentication, and configuration problems.

    The scp utility exits 0 on success, and >0 if an error occurs.
    See Also

rcp(1), sftp(1), ssh(1), ssh-add(1), ssh-agent(1), ssh-keygen(1), ssh_config(5), sshd(8)
History

scp is based on the rcp(1) program in BSD source code from the Regents of the University of California.
Authors

Timo Rinne <tri@iki.fi>
Tatu Ylonen <ylo@cs.hut.fi>

BSD November 6, 2011 BSD
Referenced By
amaddclient(8), cpdup(1), darcs(1), htcp(1), mirrordir(1), openvpn(8), rbldnsd(8), rssh(1), rssh.conf(5), scponly(8), ztelnet(1) 

		16.2.2 My examples
copy to remote server:
[yizaq@yizaq-lnx:Tue Nov 15:/view/yizaq__yizaq_5_4.int.acs5_0.lx/vob/nm_acs/acs/install]$ scp calculateQuotas.sh  root@10.56.24.161:/opt/CSCOacs/config/calculateQuotas.sh
Warning: Permanently added '10.56.24.161' (RSA) to the list of known hosts.
Password: 
calculateQuotas.sh                                                              100% 1882     1.8KB/s   00:00    

		16.2.3 Tips & Tricks with ssh and scp

Quite a handy thing about scp is that it supports asterisks. You can copy all files in a remote directory in a way like this:

    [rechosen@localhost ~]$ scp yourusername@yourserver:/home/yourusername/* .

And you can also just copy a whole directory by specifying the -r (recursive) option:

    [rechosen@localhost ~]$ scp -r yourusername@yourserver:/home/yourusername/ .

Both of these also work when copying to a (remote) server or copying between a (remote) server and another (remote) server.

The ssh command can come in handy if you don't know the exact location of the file you want to copy with scp. First, ssh to the (remote) server:

    [rechosen@localhost ~]$ ssh yourusername@yourserver

Then browse to the right directory with cd. This is essential Linux terminal knowledge, so I won't explain it here. When you're in the right directory, you can get the full path with this command:

    [rechosen@localhost ~]$ pwd

Note: pwd is an abbreviation of Print Working Directory, which is a useful way to remember the command.

You can then copy this output, leave the ssh shell by pressing Ctrl + D, and then paste the full directory path in your scp command. This saves a lot of remembering and typing!

You can also limit the bandwidth scp may use when copying. This is very useful if you're wanting to copy a huge amount of data without suffering from slow internet for a long time. Limiting bandwidth is done this way:

    scp -l bandwidthlimit yourusername@yourserver:/home/yourusername/* .

The bandwidth is specified in Kbit/sec. What does this mean? Eight bits is one byte. If you want to copy no faster than 10 Kbyte/sec, set the limit to 80. If you want to copy no faster than 80 Kbyte/sec, set the limit to 640. Get it? You should set the limit to eight times the maximum Kbyte/sec you want it to be. I'd recommend to set the -l option with all scp'ing you do on a connection that other people need to use, too. A big amount of copying can virtually block a whole 10 Mbit network if you're using hubs.

*/


		16.2.4 scp errors

			16.2.4.1 protocol error: bad mode
- ex:
[root@ise-sus-u7 lib]# scp yizaq@pmbu-dev-vm58:/tmp/libActiveDirectoryIDStore.so .
yizaq@pmbu-dev-vm58's password: 
protocol error: bad mode

- reason some print from .bashrc is messing w/ scp protocol
- quick fix , use ssh
[root@ise-sus-u7 lib]# ssh  yizaq@pmbu-dev-vm58 "cat /tmp/libActiveDirectoryIDStore.so" > libActiveDirectoryIDStore.so
yizaq@pmbu-dev-vm58's password: 
/users/yizaq/.bashrc: line 59: /sw/licensed/rational/products/purifyplus_setup.sh: No such file or directory
/users/yizaq/.aliases: line 1305: ct: command not found
[root@ise-sus-u7 lib]# ls -l libAc*
-rw-r--r-- 1 root gadmin 1931366 Nov 19 03:53 libAcs.so
-rwxr-xr-x 1 root root    237126 Jul 24  2013 libAcsDebugLog.so
-rwxr-xr-x 1 root root    554470 Jul 24  2013 libAcsDebugLogConfig.so
-rwxr-xr-x 1 root gadmin 2206738 Feb 25 16:02 libActiveDirectoryIDStore.so
-rwxr-xr-x 1 root gadmin 2125697 Feb 25 15:44 libActiveDirectoryIDStore.so.orig

see more examples:
To send a file:

cat file | ssh ajw@dogmatix "cat > remote"
Or:

ssh ajw@dogmatix "cat > remote" < file
To receive a file:

ssh ajw@dogmatix "cat remote" > copy
		16.2.5


	16.3 Read remote file via SSH

ssh server "cat /path/to/file"

If you want paging..

ssh server "cat /path/to/file" |less

	16.4 SSH login without password

		16.4.1 explanation


Your aim

You want to use Linux and OpenSSH to automize your tasks. Therefore you need an automatic login from host A / user a to Host B / user b. You don't want to enter any passwords, because you want to call ssh from a within a shell script.
How to do it

First log in on A as user a and generate a pair of authentication keys. Do not enter a passphrase:

a@A:~> ssh-keygen -t rsa
Generating public/private rsa key pair.
Enter file in which to save the key (/home/a/.ssh/id_rsa): 
Created directory '/home/a/.ssh'.
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /home/a/.ssh/id_rsa.
Your public key has been saved in /home/a/.ssh/id_rsa.pub.
The key fingerprint is:
3e:4f:05:79:3a:9f:96:7c:3b:ad:e9:58:37:bc:37:e4 a@A

Now use ssh to create a directory ~/.ssh as user b on B. (The directory may already exist, which is fine):

a@A:~> ssh b@B mkdir -p .ssh
b@B's password: 

Finally append a's new public key to b@B:.ssh/authorized_keys and enter b's password one last time:

a@A:~> cat .ssh/id_rsa.pub | ssh b@B 'cat >> .ssh/authorized_keys'
b@B's password: 

From now on you can log into B as b from A as a without password:

a@A:~> ssh b@B hostname
B

A note from one of our readers: Depending on your version of SSH you might also have to do the following changes:

    Put the public key in .ssh/authorized_keys2
    Change the permissions of .ssh to 700
    Change the permissions of .ssh/authorized_keys2 to 640

		16.4.2 My example
a. Make keys on source machine
[yizaq@yizaq-WS:Mon Jul 02:/cygdrive/c/Documents and Settings/yizaq/Desktop:]$ ssh-keygen -t rsa
Generating public/private rsa key pair.
Enter file in which to save the key (/home/yizaq/.ssh/id_rsa): 
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /home/yizaq/.ssh/id_rsa.
Your public key has been saved in /home/yizaq/.ssh/id_rsa.pub.
The key fingerprint is:
21:98:b3:4d:63:ad:a3:b4:2d:2d:ea:3f:1e:bf:9e:f9 yizaq@yizaq-WS
The key's randomart image is:
+--[ RSA 2048]----+
|                 |
|     o .         |
|    + = o        |
|     * + .       |
|    o + S        |
|   . = .         |
|    * o          |
|   ..= o         |
| .oooo*oE        |
+-----------------+

b. make .ssh on target machine
[yizaq@yizaq-WS:Mon Jul 02:/cygdrive/c/Documents and Settings/yizaq/Desktop:]$ ssh root@10.56.24.159 mkdir -p .ssh
Copyright(c) 2012 Cisco Systems, Inc. All rights Reserved

Password: 

c. copy public key to target machine
- From Cygwin
[yizaq@yizaq-WS:Mon Jul 02:/cygdrive/c/Documents and Settings/yizaq/Desktop:]$ cat /home/yizaq/.ssh/id_rsa.pub | ssh root@10.56.24.159 'cat >> .ssh/authorized_keys'
cat /home/yizaq/.ssh/id_rsa.pub | ssh root@10.56.24.161 'cat >> .ssh/authorized_keys'
Copyright(c) 2012 Cisco Systems, Inc. All rights Reserved

Password: 

- From Linux
cat ~/.ssh/id_rsa.pub | ssh root@10.56.24.159 'cat >> .ssh/authorized_keys'

d. possible miscellaneous steps
[yizaq@yizaq-WS:Mon Jul 02:/cygdrive/c/Documents and Settings/yizaq/Desktop:]$ ssh root@10.56.24.159 chmod 700 .ssh
[yizaq@yizaq-WS:Mon Jul 02:/cygdrive/c/Documents and Settings/yizaq/Desktop:]$ cat /home/yizaq/.ssh/id_rsa.pub | ssh root@10.56.24.159 'cat >> .ssh/authorized_keys2'

	16.5
17. IO redirection

	17.1 tee , tags: tee 
tee is normally used to split the output of a program so that it can be seen on the display and also be saved in a file. The command can also be used to capture intermediate output before the data is altered by another command or program. The tee command reads standard input, then writes its content to standard output and simultaneously copies it into the specified file(s) or variables. The syntax differs depending on the command's implementation:

Unix-like

tee [ -a ] [ -i ] [ File ... ]

Arguments:

    * File One or more files that will receive the "tee-d" output.

Flags:

    * -a Appends the output to the end of File instead of writing over it.
    * -i Ignores interrupts.

The command returns the following exit values (exit status):

    * 0 The standard input was successfully copied to all output files.
    * >0 An error occurred.

Note: If a write to any successfully opened File operand is not successful, writes to other successfully opened File operands and standard output will continue, but the exit value will be >0.

4DOS and 4NT

TEE [/A] file...

Arguments:

    * file One or more files that will receive the "tee-d" output.

Flags:

    * /A Append the pipeline content to the output file(s) rather than overwriting them.

Note: When tee is used with a pipe, the output of the previous command is written to a temporary file. When that command finishes, tee reads the temporary file, displays the output, and writes it to the file(s) given as command-line argument.

Windows PowerShell

tee [-FilePath] <String> [-InputObject <PSObject>]
tee -Variable <String> [-InputObject <PSObject>]

Arguments:

    * -InputObject <PSObject> Specifies the object input to the cmdlet. The parameter accepts variables that contain the objects and commands or expression that return the objects.
    * -FilePath <String> Specifies the file where the cmdlet stores the object. The parameter accepts wildcard characters that resolve to a single file.
    * -Variable <String> A reference to the input objects will be assigned to the specified variable.

Note: tee is implemented as a ReadOnly command alias. The internal cmdlet name is Microsoft.PowerShell.Utility\Tee-Object.

Examples

Unix-like

    * To view and save the output from a command (lint) at the same time:

lint program.c | tee program.lint

This displays the standard output of the command lint program.c at the workstation, and at the same time saves a copy of it in the file program.lint. If a file named program.lint already exists, it is deleted and replaced.

    * To view and save the output from a command to an existing file:

lint program.c | tee -a program.lint

This displays the standard output of the lint program.c command at the workstation and at the same time appends a copy of it to the end of the program.lint file. If the program.lint file does not exist, it is created.

    * To allow escalation of permissions:

echo "Body of file..." | sudo tee root_owned_file > /dev/null

This example shows tee being used to bypass an inherent limitation in the sudo command. sudo is unable to pipe the standard output to a file. By dumping its stdout stream into /dev/null, we also suppress the mirrored output in the console.

4DOS and 4NT

This example searches the file wikipedia.txt for any lines containing the string "4DOS", makes a copy of the matching lines in 4DOS.txt, sorts the lines, and writes them to the output file 4DOSsorted.txt:

c:\> find "4DOS" wikipedia.txt | tee 4DOS.txt | sort > 4DOSsorted.txt

Windows PowerShell

    * To view and save the output from a command at the same time:

ipconfig | tee OutputFile.txt

This displays the standard output of the command ipconfig at the console window, and simultaneously saves a copy of it in the file OutputFile.txt.

    * To display and save all running processes, filtered so that only programs starting with svc and owning more than 1000 handles are outputted:

gps | where { $_.Name -like "svc*" } | tee ABC.txt | where { $_.Handles -gt 1000 }

This example shows that the piped input for tee can be filtered and that tee is used to display that output, which is filtered again so that only processes owning more than 1000 handles are displayed, and writes the unfiltered output to the file ABC.txt.

	17.2 Manual
tee(1) - Linux man page

Name

tee - read from standard input and write to standard output and files
Synopsis

tee [OPTION]... [FILE]...
Description


 
Copy standard input to each FILE, and also to standard output.

-a, --append
append to the given FILEs, do not overwrite
-i, --ignore-interrupts
ignore interrupt signals
--help
display this help and exit
--version
output version information and exit
If a FILE is -, copy again to standard output.

Author

Written by Mike Parker, Richard M. Stallman, and David MacKenzie.
Reporting Bugs

Report bugs to <bug-coreutils@gnu.org>.
Copyright

Copyright � 2006 Free Software Foundation, Inc.
This is free software. You may redistribute copies of it under the terms of the GNU General Public License <http://www.gnu.org/licenses/gpl.html>. There is NO WARRANTY, to the extent permitted by law.
See Also

The full documentation for tee is maintained as a Texinfo manual. If the info and tee programs are properly installed at your site, the command
info tee
should give you access to the complete manual.

Referenced By

auto-build(1), pee(1), tee(2), tpipe(1)

	17.3 How to use tee with stdout and stderr?
cmd  2>&1 | tee xyz.log

	17.4

18. Troubleshoot

	18.1 login 

		18.1.1 Direct login fails  "GDM could not write to your authorization file"
do df -h /
df -h /users/yizaq

delete some files and make space (check /tmp!)
Filesystem            Size  Used Avail Use% Mounted on
/dev/sda2             7.9G  5.2G  2.4G  70% /



	18.2 Scripts

		18.2.1 line 1: ﻿#!/usr/bin/bash: No such file or directory

-problem:
[yizaq@csi-pmbu17-lnx:Thu Jul 28:1011:20:/data/trunk/branches/ise_denali_br/cpm]$ ~/work/scripts/work_utils/pushaod.sh ./install/isos/ise/target/ise-2.2.0.245.NOT_FOR_RELEASE.x86_64.iso
/users/yizaq/work/scripts/work_utils/pushaod.sh: line 1: ﻿#!/usr/bin/bash: No such file or directory

-fix:
	-- Identify issue:
[yizaq@csi-pmbu17-lnx:Thu Jul 28:1182:204:~/work/scripts/work_utils]$ head -n 1 pushaod.sh | LC_ALL=C od -tc
0000000 357 273 277   #   !   /   u   s   r   /   b   i   n   /   b   a
0000020   s   h                                      \n
0000034

If the output starts with 357 273 277 it is an UTF8 byte order mark. In vim you can use set nobomb to remove it.

Neither the kernel (which interprets the she-bang line), nor your system's sh (which is invoked for scripts that don't have a she-bang) recognise that BOM, so what you're seeing here is:

The kernel returns ENOEXEC upon executing that script as it's not a recognised executable (it's not a native binary executable, and it's not a script as it doesn't start with #!).
Your interactive shell, upon that ENOEXEC, calls sh on it.
That sh reads and interprets the file. The first line is not a comment, the # is not preceded by a blank, so it does try to execute <BOM>#/bin/bash, and reports an error message.

- Verify fix
[yizaq@csi-pmbu17-lnx:Thu Jul 28:1183:205:~/work/scripts/work_utils]$ head -n 1 pushaod.sh | LC_ALL=C od -tc
0000000   #   !   /   u   s   r   /   b   i   n   /   b   a   s   h    
0000020                                  \n
0000031


	18.3

19. LDAP

	19.1 LDAP search
| 1)	Login to ACS as root.
| 2)	Find the domain controller that can provide the groups (global catalog) and use it within the LDAP query (in the following example the DC is sustain.sus).
| 3)	The output is the  list of the groups as you can see below.  

[root@acs-sus-e1b ~]# /opt/CSCOacs/runtime/adagent/bin/./ACS_AD_Runner.sh ldapsearch -m -H gc://acs-sus-s5.sustain.sus -E pr=120 -z 101 -r -S distinguishedName -b 'DC=sustain, DC=sus' "(&(objectClass=group)(cn=*))" distinguishedName groupType

/opt/CSCOacs/runtime/adagent/bin/./ACS_AD_Runner.sh ldapsearch -m -H gc://a3-adf1.amer.acs.com -E pr=120 -z 101 -r -S distinguishedName -b 'DC=canada,DC=north,DC=amer,DC=acs,DC=com' "(&(objectClass=group)(cn=*))" distinguishedName groupType -d 1

Executed with privileges of root
executing /opt/CSCOacs/runtime/adagent/bin/ldapsearch -m -H gc://acs-sus-s5.sustain.sus -E pr=120 -z 101 -r -S distinguishedName -b DC=sustain, DC=sus (&(objectClass=group)(cn=*)) distinguishedName groupType
SASL/GSSAPI authentication started
SASL SSF: 56
SASL installing layers
# extended LDIF
#
# LDAPv3
# base <DC=sustain, DC=sus> with scope sub
# filter: (&(objectClass=group)(cn=*))
# requesting: distinguishedName groupType 
# with pagedResults control: size=120
#

# Account Operators, Builtin, sustain.sus
dn: CN=Account Operators,CN=Builtin,DC=sustain,DC=sus
distinguishedName: CN=Account Operators,CN=Builtin,DC=sustain,DC=sus
groupType: -2147483643

# Administrators, Builtin, sustain.sus
dn: CN=Administrators,CN=Builtin,DC=sustain,DC=sus
distinguishedName: CN=Administrators,CN=Builtin,DC=sustain,DC=sus
groupType: -2147483643

# Backup Operators, Builtin, sustain.sus
dn: CN=Backup Operators,CN=Builtin,DC=sustain,DC=sus
distinguishedName: CN=Backup Operators,CN=Builtin,DC=sustain,DC=sus
groupType: -2147483643

-------------------------------------------------------
[yizaq@yizaq-lnx:Tue Jun 01:/opt/CSCOacs/runtime/adagent/bin]$ sudo ./ACS_AD_Runner.sh ldapsearch -m -Q -LLL -H "gc://" -b "dc=canada,dc=north,dc=amer,dc=acs,dc=com" "(serviceprincipalname=host/yizaq-lnx)" dn 
Executed with privileges of root
executing /opt/CSCOacs/runtime/adagent/bin/ldapsearch -m -Q -LLL -H gc:// -b dc=canada,dc=north,dc=amer,dc=acs,dc=com (serviceprincipalname=host/yizaq-lnx) dn
dn: CN=yizaq-lnx,CN=Computers,DC=canada,DC=north,DC=amer,DC=acs,DC=com

[yizaq@yizaq-lnx:Tue Jun 01:/opt/CSCOacs/runtime/adagent/bin]$ sudo ./ACS_AD_Runner.sh ldapsearch -m -Q -LLL -H "ldap://" -b "dc=canada,dc=north,dc=amer,dc=acs,dc=com" "(serviceprincipalname=host/yizaq-lnx)" dn 
Executed with privileges of root
executing /opt/CSCOacs/runtime/adagent/bin/ldapsearch -m -Q -LLL -H ldap:// -b dc=canada,dc=north,dc=amer,dc=acs,dc=com (serviceprincipalname=host/yizaq-lnx) dn
dn: CN=yizaq-lnx,CN=Computers,DC=canada,DC=north,DC=amer,DC=acs,DC=com

[yizaq@yizaq-lnx:Tue Jun 01:/opt/CSCOacs/runtime/adagent/bin]$ sudo ./ACS_AD_Runner.sh ldapsearch -m -Q -LLL -H "ldap://" -b "dc=canada,dc=north,dc=amer,dc=acs,dc=com" "(serviceprincipalname=host/yizaq-lnx)" dn serviceprincipalname 
Executed with privileges of root
executing /opt/CSCOacs/runtime/adagent/bin/ldapsearch -m -Q -LLL -H ldap:// -b dc=canada,dc=north,dc=amer,dc=acs,dc=com (serviceprincipalname=host/yizaq-lnx) dn serviceprincipalname
dn: CN=yizaq-lnx,CN=Computers,DC=canada,DC=north,DC=amer,DC=acs,DC=com
servicePrincipalName: nfs/yizaq-lnx.canada.north.amer.acs.com
servicePrincipalName: nfs/yizaq-lnx
servicePrincipalName: http/yizaq-lnx.canada.north.amer.acs.com
servicePrincipalName: http/yizaq-lnx
servicePrincipalName: host/yizaq-lnx.canada.north.amer.acs.com
servicePrincipalName: host/yizaq-lnx
servicePrincipalName: ftp/yizaq-lnx.canada.north.amer.acs.com
servicePrincipalName: ftp/yizaq-lnx
servicePrincipalName: cifs/yizaq-lnx.canada.north.amer.acs.com
servicePrincipalName: cifs/yizaq-lnx

[yizaq@yizaq-lnx:Tue Jun 01:/opt/CSCOacs/runtime/adagent/bin]$ sudo ./ACS_AD_Runner.sh ldapsearch -m -Q -LLL -H "gc://" -b "dc=canada,dc=north,dc=amer,dc=acs,dc=com" "(serviceprincipalname=host/yizaq-lnx)" dn serviceprincipalname 
Executed with privileges of root
executing /opt/CSCOacs/runtime/adagent/bin/ldapsearch -m -Q -LLL -H gc:// -b dc=canada,dc=north,dc=amer,dc=acs,dc=com (serviceprincipalname=host/yizaq-lnx) dn serviceprincipalname
dn: CN=yizaq-lnx,CN=Computers,DC=canada,DC=north,DC=amer,DC=acs,DC=com
servicePrincipalName: nfs/yizaq-lnx.canada.north.amer.acs.com
servicePrincipalName: nfs/yizaq-lnx
servicePrincipalName: http/yizaq-lnx.canada.north.amer.acs.com
servicePrincipalName: http/yizaq-lnx
servicePrincipalName: host/yizaq-lnx.canada.north.amer.acs.com
servicePrincipalName: host/yizaq-lnx
servicePrincipalName: ftp/yizaq-lnx.canada.north.amer.acs.com
servicePrincipalName: ftp/yizaq-lnx
servicePrincipalName: cifs/yizaq-lnx.canada.north.amer.acs.com
servicePrincipalName: cifs/yizaq-lnx

-------------------------------------------------------


	19.2 Determine gc of domain
[yizaq@yizaq-lnx:Mon May 31:/opt/CSCOacs/runtime/adagent/bin]$ nslookup gc._msdcs.amer.acs.com
Server:         10.56.60.150
Address:        10.56.60.150#53

Name:   gc._msdcs.amer.acs.com
Address: 10.56.60.38

or, use Centrify's adinfo -g

	19.3 Determine DCs of domain
[yizaq@yizaq-lnx:Mon May 31:/opt/CSCOacs/runtime/adagent/bin]$ nslookup 
> set type=ptr
> 10.56.60.38
Server:         10.56.60.150
Address:        10.56.60.150#53

38.60.56.10.in-addr.arpa        name = a3-adf1.amer.acs.com.

DC is a3-adf1.amer.acs.com.

or, use Centrify's adinfo -g
	19.3


20. Examples of my use cases

	20.1 sort -k example
use for monitoring command, ex:
while 1 ; do ls -lrth acs_5_1* | tail | sort -k 9 ; echo "------------------";  sleep 60; done

21. Monit

	21.1 Examples
Configuration Examples
Real-world configuration examples

Here are some real-world configuration examples for monit. It can be helpful to look at the examples given here to see how a service is running, where it put its pidfile, how to call the start and stop methods for a service, etc.

You are welcome to cut & paste configuration into your own monitrc control file. Please check and edit as needed, some IP-addresses and paths mentioned here may or will differ from your system.

    * System Services 

Cron (program timer)
Gdm (gnome desktop manager)
Inetd (internet service manager)
Syslogd (system logfile daemon)
Xfs (X font server)
YPBind (Yellow page bind daemon)
Net-SNMP (SNMP agent)
NTP (time server)
Nscd (name service caching daemon)

    * Name Services 

Bind (chrooted)

    * AAA Services 

FreeRADIUS

    * FTP Services 

Proftpd

    * Login Services 

SSHD

    * WWW Services 

Apache (web server)
Mongrel Cluster
Zope (appication server)
Squid (http/ftp proxy)
Privoxy (spamfilter proxy)

    * Mail Services 

Postfix (mail server)
Exim (mail server)
sendmail (mail server)
Qpopper (pop3 server)
Dovecot (imap secure server)
Spamassassin daemon (spam scan daemon)
Amavis-new (mail virus scanner)
Policyd (Postfix access policy delegation daemon)

    * Virus Scanner 

Sophie (virus scan daemon)
Trophie (virus scan daemon)
Clamavd (virus scan daemon)

    * Printing Services 

LPRng (printer daemon)

    * Database Services 

MySQL Server
OpenLDAP Server
PostgreSQL Server

    * File Services 

Samba (windows file/domain server)

    * Sun ONE Services 

iPlanetDirectoryServer (Sun ONE)
iPlanetMessagingServer processes (Sun ONE)
iPlanetCalendarServer processes (Sun ONE)

    * Misc Services 

apcupsd (APC ups daemon)
Webmin (remote admin service)
STunnel (SSL tunnel)

    * Misc Usage 

Watch and analyze crashdumps (Solaris)
Watch and analyze crashdumps (Linux)
Start and stop tcpdump based on condition
Rotate tcpdump until condition occures
MySQL event driven process list
Logrotate configuration
aMule, p2p app.
Kissdx, network streaming server for some DVDs
Getting top otput by mail on event
System Services

Cron (program timer)

When used with Solaris the init.d script needs a modification. Add the following line after start of cron:

 /usr/bin/pgrep -x -u 0 -P 1 cron > /var/run/cron.pid 

 check process cron with pidfile /var/run/cron.pid
   group system
   start program = "/etc/init.d/cron start"
   stop  program = "/etc/init.d/cron stop"
   if 5 restarts within 5 cycles then timeout
   depends on cron_rc

 check file cron_rc with path /etc/init.d/cron
   group system
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

Gdm (gnome desktop manager)

 check process gdm with pidfile /var/run/gdm.pid
   start program = "/etc/init.d/gdm start"
   stop program = "/etc/init.d/gdm stop"
   if 5 restarts within 5 cycles then timeout

Inetd (internet service manager)

 check process inetd with pidfile /var/run/inetd.pid
   start program = "/etc/init.d/inetd start"
   stop program = "/etc/init.d/inetd stop"
   if failed host 192.168.1.1 port 25 protocol smtp then restart  # e.g. exim 
   if failed host 192.168.1.1 port 515 then restart               # e.g. cups-lpd
   if failed host 192.168.1.1 port 113 then restart               # e.g. ident
   if 5 restarts within 5 cycles then timeout

Syslogd (system logfile daemon)

 check process syslogd with pidfile /var/run/syslogd.pid
   start program = "/etc/init.d/sysklogd start"
   stop program = "/etc/init.d/sysklogd stop"
   if 5 restarts within 5 cycles then timeout

 check file syslogd_file with path /var/log/syslog
   if timestamp > 65 minutes then alert # Have you seen "-- MARK --"?

Xfs (X font server)

 check process xfs with pidfile /var/run/xfs.pid
   start program = "/etc/init.d/xfs start"
   stop program = "/etc/init.d/xfs stop"
   if 5 restarts within 5 cycles then timeout

YPBind (Yellow page bind daemon)

 check process ypbind with pidfile /var/run/ypbind.pid
   start program = "/etc/init.d/nis start"
   stop program = "/etc/init.d/nis stop"
   if 5 restarts within 5 cycles then timeout

Net-SNMP (SNMP agent)

 check process snmpd with pidfile /var/run/snmpd
   start program = "/etc/init.d/snmpd start"
   stop program = "/etc/init.d/snmpd stop"
   if failed host 192.168.1.1 port 161 type udp then restart
   if failed host 192.168.1.1 port 199 type tcp then restart
   if 5 restarts within 5 cycles then timeout

NTP (time server)

 check process ntpd with pidfile /var/run/ntpd.pid
   start program = "/etc/init.d/ntpd start"
   stop  program = "/etc/init.d/ntpd stop"
   if failed host 127.0.0.1 port 123 type udp then alert
   if 5 restarts within 5 cycles then timeout

Nscd (name service caching daemon)

 check process nscd with pidfile /var/run/nscd/nscd.pid
   start program = "/etc/init.d/nscd start"
   stop  program = "/etc/init.d/nscd stop"
   if 5 restarts within 5 cycles then timeout

Name Services

Bind (chrooted)

 check process named with pidfile /var/named/chroot/var/run/named/named.pid
   start program = "/etc/init.d/named start"
   stop program = "/etc/init.d/named stop"
   if failed host 127.0.0.1 port 53 type tcp protocol dns then alert
   if failed host 127.0.0.1 port 53 type udp protocol dns then alert
   if 5 restarts within 5 cycles then timeout

AAA Services

FreeRADIUS (SVN only, not Monit 5.0)

 check process radiusd with pidfile /var/named/chroot/var/run/radiusd/radiusd.pid
   start program = "/etc/init.d/radiusd start"
   stop program = "/etc/init.d/radiusd stop"
   if failed host 127.0.0.1 port 1812 type udp protocol radius secret testing123 then alert
   if failed host 127.0.0.1 port 1812 type udp protocol radius secret testing123 then alert
   if 5 restarts within 5 cycles then timeout

FTP Services

Proftpd

 check process proftpd with pidfile /var/run/proftpd.pid
   start program = "/etc/init.d/proftpd start"
   stop program  = "/etc/init.d/proftpd stop"
   if failed port 21 protocol ftp then restart
   if 5 restarts within 5 cycles then timeout

Login Services

SSHD

 check process sshd with pidfile /var/run/sshd.pid
   start program  "/etc/init.d/sshd start"
   stop program  "/etc/init.d/sshd stop"
   if failed port 22 protocol ssh then restart
   if 5 restarts within 5 cycles then timeout

WWW Services

Apache (web server)

Hint: It is recommended to use a "token" file (an empty file) for monit to request. That way, it is easy to filter out all the requests made by monit in the httpd access log file. Here's a trick shared by Marco Ermini, place the following in httpd.conf to stop apache from loggin any requests done by monit:

  SetEnvIf        Request_URI "^\/monit\/token$" dontlog
  CustomLog       logs/access.log common env=!dontlog

In some cases init scripts for apache and apache-ssl are separated, e.g. Debian Linux.

 check process apache with pidfile /opt/apache_misc/logs/httpd.pid
   group www
   start program = "/etc/init.d/apache start"
   stop  program = "/etc/init.d/apache stop"
   if failed host localhost port 80 
        protocol HTTP request "/~hauk/monit/token" then restart
   if failed host 192.168.1.1 port 443 type TCPSSL 
        certmd5 12-34-56-78-90-AB-CD-EF-12-34-56-78-90-AB-CD-EF
	protocol HTTP request http://localhost/~hauk/monit/token  then restart
   if 5 restarts within 5 cycles then timeout
   depends on apache_bin
   depends on apache_rc

 check file apache_bin with path /opt/apache/bin/httpd
   group www
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

 check file apache_rc with path /etc/init.d/apache
   group www
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

Mongrel Cluster

Each mongrel instance will need it's own entry, and make sure to change the port (8000 in this example) to reflect your mongrel_cluster.yml file.

check process mongrel8000
  with pidfile /path/to/pidfile/mongrel.8000.pid
  group mongrels
  start program = "/bin/mongrel_rails cluster::start -C /path/to/mongrel_cluster.yml --clean --only 8000"
  stop program = "/bin/mongrel_rails cluster::stop -C /path/to/mongrel_cluster.yml --clean --only 8000"
  if failed port 8000 protocol HTTP
    request /system/token
    with timeout 10 seconds
    then restart
  if 5 restarts within 5 cycles
    then timeout

Note: /system/token requests an empty file called token, as recommended in the apache section above.

Zope (application server)

 check process zope with pidfile /opt/Zope/var/zProcessManager.pid
   start program = "/etc/init.d/zope start"
   stop  program = "/etc/init.d/zope stop"
   group www
   if failed host 192.168.1.1 port 8080 protocol HTTP then restart
   if 5 restarts within 5 cycles then timeout
   every 5

Squid (http/ftp proxy)

 check process squid with pidfile /opt/squid/logs/squid.pid
   group www
   start program = "/etc/init.d/squid start"
   stop  program = "/etc/init.d/squid stop"
   if failed host 192.168.1.1 port 3128  then restart
   if 5 restarts within 5 cycles then timeout
   depends on squid_bin
   depends on squid_rc

 check file squid_bin with path /opt/squid/bin/squid
   group www
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

 check file squid_rc with path /etc/init.d/squid
   group www
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

Privoxy (spamfilter proxy)

 check process privoxy with pidfile /opt/privoxy/var/privoxy.pid
   group www
   start program = "/etc/init.d/privoxy start"
   stop  program = "/etc/init.d/privoxy stop"
   if 5 restarts within 5 cycles then timeout
   if failed host 192.168.1.1 port 8118  then restart
   depends on privoxy_bin
   depends on privoxy_rc

 check file privoxy_bin with path /opt/privoxy/sbin/privoxy
   group www
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

 check file privoxy_rc with path /etc/init.d/privoxy
   group www
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

Mail Services

Postfix (mail server)

 check process postfix with pidfile /var/spool/postfix/pid/master.pid
   group mail
   start program = "/etc/init.d/postfix start"
   stop  program = "/etc/init.d/postfix stop"
   if failed port 25 protocol smtp then restart
   if 5 restarts within 5 cycles then timeout
   depends on postfix_rc

 check file postfix_rc with path /etc/init.d/postfix
   group mail
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

Exim (mail server)

 check process exim with pidfile /var/run/exim.pid
   group mail
   start program = "/etc/init.d/exim start"
   stop  program = "/etc/init.d/exim stop"
   if failed port 25 protocol smtp then restart
   if 5 restarts within 5 cycles then timeout
   depends on exim_bin
   depends on exim_rc

 check file exim_bin with path /usr/sbin/exim
   group mail
   if failed checksum then unmonitor
   if failed permission 4755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

 check file exim_rc with path /etc/init.d/exim
   group mail
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor 

Sendmail (mail server)

 check process sendmail with pidfile /var/run/sendmail.pid
   group mail
   start program = "/etc/init.d/sendmail start"
   stop  program = "/etc/init.d/sendmail stop"
   if failed port 25 protocol smtp then restart
   if 5 restarts within 5 cycles then timeout
   depends on sendmail_bin
   depends on sendmail_rc

 check file sendmail_bin with path /usr/lib/sendmail
   group mail
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

 check file sendmail_rc with path /etc/init.d/sendmail
   group mail
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

Qpopper (pop3 server)

 check process qpopper with pidfile /var/run/popper.pid
   group mail
   start program = "/etc/init.d/qpopper start"
   stop  program = "/etc/init.d/qpopper stop"
   if 5 restarts within 5 cycles then timeout
   if failed port 110 type TCP protocol POP then restart
   depends on qpopper_bin
   depends on qpopper_rc

 check file qpopper_bin with path /opt/sbin/popper
   group mail
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

 check file qpopper_rc with path /etc/init.d/qpopper
   group mail
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

Dovecot (imap secure server)

check process dovecot with pidfile /var/run/dovecot/master.pid
   start program = "/etc/init.d/dovecot start"
   stop program = "/etc/init.d/dovecot stop"
   group mail
   if failed host mail.yourdomain.tld port 993 type tcpssl sslauto protocol imap for 5 cycles then restart
   if 3 restarts within 5 cycles then timeout
   depends dovecot_init
   depends dovecot_bin
check file dovecot_init with path /etc/init.d/dovecot
   group mail
check file dovecot_bin with path /usr/sbin/dovecot
   group mail

Spamassassin daemon (spam scan daemon)

 check process spamd with pidfile /var/run/spamd.pid
   group mail
   start program = "/etc/init.d/spamd start"
   stop  program = "/etc/init.d/spamd stop"
   if 5 restarts within 5 cycles then timeout
   if cpu usage > 99% for 5 cycles then alert
   if mem usage > 99% for 5 cycles then alert
   depends on spamd_bin
   depends on spamd_rc

 check file spamd_bin with path /usr/local/bin/spamd
   group mail
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

 check file spamd_rc with path /etc/init.d/spamd
   group mail
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

Amavis-new (mail virus scanner)

 check process amavisd with pidfile /opt/virus/amavis-new/var/run/amavisd.pid
   group mail
   start program = "/etc/init.d/amavis-new start"
   stop  program = "/etc/init.d/amavis-new stop"
   if failed port 10024 protocol smtp then restart
   if 5 restarts within 5 cycles then timeout
   depends on amavisd_bin
   depends on amavisd_rc

 check file amavisd_bin with path /opt/virus/amavis-new/bin/amavisd
   group mail
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

 check file amavisd_rc with path /etc/init.d/amavis-new
   group mail
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

Policyd (Postfix policy delegation daemon)

 check process policyd with pidfile /var/run/policyd.pid
   group mail
   start program = "/etc/init.d/policyd start"
   stop  program = "/etc/init.d/policyd stop"
   if failed port 10031 protocol postfix-policy then restart
   if 5 restarts within 5 cycles then timeout
   depends on policyd_bin
   depends on policyd_rc
   depends on cleanup_bin

 check file policyd_bin with path /usr/local/policyd/policyd
   group mail
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

 check file policyd_rc with path /etc/init.d/policyd
   group mail
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

 check file cleanup_bin with path /usr/local/policyd/cleanup
   group mail
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

Virus Scanner

Sophie (virus scan daemon)

 check process sophie with pidfile /var/run/sophie.pid
   group virus
   start program = "/etc/init.d/sophie start"
   stop  program = "/etc/init.d/sophie stop"
   if failed unixsocket /var/run/sophie then restart
   if 5 restarts within 5 cycles then timeout

Virus Scanner

Sophie (virus scan daemon)

 check process sophie with pidfile /var/run/sophie.pid
   group virus
   start program = "/etc/init.d/sophie start"
   stop  program = "/etc/init.d/sophie stop"
   if failed unixsocket /var/run/sophie then restart
   if 5 restarts within 5 cycles then timeout
   depends on sophie_bin
   depends on sophie_rc

 check file sophie_bin with path /opt/virus/sophie/sophie
   group virus
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

 check file sophie_rc with path /etc/init.d/sophie
   group virus
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

Trophie (virus scan daemon)

 check process trophie with pidfile /var/run/trophie.pid
   group virus
   start program = "/etc/init.d/trophie start"
   stop  program = "/etc/init.d/trophie stop"
   if failed unixsocket /var/run/trophie then restart
   if 5 restarts within 5 cycles then timeout
   depends on trophie_bin
   depends on trophie_rc

 check file trophie_bin with path /opt/virus/trophie/trophie
   group virus
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

 check file trophie_rc with path /etc/init.d/trophie
   group virus
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

Clamav (virus scan daemon)

 check process clamavd with pidfile /var/run/clamd.pid
   group virus
   start program = "/etc/init.d/clamavd start"
   stop  program = "/etc/init.d/clamavd stop"
   if failed unixsocket /var/run/clamd then restart
   if 5 restarts within 5 cycles then timeout
   depends on clamavd_bin
   depends on clamavd_rc

 check file clamavd_bin with path /opt/virus/clamavd/clamavd
   group virus
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

 check file clamavd_rc with path /etc/init.d/clamavd
   group virus
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

Database Services

MySQL Server

The name of the pidfile consists usually of the fully quallified domainname and pidfile as extension.

check process mysql with pidfile /opt/mysql/data/myserver.mydomain.pid
   group database
   start program = "/etc/init.d/mysql start"
   stop program = "/etc/init.d/mysql stop"
   if failed host 192.168.1.1 port 3306 protocol mysql then restart
   if 5 restarts within 5 cycles then timeout
   depends on mysql_bin
   depends on mysql_rc

 check file mysql_bin with path /opt/mysql/bin/mysqld
   group database
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

 check file mysql_rc with path /etc/init.d/mysql
   group database
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

OpenLDAP slapd (Debian package)

check process slapd with pidfile /var/run/slapd.pid
   group database
   start program = "/etc/init.d/slapd start"
   stop program = "/etc/init.d/slapd stop"
   if failed host 192.168.1.1 port 389 protocol ldap3 then restart
   if 5 restarts within 5 cycles then timeout
   depends on slapd_bin
   depends on slapd_rc

 check file slapd_bin with path /usr/sbin/slapd
   group database
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

 check file slapd_rc with path /etc/init.d/slapd
   group database
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

PostgreSQL

Generally choosing either the socket or a TCP/IP connect is sufficient.

 check process postgres with pidfile /var/postgres/postmaster.pid
   group database
   start program = "/etc/init.d/postgresql start"
   stop  program = "/etc/init.d/postgresql stop"
   if failed unixsocket /var/run/postgresql/.s.PGSQL.5432 protocol pgsql 
      then restart
   if failed host 192.168.1.1 port 5432 protocol pgsql then restart
   if 5 restarts within 5 cycles then timeout

File Services

Samba (windows file/domain server)

Hint: For enhanced controllability of the service it is handy to split up the samba init file into two pieces, one for smbd (the file service) and one for nmbd (the name service).

 check process smbd with pidfile /opt/samba2.2/var/locks/smbd.pid
   group samba
   start program = "/etc/init.d/smbd start"
   stop  program = "/etc/init.d/smbd stop"
   if failed host 192.168.1.1 port 139 type TCP  then restart
   if 5 restarts within 5 cycles then timeout
   depends on smbd_bin

 check file smbd_bin with path /opt/samba2.2/sbin/smbd
   group samba
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

 check process nmbd with pidfile /opt/samba2.2/var/locks/nmbd.pid
   group samba
   start program = "/etc/init.d/nmbd start"
   stop  program = "/etc/init.d/nmbd stop"
   if failed host 192.168.1.1 port 138 type UDP  then restart
   if failed host 192.168.1.1 port 137 type UDP  then restart
   if 5 restarts within 5 cycles then timeout
   depends on nmbd_bin

 check file nmbd_bin with path /opt/samba2.2/sbin/nmbd
   group samba
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

Printing Services

LPRng (printer daemon)

 check process lprng with pidfile /var/run/lpd.515
   group printer
   start program = "/etc/init.d/lprng start"
   stop  program = "/etc/init.d/lprng stop"
   if failed host 192.168.1.1 port 515 type TCP  then restart
   if 5 restarts within 5 cycles then timeout
   depends on lprng_bin
   depends on lprng_rc

 check file lprng_bin with path /opt/lprng/sbin/lpd
   group printer
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

 check file lprng_rc with path /etc/init.d/lprng
   group printer
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

Sun ONE Services

iPlanetDirectoryServer slapd

 check process ldap-master
  with pidfile /usr/iplanet/ldapmaster/slapd-master-1/logs/pid
   start program  "/usr/iplanet/ldapmaster/slapd-master-1/start-slapd"
   stop program  "/usr/iplanet/ldapmaster/slapd-master-1/stop-slapd"
   if 5 restarts within 5 cycles then timeout
   if failed host 192.168.1.1 port 389 protocol ldap3 then restart

iPlanetMessagingServer MTA dispatcher

 check process mta-dispatcher 
  with pidfile /usr/iplanet/msg-ims-1/config/pidfile.imta_dispatch
   start program  "/usr/iplanet/msg-ims-1/imsimta start dispatcher"
   stop program  "/usr/iplanet/msg-ims-1/imsimta stop dispatcher"
   group messaging
   if 5 restarts within 5 cycles then timeout
   if failed host 192.168.1.1 port 25 protocol smtp then restart

iPlanetMessagingServer MTA job controler

 check process mta-job_controller 
  with pidfile /usr/iplanet/msg-ims-1/config/pidfile.imta_jbc
   start program  "/usr/iplanet/msg-ims-1/imsimta start job_controller"
   stop program  "/usr/iplanet/msg-ims-1/imsimta stop job_controller"
   group messaging
   if 5 restarts within 5 cycles then timeout
   if failed host 192.168.1.1 port 28442 then restart

iPlanetMessagingServer stored

 check process store with pidfile /usr/iplanet/msg-ims-1/config/pidfile.store
   start program  "/usr/iplanet/msg-ims-1/start-msg store"
   stop program  "/usr/iplanet/msg-ims-1/stop-msg store"
   if 5 restarts within 5 cycles then timeout
   group messaging

 check file stored.ckp with path /usr/iplanet/msg-ims-1/config/stored.ckp
   if timestamp > 10 minutes then alert
   group messaging

 check file stored.lcu with path /usr/iplanet/msg-ims-1/config/stored.lcu
   if timestamp > 15 minutes then alert
   group messaging

 check file stored.per with path /usr/iplanet/msg-ims-1/config/stored.per
   if timestamp > 70 minutes then alert
   group messaging

iPlanetMessagingServer mshttpd

 check process webmail with pidfile /usr/iplanet/msg-ims-1/config/pidfile.http
   start program  "/usr/iplanet/msg-ims-1/start-msg http"
   stop program  "/usr/iplanet/msg-ims-1/stop-msg http"
   group messaging
   if 5 restarts within 5 cycles then timeout
   if failed host 192.168.1.1 port 80 protocol http then restart

iPlanetMessagingServer popd

 check process pop3 with pidfile /usr/iplanet/msg-ims-1/config/pidfile.pop
   start program  "/usr/iplanet/msg-ims-1/start-msg pop"
   stop program  "/usr/iplanet/msg-ims-1/stop-msg pop"
   group messaging
   if 5 restarts within 5 cycles then timeout
   if failed host 192.168.1.1 port 110 protocol pop then restart

iPlanetMessagingServer imapd

 check process imap4 with pidfile /usr/iplanet/msg-ims-1/config/pidfile.imap
   start program  "/usr/iplanet/msg-ims-1/start-msg imap"
   stop program  "/usr/iplanet/msg-ims-1/stop-msg imap"
   group messaging
   if 5 restarts within 5 cycles then timeout
   if failed host 192.168.1.1 port 143 protocol imap then restart

iPlanetMessagingServer madmand (SNMP subagent)

 check process snmp-subagent 
   with pidfile /usr/iplanet/msg-ims-1/config/pidfile.snmp
   start program  "/usr/iplanet/msg-ims-1/start-msg snmp"
   stop program  "/usr/iplanet/msg-ims-1/stop-msg snmp"
   group messaging
   if 5 restarts within 5 cycles then timeout

iPlanetMessagingServer MMP (POP3/IMAP4/SMTP proxy)

 check process mmp with pidfile /usr/iplanet/mmp-ims2/pidfile
   start program  "/usr/iplanet/mmp-ims2/AService.rc start"
   stop program  "/usr/iplanet/mmp-ims2/AService.rc stop"
   group messaging
   if 5 restarts within 5 cycles then timeout
   if failed host 192.168.1.2 port 110 protocol pop then restart
   if failed host 192.168.1.2 port 143 protocol imap then restart

iPlanetCalendarServer csadmind

 check process calendar-admin 
   with pidfile /usr/iplanet/SUNWics5/cal/bin/config/pidfile.admin
   start program  "/usr/iplanet/SUNWics5/cal/bin/csstart service admin"
   stop program  "/usr/iplanet/SUNWics5/cal/bin/csstop service admin"
   group calendar
   if 5 restarts within 5 cycles then timeout

iPlanetCalendarServer cshttpd

 check process calendar-http 
   with pidfile /usr/iplanet/SUNWics5/cal/bin/config/pidfile.http
   start program  "/usr/iplanet/SUNWics5/cal/bin/csstart service http"
   stop program  "/usr/iplanet/SUNWics5/cal/bin/csstop service http"
   group calendar
   if 5 restarts within 5 cycles then timeout
   if failed host 192.168.1.3 port 80 protocol http then restart

iPlanetCalendarServer csdwpd (database wire protocol)

 check process calendar-dwp 
   with pidfile /usr/iplanet/SUNWics5/cal/bin/config/pidfile.dwp
   start program  "/usr/iplanet/SUNWics5/cal/bin/csstart service dwp"
   stop program  "/usr/iplanet/SUNWics5/cal/bin/csstop service dwp"
   group calendar
   if 5 restarts within 5 cycles then timeout
   if failed host 192.168.1.3 port 9779 protocol dwp then restart
   if cpu usage > 2% for 5 cycles then restart   # There's a leak in csdwpd

iPlanetCalendarServer csnotifyd

 check process calendar-notify 
   with pidfile /usr/iplanet/SUNWics5/cal/bin/config/pidfile.notify
   start program  "/usr/iplanet/SUNWics5/cal/bin/csstart service notify"
   stop program  "/usr/iplanet/SUNWics5/cal/bin/csstop service notify"
   group calendar
   if 5 restarts within 5 cycles then timeout

iPlanetCalendarServer enpd (event notification service broker)

 check process calendar-ens 
   with pidfile /usr/iplanet/SUNWics5/cal/bin/config/pidfile.ens
   start program  "/usr/iplanet/SUNWics5/cal/bin/csstart service ens"
   stop program  "/usr/iplanet/SUNWics5/cal/bin/csstop service ens"
   group calendar
   if 5 restarts within 5 cycles then timeout
   if failed host 192.168.1.3 port 7997 then restart

Misc Services

Apcupsd (APC ups daemon)

 check process apcupsd with pidfile /var/run/apcupsd.pid
   group ups
   start program = "/etc/init.d/apcupsd start"
   stop  program = "/etc/init.d/apcupsd stop"
   if 5 restarts within 5 cycles then timeout
   if failed host 192.168.1.3 port 7000 type TCP  then restart
   depends on apcupsd_bin
   depends on apcupsd_rc

 check file apcupsd_bin with path /opt/apcupsd/sbin/apcupsd
   group ups
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

 check file apcupsd_rc with path /etc/init.d/apcupsd
   group ups
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

Webmin (remote admin service)

 check process webmin with pidfile /var/webmin/miniserv.pid
   group webmin
   start program = "/etc/init.d/webmin start"
   stop  program = "/etc/init.d/webmin stop"
   if failed host 192.168.1.3 port 10000 then restart
   if 5 restarts within 5 cycles then timeout

 check file webmin_rc with path /etc/init.d/webmin
   group webmin
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

aMule (p2p program - daemon version)

  check process aMule with pidfile /home/$USER/.aMule/muleLock
    start program = "/etc/init.d/amule-daemon start"
    stop program  = "/etc/init.d/amule-daemon stop"

kissdx (Streaming app for some DVDs)

  check process kissdx with pidfile /var/run/kissdx
    start program = "/etc/init.d/kissdx"
    stop program  = "/usr/bin/killall kissdxp"
    if 5 restarts within 5 cycles then timeout

STunnel (SSL tunnel)

 check process stunnel_pop3 with pidfile /opt/var/stunnel/stunnel.110.pid
   start program = "/etc/init.d/stunnel start_pop3"
   stop  program = "/etc/init.d/stunnel stop_pop3"
   if failed host 192.168.1.1 port 143 type TCPSSL protocol POP then restart
   group stunnel
   depends stunnel_init
   depends stunnel_bin

 check process stunnel_swat with pidfile /opt/var/stunnel/stunnel.901.pid
   start program = "/etc/init.d/stunnel start_swat"
   stop  program = "/etc/init.d/stunnel stop_swat"
   if failed host 192.168.1.1 port 995 type TCPSSL then restart
   group stunnel
   depends stunnel_bin
   depends stunnel_rc

 check file stunnel_bin with path /opt/sbin/stunnel
   group stunnel
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

 check file stunnel_rc with path /etc/init.d/stunnel
   group stunnel
   if failed checksum then unmonitor
   if failed permission 755 then unmonitor
   if failed uid root then unmonitor
   if failed gid root then unmonitor

Misc Usage

Watch and analyze httpd crashdumps (Solaris) Setuid coredump allowed:

 coreadm -e proc-setid

Monit set to watch the core timestamp change and send the backtrace:

 check file httpd_core with path /usr/apache/core
   if changed timestamp
      then exec "/bin/bash -c '/usr/bin/pstack /usr/apache/core |\
           mailx -s httpd_crash foo@bar'"

Watch and analyze httpd crashdumps (Linux) Central coredump prepared:

 mkdir -p /var/crash/core
 chmod 1777 /var/crash/core
 sysctl -w kernel.core_pattern = /var/crash/core/core.%e.%t.%p
 sysctl -w kernel.core_setuid_ok = 0
 sysctl -w kernel.core_uses_pid = 1
 echo -e "bt\nquit" &gt; /etc/gdb.batch
 echo "ulimit -c unlimited" &gt;&gt; /etc/sysconfig/httpd
 echo "CoreDumpDirectory /var/crash/core" &gt; /etc/httpd/conf.d/core.conf

Crontab based core aging:

 10 1 * * * /usr/bin/find /var/crash/core/ -type f -mtime +1 -exec rm -f {} \;

Monit set to watch the directory timestamp change and send last core backtrace:

 check directory httpd_core with path /var/crash/core
   if changed timestamp then exec "/bin/bash -c 'if [ `/bin/cat /tmp/monit_httpd_core.tmp | head -1` != `/bin/ls /var/crash/core/core.httpd* | tail -1` ]; then /usr/bin/gdb -x /etc/gdb.batch /usr/sbin/httpd `/bin/ls /var/crash/core/core.httpd* | tail -1 | tee /tmp/monit_httpd_core.tmp` | mail -s httpd_crash admin@foo.bar webmaster@foo.bar; fi'"

Start and stop tcpdump based on condition As soon as the remote SMTP service of host bar is not available tcpdump is started. When the connection is available again, tcpdump is stopped. Only first ocurrence is catched (noexec flag is created to prevent another outage monitoring).

 check host bar with address 10.1.1.2
   if failed port 25 protocol smtp then exec "/bin/bash -c 'if [ ! -f /tmp/noexec ]; then touch /tmp/noexec; tcpdump -w /tmp/foo_bar.dump host bar; fi'" else if recovered then exec "killall tcpdump"

Rotate tcpdump until condition occures This allows to let tcpdump write the data to file and rotate it to keep the size of the dump small until network problem occures (we don't need to flood the filesystem with data which are ok). As soon as the problem occures, monit sets noexec flag => the dump contains the data which preceded the problem as well.

Script for tcpdump and rotation created (/tmp/dumprotate):

 #!/bin/bash
 killall tcpdump
 if [ ! -f /tmp/noexec ]
 then
   tcpdump -w /tmp/foo_bar.dump host bar
 fi

The script is started from cron each 30 minutes:

 0,30 * * * * /tmp/dumprotate

Monit watches the host availablity and as soon as it failed, sets noexec flag (with 5 minutes extent):

 check host bar with address 10.1.1.2
   if failed port 25 protocol smtp then exec "/bin/bash -c 'sleep 300; touch /tmp/noexec'"

MySQL event driven process list This allows to obtain process list of mysql threads as soon as mysql refuses connections. For example we needed to know why mysql returned "Too many connections" to clients occasionaly. (note that for simplicity in this example is showed mysql root account without password - you realy should use restricted account ;)

 check process mysqld with pidfile /var/run/mysqld.pid
   if failed port 3306 protocol mysql then exec "/bin/bash -c '(date && /usr/bin/mysqladmin -u root processlist && echo) >> /tmp/mysql_processlist'"

Logrotate configuration for monit

/var/log/monit.log {
    missingok
    notifempty
    size 100k
    create 0644 root root
    postrotate
        /bin/kill -HUP `cat /var/run/monit.pid 2>/dev/null` 2> /dev/null || true
    endscript
}

Getting top otput by mail on event

check file myfile with path /tmp/fo.bar
  if changed timestamp then exec "/bin/bash -c 'top -bn1 | mail -s top admin@foo.bar'"


	21.2 FAQ
Table of Contents

    * Pid File
    * Connection
    * Execution
    * Running Monit from Init 

Pid file

top

Keywords: pid file
Q: If a program crashes without removing its pid file, will monit recognize that the program is not running?

A: Yes, Monit will always check that the pid number in a pid file belongs to a running process. If a program crashes and dies in a "normal" manner, then the process ID (pid) will not exist and monit will know that the program is not running and restart it even if a pid file exist. Some servers can crash and leave a zombie process, and appear to run. Monit also test for zombie processes and will raise an alert if a process has become a zombie.

Keywords: pid file
Q: I have a program that does not create its own pid file. Since monit requires all programs to have a pid file, what do I do?

A: Create a wrapper script and have the script create a pid file before it starts the program. Below you can find an example script for starting an imaginary program (a Java program in this case). Assuming that the script is saved in a file called /bin/xyz, you can call this script from monit by using the following in monitrc:

 check process xyz with pidfile /var/run/xyz.pid
       start = "/bin/xyz start"
       stop = "/bin/xyz stop"

The wrapper script:

 #!/bin/bash
 export JAVA_HOME=/usr/local/java/
 CLASSPATH=ajarfile.jar:.

 case $1 in
    start)
       echo $$ > /var/run/xyz.pid;
       exec 2>&1 java -cp ${CLASSPATH} org.something.with.main 1>/tmp/xyz.out 
       ;;
     stop)  
       kill `cat /var/run/xyz.pid` ;;
     *)  
       echo "usage: xyz {start|stop}" ;;
 esac
 exit 0

Keywords: pid file
Q: Tomcat (The Jakarta Servlet Container) does not create a pid file and will put the server in the background.

A: Edit The catalina.sh script and find and remove the '&' character which will put the Tomcat server in the background. Then call tomcats startup.sh and shutdown.sh scripts from a wrapper script like the one mentioned above.

or

If your catalina.sh contains lines with $CATALINA_PID, you can just set CATALINA_PID=/path/file.pid enviroment variable.
Connection

top

Keywords: Connection testing
Q: I have started monit with HTTP support, but when I telnet into the monit http port the connection closes.

A: If you use the host allow statement, monit will promptly close all connections from hosts it does not find in the host allow list. So make sure that you use the official name for your host or its IP address. If you have a firewall running also make sure that it does not block connections on the monit port.
Execution

top

Keywords: Executing programs
Q: I'm having trouble getting monit to execute any "start" or "stop" program commands. The log file says that they're being executed, and I can't find anything wrong when I run monit in verbose mode.

A: For security reasons monit purges the environment and only sets a spartan PATH variable that contains /bin, /usr/bin, /sbin and /usr/sbin. If your program or script dies, the reason could be that it expects certain environment variables or to find certain programs via PATH. If this is the case you should set the environment variables you need directly in the start or stop script called by monit.
Running Monit from Init

top

Keywords: init
Q: How can I run monit from init so it can be respawned in case monit dies unexpectedly?

A: It is recommended that you use Monit version 5 or later when running Monit from init. Use either the 'set init' statement in monits configuration file or use the -I option from the command line. Here's a sample /etc/inittab entry for monit:

 # Run monit in standard runlevels
 mo:2345:respawn:/usr/local/sbin/monit -Ic /etc/monitrc

After you have modified inits configuration file, you can run the following command to re-examine the runlevel and start monit:

 telinit q

If monit is used to monitor services that are also started at boot time (e.g. services started via SYSV init rc scripts or via inittab) then in some situations a special race condition can occur. That is; if a service is slow to start, monit can assume that the service is not running and possibly try to start it and raise an alert, while, in fact the service is already about to start or already in its startup sequence. If you experience this problem, here are a couple of strategies you can use to prevent this type of race condition:

|1. Start critical services directly from monit:

This is the recommended solution - let monit takeover the responsibility for starting services. To use this strategy you must turn off the systems automatic start and stop for all services handled by monit.

On RedHat, you can for example use:

  chkconfig myservice off

on Debian:

  update-rc.d -f myservice remove

a general example:

  mv /etc/rc2.d/S99myservice /etc/rc2.d/s99myservice

If monit is started from a rc script, then to start and stop the service at systems shutdown, you should add the following lines to monit's rc script:

on start:

  /usr/local/bin/monit -c /etc/monitrc start myprocess

on stop:

  /usr/local/bin/monit -c /etc/monitrc stop myprocess

or if monit handles more than one service, simply start/stop all services by using:

on start:

  /usr/local/bin/monit -c /etc/monitrc start all

on stop:

  /usr/local/bin/monit -c /etc/monitrc stop all

If monit instead is started from init then, add a second line to inittab to stop the service:

  mo:2345:respawn:/usr/local/bin/monit -Ic /etc/monitrc
  mon:2345:wait:/usr/local/bin/monit -Ic /etc/monitrc start myprocess
  moff:06:wait:/usr/local/bin/monit -Ic /etc/monitrc stop myprocess

or to stop all services handled by monit:

  mo:2345:respawn:/usr/local/bin/monit -Ic /etc/monitrc
  mon:2345:wait:/usr/local/bin/monit -Ic /etc/monitrc start all
  moff:06:wait:/usr/local/bin/monit -Ic /etc/monitrc stop all

Services handled by monit must have start and stop methods defined so monit can start and stop a service. For instance:

  check process myservice with pidfile /var/run/myservice.pid
        start program = "/etc/init.d/myservice start"
        stop program = "/etc/init.d/myservice stop"

|2. Make init wait for a service to start:

This solution will make the init process wait for the service to start before it will continue to start other services. If you are running monit from init, you must enter monit's line at the end of /etc/inittab (A short example):

  si::sysinit:/etc/init.d/rcS
  ...
  l2:2:wait:/etc/init.d/rc 2
  ...
  mo:2345:respawn:/usr/local/bin/monit -Ic /etc/monitrc

The rc script for the monitored service must be so, that it will not return unless the service was started or start of the service timed out. Creative use of sleep(1) may be sufficient.

As in the above example, services handled by monit must have start and stop methods defined.

id=__monit_man_page__
	21.3 MAN page

monit(1) - Linux man page

Name

monit - utility for monitoring services on a Unix system

Synopsis

monit [options] {arguments}

Description


 
monit is a utility for managing and monitoring processes, files, directories and devices on a Unix system. Monit conducts automatic maintenance and repair and can execute meaningful causal actions in error situations. E.g. monit can start a process if it does not run, restart a process if it does not respond and stop a process if it uses to much resources. You may use monit to monitor files, directories and devices for changes, such as timestamps changes, checksum changes or size changes.

Monit is controlled via an easy to configure control file based on a free-format, token-oriented syntax. Monit logs to syslog or to its own log file and notifies you about error conditions via customizable alert messages. Monit can perform various TCP/IP network checks, protocol checks and can utilize SSL for such checks. Monit provides a http(s) interface and you may use a browser to access the monit program.

General Operation

The behavior of monit is controlled by command-line options and a run control file, ~/.monitrc, the syntax of which we describe in a later section. Command-line options override .monitrc declarations.

The following options are recognized by monit. However, it is recommended that you set options (when applicable) directly in the .monitrc control file.

General Options and Arguments

-c file Use this control file

-d n Run as a daemon once per n seconds

-g Set group name for start, stop, restart and status

-l logfile Print log information to this file

-p pidfile Use this lock file in daemon mode

-s statefile Write state information to this file

-I Do not run in background (needed for run from init)

-t Run syntax check for the control file

-v Verbose mode, work noisy (diagnostic output)

-H [filename] Print MD5 and SHA1 hashes of the file or of stdin if the filename is omitted; monit will exit afterwards

-V Print version number and patch level

-h Print a help text

In addition to the options above, monit can be started with one of the following action arguments; monit will then execute the action and exit without transforming itself to a daemon.

start all Start all services listed in the control file and enable monitoring for them. If the group option is set, only start and enable monitoring of services in the named group.

start name Start the named service and enable monitoring for it. The name is a service entry name from the monitrc file.

stop all Stop all services listed in the control file and disable their monitoring. If the group option is set, only stop and disable monitoring of the services in the named group.

stop name Stop the named service and disable its monitoring. The name is a service entry name from the monitrc file.

restart all Stop and start all services. If the group option is set, only restart the services in the named group.

restart name Restart the named service. The name is a service entry name from the monitrc file.

monitor all Enable monitoring of all services listed in the control file. If the group option is set, only start monitoring of services in the named group.

monitor name Enable monitoring of the named service. The name is a service entry name from the monitrc file. Monit will also enable monitoring of all services this service depends on.

unmonitor all Disable monitoring of all services listed in the control file. If the group option is set, only disable monitoring of services in the named group.

unmonitor name Disable monitoring of the named service. The name is a service entry name from the monitrc file. Monit will also disable monitoring of all services that depends on this service.

status Print full status information for each service.

summary Print short status information for each service.

reload Reinitialize a running monit daemon, the daemon will reread its configuration, close and reopen log files.

quit Kill a monit daemon process

validate Check all services listed in the control file. This action is also the default behavior when monit runs in daemon mode.

What To Monitor

You may use monit to monitor daemon processes or similar programs running on localhost. Monit is particular useful for monitoring daemon processes, such as those started at system boot time from /etc/init.d/. For instance sendmail, sshd, apache and mysql. In difference to many monitoring systems, monit can act if an error situation should occur, e.g.; if sendmail is not running, monit can start sendmail or if apache is using to much system resources (e.g. if a DoS attack is in progress) monit can stop or restart apache and send you an alert message. Monit does also monitor process characteristics, such as; if a process has become a zombie and how much memory or cpu cycles a process is using.

You may also use monit to monitor files, directories and devices on localhost. Monit can monitor these items for changes, such as timestamps changes, checksum changes or size changes. This is also useful for security reasons - you can monitor the md5 checksum of files that should not change.

You may even use monit to monitor remote hosts. First and foremost monit is a utility for monitoring and mending services on localhost, but if a service depends on a remote service, e.g. a database server or an application server, it might by useful to be able to test a remote host as well.

You may monitor the general system-wide resources such as cpu usage, memory and load average.

How To Monitor

monit is configured and controlled via a control file called monitrc. The default location for this file is ~/.monitrc. If this file does not exist, monit will try /etc/monitrc, then /usr/local/etc/monitrc and finally ./monitrc.

A monit control file consists of a series of service entries and global option statements in a free-format, token-oriented syntax. Comments begin with a # and extend through the end of the line. There are three kinds of tokens in the control file: grammar keywords, numbers and strings.

On a semantic level, the control file consists of three types of statements:

|1. Global set-statements
A global set-statement starts with the keyword set and the item to configure.
|2. Global include-statement
The include statement consists of the keyword include and a glob string.
|3. One or more service entry statements.
A service entry starts with the keyword check followed by the service type.
This is the hello galaxy version of a monit control file:
#
# monit control file
#

set daemon 120 # Poll at 2-minute intervals
set logfile syslog facility log_daemon
set alert foo@bar.baz
set httpd port 2812 and use address localhost
    allow localhost   # Allow localhost to connect
    allow admin:monit # Allow Basic Auth

check system myhost.mydomain.tld
   if loadavg (1min) > 4 then alert
   if loadavg (5min) > 2 then alert
   if memory usage > 75% then alert
   if cpu usage (user) > 70% then alert
   if cpu usage (system) > 30% then alert
   if cpu usage (wait) > 20% then alert

check process apache
   with pidfile "/usr/local/apache/logs/httpd.pid"
   start program = "/etc/init.d/httpd start"
   stop program = "/etc/init.d/httpd stop"
   if 2 restarts within 3 cycles then timeout
   if totalmem > 100 Mb then alert
   if children > 255 for 5 cycles then stop
   if cpu usage > 95% for 3 cycles then restart
   if failed port 80 protocol http then restart
   group server
   depends on httpd.conf, httpd.bin

check file httpd.conf
    with path /usr/local/apache/conf/httpd.conf
    # Reload apache if the httpd.conf file was changed
    if changed checksum
       then exec "/usr/local/apache/bin/apachectl graceful"

check file httpd.bin
    with path /usr/local/apache/bin/httpd
    # Run /watch/dog in the case that the binary was changed
    # and alert in the case that the checksum value recovered
    # later
    if failed checksum then exec "/watch/dog"
       else if recovered then alert

include /etc/monit/mysql.monitrc
include /etc/monit/mail/*.monitrc
This example illustrate a service entry for monitoring the apache web server process as well as related files. The meaning of the various statements will be explained in the following sections.
Logging

monit will log status and error messages to a log file. Use the set logfile statement in the monitrc control file. To setup monit to log to its own logfile, use e.g. set logfile /var/log/monit.log. If syslog is given as a value for the -l command-line switch (or the keyword set logfile syslog is found in the control file) monit will use the syslog system daemon to log messages. The priority is assigned to each message based on the context. To turn off logging, simply do not set the logfile in the control file (and of course, do not use the -l switch)

Daemon Mode

The -d interval command-line switch runs monit in daemon mode. You must specify a numeric argument which is a polling interval in seconds.

In daemon mode, monit detaches from the console, puts itself in the background and runs continuously, monitoring each specified service and then goes to sleep for the given poll interval.

Simply invoking

       monit -d 300
will poll all services described in your ~/.monitrc file every 5 minutes.
It is strongly recommended to set the poll interval in your ~/.monitrc file instead, by using set daemon n, where n is an integer number of seconds. If you do this, monit will always start in daemon mode (as long as no action arguments are given).

Monit makes a per-instance lock-file in daemon mode. If you need more monit instances, you will need more configuration files, each pointing to its own lock-file.

Calling monit with a monit daemon running in the background sends a wake-up signal to the daemon, forcing it to check services immediately.

The quit argument will kill a running daemon process instead of waking it up.

Init Support

Monit can run and be controlled from init. If monit should crash, init will re-spawn a new monit process. Using init to start monit is probably the best way to run monit if you want to be certain that you always have a running monit daemon on your system. (It's obvious, but never the less worth to stress; Make sure that the control file does not have any syntax errors before you start monit from init. Also, make sure that if you run monit from init, that you do not start monit from a startup scripts as well).

To setup monit to run from init, you can either use the 'set init' statement in monit's control file or use the -I option from the command line and here is what you must add to /etc/inittab:

# Run monit in standard run-levels
mo:2345:respawn:/usr/local/bin/monit -Ic /etc/monitrc
After you have modified init's configuration file, you can run the following command to re-examine /etc/inittab and start monit:
telinit q
For systems without telinit:
kill -1 1
If monit is used to monitor services that are also started at boot time (e.g. services started via SYSV init rc scripts or via inittab) then, in some cases, a race condition could occur. That is; if a service is slow to start, monit can assume that the service is not running and possibly try to start it and raise an alert, while, in fact the service is already about to start or already in its startup sequence. Please see the FAQ for solutions to this problem.
Include Files

The monit control file, monitrc, can include additional configuration files. This feature helps to maintain a certain structure or to place repeating settings into one file. Include statements can be placed at virtually any spot. The syntax is the following:

INCLUDE globstring
The globstring is any kind of string as defined in glob(7). Thus, you can refer to a single file or you can load several files at once. In case you want to use whitespace in your string the globstring need to be embedded into quotes (') or double quotes ("). For example,
INCLUDE "/etc/monit/monit configuration files/printer.*.monitrc"
loads any file matching the single globstring. If the globstring matches a directory instead of a file, it is silently ignored.
INCLUDE statements in included files are parsed as in the main control file.

If the globstring matches several results, the files are included in a non sorted manner. If you need to rely on a certain order, you might need to use single include statements.

Group Support

Service entries in the control file, monitrc, can be grouped together by the group statement. The syntax is simply (keyword in capital):

GROUP groupname
With this statement it is possible to group similar service entries together and manage them as a whole. Monit provides functions to start, stop and restart a group of services, like so:
To start a group of services from the console:

monit -g <groupname> start
To stop a group of services:
monit -g <groupname> stop
To restart a group of services:
monit -g <groupname> restart
Monitoring Mode

Monit supports three monitoring modes per service: active, passive and manual. See also the example section below for usage of the mode statement.

In active mode, monit will monitor a service and in case of problems monit will act and raise alerts, start, stop or restart the service. Active mode is the default mode.

In passive mode, monit will passively monitor a service and specifically not try to fix a problem, but it will still raise alerts in case of a problem.

For use in clustered environments there is also a manual mode. In this mode, monit will enter active mode only if a service was brought under monit's control, for example by executing the following command in the console:

monit start sybase
(monit will call sybase's start method and enable monitoring)
If a service was not started by monit or was stopped or disabled for example by:
monit stop sybase
(monit will call sybase's stop method and disable monitoring)
monit will not monitor the service. This allows for having services configured in monitrc and start it with monit only if it should run. This feature can be used to build a simple failsafe cluster. To see how, read more about how to setup a cluster with monit using the heartbeat system in the examples sections below.
Alert Messages

Monit will raise an email alert in the following situations:

o A service timed out
o A service does not exist
o A service related data access problem
o A service related program execution problem
o A service is of invalid object type
o A icmp problem
o A port connection problem
o A resource statement match
o A file checksum problem
o A file size problem
o A file/directory timestamp problem
o A file/directory/device permission problem
o A file/directory/device uid problem
o A file/directory/device gid problem
Monit will send an alert each time a monitored object changed. This involves:
o Monit started, stopped or reloaded
o A file checksum changed
o A file size changed
o A file content match
o A file/directory timestamp changed
You use the alert statement to notify monit that you want alert messages sent to an email address. If you do not specify an alert statement, monit will not send alert messages.
There are two forms of alert statement:

o Global - common for all services
o Local  - per service
In both cases you can use more than one alert statement. In other words, you can send many different emails to many different addresses. (in case you now got a new business idea: monit is not really suitable for sending spam).
Recipients in the global and in the local lists are alerted when a service failed, recovered or changed. If the same email address is in the global and in the local list, monit will send only one alert. Local (per service) defined alert email addresses override global addresses in case of a conflict. Finally, you may choose to only use a global alert list (recommended), a local per service list or both.

It is also possible to disable the global alerts localy for particular service(s) and recipients.

Setting a global alert statement

If a change occurred on a monitored services, monit will send an alert to all recipients in the global list who have registered interest for the event type. Here is the syntax for the global alert statement:

SET ALERT mail-address [ [ NOT ] {events}] [ MAIL-FORMAT {mail-format}] [ REMINDER number]
Simply using the following in the global section of monitrc:
set alert foo@bar
will send a default email to the address foo@bar whenever an event occurred on any service. Such an event may be that a service timed out, a service was doesn't exist or a service does exist (on recovery) and so on. If you want to send alert messages to more email addresses, add a set alert 'email' statement for each address.
For explanations of the events, MAIL-FORMAT and REMINDER keywords above, please see below.

When you want to enable global alert recipient which will receive all event alerts except some type, you can also use the NOT negation option ahead of events list which allows you to set the recipient for "all but specified events" (see bellow for more details).

Setting a local alert statement

Each service can also have its own recipient list.

ALERT mail-address [ [ NOT ] {events}] [ MAIL-FORMAT {mail-format}] [ REMINDER number]
or
NOALERT mail-address
If you only want an alert message sent for certain events for certain service(s), for example only for timeout events or only if a service died, then postfix the alert-statement with a filter block:
check process myproc with pidfile /var/run/my.pid
  alert foo@bar only on { timeout, nonexist }
  ...
(only and on are noise keywords, ignored by monit. As a side note; Noise keywords are used in the control file grammar to make an entry resemble English and thus make it easier to read (or, so goes the philosophy). The full set of available noise keywords are listed below in the Control File section).
You can also set the alert to send all events except specified using the list negation - the word not ahead of the event list. For example when you want to receive alerts for all events except the monit instance related, you can write (note that the noise words 'but' and 'on' are optional):

check system myserver
  alert foo@bar but not on { instance }
  ...
instead of:
alert foo@bar on { change
                   checksum
                   data
                   exec
                   gid
                   icmp
                   invalid
                   match
                   nonexist
                   permission
                   size
                   timeout
                   timestamp }
This will enable all alerts for foo@bar, except the monit instance related alerts.
Event filtering can be used to send a mail to different email addresses depending on the events that occurred. For instance:

alert foo@bar { nonexist, timeout, resource, icmp, connection }
alert security@bar on { checksum, permission, uid, gid }
alert manager@bar
This will send an alert message to foo@bar whenever a nonexist, timeout, resource or connection problem occurs and a message to security@bar if a checksum, permission, uid or gid problem occurs. And finally, a message to manager@bar whenever any error event occurs.
This is the list of events you can use in a mail-filter: uid, gid, size, nonexist, data, icmp, instance, invalid, exec, changed, timeout, resource, checksum, match, timestamp, connection, permission

You can also disable the alerts localy using the NOALERT statement. This is useful for example when you have lot of services monitored, used the global alert statement, but don't want to receive alerts for some minor subset of services:

noalert appadmin@bar
For example when you will place the noalert statement to the 'check system', the given user won't receive the system related alerts (such as monit instance started/stopped/reloaded alert, system overloaded alert, etc.) but will receive the alerts for all other monitored services.
The following example will alert foo@bar on all events on all services by default, except the service mybar which will send an alert only on timeout. The trick is based on the fact that local definition of the same recipient overrides the global setting (including registered events and mail format):

set alert foo@bar

check process myfoo with pidfile /var/run/myfoo.pid
  ...
check process mybar with pidfile /var/run/mybar.pid
  alert foo@bar only on { timeout }
The 'instance' alert type report events related to monit internals, such as when a monit instance was started, stopped or reloaded.
If the MTA (mailserver) for sending alerts is not available, monit can queue events on the local file-system until the MTA recover. Monit will then post queued events in order with their original timestamp so the events are not lost. This feature is most useful if monit is used together with e.g. m/monit and when event history is important.

Alert message layout

monit provides a default mail message layout that is short and to the point. Here's an example of a standard alert mail sent by monit:

From: monit@tildeslash.com
Subject: monit alert -- Does not exist apache
To: hauk@tildeslash.com
Date: Thu, 04 Sep 2003 02:33:03 +0200

Does not exist Service apache

       Date:   Thu, 04 Sep 2003 02:33:03 +0200
       Action: restart
       Host:   www.tildeslash.com

Your faithful employee,
monit
If you want to, you can change the format of this message with the optional mail-format statement. The syntax for this statement is as follows:
mail-format {
     from: monit@localhost
  subject: $SERVICE $EVENT at $DATE
  message: Monit $ACTION $SERVICE at $DATE on $HOST: $DESCRIPTION.
           Yours sincerely,
           monit
}
Where the keyword from: is the email address monit should pretend it is sending from. It does not have to be a real mail address, but it must be a proper formated mail address, on the form: name@domain. The keyword subject: is for the email subject line. The subject must be on only one line. The message: keyword denotes the mail body. If used, this keyword should always be the last in a mail-format statement. The mail body can be as long as you want and must not contain the '}' character.
All of these format keywords are optional but you must provide at least one. Thus if you only want to change the from address monit is using you can do:

set alert foo@bar with mail-format { from: bofh@bar.baz }
From the previous example you will notice that some special $XXX variables was used. If used, they will be substituted and expanded into the text with these values:
* $EVENT
A string describing the event that occurred. The values are
fixed and are:

Event:    | Failure state:          | Recovery state:
---------------------------------------------------------------
CHANGED   | "Changed"               | "Changed back"
CHECKSUM  | "Checksum failed"       | "Checksum passed"
CONNECTION| "Connection failed"     | "Connection passed"
DATA      | "Data access error"     | "Data access succeeded"
EXEC      | "Execution failed"      | "Execution succeeded"
GID       | "GID failed"            | "GID passed"
ICMP      | "ICMP failed"           | "ICMP passed"
INSTANCE  | "Monit instance changed"| "Monit instance changed not"
INVALID   | "Invalid type"          | "Type passed"
MATCH     | "Regex match"           | "No regex match"
NONEXIST  | "Does not exist"        | "Exists"
PERMISSION| "Permission failed"     | "Permission passed"
RESOURCE  | "Resource limit matched"| "Resource limit passed"
SIZE      | "Size failed"           | "Size passed"
TIMEOUT   | "Timeout"               | "Timeout recovery"
TIMESTAMP | "Timestamp failed"      | "Timestamp passed"
UID       | "UID failed"            | "UID passed"
* $SERVICE
The service entry name in monitrc
* $DATE
The current time and date (RFC 822 date style).
* $HOST
The name of the host monit is running on
* $ACTION
The name of the action which was done. Action names are fixed
and are:

Action:  | Name:
--------------------
ALERT    | "alert"
EXEC     | "exec"
MONITOR  | "monitor"
RESTART  | "restart"
START    | "start"
STOP     | "stop"
UNMONITOR| "unmonitor"
* $DESCRIPTION
The description of the error condition
Setting a global mail format
It is possible to set a standard mail format with the following global set-statement (keywords are in capital):

SET MAIL-FORMAT {mail-format}
Format set with this statement will apply to every alert statement that does not have its own specified mail-format. This statement is most useful for setting a default from address for messages sent by monit, like so:
set mail-format { from: monit@foo.bar.no }
Setting a error reminder
Monit by default sends just one error notification when the service failed and another one when it has recovered. If you want to be notified more then once in the case that the service remains failed, you can use the reminder option of alert statement (keywords are in capital):

ALERT ... [ WITH ] REMINDER [ ON ] number [ CYCLES ]
For example if you want to be notified each tenth cycle when the service remains failed, you can use:
alert foo@bar with reminder on 10 cycles
If you want to be notified on each failed cycle, you can use:
alert foo@bar with reminder on 1 cycle
Setting a mail server for alert messages
The mail server monit should use to send alert messages is defined with a global set statement (keywords are in capital and optional statements in [brackets]):

SET MAILSERVER {host name [PORT port]|ip-address [PORT port]}+
               [with TIMEOUT X SECONDS]
The port statement allows to use SMTP servers other then those listening on port 25. If omitted, port 25 is used for the connection.
As you can see, it is possible to set several SMTP servers. If monit cannot connect to the first server in the list it will try the second server and so on. Monit has a default 5 seconds connection timeout and if the SMTP server is slow, monit could timeout when connecting or reading from the server. You can use the optional timeout statement to explicit set the timeout to a higher value if needed. Here is an example for setting several mail servers:

set mailserver mail.tildeslash.com, mail.foo.bar port 10025,
    localhost with timeout 15 seconds
Here monit will first try to connect to the server "mail.tildeslash.com", if this server is down monit will try "mail.foo.bar" on port 10025 and finally "localhost". We do also set an explicit connect and read timeout; If monit cannot connect to the first SMTP server in the list within 15 seconds it will try the next server and so on. The set mailserver .. statement is optional and if not defined monit defaults to use localhost as the SMTP server.
Event queue

Monit provide optionally queueing of event alerts that cannot be sent. For example, if no mail-server is available at the moment, monit can store events in a queue and try to reprocess them at the next cycle. As soon as the mail-server recover, monit will post the queued events. The queue is persistent across monit restarts and provided that the back-end filesystem is persistent too, across system restart as well.

By default, the queue is disabled and if the alert handler fails, monit will simply drop the alert message. To enable the event queue, add the following statement to the monit control file:

SET EVENTQUEUE BASEDIR <path> [SLOTS <number>]
The <path> is the path to the directory where events will be stored. Optionally if you want to limit the queue size (maximum events count), use the slots option. If the slots option is not used, monit will store as many events as the backend filesystem allows.
Example:

set eventqueue
    basedir /var/monit
    slots 5000
The events are stored in binary format, one file per event. The file size is ca. 130 bytes or a bit more (depending on the message length). The file name is composed of the unix timestamp, underscore and the service name, for example:
/var/monit/1131269471_apache
If you are running more then one monit instance on the same machine, you must use separated event queue directories to avoid sending wrong alerts to the wrong addresses.
If you want to purge the queue by hand (remove queued event-files), monit should be stopped before the removal.

Service Timeout

monit provides a service timeout mechanism for situations where a service simply refuses to start or respond over a longer period. In cases like this, and particularly if monit's poll-cycle is low, monit will simply increase the machine load by trying to restart the service.

The timeout mechanism monit provides is based on two variables, i.e. the number the service has been started and the number of poll-cycles. For example, if a service had x restarts within y poll-cycles (where x <= y) then monit will timeout and not (re)start the service on the next cycle. If a timeout occurs monit will send you an alert message if you have register interest for this event.

The syntax for the timeout statement is as follows (keywords are in capital):

IF NUMBER RESTART NUMBER CYCLE (S) THEN TIMEOUT
Where the first number is the number of service restarts and the second, the number of poll-cycles. If the number of cycles was reached without a timeout, the service start-counter is reset to zero. This provides some granularity to catch exceptional cases and do a service timeout, but let occasional service start and restarts happen without having an accumulated timeout.
Here is an example where monit will timeout (not check the service) if the service was restarted 2 times within 3 cycles:

if 2 restarts within 3 cycles then timeout
To have monit check the service again after a timeout, run 'monit monitor service' from the command line. This will remove the timeout lock in the daemon and make the daemon start and check the service again.
Service Tests

Monit provides several tests you may utilize in a service entry to test a service. Basically here are two classes of tests: variable and constant object tests.

Constant object tests are related to failed/passed state. In the case of error, monit will watch whether the failed parameter will recover - in such case it will handle recovery related action. General format:

IF < TEST > [[<X>] [ TIMES WITHIN ] <Y> CYCLES ] THEN ACTION [ ELSE IF PASSED [[<X>] [ TIMES WITHIN ] <Y> CYCLES ] THEN ACTION ]
For constant object tests if the < TEST > should validate to true, then the selected action is executed each cycle the condition remains true. The value for comparison is constant. Recovery action is evaluated only once (on failed->passed state change only). The ' ELSE IF PASSED ' part is optional - if omitted, monit will do alert action on recovery by default. The alert is delivered only once on each state change unless overridden by 'reminder' alert option.
Variable object tests begins with ' IF CHANGED ' statement and serves for monitoring of object, which property can change legally - monit watches whether the value will change again. You can use it just for alert or to involve some automatic action, as for example to reload monitored process after its configuration file was changed. Variable tests are supported for 'checksum', 'size', 'pid, 'ppid' and 'timestamp' tests only, if you consider that other tests can be useful in variable form too, please let us know.

IF CHANGED < TEST > [[<X>] [ TIMES WITHIN ] <Y> CYCLES ] THEN ACTION
For variable object tests if the < TEST > should validate to true, then the selected action is executed once and monit will watch for another change. The value for comparison is a variable where the last result becomes the actual value, which is compared in future cycles. The alert is delivered each time the condition becomes true.
You can restrict the event ratio needed to change the state:

... [[<X>] [ TIMES WITHIN ] <Y> CYCLES ] ...
This part is optional and is supported by all testing rules. It defines how many event occurrences during how many cycles are needed to trigger the following action. You can use it in several ways - the core syntax is:
[<X>] <Y> CYCLES
It is possible to use filling words which give the rule better first-sight sense. You can use any filling words such as: FOR , TIMES , WITHIN , thus for example:
if failed port 80 for 3 times within 5 cycles then alert
or
if failed port 80 for 10 cycles then unmonitor
When you don't specify the <X>, it equals to <Y> by default, thus the rule applies when <Y> consecutive cycles of inverse event occurred (relatively to the current service state).
When you omit it at all, monit will by default change state on first inverse event, which is equivalent to this notation:

1 times within 1 cycles
It is possible to use this option for failed, passed/recovered or changed rules. More complex examples:
check device rootfs with path /dev/hda1
 if space usage > 80% 5 times within 15 cycles
    then alert
    else if passed for 10 cycles then alert
 if space usage > 90% for 5 cycles then
    exec '/try/to/free/the/space'
 if space usage > 99% then exec '/stop/processess'
Note that the maximal cycles count which can be used in the rule is limited by the size of 'long long' data type on your platform. This provides 64 cycles on usual platforms currently. In the case that you use unsupported value, the configuration parser will tell you the limits during monit startup.
You must select an action to be executed from this list:

*
ALERT sends the user an alert event on each state change (for constant object tests) or on each change (for variable object tests).

*

RESTART restarts the service and sends an alert. Restart is conducted by first calling the service's registered stop method and then the service's start method.

*

START starts the service by calling the service's registered start method and send an alert.

*

STOP stops the service by calling the service's registered stop method and send an alert. If monit stops a service it will not be checked by monit anymore nor restarted again later. To reactivate monitoring of the service again you must explicitly enable monitoring from the web interface or from the console, e.g. 'monit monitor apache'.

*

EXEC may be used to execute an arbitrary program and send an alert. If you choose this action you must state the program to be executed and if the program require arguments you must enclose the program and its arguments in a quoted string. You may optionally specify the uid and gid the executed program should switch to upon start. For instance:

exec "/usr/local/tomcat/bin/startup.sh"
     as uid nobody and gid nobody
This may be useful if the program to be started cannot change to a lesser privileged user and group. This is typically needed for Java Servers. Remember, if monit is run by the superuser, then all programs executed by monit will be started with superuser privileges unless the uid and gid extension was used.
*
MONITOR will enable monitoring of the service and send an alert.

*

UNMONITOR will disable monitoring of the service and send an alert. The service will not be checked by monit anymore nor restarted again later. To reactivate monitoring of the service you must explicitly enable monitoring from monit's web interface or from the console using the monitor argument.

RESOURCE TESTING
Monit can examine how much system resources a services are using. This test may only be used within a system or process service entry in the monit control file.

Depending on the system or process characteristics, services can be stopped or restarted and alerts can be generated. Thus it is possible to utilize systems which are idle and to spare system under high load.

The full syntax for the resource-statements used for resource testing is as follows (keywords are in capital and optional statements in [brackets]),

IF resource operator value [[<X>] <Y> CYCLES ] THEN action [ ELSE IF PASSED [[<X>] <Y> CYCLES ] THEN action]
resource is a choice of " CPU ", " CPU ([user|system|wait])", " MEMORY ", " CHILDREN ", " TOTALMEMORY ", " LOADAVG ([1min|5min|15min])". Some resources can be used inside of system service container, some in process service container and some in both:
System only resource tests:

CPU ([user|system|wait]) is the percent of time that the system spend in user or system/kernel space. Some systems such as linux 2.6 supports 'wait' indicator as well.

Process only resource tests:

CPU is the CPU usage of the process and its children in parts of hundred (percent).

CHILDREN is the number of child processes of the process.

TOTALMEMORY is the memory usage of the process and its child processes in either percent or as an amount (Byte, kB, MB , GB ).

System and process resource tests:

MEMORY is the memory usage of the system or in the process context of the process without its child processes in either percent (of the systems total) or as an amount (Byte, kB, MB , GB ).

LOADAVG ([1min|5min|15min]) refers to the system's load average. The load average is the number of processes in the system run queue, averaged over the specified time period.

operator is a choice of "<", ">", "!=", "==" in C notation, "gt", "lt", "eq", "ne" in shell sh notation and "greater", "less", "equal", "notequal" in human readable form (if not specified, default is EQUAL ).

value is either an integer or a real number (except for CHILDREN ). For CPU , MEMORY and TOTALMEMORY you need to specify a unit. This could be "%" or if applicable "B" (Byte), "kB" (1024 Byte), " MB " (1024 KiloByte) or " GB " (1024 MegaByte).

action is a choice of " ALERT ", " RESTART ", " START ", " STOP ", " EXEC ", " MONITOR " or " UNMONITOR ".

To calculate the cycles, a counter is raised whenever the expression above is true and it is lowered whenever it is false (but not below 0). All counters are reset in case of a restart.

The following is an example to check that the CPU usage of a service is not going beyond 50% during five poll cycles. If it does, monit will restart the service:

if cpu is greater than 50% for 5 cycles then restart
See also the example section below.
FILE CHECKSUM TESTING

The checksum statement may only be used in a file service entry. If specified in the control file, monit will compute a md5 or sha1 checksum for a file.

The checksum test in constant form is used to verify that a file does not change. Syntax (keywords are in capital):

IF FAILED [MD5|SHA1] CHECKSUM [ EXPECT checksum] [[<X>] <Y> CYCLES ] THEN action [ ELSE IF PASSED [[<X>] <Y> CYCLES ] THEN action]
The checksum test in variable form is used to watch for file changes. Syntax (keywords are in capital):
IF CHANGED [MD5|SHA1] CHECKSUM [[<X>] <Y> CYCLES ] THEN action
The choice of MD5 or SHA1 is optional. MD5 features a 256 bit and SHA1 a 320 bit checksum. If this option is omitted monit tries to guess the method from the EXPECT string or uses MD5 as default.
expect is optional and if used it specifies a md5 or sha1 string monit should expect when testing a file's checksum. If expect is used, monit will not compute an initial checksum for the file, but instead use the string you submit. For example:

if failed checksum and
   expect the sum 8f7f419955cefa0b33a2ba316cba3659
then alert
You can, for example, use the GNU utility md5sum(1) or sha1sum(1) to create a checksum string for a file and use this string in the expect-statement.
action is a choice of " ALERT ", " RESTART ", " START ", " STOP ", " EXEC ", " MONITOR " or " UNMONITOR ".

The checksum statement in variable form may be used to check a file for changes and if changed, do a specified action. For instance to reload a server if its configuration file was changed. The following illustrate this for the apache web server:

check file httpd.conf path /usr/local/apache/conf/httpd.conf
    if changed sha1 checksum
       then exec "/usr/local/apache/bin/apachectl graceful"
If you plan to use the checksum statement for security reasons, (a very good idea, by the way) and to monitor a file or files which should not change, then please use constant form and also read the DEPENDENCY TREE section below to see a detailed example on how to do this properly.
Monit can also test the checksum for files on a remote host via the HTTP protocol. See the CONNECTION TESTING section below.

TIMESTAMP TESTING

The timestamp statement may only be used in a file, fifo or directory service entry.

The timestamp test in constant form is used to verify various timestamp conditions. Syntax (keywords are in capital):

IF TIMESTAMP [[operator] value [unit]] [[<X>] <Y> CYCLES ] THEN action [ ELSE IF PASSED [[<X>] <Y> CYCLES ] THEN action]
The timestamp statement in variable form is simply to test an existing file or directory for timestamp changes and if changed, execute an action. Syntax (keywords are in capital):
IF CHANGED TIMESTAMP [[<X>] <Y> CYCLES ] THEN action
operator is a choice of "<", ">", "!=", "==" in C notation, " GT ", " LT ", " EQ ", " NE " in shell sh notation and " GREATER ", " LESS ", " EQUAL ", " NOTEQUAL " in human readable form (if not specified, default is EQUAL ).
value is a time watermark.

unit is either " SECOND ", " MINUTE ", " HOUR " or " DAY " (it is also possible to use " SECONDS ", " MINUTES ", " HOURS ", or " DAYS ").

action is a choice of " ALERT ", " RESTART ", " START ", " STOP ", " EXEC ", " MONITOR " or " UNMONITOR ".

The variable timestamp statement is useful for checking a file for changes and then execute an action. This version was written particularly with configuration files in mind. For instance, if you monitor the apache web server you can use this statement to reload apache if the httpd.conf (apache's configuration file) was changed. Like so:

check file httpd.conf with path /usr/local/apache/conf/httpd.conf
  if changed timestamp
     then exec "/usr/local/apache/bin/apachectl graceful"
The constant timestamp version is useful for monitoring systems able to report its state by changing the timestamp of certain state files. For instance the iPlanet Messaging server stored process system updates the timestamp of:
o stored.ckp
o stored.lcu
o stored.per
If a task should fail, the system keeps the timestamp. To report stored problems you can use the following statements:
check file stored.ckp with path /msg-foo/config/stored.ckp
  if timestamp > 1 minute then alert

check file stored.lcu with path /msg-foo/config/stored.lcu
  if timestamp > 5 minutes then alert

check file stored.per with path /msg-foo/config/stored.per
  if timestamp > 1 hour then alert
As mentioned above, you can also use the timestamp statement for monitoring directories for changes. If files are added or removed from a directory, its timestamp is changed:
check directory mydir path /foo/directory
 if timestamp > 1 hour then alert
or
check directory myotherdir path /foo/secure/directory
 if timestamp < 1 hour then alert
The following example is a hack for restarting a process after a certain time. Sometimes this is a necessary workaround for some third-party applications, until the vendor fix a problem:
check file server.pid path /var/run/server.pid
      if timestamp > 7 days
         then exec "/usr/local/server/restart-server"
FILE SIZE TESTING
The size statement may only be used in a file service entry. If specified in the control file, monit will compute a size for a file.

The size test in constant form is used to verify various size conditions. Syntax (keywords are in capital):

IF SIZE [[operator] value [unit]] [[<X>] <Y> CYCLES ] THEN action [ ELSE IF PASSED [[<X>] <Y> CYCLES ] THEN action]
The size statement in variable form is simply to test an existing file for size changes and if changed, execute an action. Syntax (keywords are in capital):
IF CHANGED SIZE [[<X>] <Y> CYCLES ] THEN action
operator is a choice of "<", ">", "!=", "==" in C notation, " GT ", " LT ", " EQ ", " NE " in shell sh notation and " GREATER ", " LESS ", " EQUAL ", " NOTEQUAL " in human readable form (if not specified, default is EQUAL ).
value is a size watermark.

unit is a choice of "B"," KB "," MB "," GB " or long alternatives "byte", "kilobyte", "megabyte", "gigabyte". If it is not specified, "byte" unit is assumed by default.

action is a choice of " ALERT ", " RESTART ", " START ", " STOP ", " EXEC ", " MONITOR " or " UNMONITOR ".

The variable size test form is useful for checking a file for changes and send an alert or execute an action. Monit will register the size of the file at startup and monitor the file for changes. As soon as the value changed, monit will do specified action, reset the registered value to new result and continue to monitor, whether the size changed again.

One example of use for this statement is to conduct security checks, for instance:

check file su with path /bin/su
      if changed size then exec "/sbin/ifconfig eth0 down"
which will "cut the cable" and stop a possible intruder from compromising the system further. This test is just one of many you may use to increase the security awareness on a system. If you plan to use monit for security reasons we recommend that you use this test in combination with other supported tests like checksum, timestamp, and so on.
The constant size test form may be useful in similar or different contexts. It can, for instance, be used to test if a certain file size was exceeded and then alert you or monit may execute a certain action specified by you. An example is to use this statement to rotate log files after they have reached a certain size or to check that a database file does not grow beyond a specified threshold.

To rotate a log file:

check file myapp.log with path /var/log/myapp.log
   if size > 50 MB then
      exec "/usr/local/bin/rotate /var/log/myapp.log myapp"
where /usr/local/bin/rotate may be a simple script, such as:
#/bin/bash
/bin/mv $1 $1.'date +%y-%m-%d'
/usr/bin/pkill -HUP $2
Or you may use this statement to trigger the logrotate(8) program, to do an "emergency" rotate. Or to send an alert if a file becomes a known bottleneck if it grows behind a certain size because of limits in a database engine:
check file mydb with path /data/mydatabase.db
      if size > 1 GB then alert
This is a more restrictive form of the first example where the size is explicitly defined (note that the real su size is system dependent):
check file su with path /bin/su
      if size != 95564 then exec "/sbin/ifconfig eth0 down"
FILE CONTENT TESTING
The match statement allows you to test the content of a text file by using regular expressions. This is a great feature if you need to periodically test files, such as log files, for certain patterns. If a pattern match, monit defaults to raise an alert, other actions are also possible.

The syntax (keywords in capital) for using this function is:

IF [ NOT ] MATCH {regex|path} [[<X>] <Y> CYCLES ] THEN action
regex is a string containing the extended regular expression. See also regex(7).
path is an absolute path to a file containing extended regular expression on every line. See also regex(7).

action is a choice of " ALERT ", " RESTART ", " START ", " STOP ", " EXEC ", " MONITOR " or " UNMONITOR ".

You can use the NOT statement to invert a match.

The content is only being checked every cycle. If content is being added and removed between two checks they are unnoticed.

On startup the read position is set to the end of the file and monit continue to scan to the end of file on each cycle. But if the file size should decrease or inode change the read position is set to the start of the file.

Only lines ending with a newline character are inspected. Thus, lines are being ignored until they have been completed with this character. Also note that only the first 511 characters of a line are inspected.

IGNORE [ NOT ] MATCH {regex|path}
Lines matching an IGNORE are not inspected during later evaluations. IGNORE MATCH has always precedence over IF MATCH .
All IGNORE MATCH statements are evaluated first, in the order of their appearance. Thereafter, all the IF MATCH statements are evaluated.

A real life example might look like this:

check file syslog with path /var/log/syslog
  ignore match
      "^\w{3} [ :0-9]{11} [._[:alnum:]-]+ monit\[[0-9]+\]:"
  ignore match /etc/monit/ignore.regex
  if match
      "^\w{3} [ :0-9]{11} [._[:alnum:]-]+ mrcoffee\[[0-9]+\]:"
  if match /etc/monit/active.regex then alert
FILESYSTEM FLAGS TESTING
monit tests the filesystem flags of devices for change. This test is implicit and monit will send alert in the case of failure by default.

You may override the default action using below rule (it may only be used within a device service entry in the monit control file).

This test is useful for detecting changes of the filesystem flags such as when the filesystem became read-only based on disk errors or the mount flags were changed (such as nosuid). Each platform provides different flags set. POSIX defined the RDONLY and NOSUID flags which should work on all platforms. Some platforms (such as FreeBSD) present another flags in addition.

The syntax for the fsflags statement is:

IF CHANGED FSFLAGS [[<X>] <Y> CYCLES ] THEN action
action is a choice of " ALERT ", " RESTART ", " START ", " STOP ", " EXEC ", " MONITOR " or " UNMONITOR ".
Example:

check device rootfs with path /
      if changed fsflags then exec "/my/script"
      alert root@localhost
SPACE TESTING
Monit can test devices/file systems and check for space usage. This test may only be used within a device service entry in the monit control file.

Monit will check a device's total space usage. If you only want to check available space for non-superuser, you must set the watermark appropriately (i.e. total space minus reserved blocks for the superuser).

You can obtain (and set) the superuser's reserved blocks size, for example by using the tune2fs utility on Linux. On Linux 5% of available blocks are reserved for the superuser by default. To list the reserved blocks for the superuser:

[root@berry monit]# tune2fs -l /dev/hda1| grep "Reserved block"
Reserved block count:     319994
Reserved blocks uid:      0 (user root)
Reserved blocks gid:      0 (group root)
On solaris 10% of the blocks are reserved. You can also use tunefs on solaris to change values on a live filesystem.
The full syntax for the space statement is:

IF SPACE operator value unit [[<X>] <Y> CYCLES ] THEN action [ ELSE IF PASSED [[<X>] <Y> CYCLES ] THEN action]
operator is a choice of "<",">","!=","==" in c notation, "gt", "lt", "eq", "ne" in shell sh notation and "greater", "less", "equal", "notequal" in human readable form (if not specified, default is EQUAL ).
unit is a choice of "B"," KB "," MB "," GB ", "%" or long alternatives "byte", "kilobyte", "megabyte", "gigabyte", "percent".

action is a choice of " ALERT ", " RESTART ", " START ", " STOP ", " EXEC ", " MONITOR " or " UNMONITOR ".

INODE TESTING

If supported by the file-system, you can use monit to test for inodes usage. This test may only be used within a device service entry in the monit control file.

If the device becomes unavailable, monit will call the entry's registered start method, if it is defined and if monit is running in active mode. If monit runs in passive mode or the start methods is not defined, monit will just send an error alert.

The syntax for the inode statement is:

IF INODE (S) operator value [unit] [[<X>] <Y> CYCLES ] THEN action [ ELSE IF PASSED [[<X>] <Y> CYCLES ] THEN action]
operator is a choice of "<",">","!=","==" in c notation, "gt", "lt", "eq", "ne" in shell sh notation and "greater", "less", "equal", "notequal" in human readable form (if not specified, default is EQUAL ).
unit is optional. If not specified, the value is an absolute count of inodes. You can use the "%" character or the longer alternative "percent" as a unit.

action is a choice of " ALERT ", " RESTART ", " START ", " STOP ", " EXEC ", " MONITOR " or " UNMONITOR ".

PERMISSION TESTING

Monit can monitor the permissions. This test may only be used within a file, fifo, directory or device service entry in the monit control file.

The syntax for the permission statement is:

IF FAILED PERM ( ISSION ) octalnumber [[<X>] <Y> CYCLES ] THEN action [ ELSE IF PASSED [[<X>] <Y> CYCLES ] THEN action]
octalnumber defines permissions for a file, a directory or a device.
action is a choice of " ALERT ", " RESTART ", " START ", " STOP ", " EXEC ", " MONITOR " or " UNMONITOR ".

The web interface will show a permission warning if the test failed.

We recommend that you use the UNMONITOR action in a permission statement. The rationale for this feature is security and that monit does not start a possible cracked program or script. Example:

check file monit.bin with path "/usr/local/bin/monit"
      if failed permission 0555 then unmonitor
      alert foo@bar
If the test fails, monit will simply send an alert and stop monitoring the file and propagate an unmonitor action upward in a depend tree.
UID TESTING

monit can monitor the owner user id (uid). This test may only be used within a file, fifo, directory or device service entry in the monit control file.

The syntax for the uid statement is:

IF FAILED UID user [[<X>] <Y> CYCLES ] THEN action [ ELSE IF PASSED [[<X>] <Y> CYCLES ] THEN action]
user defines a user id either in numeric or in string form.
action is a choice of " ALERT ", " RESTART ", " START ", " STOP ", " EXEC ", " MONITOR " or " UNMONITOR ".

The web interface will show a uid warning if the test should fail.

We recommend that you use the UNMONITOR action in a uid statement. The rationale for this feature is security and that monit does not start a possible cracked program or script. Example:

check file passwd with path /etc/passwd
      if failed uid root then unmonitor
      alert root@localhost
If the test fails, monit will simply send an alert and stop monitoring the file and propagate an unmonitor action upward in a depend tree.
GID TESTING

monit can monitor the owner group id (gid). This test may only be used within a file, fifo, directory or device service entry in the monit control file.

The syntax for the gid statement is:

IF FAILED GID user [[<X>] <Y> CYCLES ] THEN action [ ELSE IF PASSED [[<X>] <Y> CYCLES ] THEN action]
user defines a group id either in numeric or in string form.
action is a choice of " ALERT ", " RESTART ", " START ", " STOP ", " EXEC ", " MONITOR " or " UNMONITOR ".

The web interface will show a gid warning if the test should fail.

We recommend that you use the UNMONITOR action in a gid statement. The rationale for this feature is security and that monit does not start a possible cracked program or script. Example:

check file shadow with path /etc/shadow
      if failed gid root then unmonitor
      alert root@localhost
If the test fails, monit will simply send an alert and stop monitoring the file and propagate an unmonitor action upward in a depend tree.
PID TESTING

monit tests the process id (pid) of processes for change. This test is implicit and monit will send alert in the case of failure by default.

You may override the default action using below rule (it may only be used within a process service entry in the monit control file).

The syntax for the pid statement is:

IF CHANGED PID [[<X>] <Y> CYCLES ] THEN action
action is a choice of " ALERT ", " RESTART ", " START ", " STOP ", " EXEC ", " MONITOR " or " UNMONITOR ".
This test is useful to detect possible process restarts which has occurred in the timeframe between two monit testing cycles. In the case that the restart was fast and the process provides expected service (i.e. all tests passed) you will be notified that the process was replaced.

For example sshd daemon can restart very quickly, thus if someone changes its configuration and do sshd restart outside of monit control, you will be notified that the process was replaced by new instance (or you can optionaly do some other action such as preventively stop sshd).

Another example is MySQL Cluster which has its own watchdog with process restart ability. You can use monit for redundant monitoring. Monit will just send alert in the case that the MySQL cluster restarted the node quickly.

Example:

check process sshd with pidfile /var/run/sshd.pid
      if changed pid then exec "/my/script"
      alert root@localhost
PPID TESTING
monit tests the process parent id (ppid) of processes for change. This test is implicit and monit will send alert in the case of failure by default.

You may override the default action using below rule (it may only be used within a process service entry in the monit control file).

The syntax for the ppid statement is:

IF CHANGED PPID [[<X>] <Y> CYCLES ] THEN action
action is a choice of " ALERT ", " RESTART ", " START ", " STOP ", " EXEC ", " MONITOR " or " UNMONITOR ".
This test is useful for detecting changes of a process parent.

Example:

check process myproc with pidfile /var/run/myproc.pid
      if changed ppid then exec "/my/script"
      alert root@localhost
CONNECTION TESTING
Monit is able to perform connection testing via networked ports or via Unix sockets. A connection test may only be used within a process or within a host service entry in the monit control file.

If a service listens on one or more sockets, monit can connect to the port (using either tcp or udp) and verify that the service will accept a connection and that it is possible to write and read from the socket. If a connection is not accepted or if there is a problem with socket read/write, monit will assume that something is wrong and execute a specified action. If monit is compiled with openssl, then ssl based network services can also be tested.

The full syntax for the statement used for connection testing is as follows (keywords are in capital and optional statements in [brackets]),

IF FAILED [host] port [type] [protocol|{send/expect}+] [timeout] [[<X>] <Y> CYCLES ] THEN action [ ELSE IF PASSED [[<X>] <Y> CYCLES ] THEN action]
or for Unix sockets,
IF FAILED [unixsocket] [type] [protocol|{send/expect}+] [timeout] [[<X>] <Y> CYCLES ] THEN action [ ELSE IF PASSED [[<X>] <Y> CYCLES ] THEN action]
host:HOST hostname. Optionally specify the host to connect to. If the host is not given then localhost is assumed if this test is used inside a process entry. If this test was used inside a remote host entry then the entry's remote host is assumed. Although host is intended for testing name based virtual host in a HTTP server running on local or remote host, it does allow the connection statement to be used to test a server running on another machine. This may be useful; For instance if you use Apache httpd as a front-end and an application-server as the back-end running on another machine, this statement may be used to test that the back-end server is running and if not raise an alert.
port:PORT number. The port number to connect to

unixsocket:UNIXSOCKET PATH . Specifies the path to a Unix socket. Servers based on Unix sockets, always runs on the local machine and does not use a port.

type:TYPE {TCP|UDP|TCPSSL}. Optionally specify the socket type monit should use when trying to connect to the port. The different socket types are; TCP , UDP or TCPSSL , where TCP is a regular stream based socket, UDP is a datagram socket and TCPSSL specify that monit should use a TCP socket with SSL when connecting to a port. The default socket type is TCP . If TCPSSL is used you may optionally specify the SSL/TLS protocol to be used and the md5 sum of the server's certificate. The TCPSSL options are:

TCPSSL [SSLAUTO|SSLV2|SSLV3|TLSV1] [CERTMD5 md5sum]
proto(col):PROTO {protocols}. Optionally specify the protocol monit should speak when a connection is established. At the moment monit knows how to speak: APACHE-STATUS DNS DWP FTP HTTP IMAP CLAMAV LDAP2 LDAP3 MYSQL NNTP NTP3 POP POSTFIX-POLICY RDATE RSYNC SMTP SSH TNS PGSQL If you have compiled monit with ssl support, monit can also speak the SSL variants such as: HTTPS FTPS POPS IMAPS To use the SSL protocol support you need to define the socket as SSL and use the general protocol name (for example in the case of HTTPS ) : TYPE TCPSSL PROTOCOL HTTP If the server's protocol is not found in this list, simply do not specify the protocol and monit will utilize a default test, including testing if it is possible to read and write to the port. This default test is in most cases more than good enough to deduce if the server behind the port is up or not.
The protocol statement is:

[PROTO(COL) {name} [REQUEST {"/path"} [with CHECKSUM checksum]]
As you can see, you may specify a request after the protocol, at the moment only the HTTP protocol supports the request option. See also below for an example.
In addition to the standard protocols, the APACHE-STATUS protocol is a test of a specific server type, rather than a generic protocol. Server performance is examined using the status page generated by Apache's mod_status, which is expected to be at its default address of http://www.example.com/server-status. Currently the APACHE-STATUS protocol examines the percentage of Apache child processes which are

o logging (loglimit)
o closing connections (closelimit)
o performing DNS lookups (dnslimit)
o in keepalive with a client (keepalivelimit)
o replying to a client (replylimit)
o receiving a request (requestlimit)
o initialising (startlimit)
o waiting for incoming connections (waitlimit)
o gracefully closing down (gracefullimit)
o performing cleanup procedures (cleanuplimit)
Each of these quantities can be compared against a value relative to the total number of active Apache child processes. If the comparison expression is true the chosen action is performed.
The apache-status protocol statement is formally defined as (keywords in uppercase):

PROTO(COL) {limit} OP PERCENT [OR {limit} OP PERCENT]*
where {limit} is one or more of: loglimit, closelimit, dnslimit, keepalivelimit, replylimit, requestlimit, startlimit, waitlimit gracefullimit or cleanuplimit. The operator OP is one of: [<|=|>].
You can combine all of these test into one expression or you can choose to test a certain limit. If you combine the limits you must or' them together using the OR keyword.

Here's an example were we test for a loglimit more than 10 percent, a dnslimit over 25 percent and a wait limit less than 20 percent of processes. See also more examples below in the example section.

protocol apache-status
               loglimit > 10% or
               dnslimit > 50% or
               waitlimit < 20%
then alert
Obviously, do not use this test unless the httpd server you are testing is Apache Httpd and mod_status is activated on the server.
send/expect: {SEND|EXPECT} "string" .... If monit does not support the protocol spoken by the server, you can write your own protocol-test using send and expect strings. The SEND statement sends a string to the server port and the EXPECT statement compares a string read from the server with the string given in the expect statement. If your system supports POSIX regular expressions, you can use regular expressions in the expect string, see regex(7) to learn more about the types of regular expressions you can use in an expect string. Otherwise the string is used as it is. The send/expect statement is:

[{SEND|EXPECT} "string"]+
Note that monit will send a string as it is, and you must remember to include CR and LF in the string sent to the server if the protocol expect such characters to terminate a string (most text based protocols used over Internet does). Likewise monit will read up to 256 bytes from the server and use this string when comparing the expect string. If the server sends strings terminated by CRLF , (i.e. "\r\n") you may remember to add the same terminating characters to the string you expect from the server.
You can use non-printable characters in a send string if needed. Use the hex notation, \0xHEXHEX to send any char in the range \0x00-\0xFF, that is, 0-255 in decimal. This may be useful when testing some network protocols, particularly those over UDP . An example, to test a quake 3 server you can use the following,

send "\0xFF\0xFF\0xFF\0xFFgetstatus"
expect "sv_floodProtect|sv_maxPing"
Finally, send/expect can be used with any socket type, such as TCP sockets, UNIX sockets and UDP sockets.
timeout:with TIMEOUT x SECONDS . Optionally specifies the connect and read timeout for the connection. If monit cannot connect to the server within this time it will assume that the connection failed and execute the specified action. The default connect timeout is 5 seconds.

action is a choice of " ALERT ", " RESTART ", " START ", " STOP ", " EXEC ", " MONITOR " or " UNMONITOR ".

Connection testing using the URL notation

You can test a HTTP server using the compact URL syntax. This test also allow you to use POSIX regular expressions to test the content returned by the HTTP server.

The full syntax for the URL statement is as follows (keywords are in capital and optional statements in [brackets]):

IF FAILED URL ULR-spec
   [CONTENT {==|!=} "regular-expression"]
   [TIMEOUT number SECONDS] [[<X>] <Y> CYCLES]
   THEN action
   [ELSE IF PASSED [[<X>] <Y> CYCLES] THEN action]
Where URL-spec is an URL on the standard form as specified in RFC 2396:
<protocol>://<authority><path>?<query>
Here is an example on an URL where all components are used:
http://user:password@www.foo.bar:8080/document/?querystring#ref
If a username and password is included in the URL monit will attempt to login at the server using Basic Authentication.
Testing the content returned by the server is optional. If used, you can test if the content match or does not match a regular expression. Here's an example on how the URL statement can be used in a check service:

check host FOO with address www.foo.bar
     if failed url
        http://user:password@www.foo.bar:8080/?querystring
        and content == 'action="j_security_check"'
     then ...
Monit will look at the content-length header returned by the server and download this amount before testing the content. That is, if the content-length is more than 1Mb or this header is not set by the server monit will default to download up to 1 Mb and not more.
Only the http(s) protocol is supported in an URL statement. If the protocol is https monit will use SSL when connecting to the server.

Remote host ping test

In addition monit can perform ICMP Echo tests in remote host checks. The icmp test may only be used in a check host entry and monit must run with super user privileges, that is, the root user must run monit. The reason is that the icmp test utilize a raw socket to send the icmp packet and only the super user is allowed to create a raw socket.

The full syntax for the ICMP Echo statement used for ping testing is as follows (keywords are in capital and optional statements in [brackets]):

IF FAILED ICMP TYPE ECHO
   [COUNT number] [WITH] [TIMEOUT number SECONDS]
     [[<X>] <Y> CYCLES]
   THEN action
   [ELSE IF PASSED [[<X>] <Y> CYCLES] THEN action]
The rules for action and timeout are the same as those mentioned above in the CONNECTION TESTING section. The count parameter specifies how many consecutive echo requests will be send to the host in one cycle. In the case that no reply came within timeout frame, monit reports error. When at least one reply was received, the test will pass. Monit sends by default three echo requests in one cycle to prevent the random packet loss from generating false alarm (i.e. up to 66% packet loss is tolerated). You can set the count option to different value, which can serve as error ratio. For example in the case that you require 100% ping success, you can set the count to 1 (i.e. just one attempt will be send, when the packet was lost, then error will be reported).
An icmp ping test is useful for testing if a host is up, before testing ports at the host. If an icmp ping test is used in a check host entry, this test is run first and if the ping test should fail we assume that the connection to the host is down and monit does not continue to test any ports. Here's an example:

check host xyzzy with address xyzzy.org
      if failed icmp type echo count 5 with timeout 15 seconds
         then alert
      if failed port 80 proto http then alert
      if failed port 443 type TCPSSL proto http then alert
      alert foo@bar
In this case, if the icmp test should fail you will get one alert and only one alert as long as the host is down, and equally important, monit will not test port 80 and port 443. Likewise if the icmp ping test should succeed (again) monit will continue to test both port 80 and 443.
Keep in mind though that some firewalls can block icmp packages and thus render the test useless.

Examples

To check a port connection and receive an alert if monit cannot connect to the port, use the following statement:

if failed port 80 then alert
In this case the machine in question is assumed to be the default host. For a process entry it's localhost and for a remote host entry it's the address of the remote host. Monit will conduct a tcp connection to the host at port 80 and use tcp by default. If you want to connect with udp, you can specify this after the port-statement;
if failed port 53 type udp protocol dns then alert
Monit will stop trying to connect to the port after 5 seconds and assume that the server behind the port is down. You may increase or decrease the connect timeout by explicit add a connection timeout. In the following example the timeout is increased to 15 seconds and if monit cannot connect to the server within 15 seconds the test will fail and an alert message is sent.
if failed port 80 with timeout 15 seconds then alert
If a server is listening to a Unix socket the following statement can be used:
if failed unixsocket /var/run/sophie then alert
A Unix socket is used by some servers for fast (interprocess) communication on localhost only. A Unix socket is specified by a path and in the example above the path, /var/run/sophie, specifies a Unix socket.
If your machine answers for several virtual hosts you can prefix the port statement with a host-statement like so:

if failed host www.sol.no port 80 then alert
if failed host 80.69.226.133 port 443 then alert
if failed host kvasir.sol.no port 80 then alert
And as mentioned above, if you do not specify a host-statement, localhost or address is assumed.
Monit also knows how to speak some of the more popular Internet protocols. So, besides testing for connections, monit can also speak with the server in question to verify that the server works. For example, the following is used to test a http server:

if failed host www.tildeslash.com port 80 proto http
   then restart
Some protocols also support a request statement. This statement can be used to ask the server for a special document entity.
Currently only the HTTP protocol module supports the request statement, such as:

if failed host www.myhost.com port 80 protocol http
   and request "/data/show.php?a=b&c=d"
then restart
The request must contain an URI string specifying a document from the http server. The string will be URL encoded by monit before it sends the request to the http server, so it's okay to use URL unsafe characters in the request. If the request statement isn't specified, the default web server page will be requested.
You can also test the checksum for documents returned by a http server. You can use either MD5 sums:

if failed port 80 protocol http
   and request "/page.html"
       with checksum 8f7f419955cefa0b33a2ba316cba3659
then alert
Or you can use SHA1 sums:
if failed port 80 protocol http
   and request "/page.html"
       with checksum e428302e260e0832007d82de853aa8edf19cd872
then alert
monit will compute a checksum (either MD5 or SHA1 is used, depending on length of the hash) for the document (in the above case, /page.html) and compare the computed checksum with the expected checksum. If the sums does not match then the if-tests action is performed, in this case alert. Note that monit will not test the checksum for a document if the server does not set the HTTP Content-Length header. A HTTP server should set this header when it server a static document (i.e. a file). A server will often use chunked transfer encoding instead when serving dynamic content (e.g. a document created by a CGI-script or a Servlet), but to test the checksum for dynamic content is not very useful. There are no limitation on the document size, but keep in mind that monit will use time to download the document over the network so it's probably smart not to ask monit to compute a checksum for documents larger than 1Mb or so, depending on you network connection of course. Tip; If you get a checksum error even if the document has the correct sum, the reason may be that the download timed out. In this case, explicit set a longer timeout than the default 5 seconds.
As mentioned above, if the server protocol is not supported by monit you can write your own protocol test using send/expect strings. Here we show a protocol test using send/expect for an imaginary "Ali Baba and the Forty Thieves" protocol:

if failed host cave.persia.ir port 4040
   send "Open, Sesame!\r\n"
   expect "Please enter the cave\r\n"
   send "Shut, Sesame!\r\n"
   expect "See you later [A-Za-z ]+\r\n"
then restart
The TCPSSL statement can optionally test the md5 sum of the server's certificate. You must state the md5 certificate string you expect the server to deliver and upon a connect to the server, the server's actual md5 sum certificate string is tested. Any other symbol but [A-Fa-f0-9] is being ignored in that sting. Thus it is possible to copy and paste the output of e.g. openssl. If they do not match, the connection test fails. If the ssl version handshake does not work properly you can also force a specific ssl version, as we demonstrate in this example:
if failed host shop.sol.no port 443
   type TCPSSL SSLV3 # Force monit to use ssl version 3
   # We expect the server to return this  md5 certificate sum
   # as either 12-34-56-78-90-AB-CD-EF-12-34-56-78-90-AB-CD-EF
   # or e.g.   1234567890ABCDEF1234567890ABCDEF
   # or e.g.   1234567890abcdef1234567890abcdef
   # what ever come in more handy (see text above)
   CERTMD5 12-34-56-78-90-AB-CD-EF-12-34-56-78-90-AB-CD-EF
   protocol http
then restart
Here's an example where a connection test is used inside a process entry:
check process apache with pidfile /var/run/apache.pid
      start program = "/etc/init.d/httpd start"
      stop program = "/etc/init.d/httpd stop"
      if failed host www.tildeslash.com port 80 then restart
Here, a connection test is used in a remote host entry:
check host up2date with address ftp.redhat.com
      if failed port 21 and protocol ftp then alert
Since we did not explicit specify a host in the above test, monit will connect to port 21 at ftp.redhat.com. Apropos, the host address can be specified as a dotted IP address string or as hostname in the DNS . The following is exactly[*] the same test, but here an ip address is used instead:
check host up2date with address 66.187.232.30
      if failed port 21 and protocol ftp then alert
[*] Well, not quite, since we specify an ip-address directly we will bypass any DNS round-robin setup, but that's another story.
For more examples, see the example section below.

Monit Httpd

If specified in the control file, monit will start a monit daemon with http support. From a Browser you can then start and stop services, disable or enable service monitoring as well as view the status of each service. Also, if monit logs to its own file, you can view the content of this logfile in a Browser.

The control file statement for starting a monit daemon with http support is a global set-statement:

set httpd port 2812
And you can use this URL , http://localhost:2812/, to access the daemon from a browser. The port number, in this case 2812, can be any number that you are allowed to bind to.
If you have compiled monit with openssl, you can also start the httpd server with ssl support, using the following expression:

set httpd port 2812
    ssl enable
    pemfile /etc/certs/monit.pem
And you can use this URL , https://localhost:2812/, to access the monit web server over an ssl encrypted connection.
The pemfile, in the example above, holds both the server's private key and certificate. This file should be stored in a safe place on the filesystem and should have strict permissions, that is, no more than 0700.

In addition, if you want to check for client certificates you can use the CLIENTPEMFILE statement. In this case, a connecting client has to provided a certificate known by monit in order to connect. This file also needs to have all necessary CA certificates. A configuration could look like:

set httpd port 2812
    ssl enable
    pemfile /etc/certs/monit.pem
    clientpemfile /etc/certs/monit-client.pem
By default self signed client certificates are not allowed. If you want to use a self signed certificate from a client it has to be allowed explicitly with the ALLOWSELFCERTIFICATION statement.
For more information on how to use monit with SSL and for more information about certificates and generating pem files, please consult the README .SSL file accompanying the software.

If you only want the http server to accept connect requests to one host addresses you can specify the bind address either as an IP number string or as a hostname. In the following example we bind the http server to the loopback device. In other words the http server will only be reachable from localhost:

set httpd port 2812 and use the address 127.0.0.1
or
set httpd port 2812 and use the address localhost
If you do not use the ADDRESS statement the http server will accept connections on any/all local addresses.
It is possible to hide monit's httpd server version, which usually is available in httpd header responses and in error pages.

set httpd port 2812
  ...
  signature {enable|disable}
Use disable to hide the server signature - monit will only report its name (e.g. 'monit' instead of for example 'monit 4.2'). By default the version signature is enabled. It is worth to stress that this option provides no security advantage and falls into the "security through obscurity" category.
If you remove the httpd statement from the config file, monit will stop the httpd server on configuration reload. Likewise if you change the port number, monit will restart the http server using the new specified port number.

The status page displayed by the monit web server is automatically refreshed with the same poll time set for the monit daemon.

Note:

We strongly recommend that you start monit with http support (and bind the server to localhost, only, unless you are behind a firewall). The built-in web-server is small and does not use much resources, and more importantly, monit can use the http server for interprocess communication between a monit client and a monit daemon.

For instance, you must start a monit daemon with http support if you want to be able to use the following console commands. (That is; most of the available console commands).

'monit stop all'
'monit start all'
'monit stop service'
'monit start service'
'monit restart service'
'monit monitor service'
'monit unmonitor service'
'monit -g groupname stop all'
'monit -g groupname start all'
'monit -g groupname restart all'
'monit -g groupname monitor all'
'monit -g groupname unmonitor all'
If a monit daemon is running in the background we will ask the daemon (via the HTTP protocol) to execute the above commands. That is, the daemon is requested to start and stop the services. This ensures that a daemon will not restart a service that you requested to stop and that (any) timeout lock will be removed from a service when you start it.
Monit HTTPD Authentication

monit supports two types of authentication schema's for connecting to the httpd server, (three, if you count SSL client certificate validation). Both schema's can be used together or by itself. You must choose at least one.

Host and network allow list

The http server maintains an access-control list of hosts and networks allowed to connect to the server. You can add as many hosts as you want to, but only hosts with a valid domain name or its IP address are allowed. If you specify a hostname that does not resolve, monit will write an error message in the console and not start. Networks require a network IP and a netmask to be accepted.

The http server will query a name server to check any hosts connecting to the server. If a host (client) is trying to connect to the server, but cannot be found in the access list or cannot be resolved, the server will shutdown the connection to the client promptly.

Control file example:

set httpd port 2812
    allow localhost
    allow my.other.work.machine.com
    allow 10.1.1.1
    allow 192.168.1.0/255.255.255.0
    allow 10.0.0.0/8
Clients, not mentioned in the allow list that tries to connect to the server are logged with their ip-address.
Basic Authentication

This authentication schema is HTTP specific and described in more detail in RFC 2617.

In short; a server challenge a client (e.g. a Browser) to send authentication information (username and password) and if accepted, the server will allow the client access to the requested document.

The biggest weakness with Basic Authentication is that the username and password is sent in clear-text (i.e. base64 encoded) over the network. It is therefor recommended that you do not use this authentication method unless you run the monit http server with ssl support. With ssl support it is completely safe to use Basic Authentication since all http data, including Basic Authentication headers will be encrypted.

monit will use Basic Authentication if an allow statement contains a username and a password separated with a single ':' character, like so; allow username:password. The username and password must be written in clear-text.

Alternatively you can use files in "htpasswd" format (one user:passwd entry per line), like so: allow [cleartext|crypt|md5] /path [users]. By default cleartext passwords are read. In case the passwords are digested it is necessary to specify the cryptographic method. In order to select the users their names can be added to the allow statement. Otherwise all users are added.

Example:

set httpd port 2812
    allow hauk:password
    allow md5 /etc/httpd/htpasswd john paul ringo george
If you use this method together with a host list, then only clients from the listed hosts will be allowed to connect to the monit http server and each client will be asked to provide a username and a password.
Example:

set httpd port 2812
    allow localhost
    allow 10.1.1.1
    allow hauk:password
If you only want to use Basic Authentication, then just provide allow entries with username and password, like so:
set httpd port 2812
    allow hauk:password
    allow admin:password
Finally it is possible to define some users as read-only. A read-only user can read the monit web pages but will not get access to push-buttons and cannot change a service from the web interface.
set httpd port 2812
    allow admin:password
    allow hauk:password read-only
A user is set to read-only by using the read-only keyword after username:password. In the above example the user hauk is defined as a read-only user, while the admin user has all access rights.
NB ! a monit client will use the first username:password pair in an allow list and you should not define the first user as a read-only user. If you do, monit console commands will not work.

If you use Basic Authentication it is a good idea to set the access permission for the control file (~/.monitrc) to only readable and writable for the user running monit, because the password is written in clear-text. (Use this command, /bin/chmod 600 ~/.monitrc). In fact, since monit version 3.0, monit will complain and exit if the control file is readable by others.

Clients trying to connect to the server but supply the wrong username and/or password are logged with their ip-address.

If the monit command line interface is being used at least one cleartext password is necessary. Otherwise, the monit command line interface will not be able to connect to the monit daemon server.

Dependencies

If specified in the control file, monit can do dependency checking before start, stop, monitoring or unmonitoring of services. The dependency statement may be used within any service entries in the monit control file.

The syntax for the depend statement is simply:

DEPENDS on service[, service [,...]]
Where service is a service entry name, for instance apache or datafs.
You may add more than one service name of any type or use more than one depend statement in an entry.

Services specified in a depend statement will be checked during stop/start/monitor/unmonitor operations. If a service is stopped or unmonitored it will stop/unmonitor any services that depends on itself. Likewise, if a service is started, it will first stop any services that depends on itself and after it is started, start all depending services again. If the service is to be monitored (enable monitoring), all services which this service depends on will be monitored before enabling monitoring of this service.

Here is an example where we set up an apache service entry to depend on the underlying apache binary. If the binary should change an alert is sent and apache is not monitored anymore. The rationale is security and that monit should not execute a possibly cracked apache binary.

(1) check process apache
(2)    with pidfile "/usr/local/apache/logs/httpd.pid"
(3)    ...
(4)    depends on httpd
(5)
(6) check file httpd with path /usr/local/apache/bin/httpd
(7)    if failed checksum then unmonitor
The first entry is the process entry for apache shown before (abbreviated for clarity). The fourth line sets up a dependency between this entry and the service entry named httpd in line 6. A depend tree works as follows, if an action is conducted in a lower branch it will propagate upward in the tree and for every dependent entry execute the same action. In this case, if the checksum should fail in line 7 then an unmonitor action is executed and the apache binary is not checked anymore. But since the apache process entry depends on the httpd entry this entry will also execute the unmonitor action. In short, if the checksum test for the httpd binary file should fail, both the check file httpd entry and the check process apache entry is set in un-monitoring mode.
A dependency tree is a general construct and can be used between all types of service entries and span many levels and propagate any supported action (except the exec action which will not propagate upward in a dependency tree for obvious reasons).

Here is another different example. Consider the following common server setup:

WEB-SERVER -> APPLICATION-SERVER -> DATABASE -> FILESYSTEM
    (a)               (b)             (c)          (d)
You can set dependencies so that the web-server depends on the application server to run before the web-server starts and the application server depends on the database server and the database depends on the file-system to be mounted before it starts. See also the example section below for examples using the depend statement.
Here we describe how monit will function with the above dependencies:

If no servers are running
monit will start the servers in the following order: d, c, b, a
If all servers are running
When you run 'monit stop all' this is the stop order: a, b, c, d. If you run 'monit stop d' then a, b and c are also stopped because they depend on d and finally d is stopped.
If a does not run
When monit runs it will start a
If b does not run
When monit runs it will first stop a then start b and finally start a again.
If c does not run
When monit runs it will first stop a and b then start c and finally start b then a.
If d does not run
When monit runs it will first stop a, b and c then start d and finally start c, b then a.
If the control file contains a depend loop.
A depend loop is for example; a->b and b->a or a->b->c->a.
When monit starts it will check for such loops and complain and exit if a loop was found. It will also exit with a complaint if a depend statement was used that does not point to a service in the control file.

The Run Control File

The preferred way to set up monit is to write a .monitrc file in your home directory. When there is a conflict between the command-line arguments and the arguments in this file, the command-line arguments take precedence. To protect the security of your control file and passwords the control file must have permissions no more than 0700 (u=xrw,g=,o=); monit will complain and exit otherwise.

Run Control Syntax

Comments begin with a '#' and extend through the end of the line. Otherwise the file consists of a series of service entries or global option statements in a free-format, token-oriented syntax.

There are three kinds of tokens: grammar keywords, numbers (i.e. decimal digit sequences) and strings. Strings can be either quoted or unquoted. A quoted string is bounded by double quotes and may contain whitespace (and quoted digits are treated as a string). An unquoted string is any whitespace-delimited token, containing characters and/or numbers.

On a semantic level, the control file consists of two types of entries:

|1. Global set-statements
A global set-statement starts with the keyword set and the item to configure.
|2. One or more service entry statements.
Each service entry consists of the keywords 'check', followed by the service type. Each entry requires a <unique> descriptive name, which may be freely chosen. This name is used by monit to refer to the service internally and in all interactions with the user.
Currently, six types of check statements are supported:
|1. CHECK PROCESS <unique name> PIDFILE <path>
<path> is the absolute path to the program's pidfile. If the pidfile does not exist or does not contain the pid number of a running process, monit will call the entry's start method if defined, If monit runs in passive mode or the start methods is not defined, monit will just send alerts on errors.
|2. CHECK FILE <unique name> PATH <path>
<path> is the absolute path to the file. If the file does not exist or disappeared, monit will call the entry's start method if defined, if <path> does not point to a regular file type (for instance a directory), monit will disable monitoring of this entry. If monit runs in passive mode or the start methods is not defined, monit will just send alerts on errors.
|3. CHECK FIFO <unique name> PATH <path>
<path> is the absolute path to the fifo. If the fifo does not exist or disappeared, monit will call the entry's start method if defined, if <path> does not point to a fifo type (for instance a directory), monit will disable monitoring of this entry. If monit runs in passive mode or the start methods is not defined, monit will just send alerts on errors.
|4. CHECK DEVICE <unique name> PATH <path>
<path> is the path to the device block special file, mount point, file or a directory which is part of a filesystem. It is recommended to use a block special file directly (for example /dev/hda1 on Linux or /dev/dsk/c0t0d0s1 on Solaris, etc.) If you use a mount point (for example /data), be careful, because if the device is unmounted the test will still be true because the mount point exist.
If the device becomes unavailable, monit will call the entry's start method if defined. if <path> does not point to a device, monit will disable monitoring of this entry. If monit runs in passive mode or the start methods is not defined, monit will just send alerts on errors.

|5. CHECK DIRECTORY <unique name> PATH <path>
<path> is the absolute path to the directory. If the directory does not exist or disappeared, monit will call the entry's start method if defined, if <path> does not point to a directory, monit will disable monitoring of this entry. If monit runs in passive mode or the start methods is not defined, monit will just send alerts on errors.
|6. CHECK HOST <unique name> ADDRESS <host address>
The host address can be specified as a hostname string or as an ip-address string on a dotted decimal format. Such as, tildeslash.com or "64.87.72.95".
|7. CHECK SYSTEM <unique name>
The system name is usualy hostname, but any descriptive name can be used. This test allows to check general system resources such as CPU usage (percent of time spent in user, system and wait), total memory usage or load average.
You can use noise keywords like 'if', 'and', 'with(in)', 'has', 'using', 'use', 'on(ly)', 'usage' and 'program(s)' anywhere in an entry to make it resemble English. They're ignored, but can make entries much easier to read at a glance. The punctuation characters ';' ',' and '=' are also ignored. Keywords are case insensitive.
Here are the legal global keywords:

Keyword         Function
----------------------------------------------------------------
set daemon      Set a background poll interval in seconds.
set init        Set monit to run from init. monit will not
                transform itself into a daemon process.
set logfile     Name of a file to dump error- and status-
                messages to. If syslog is specified as the
                file, monit will utilize the syslog daemon
                to log messages. This can optionally be
                followed by 'facility <facility>' where
                facility is 'log_local0' - 'log_local7' or
                'log_daemon'. If no facility is specified,
                LOG_USER is used.
set mailserver  The mailserver used for sending alert
                notifications. If the mailserver is not
                defined, monit will try to use 'localhost'
                as the smtp-server for sending mail. You
                can add more mail servers, if monit cannot
                connect to the first server it will try the
                next server and so on.
set mail-format Set a global mail format for all alert
                messages emitted by monit.
set pidfile     Explicit set the location of the monit lock
                file. E.g. set pidfile /var/run/xyzmonit.pid.
set statefile   Explicit set the location of the file monit
                will write state data to. If not set, the
                default is $HOME/.monit.state.
set httpd port  Activates monit http server at the given
                port number.
ssl enable      Enables ssl support for the httpd server.
                Requires the use of the pemfile statement.
ssl disable     Disables ssl support for the httpd server.
                It is equal to omitting any ssl statement.
pemfile         Set the pemfile to be used with ssl.
clientpemfile   Set the pemfile to be used when client
                certificates should be checked by monit.
address         If specified, the http server will only
                accept connect requests to this addresses
                This statement is an optional part of the
                set httpd statement.
allow           Specifies a host or IP address allowed to
                connect to the http server. Can also specify
                a username and password allowed to connect
                to the server. More than one allow statement
                are allowed. This statement is also an
                optional part of the set httpd statement.
read-only       Set the user defined in username:password
                to read only. A read-only user cannot change
                a service from the monit web interface.
include         include a file or files matching the globstring

Here are the legal service entry keywords:

Keyword         Function
----------------------------------------------------------------
check           Starts an entry and must be followed by the type
                of monitored service {device|directory|file|host
                process|system} and a descriptive name for the
                service.
pidfile         Specify the  process pidfile. Every
                process must create a pidfile with its
                current process id. This statement should only
                be used in a process service entry.
path            Must be followed by a path to the block
                special file for filesystem (device), regular
                file, directory or a process's pidfile.
group           Specify a groupname for a service entry.
start           The program used to start the specified
                service. Full path is required. This
                statement is optional, but recommended.
stop            The program used to stop the specified
                service. Full path is required. This
                statement is optional, but recommended.
pid and ppid    These keywords may be used as standalone
                statements in a process service entry to
                override the alert action for change of
                process pid and ppid.
uid and gid     These keywords are either 1) an optional part of
                a start, stop or exec statement. They may be
                used to specify a user id and a group id the
                program (process) should switch to upon start.
                This feature can only be used if the superuser
                is running monit. 2) uid and gid may also be
                used as standalone statements in a file service
                entry to test a file's uid and gid attributes.
host            The hostname or IP address to test the port
                at. This keyword can only be used together
                with a port statement or in the check host
                statement.
port            Specify a TCP/IP service port number which
                a process is listening on. This statement
                is also optional. If this statement is not
                prefixed with a host-statement, localhost is
                used as the hostname to test the port at.
type            Specifies the socket type monit should use when
                testing a connection to a port. If the type
                keyword is omitted, tcp is used. This keyword
                must be followed by either tcp, udp or tcpssl.
tcp             Specifies that monit should use a TCP
                socket type (stream) when testing a port.
tcpssl          Specifies that monit should use a TCP socket
                type (stream) and the secure socket layer (ssl)
                when testing a port connection.
udp             Specifies that monit should use a UDP socket
                type (datagram) when testing a port.
certmd5         The md5 sum of a certificate a ssl forged
                server has to deliver.
proto(col)      This keyword specifies the type of service
                found at the port. monit knows at the moment
                how to speak HTTP, SMTP, FTP, POP, IMAP, MYSQL,
                NNTP, SSH, DWP, LDAP2, LDAP3, RDATE, NTP3, DNS,
                POSTFIX-POLICY, APACHE-STATUS, TNS, PGSQL and
                RSYNC.
                You're welcome to write new protocol test
                modules. If no protocol is specified monit will
                use a default test which in most cases are good
                enough.
request         Specifies a server request and must come
                after the protocol keyword mentioned above.
                 - for http it can contain an URL and an
                   optional query string.
                 - other protocols does not support this
                   statement yet
send/expect     These keywords specify a generic protocol.
                Both require a string whether to be sent or
                to be matched against (as extended regex if
                supported).  Send/expect can not be used
                together with the proto(col) statement.
unix(socket)    Specifies a Unix socket file and used like
                the port statement above to test a Unix
                domain network socket connection.
URL             Specify an URL string which monit will use for
                connection testing.
content         Optional sub-statement for the URL statement.
                Specifies that monit should test the content
                returned by the server against a regular
                expression.
timeout x sec.  Define a network port connection timeout. Must
                be followed by a number in seconds and the
                keyword, seconds.
timeout         Define a service timeout. Must be followed by
                two digits. The first digit is max number of
                restarts for the service. The second digit
                is the cycle interval to test restarts.
                This statement is optional.
alert           Specifies an email address for notification
                if a service event occurs. Alert can also
                be postfixed, to only send a message for
                certain events. See the examples above. More
                than one alert statement is allowed in an
                entry. This statement is also optional.
noalert         Specifies an email address which don't want
                to receive alerts. This statement is also
                optional.
restart, stop   These keywords may be used as actions for
unmonitor,      various test statements. The exec statement is
start and       special in that it requires a following string
exec            specifying the program to be execute. You may
                also specify an UID and GID for the exec
                statement. The program executed will then run
                using the specified user id and group id.
mail-format     Specifies a mail format for an alert message
                This statement is an optional part of the
                alert statement.
checksum        Specify that monit should compute and monitor a
                file's md5/sha1 checksum. May only be used in a
                check file entry.
expect          Specifies a md5/sha1 checksum string monit
                should expect when testing the checksum. This
                statement is an optional part of the checksum
                statement.
timestamp       Specifies an expected timestamp for a file
                or directory. More than one timestamp statement
                are allowed. May only be used in a check file or
                check directory entry.
changed         Part of a timestamp statement and used as an
                operator to simply test for a timestamp change.
every           Validate this entry only at every n poll cycle.
                Useful in daemon mode when the cycle is short
                and a service takes some time to start.
mode            Must be followed either by the keyword active,
                passive or manual. If active, monit will restart
                the service if it is not running (this is the
                default behavior). If passive, monit will not
                (re)start the service if it is not running - it
                will only monitor and send alerts (resource
                related restart and stop options are ignored
                in this mode also). If manual, monit will enter
                active mode only if a service was started under
                monit's control otherwise the service isn't
                monitored.
cpu             Must be followed by a compare operator, a number
                with "%" and an action. This statement is used
                to check the cpu usage in percent of a process
                with its children over a number of cycles. If
                the compare expression matches then the
                specified action is executed.
mem             The equivalent to the cpu token for memory of a
                process (w/o children!).  This token must be
                followed by a compare operator a number with
                unit {B|KB|MB|GB|%|byte|kilobyte|megabyte|
                gigabyte|percent} and an action.
loadavg         Must be followed by [1min,5min,15min] in (), a
                compare operator, a number and an action. This
                statement is used to check the system load
                average over a number of cycles. If the compare
                expression matches then the specified action is
                executed.
children        This is the number of child processes spawn by a
                process. The syntax is the same as above.
totalmem        The equivalent of mem, except totalmem is an
                aggregation of memory, not only used by a
                process but also by all its child
                processes. The syntax is the same as above.
space           Must be followed by a compare operator, a
                number, unit {B|KB|MB|GB|%|byte|kilobyte|
                megabyte|gigabyte|percent} and an action.
inode(s)        Must be followed by a compare operator, integer
                number, optionally by percent sign (if not, the
                limit is absolute) and an action.
perm(ission)    Must be followed by an octal number describing
                the permissions.
size            Must be followed by a compare operator, a
                number, unit {B|KB|MB|GB|byte|kilobyte|
                megabyte|gigabyte} and an action.
depends (on)    Must be followed by the name of a service this
                service depends on.
Here's the complete list of reserved keywords used by monit:
if, then, else, set, daemon, logfile, syslog, address, httpd, ssl, enable, disable, pemfile, allow, read-only, check, init, count, pidfile, statefile, group, start, stop, uid, gid, connection, port(number), unix(socket), type, proto(col), tcp, tcpssl, udp, alert, noalert, mail-format, restart, timeout, checksum, resource, expect, send, mailserver, every, mode, active, passive, manual, depends, host, default, http, ftp, smtp, pop, ntp3, nntp, imap, clamav, ssh, dwp, ldap2, ldap3, tns, request, cpu, mem, totalmem, children, loadavg, timestamp, changed, second(s), minute(s), hour(s), day(s), space, inode, pid, ppid, perm(ission), icmp, process, file, directory, device, size, unmonitor, rdate, rsync, data, invalid, exec, nonexist, policy, reminder, instance, eventqueue, basedir, slot(s), system and failed

And here is a complete list of noise keywords ignored by monit:

is, as, are, on(ly), with(in), and, has, using, use, the, sum, program(s), than, for, usage, was, but.

Note: If the start or stop programs are shell scripts, then the script must begin with "#!" and the remainder of the first line must specify an interpreter for the program. E.g. "#!/bin/sh"

It's possible to write scripts directly into the start and stop entries by using a string of shell-commands. Like so:

start="/bin/bash -c 'echo $$ > pidfile; exec program'"
stop="/bin/bash -c 'kill -s SIGTERM 'cat pidfile''"
CONFIGURATION EXAMPLES
The simplest form is just the check statement. In this example we check to see if the server is running and log a message if not:

check process resin with pidfile /usr/local/resin/srun.pid
To have monit start the server if it's not running, add a start statement:
check process resin with pidfile /usr/local/resin/srun.pid
      start program = "/usr/local/resin/bin/srun.sh start"
Here's a more advanced example for monitoring an apache web-server listening on the default port number for HTTP and HTTPS . In this example monit will restart apache if it's not accepting connections at the port numbers. The method monit use for a process restart is to first execute the stop-program, wait for the process to stop and then execute the start-program. (If monit was unable to stop or start the service a failed alert message will be sent if you have requested alert messages to be sent).
check process apache with pidfile /var/run/httpd.pid
      start program = "/etc/init.d/httpd start"
      stop program  = "/etc/init.d/httpd stop"
      if failed port 80 then restart
      if failed port 443 with timeout 15 seconds then restart
This example demonstrate how you can run a program as a specified user (uid) and with a specified group (gid). Many daemon programs will do the uid and gid switch by them self, but for those programs that does not (e.g. Java programs), monit's ability to start a program as a certain user can be very useful. In this example we start the Tomcat Java Servlet Engine as the standard nobody user and group. Please note that monit will only switch uid and gid for a program if the super-user is running monit, otherwise monit will simply ignore the request to change uid and gid.
check process tomcat with pidfile /var/run/tomcat.pid
      start program = "/etc/init.d/tomcat start"
            as uid nobody and gid nobody
      stop program  = "/etc/init.d/tomcat stop"
            # You can also use id numbers instead and write:
            as uid 99 and with gid 99
      if failed port 8080 then alert
In this example we use udp for connection testing to check if the name-server is running and also use timeout and alert:
check process named with pidfile /var/run/named.pid
      start program = "/etc/init.d/named start"
      stop program  = "/etc/init.d/named stop"
      if failed port 53 use type udp protocol dns then restart
      if 3 restarts within 5 cycles then timeout
The following example illustrate how to check if the service 'sophie' is answering connections on its Unix domain socket:
check process sophie with pidfile /var/run/sophie.pid
      start program = "/etc/init.d/sophie start"
      stop  program = "/etc/init.d/sophie stop"
      if failed unix /var/run/sophie then restart
In this example we check an apache web-server running on localhost that answers for several IP-based virtual hosts or vhosts, hence the host statement before port:
check process apache with pidfile /var/run/httpd.pid
      start "/etc/init.d/httpd start"
      stop  "/etc/init.d/httpd stop"
      if failed host www.sol.no port 80 then alert
      if failed host shop.sol.no port 443 then alert
      if failed host chat.sol.no port 80 then alert
      if failed host www.tildeslash.com port 80 then alert
To make sure that monit is communicating with a http server a protocol test can be added:
check process apache with pidfile /var/run/httpd.pid
      start "/etc/init.d/httpd start"
      stop  "/etc/init.d/httpd stop"
      if failed host www.sol.no port 80
         protocol HTTP
         then alert
This example shows a different way to check a webserver using the send/expect mechanism:
check process apache with pidfile /var/run/httpd.pid
      start "/etc/init.d/httpd start"
      stop  "/etc/init.d/httpd stop"
      if failed host www.sol.no port 80
         send "GET / HTTP/1.0\r\nHost: www.sol.no\r\n\r\n"
         expect "HTTP/[0-9\.]{3} 200 .*\r\n"
         then alert
To make sure that Apache is logging successfully (i.e. no more than 60 percent of child servers are logging), use its mod_status page at www.sol.no/server-status with this special protocol test:
check process apache with pidfile /var/run/httpd.pid
      start "/etc/init.d/httpd start"
      stop  "/etc/init.d/httpd stop"
      if failed host www.sol.no port 80
      protocol apache-status loglimit > 60% then restart
This configuration can be used to alert you if 25 percent or more of Apache child processes are stuck performing DNS lookups:
check process apache with pidfile /var/run/httpd.pid
      start "/etc/init.d/httpd start"
      stop  "/etc/init.d/httpd stop"
      if failed host www.sol.no port 80
      protocol apache-status dnslimit > 25% then alert
Here we use an icmp ping test to check if a remote host is up and if not send an alert:
check host www.tildeslash.com with address www.tildeslash.com
      if failed icmp type echo count 5 with timeout 15 seconds
         then alert
In the following example we ask monit to compute and verify the checksum for the underlying apache binary used by the start and stop programs. If the the checksum test should fail, monitoring will be disabled to prevent possibly starting a compromised binary:
check process apache with pidfile /var/run/httpd.pid
      start program = "/etc/init.d/httpd start"
      stop program  = "/etc/init.d/httpd stop"
      if failed host www.tildeslash.com port 80 then restart
      depends on apache_bin

check file apache_bin with path /usr/local/apache/bin/httpd
      if failed checksum then unmonitor
In this example we ask monit to test the checksum for a document on a remote server. If the checksum was changed we send an alert:
check host tildeslash with address www.tildeslash.com
      if failed port 80 protocol http
         and request "/monit/dist/monit-4.0.tar.gz"
             with checksum f9d26b8393736b5dfad837bb13780786
      then alert
      alert hauk@tildeslash.com with mail-format {subject:
        Aaaalarm! }
Some servers are slow starters, like for example Java based Application Servers. So if we want to keep the poll-cycle low (i.e. < 60 seconds) but allow some services to take its time to start, the every statement is handy:
check process dynamo with pidfile /etc/dynamo.pid
      start program = "/etc/init.d/dynamo start"
      stop program  = "/etc/init.d/dynamo stop"
      if failed port 8840 then alert
      every 2 cycles
Here is an example where we group together two database entries so you can manage them together, e.g.; 'monit -g database start all'. The mode statement is also illustrated in the first entry and have the effect that monit will not try to (re)start this service if it is not running:
check process sybase with pidfile /var/run/sybase.pid
      start = "/etc/init.d/sybase start"
      stop  = "/etc/init.d/sybase stop"
      mode passive
      group database

check process oracle with pidfile /var/run/oracle.pid
      start program = "/etc/init.d/oracle start"
      stop program  = "/etc/init.d/oracle stop"
      mode active # Not necessary really, since it's the default
      if failed port 9001 then restart
      group database
Here is an example to show the usage of the resource checks. It will send an alert when the CPU usage of the http daemon and its child processes raises beyond 60% for over two cycles. Apache is restarted if the CPU usage is over 80% for five cycles or the memory usage over 100Mb for five cycles or if the machines load average is more than 10 for 8 cycles:
check process apache with pidfile /var/run/httpd.pid
      start program = "/etc/init.d/httpd start"
      stop program  = "/etc/init.d/httpd stop"
      if cpu > 60% for 2 cycles then alert
      if cpu > 80% for 5 cycles then restart
      if mem > 100 MB for 5 cycles then stop
      if loadavg(5min) greater than 10.0 for 8 cycles then stop
This examples demonstrate the timestamp statement with exec and how you may restart apache if its configuration file was changed.
check file httpd.conf with path /etc/httpd/httpd.conf
      if changed timestamp
         then exec "/etc/init.d/httpd graceful"
In this example we demonstrate usage of the extended alert statement and a file check dependency:
check process apache with pidfile /var/run/httpd.pid
     start = "/etc/init.d/httpd start"
     stop  = "/etc/init.d/httpd stop"
     if failed host www.tildeslash.com  port 80 then restart
     alert admin@bar on {nonexist, timeout}
       with mail-format {
             from:     bofh@$HOST
             subject:  apache $EVENT - $ACTION
             message:  This event occurred on $HOST at $DATE.
             Your faithful employee,
             monit
     }
     if 3 restarts within 5 cycles then timeout
     depend httpd_bin
     group apache

check file httpd_bin with path /usr/local/apache/bin/httpd
      if failed checksum
         and expect 8f7f419955cefa0b33a2ba316cba3659
             then unmonitor
      if failed permission 755 then unmonitor
      if failed uid root then unmonitor
      if failed gid root then unmonitor
      if changed timestamp then alert
      alert security@bar on {checksum, timestamp,
                             permission, uid, gid}
            with mail-format {subject: Alaaarrm! on $HOST}
      group apache
In this example, we demonstrate usage of the depend statement. In this case, we want to start oracle and apache. However, we've set up apache to use oracle as a back end, and if oracle is restarted, apache must be restarted as well.
check process apache with pidfile /var/run/httpd.pid
      start = "/etc/init.d/httpd start"
      stop  = "/etc/init.d/httpd stop"
      depends on oracle

check process oracle with pidfile /var/run/oracle.pid
      start = "/etc/init.d/oracle start"
      stop  = "/etc/init.d/oracle stop"
      if failed port 9001 then restart
Next, we have 2 services, oracle-import and oracle-export that need to be restarted if oracle is restarted, but are independent of each other.
check process oracle with pidfile /var/run/oracle.pid
      start = "/etc/init.d/oracle start"
      stop  = "/etc/init.d/oracle stop"
      if failed port 9001 then restart

check process oracle-import
     with pidfile /var/run/oracle-import.pid
      start = "/etc/init.d/oracle-import start"
      stop  = "/etc/init.d/oracle-import stop"
      depends on oracle

check process oracle-export
     with pidfile /var/run/oracle-export.pid
      start = "/etc/init.d/oracle-export start"
      stop  = "/etc/init.d/oracle-export stop"
      depends on oracle
Finally an example with all statements:
check process apache with pidfile /var/run/httpd.pid
      start program = "/etc/init.d/httpd start"
      stop program  = "/etc/init.d/httpd stop"
      if 3 restarts within 5 cycles then timeout
      if failed host www.sol.no  port 80 protocol http
         and use the request "/login.cgi"
             then alert
      if failed host shop.sol.no port 443 type tcpssl
         protocol http and with timeout 15 seconds
             then restart
      if cpu is greater than 60% for 2 cycles then alert
      if cpu > 80% for 5 cycles then restart
      if totalmem > 100 MB then stop
      if children > 200 then alert
      alert bofh@bar with mail-format {from: monit@foo.bar.no}
      every 2 cycles
      mode active
      depends on weblogic
      depends on httpd.pid
      depends on httpd.conf
      depends on httpd_bin
      depends on datafs
      group server

check file httpd.pid with path /usr/local/apache/logs/httpd.pid
      group server
      if timestamp > 7 days then restart
      every 2 cycles
      alert bofh@bar with mail-format {from: monit@foo.bar.no}
      depends on datafs

check file httpd.conf with path /etc/httpd/httpd.conf
      group server
      if timestamp was changed
         then exec "/usr/local/apache/bin/apachectl graceful"
      every 2 cycles
      alert bofh@bar with mail-format {from: monit@foo.bar.no}
      depends on datafs

check file httpd_bin with path /usr/local/apache/bin/httpd
      group server
      if failed checksum and expect the sum
         8f7f419955cefa0b33a2ba316cba3659 then unmonitor
      if failed permission 755 then unmonitor
      if failed uid root then unmonitor
      if failed gid root then unmonitor
      if changed size then alert
      if changed timestamp then alert
      every 2 cycles
      alert bofh@bar with mail-format {from: monit@foo.bar.no}
      alert foo@bar on { checksum, size, timestamp, uid, gid }
      depends on datafs

check device datafs with path /dev/sdb1
      group server
      start program  = "/bin/mount /data"
      stop program  =  "/bin/umount /data"
      if failed permission 660 then unmonitor
      if failed uid root then unmonitor
      if failed gid disk then unmonitor
      if space usage > 80 % then alert
      if space usage > 94 % then stop
      if inode usage > 80 % then alert
      if inode usage > 94 % then stop
      alert root@localhost

check host ftp.redhat.com with address ftp.redhat.com
      if failed icmp type echo with timeout 15 seconds
         then alert
      if failed port 21 protocol ftp
         then exec "/usr/X11R6/bin/xmessage -display
                    :0 ftp connection failed"
      alert foo@bar.com

check host www.gnu.org with address www.gnu.org
      if failed port 80 protocol http
         and request "/pub/gnu/bash/bash-2.05b.tar.gz"
             with checksum 8f7f419955cefa0b33a2ba316cba3659
      then alert
      alert rms@gnu.org with mail-format {
           subject: The gnu server may be hacked again! }
Note; only the check type, pidfile/path/address statements are mandatory, the other statements are optional and the order of the optional statements is not important.
Monit With Heartbeat

You can download heartbeat from http://www.linux-ha.org/download/. It might be useful to have a look at The Heartbeat Getting Started Guide at: http://www.linux-ha.org/GettingStarted.html

Starting up a Node

This is the normal start sequence for a cluster-node. With this sequence, there should be no error-case, which is not handled either by heartbeat or by monit. For example, if monit dies, initd restarts it. If heartbeat dies, monit restarts it. If the node dies, the heartbeat instance on the other node detects it and restart the services there.

|1. initd starts monit with group local
|2. monit starts heartbeat in local group
|3. heartbeat requests monit to start the node group
|4. monit starts the node group
Monit: /etc/monitrc
This example describes a cluster with 2 nodes. Services running on Node 1 are in the group node1 and Node 2 services are in the node2 group.

The local group entries are mode active, the node group entries are mode manual and controlled by heartbeat.

#
# local services on both hosts
#

check process heartbeat with pidfile /var/run/heartbeat.pid
      start program = "/etc/init.d/heartbeat start"
      stop  program = "/etc/init.d/heartbeat start"
      mode  active
      alert foo@bar
      group local

check process postfix with pidfile /var/run/postfix/master.pid
      start program = "/etc/init.d/postfix start"
      stop program  = "/etc/init.d/postfix stop"
      mode  active
      alert foo@bar
      group local

#
# node1 services
#

check process apache with pidfile /var/apache/logs/httpd.pid
      start program = "/etc/init.d/apache start"
      stop program  = "/etc/init.d/apache stop"
      depends named
      alert foo@bar
      mode  manual
      group node1

check process named with pidfile /var/tmp/named.pid
      start program = "/etc/init.d/named start"
      stop program  = "/etc/init.d/named stop"
      alert foo@bar
      mode  manual
      group node1

#
# node2 services
#

check process named-slave with pidfile /var/tmp/named-slave.pid
      start program = "/etc/init.d/named-slave start"
      stop program  = "/etc/init.d/named-slave stop"
      mode  manual
      alert foo@bar
      group node2

check process squid with pidfile /var/squid/logs/squid.pid
      start program = "/etc/init.d/squid start"
      stop program  = "/etc/init.d/squid stop"
      depends named-slave
      alert foo@bar
      mode  manual
      group node2
initd: /etc/inittab
Monit is started on both nodes with initd. You will need to add an entry in /etc/inittab to start monit with the same local group heartbeat is member of.

#/etc/inittab
mo:2345:respawn:/usr/local/bin/monit -d 10 -c /etc/monitrc -g local
heartbeat: /etc/ha.d/haresources
When heartbeat starts, heartbeat looks up the node entry and start the script /etc/init.d/monit-node1 or /etc/init.d/monit-node2. The script calls monit to start the specific group per node.

# /etc/ha.d/haresources
node1 IPaddr::172.16.100.1  monit-node1
node2 IPaddr::172.16.100.2  monit-node2
/etc/init.d/monit-node1
#!/bin/bash
#
# sample script for starting/stopping all services on node1
#
prog="/usr/local/bin/monit -g node1"
start()
{
      echo -n $"Starting $prog:"
      $prog start all
      echo
}

stop()
{
      echo -n $"Stopping $prog:"
      $prog stop all
      echo
}

case "$1" in
      start)
           start;;
      stop)
           stop;;
      *)
           echo $"Usage: $0 {start|stop}"
           RETVAL=1
esac
exit $RETVAL
Handling state
As mentioned elsewhere, monit save its state to a state file. If the monit process should die, upon restart monit will read its last known state from this file. This can be a problem if monit is used in a cluster, as illustrate in this scenario:

1
The active node fails, the second takes over

2

After a reboot, the failed node comes back, monit read its state file and start all the services (even manual ones) as they were running before the failure. This is a problem because services will now run on both nodes.

The solution to this problem is to remove the monit.state file in a rc-script called at boot time and before monit is started.
Files

~/.monitrc Default run control file

/etc/monitrc If the control file is not found in the default location and /etc contains a monitrc file, this file will be used instead.

./monitrc If the control file is not found in either of the previous two locations, and the current working directory contains a monitrc file, this file is used instead.

~/.monitrc.pid Lock file to help prevent concurrent runs (non-root mode).

/var/run/monit.pid Lock file to help prevent concurrent runs (root mode, Linux systems).

/etc/monit.pid Lock file to help prevent concurrent runs (root mode, systems without /var/run).

~/.monit.state monit save its state to this file and utilize information found in this file to recover from a crash. This is a binary file and its content is only of interest to monit. You may set the location of this file in the monit control file or by using the -s switch when monit is started.

Environment

No environment variables are used by monit. However, when monit execute a script or a program monit will set several environment variables which can be utilized by the executable. The following and only the following environment variables are available:

MONIT_EVENT
The event that occurred on the service
MONIT_SERVICE
The name of the service (from monitrc) on which the event occurred.
MONIT_DATE
The time and date (rfc 822 style) the event occurred
MONIT_HOST
The host the event occurred on
The following environment variables are only available for process service entries:
MONIT_PROCESS_PID
The process pid. This may be 0 if the process was (re)started,
MONIT_PROCESS_MEMORY
Process memory. This may be 0 if the process was (re)started,
MONIT_PROCESS_CHILDREN
Process children. This may be 0 if the process was (re)started,
MONIT_PROCESS_CPU_PERCENT
Process cpu%. This may be 0 if the process was (re)started,
In addition the following spartan PATH environment variable is available:
PATH=/bin:/usr/bin:/sbin:/usr/sbin
Scripts or programs that depends on other environment variables or on a more verbose PATH must provide means to set these variables by them self.
Signals

If a monit daemon is running, SIGUSR1 wakes it up from its sleep phase and forces a poll of all services. SIGTERM and SIGINT will gracefully terminate a monit daemon. The SIGTERM signal is sent to a monit daemon if monit is started with the quit action argument.

Sending a SIGHUP signal to a running monit daemon will force the daemon to reinitialize itself, specifically it will reread configuration, close and reopen log files.

Running monit in foreground while a background monit daemon is running will wake up the daemon.

Notes

This is a very silent program. Use the -v switch if you want to see what monit is doing, and tail -f the logfile. Optionally for testing purposes; you can start monit with the -Iv switch. Monit will then print debug information to the console, to stop monit in this mode, simply press CTRL^C (i.e. SIGINT ) in the same console.

The syntax (and parser) of the control file is inspired by Eric S. Raymond et al. excellent fetchmail program. Some portions of this man page does also receive inspiration from the same authors.

Authors

Jan-Henrik Haukeland <hauk@tildeslash.com>, Martin Pala <martinp@tildeslash.com>, Christian Hopp <chopp@iei.tu-clausthal.de>, Rory Toma <rory@digeo.com>

See also http://www.tildeslash.com/monit/who.html

Copyright

Copyright (C) 2000-2007 by the monit project group. All Rights Reserved. This product is distributed in the hope that it will be useful, but WITHOUT any warranty; without even the implied warranty of MERCHANTABILITY or FITNESS for a particular purpose.

See Also

GNU text utilities; md5sum(1); sha1sum(1); openssl(1); glob(7); regex(7)

	21.4
22. FTP

	22.1 In Unix, how can I issue batches of non-interactive FTP commands?

In Unix, you can use the ftp command in combination with a brief shell script to automate an FTP session. For example, if your email address were dvader@indiana.edu and you wanted to retrieve a listing of a directory named /pub/docs/plans on a host named deathstar.empire.org, you could use the following script:

#!/bin/sh
ftp -n deathstar.org <<EOT | mailx -s "Your Listing" dvader@indiana.edu
user anonymous dvader@indiana.edu
cd /pub/docs/plans
dir
quit
EOT
The first line indicates the file is a script. The second line invokes the ftp command and directs the output of the session to the mailx command. If mailx isn't on your system, try using mail or mhmail instead. The third line lists the login and password for an anonymous FTP connection. The following two lines contain the ftp commands that the script will execute on the remote host; you could substitute any valid ftp commands of your own before the word quit. Finally, once the commands have been executed, the output will be mailed in a message to dvader@indiana.edu with the subject Your Listing, as specified in the second line of the script.

To use the script, enter:

  sh script_name
Replace script_name with the name of the file containing the text of the script. If you would like to run the script in the background so that you don't have to wait for it to finish to do other work, enter:

  sh script_name &
You will receive an error message if, in your script, you refer to directories or files that don't actually exist. In the example here, we assumed that dvader already knew that the directory named /pub/docs/plans existed on the remote host.

	22.2 my aliases to automate FTP access
#utility for getting files from ftp server
#expects 3 args, ftp, dir name, file name, assumes anonymous user
_ftp_get(){
ftp -un $1 <<EOT
user anonymous  foo
bin
cd $2
get $3
quit
EOT
}


#utility for connecting to ftp server and listing a certain dir
#expects 2 args, ftp, and dir name.user name is assumed to be anonymous  
_ftp_listing(){
ftp -un $1 <<EOT
user anonymous  foo
cd $2
dir
quit
EOT
}

alias acs_ftp_ls="_ftp_listing cd-acs-ftp /upload/yizaq/"
alias acs_ftp_get="echo please make sure you specified file to get!; _ftp_get cd-acs-ftp /upload/yizaq/"

	22.3 Kermit FTP client

FTP Script Writing - How Do I Automate an FTP Session?

Most recent update: Fri Sep 10 17:52:20 2010
UNIX   WINDOWS   FTP CLIENT DOCUMENTATION   KERMIT SCRIPT LIBRARY
CLICK HERE to read about new FTP features in C-Kermit 8.0.206.
FTP SCRIPT : FTP AUTOMATION : AUTOMATE FTP : BATCH FTP : PROGRAMMABLE FTP : UNATTENDED FTP

This page is written for users of Unix operating systems -- Linux, Mac OS X, FreeBSD, AIX, HP-UX, IRIX, Solaris, etc. The Kermit FTP client is also available in Kermit 95 2.0 for Windows 9x/ME/NT/2000/XP, for which some of the applications, examples, and terminology used here might need minor adjustments (e.g. directory path syntax).
Also see: Accessing IBM Information Exchange with Kermit for a discussion of making securely authenticated and encrypted FTP connections.

Hardly a day goes by without an FTP automation question appearing in the newsgroups. Until now, the stock answers (for Unix) have been as follows (the options for Windows are sparse indeed):
Pipe commands into FTP's standard input. This works great when it works, but doesn't allow for synchronization, error handling, decision making, and so on, and can get into an awful mess when something goes wrong. For example, some FTP clients do special tricks with the password that tend to thwart piping of standard input or "here" documents into them. Also, the exact syntax of your procedure depends on which shell (sh, ksh, csh, bash, etc) you are using. Also, your password can be visible to other people through "ps" or "w" listings.
Put the commands to be executed into the .netrc file in your login directory in the form of a macro definition. Except for avoiding shell syntax differences, this is not much different than the first option, since FTP commands don't have any capability for error detection, decision making, conditional execution, etc. Note that the .netrc file can also be used to store host access information (your username and password on each host). It's a glaring security risk to have this well-known file on your disk; anybody who gains access to your .netrc also gains access to all the hosts listed in it.
Use Expect to feed commands to the FTP prompt. This improves the situation with synchronization, but:
It's cumbersome and error-prone, since it relies on the specific messages and prompts of each FTP client and server, which vary, rather than the FTP protocol itself, which is well-defined. Expect scripts break whenever the client or server prompts or text messages change, or if the messages come out in different languages.
You're still stuck with same dumb old FTP client and its limited range of function.
Use FTP libraries available for Perl, Tcl, C, etc. This might give direct programmatic access to the FTP protocol, but still offers limited functionality unless you program it yourself at a relatively low and detailed level.
Now there's a new alternative. The latest generation of Kermit software:

C-Kermit 8.0 (or later)
Kermit 95 2.0 (or later)
These programs include their own built-in FTP client, allowing FTP sessions to be automated using the same cross-platform scripting language we've been using for serial-port, modem, Telnet, and X.25 connections since the 1980s, in its advanced modern form:

http://www.columbia.edu/kermit/ck80specs.html#scripts
and has loads of features that you won't find in the regular UNIX FTP client:

http://www.columbia.edu/kermit/ftpclient.html
Here's a brief tutorial on writing C-Kermit FTP scripts. But the commands presented below are not just for scripts. You can also use them interactively, just as you would give commands to the regular UNIX or Windows FTP client, except that with Kermit you also get built-in help, context-sensitive help (if you type "?"), command recall, keyword and filename menus and completion, keyword abbreviation, and command shortcuts and macros.

Also see:

Kermit Scripting Tutorial and Library
Kermit FTP Client FAQ
EXAMPLE 1: SIMPLE ANONYMOUS DOWNLOAD

Let's begin with a simple anonymous connection to the FTP server at xyzcorp.com to download a file from the public drivers directory. The following table shows how to do this with a regular FTP client and with Kermit; the parts you type are underlined; the other parts are prompts from the shell or application:
Traditional FTP Client	Kermit FTP Client
$ ftp	$ kermit
ftp> open ftp.xyzcorp.com	C-Kermit> ftp open ftp.xyzcorp.com  
Name: anonymous	Name: anonymous
Password: user@somehost.com  	Password: user@somehost.com
ftp> cd drivers	C-Kermit> cd drivers
ftp> binary	C-Kermit> binary
ftp> get newdrivers.zip  	C-Kermit> get newdrivers.zip
ftp> bye	C-Kermit> bye
As you can see, the procedures are practically identical. The main difference is that Kermit, since it can make many kinds of connections, must be told which kind to make ("ftp open"), whereas since FTP makes only one kind, it simply opens the connection the only way it knows how. Note, however, that any error handling in the procedures above is done strictly by the user. If an error message appears, the user reads it and decides how to respond. Other differences include:

Kermit makes the connection in passive mode by default.
On multiple-file downloads, Kermit does not prompt you for each file.
Kermit does not need to be given BINARY or ASCII commands; it switches for each file automatically.
Kermit has many functional advantages, listed HERE.
To make Kermit execute these commands automatically, just put them into a file:

ftp open ftp.xyzcorp.com /anonymous
cd drivers
binary
get newdrivers.zip
bye
and then tell Kermit to execute the file, which can be done in any number of ways (use the TAKE command at the C-Kermit> prompt; give the filename as the first command-line argument; or execute the file directly, like a shell script, as explained below). The first command (FTP OPEN) includes an "/anonymous" switch, which tells Kermit to log you in anonymously, automatically supplying your e-mail address as the password, thus bypassing the prompts. Executing this file is just like the interactive procedure but without engagement of your brain for decision making in case of errors.

Note, by the way, that there are simpler ways to accomplish the same task (download a single file anonymously), e.g. by giving Kermit the URL of the file on its command line:

kermit ftp://ftp.xyzcorp.com/drivers/newdrivers.zip
Now let's write the same procedure as a Kermit script, in which we illustrate some of Kermit's capabilities for detecting and reacting to errors:

#!/usr/local/bin/kermit +
ftp open ftp.xyzcorp.com /anonymous
if fail exit 1 Connection failed
if not \v(ftp_loggedin) exit 1 Login failed
ftp cd drivers
if fail exit 1 ftp cd drivers: \v(ftp_message)
cd ~/download
if fail exit 1 cd ~/download: \v(errstring)
ftp get /binary newdrivers.zip
if fail exit 1 ftp get newdrivers.zip: \v(ftp_message)
ftp bye
exit
Here's a brief explanation, line by line:

#!/usr/local/bin/kermit +
This is the "kerbang" line. If you want to run the script "directly" (as you would a shell script), this must be the first line in the script. The Kerbang line specifies the script interpreter to be Kermit rather than the shell. Substitute the appropriate C-Kermit 8.0 path name on your computer, and be sure to give the script file execute permission. For details see:
http://www.columbia.edu/kermit/ckscripts.html
ftp open ftp.xyzcorp.com /anonymous
This command attempts to open a connection to the FTP server on the computer whose IP hostname is ftp.xyzcorp.com. If the connection is successful, the command attempts to log in as user "anonymous", using your username and hostname (in email-address format) as the password.
if fail exit 1 Connection failed
This checks whether the connection was made. If not, it prints an error message and exits with a status of 1, indicating failure. Here's where methods 1 and 2 above are lacking: if the connection fails, they'll just go ahead and try to execute the rest of the commands anyway.
if not \v(ftp_loggedin) exit 1 Login failed
At this point we know we have a connection. This command checks whether login was successful by querying the value of one of Kermit's built-in FTP status variables, \v(ftp_loggedin). If login failed, the script prints "Login failed" and exits with a failure code of 1.
ftp cd drivers
This command asks the FTP server to change its directory to "drivers". Note that all commands for the FTP server begin with the word "ftp". That's because Kermit also has similar commands to be executed on the local computer, and other similar commands to be executed by a Kermit (not FTP) server.
if fail exit 1 ftp cd drivers: \v(ftp_message)
Here we check the results of the FTP CD command. If it failed, a message such as "ftp cd drivers: No such file or directory" is printed, indicating the failing command and the FTP server's failure reason, and the script exits with a failure code. If we did not check for failure here, then the later download would fail or (worse) we might download the wrong file. Kermit's built-in \v(ftp_message) variable contains the most recent message from the FTP server. Note that when an FTP connection is active, Kermit's EXIT command also sends a BYE command to the FTP server and closes the connection.
lcd ~/download
This command tells Kermit to change its own local directory to the download subdirectory of your login directory. LCD means "Local CD".
if fail exit 1 cd ~/download: \v(errstring)
Here we check the local CD command in case it failed, for example because the directory could not be found. In this case, the local error message is printed rather than the FTP server's message (in UNIX, \v(errstring) is the error message that corresponds to the current value of errno).
ftp get /binary newdrivers.zip
Now that both client and server are in the desired directories, we ask the server to send us the newdrivers.zip file in binary mode.
if fail exit 1 ftp get newdrivers.zip: \v(ftp_message)
We check for failure in the normal manner.
ftp bye
The file was received successfully. We log out and disconnect from the server...
exit 0
And exit successfully (status code 0) from the script. If you don't include an EXIT command in the script, it won't exit; instead, it will issue C-Kermit's interactive command prompt and wait for you to type a command. The "0" is optional; Kermit's default exit status code is 0.
Let's say our script has been saved in a file called getnewdrivers. How to execute it? There are at least three ways:

At the C-Kermit> prompt, type "take getnewdrivers" (assuming the script is in C-Kermit's current directory).
At the shell prompt, type "kermit getnewdrivers" (assuming the script is in the current directory, and "kermit" is C-Kermit 8.0, and it is in your PATH).
At the shell prompt, type "getnewdrivers" (assuming the getnewdrivers script file is stored in a directory that is in your UNIX PATH; if it's not, type the full pathname).
In cases 1 and 2, the Kerbang line is not needed. Method 3 requires the getnewdrivers file to be in your PATH and that the Kerbang line of the script indicates the pathname of the C-Kermit 8.0 executable, and that the script file has execute permission:

  chmod +x getnewdrivers
EXAMPLE 2: LOGGING IN AS A REAL USER

Here we modify our script to log in as a real user:
  ftp open ftp.xyzcorp.com /user:olga /password:bigsecret
The rest of the script is the same, except perhaps now a full pathname is needed in the FTP CD command.

But it's a notoriously bad idea to put passwords in scripts or any other files (if this is news to you, please take it on faith). So how can the script log in as a real user without knowing the password in advance? There are lots of ways. The first is simply to have the script prompt for the password when it runs:

  ftp open ftp.xyzcorp.com /user:olga
Just omit the /PASSWORD switch from the FTP OPEN command and Kermit prompts you for the password at the time it's needed (if it is), and you can supply it from your keyboard (it won't echo).

Alternatively, you can have Kermit prompt you for the password in advance. This might be appropriate when it's a long-running script and the FTP step doesn't happen until much later:

  undefine \%p
  while not defined \%p {
      askq \%p Password:
  }
  ....
  ftp open ftp.xyzcorp.com /user:olga /password:\%p
  if fail exit 1 Connection failed
  undefine \%p ; Erase password from memory
Here you see some "programming": variables and loops. We ask the user to type in the password using Kermit's ASKQ command ("ask quietly", i.e. don't echo the response). Since a password is required, the WHILE loop makes Kermit keep asking until it gets one, at which time it is assigned to the variable \%p. Then when the FTP OPEN command is given, \%p is specified as the password. Since \%p is a variable, it is replaced by its definition, which is whatever the user typed. (Normally, everything in Kermit that starts with a backslash indicates some kind of replacement -- a variable, a function call, the numeric representation of a character, etc.)

As a security precaution, the second "undefine \%p" command erases the password from memory immediately after it is used, like the comment says (trailing comments in Kermit are set off from the command by a semicolon (;) or number-sign (#) surrounded by whitespace).

Now suppose you want your script to run unattended when nobody is there to type in the password. This is a classic problem. One solution is to start the script early, type the password, and then have the script wait until the the desired time to do its work, using Kermit's SLEEP command, e.g.:

  sleep 6000     ; Sleep 6000 seconds
  sleep 23:59:59 ; Sleep until just before midnight
But what if you want the script to run periodically as a cron job, in which case there isn't even a terminal at which to type in the password? Well, that's a tough one, and it's one of the reasons for the appearance of secure FTP servers and Kermit's features for taking advantage of them. But that's another story, covered elsewhere:

  http://www.columbia.edu/kermit/security.html
For example, if your version of C-Kermit was built with SSL/TLS security, and the server also supports SSL/TLS security, it is negotiated automatically. Various special commands can be used, but the only one that's required is SET AUTHENTICATION TLS VERIFY-FILE filename, that tells Kermit where to find the certificate file to be used to authenticate the FTP server.

EXAMPLE 3: PASSING PARAMETERS FROM THE COMMAND LINE

Let's generalize our little script to accept a host and a filename from the command line.
  #!/usr/local/bin/kermit +
  if < \v(argc) 3 exit 1 Usage: \%0 host file
  ftp open \%1 /anonymous
  if fail exit 1 \%1: Connection failed
  if not \v(ftp_loggedin) exit 1 Login failed
  lcd ~/download
  if fail exit 1 cd ~/download: \v(errstring)
  ftp get /binary \%2:
  if fail exit 1 ftp get \%2: \v(ftp_message)
  ftp bye
  exit
Here we have simply replaced the host and file names by variables, \%1 and \%2, whose values are set automatically by Kermit from the command-line arguments. These are similar to the $1 and $2 Shell variables.

Let's call this version of the script getfile, since it's not just getting new drivers any more; you can use it to get any file from any host that accepts anonymous logins. \v(argc) is a built-in variable that says how many "arguments" there were on the command line, including the name of the script itself.

Assuming getfile is installed as a Kerbang script in your PATH, now you can give commands such as these at the shell prompt (or in a shell script):

  getfile support.scsicorp.com drivers/scsidrivers.zip
  getfile kermit.columbia.edu kermit/archives/ckermit.tar.gz
If you run getfile without supplying the parameters it needs (host name and file name), it prints a usage message and exits with a failure code.

The command line arguments are passed to the script as:

  \%0   The name of the script
  \%1   The first command-line argument
  \%2   The second command-line argument

Of course you can have more than 2 command-line arguments.

EXAMPLE 4: SUPPLYING DEFAULT PARAMETERS

If you don't want your script to fail if you pass it insufficient parameters on the command line, you can have it supply defaults for each missing parameter. There are two ways to do this; silently supply hardwired defaults, or prompt for missing parameters. The first way:
  if not defined \%1 define \%1 ftp.xyzcorp.com
  if not defined \%2 define \%2 newdrivers.zip
The second way:

  while not defined \%1 {
      ask \%1 { Host: }
  }
  while not defined \%2 {
      ask \%2 { File: }
  }
Or a combination:

  if not defined \%1 ask \%1 { Host [ftp.xyzcorp.com]: }
  if not defined \%1 define \%1 ftp.xyzcorp.com
  if not defined \%2 ask \%2 { File [newdrivers.zip]: }
  if not defined \%2 define \%2 newdrivers.zip
EXAMPLE 5: TRANSACTION PROCESSING CASE STUDY

Now that you know the basics of Kermit FTP scripting, let's look at a real-life application. Transaction processing refers to the communication from one computer to another of reservations, orders, votes, or other actions that have real (e.g. financial or political) consequences, usually from a large number of client computers to a central computer. Each transaction should take place exactly once -- not zero times, not two or more times -- otherwise a customer could be double-billed, or might not receive merchandise that was ordered; or votes might go uncounted or be counted multiple times; or a needed rental auto might not be waiting at the airport -- or six of them might be waiting!
A simple form of transaction processing is done by moving files from one computer to another, for example insurance claims from a pharmacy or doctor's office (the client site) to an insurance clearinghouse (the central site). A "watcher" process at the central site waits for files to appear in a certain directory and then processes them. In this case we want to make sure that each file is transferred completely and correctly, and exactly once, and furthermore:

That the central-site process does not begin to process a file before it has fully arrived; and:
That one client can't overwrite another client's transactions.
In a Kermit protocol client/server setting, all of this is handled quite nicely by Kermit's "atomic file movement" features. Unfortunately, not all of these features are available in FTP protocol, most notably a way to tell the FTP server to move or rename each incoming file automatically after it has fully arrived. However, we can accomplish the same thing with a Kermit FTP client script.

In this scenario, each client site has its own login ID on the central site to prevent file collisions between different clients, and also to provide an authenticated association between the uploaded files and the clients themselves. Each client ID at the central site has two subdirectories, working and ready. Client files (orders, votes, reservations, insurance claims, whatever) are uploaded to the working directory and then moved to the ready directory when the upload is complete. The move is "atomic" -- when the file appears in the ready directory, it appears all at once, not bit by bit; thus it is truly ready for processing the instant it is visible. The central-site "watcher" process periodically looks for files to appear in each client's ready directory, and when one does appear, moves it again, this time to its own area, and processes it. Thus any files in the client's ready directory are waiting to be processed and should not be disturbed. It is the client's responsibility to ensure that each file is sent completely, and sent only once, and that it is not disturbed after it is sent. It is the central site's responsibility to move files out of the ready directory and process them.

Our script expects the name of the file to send as its first command-line argument. We begin our script by checking the argument:

  #!/usr/local/bin/kermit +
  if not defined \%1 exit 1 Usage: \%0 filename
  .filename := \fcontents(\%1)
  .nameonly := \fbasename(\m(filename))
  if not exist \m(filename) exit 1 \m(filename): File not found
  if not readable \m(filename) exit 1 \m(filename): File not readable
Script and macro formal parameters (\%1, \%2, ...) are evaluated recursively, so that if their definitions contain variables, these are evaluated too, as many levels deep as variables are found. Since Kermit variables and other replacement quantities start with backslash (\) this introduces an unfortunate conflict with DOS/Windows pathnames. Assigning the contents of \%1 variable to a macro ("filename") forces one-level deep, rather than recursive, evaluation, and this allows our script to work with DOS or Windows file specifications as well as Unix ones. (For C-Kermit 9.0, see this.)

filename is the local name of the file to be sent, which can include a path -- i.e. it doesn't necessarily have to be in Kermit's current directory. nameonly is the name of the same file, but without the path. We use this to refer to the file's name on the server. The \fbasename() function strips any directory path from the filename, in case one was given (since the path is also stripped when sending the file's name to the FTP server).

Now we make the connection in the usual way:

  undefine \%p
  while not defined \%p {
      askq \%p Password:
  }
  ftp open centralsite.com /user:clientid /password:\%p
  if fail exit 1 Connection failed
  if not \v(ftp_loggedin) exit 1 Login failed
  undefine \%p
  ftp cd working
  if fail exit 1 ftp cd working: \v(ftp_message)
  lcd ~/upload
  if fail exit 1 lcd ~/upload: \v(errstring)
Now we upload the file:

  ftp put /delete \m(filename)
  if fail exit 1 ftp put \m(filename): \v(ftp_message)
Notice that failure leaves the partial file (if any) in the working directory, where the central-site watcher process does not look for it. Thus transient failures do no harm. The script can be run again later. The /DELETE switch on the PUT command removes the source file after, and only if, it was uploaded successfully; this prevents it from being uploaded again (you could also have it moved or renamed). This way, even if the script is run again for the same file, it will fail immediately because the file is no longer there. Or, if a file of the same name is in the same place, it is a new file that should be uploaded.

Now we can move the uploaded file from the server's working directory to its ready directory (the syntax assumes a UNIX-like file system on server):

  ftp rename \m(nameonly) ../ready/\m(nameonly)
  if fail exit 1 ftp rename \m(nameonly): \v(ftp_message)
But wait, what if the destination file already exists in the server's ready directory? This would indicate that a previous transaction with the same name had not yet been processed. We should allow for this possibility:

FTP CHECK filename
Succeeds if the file exists, fails if the file doesn't exist.
Here is the final version of our script:

  #!/usr/local/bin/kermit +

  ; Verify command-line parameter (name of file to send)
  ;
  if not defined \%1 exit 1 Usage: \%0 filename
  .filename := \fcontents(\%1)
  .nameonly := \fbasename(\m(filename))

  if not exist \m(filename) exit 1 \m(filename): File not found
  if not readable \m(filename) exit 1 \m(filename): File not readable

  ; Prompt for server password (OR USE SECURE FTP IF AVAILABLE!)
  ;
  undefine \%p
  while not defined \%p {
      askq \%p Password:
  }
  ; Open the connection and log in
  ;
  ftp open centralsite.com /user:clientid /password:\%p
  if fail exit 1 Connection failed
  if not \v(ftp_loggedin) exit 1 Login failed
  undefine \%p

  ; Check if file of same name already exists on the server
  ;
  ftp cd ready
  if fail exit 1 ftp cd ready: \v(ftp_message)
  lcd ~/upload
  if fail exit 1 lcd ~/upload: \v(errstring)
  ftp check \m(nameonly)
  if success exit 1 \m(nameonly): Already exists in server ready directory.

  ; OK to send - cd to server's working directory.
  ;
  ftp cdup
  if fail exit 1 ftp cdup: \v(ftp_message)
  ftp cd working
  if fail exit 1 ftp cd working: \v(ftp_message)

  ; Now we upload the file and delete the local copy if successful.
  ;
  ftp put /delete \m(filename)
  if fail exit 1 ftp put \m(filename): \v(ftp_message)

  ; Move the uploaded copy to the ready directory
  ;
  ftp rename \m(nameonly) ../ready/\m(nameonly)
  if fail exit 1 ftp rename \m(nameonly): \v(ftp_message)

  bye
  exit 0
Call the script file upload, make sure the Kerbang line indicates the C-Kermit 8.0 path, give it execute permission, and then run it from the shell prompt as:

  $ upload claim01.dat
If it didn't succeed, the error message will tell you why and you can take corrective action and run it again. If you run it again without taking corrective action, no harm is done -- either it will work or it will fail. If it works and you run it again on the same file, it will fail harmlessly because the original file is gone.

EXAMPLE 6: TRANSACTION PROCESSING - MULTIPLE FILES

The previous example showed how to upload a single file in a transaction processing environment. Let's generalize this to allow sending multiple files, assuming the same Working/Ready directory layout.
C-Kermit 8.0 also includes GET and PUT options (switches) to rename server files after successful transfer, whose use could shorten our transaction processing script, and are especially useful when transferring multiple files in a single operation: [M]GET or [M]PUT /SERVER-RENAME:template. Here is the previous script modified to accept a wildcard or directory name as \%1:

  #!/usr/local/bin/kermit +

  ; Verify command-line parameter - source file(s) or directory
  ;
  if not defined \%1 exit 1 Usage: \%0 filespec or directory name
  if defined \%2 {
      echo "Fatal - Multiple arguments not supported."
      echo " You may give a single argument that is a filename,"
      echo " or a wildcard to match multiple files, or the name"
      echo " of a directory.  If you give a directory name, all"
      echo " files will be sent from that directory."
      exit 1
  }
  .filespec := \fcontents(\%1)
  if directory \m(filespec) {
      lcd \m(filespec)
      if fail exit 1 - LCD \m(filespec) failed
      .filespec := *
  }
  if not \ffiles(\m(filespec)) exit 1 \m(filespec): No files match

  ; Prompt for server password (OR USE SECURE FTP IF AVAILABLE!)
  ;
  undefine \%p
  while not defined \%p {
      askq \%p Password:
  }
  ; Open the connection and log in
  ;
  ftp open centralsite.com /user:clientid /password:\%p
  if fail exit 1 Connection failed
  if not \v(ftp_loggedin) exit 1 Login failed
  undefine \%p

  ; Make sure Ready directory is empty
  ;
  ftp check ready/*
  if success exit 1 Ready directory is not empty

  ; OK to send - cd to server's working directory.
  ;
  ftp cd working
  if fail exit 1 ftp cd working: \v(ftp_message)

  ; Now we upload the files, deleting each local copy and moving each
  ; uploaded copy when successful.
  ;
  ftp mput /delete /server-rename:../ready/\v(filename) \m(filespec)
  if fail exit 1 ftp mput \m(filespec): \v(ftp_message)

  bye
  exit 0
As written, the script accepts a single argument, which can be a filename, a wildcard to denote a group of files (which will need to be quoted if you invoke this script from the shell), or a directory name (in which case the the script will CD to the directory and the upload all the files from it). Of course the script could be modified to accept a list of arguments and/or various options.

All the work is done by the FTP MPUT command. The /DELETE switch says to delete each file that is sent successfully, and the /SERVER-RENAME: switch says to rename the file into the ../ready directory as soon as it is fully received. \v(filename) is a built-in variable for use in file-group transfers that contains the name of the current file at the time it is being processed; it for use with switches such as /SERVER-RENAME that rename each file in a group on the fly.

In this case, we require that the ../ready directory be empty, since FTP MPUT does not have a way to avoid renaming collisions a per-file basis. If there is any interest in such a feature, it can be added in a future release. In the meantime, per-file checking can be accomplished with a loop.

EXAMPLE 7: AUTOMATING SECURE FTP SESSIONS

Often when making secure connections, you are prompted interactively for certain information or permission to proceed. These prompts can stop an automated procedure. To avoid them, you must give the appropriate commands to disable them, and/or supply the prompted-for information beforehand. Here are a few hints:
Make sure that SET TAKE ERROR and SET MACRO ERROR are both OFF. This is the default, but in case you have set either one of these ON in your script or initialization file, this makes the script halt on any kind of error. Normally you would want to check each operation for success or failure and take appropriate action.
On SSL and TLS connections, you may be asked whether it is OK to proceed with a connection to server that presents a self-signed certificate. You can use the SET AUTHENTICATION SSL (or TLS) VERIFY or SET AUTH SSL (or TLS) CERTS-OK commands to avoid this prompt by not requesting a certificate from the peer.
(More to be added...)
WHERE TO GO FROM HERE

The next step is to explore what other features are available. Kermit's feature set is rich, far beyond what you'd find in the typical FTP client. Suppose, for example, you want to upload all the files that are less than five days old, which might be any mixture of text and binary files, from a certain directory. Once the connection is made and desired directories are selected on the client and server, the command is surprisingly simple:
  ftp put /after:-5days *
Notice there is nothing about text or binary mode in the command. That's because Kermit automatically switches into the appropriate mode for each file that it sends.

Or suppose you want to send all the files that are larger than one million bytes and whose names start with 'c' or 'w' except if the file's name is core or its name ends with .log:

  ftp put /except:{{core}{*.log}} /larger:1000000 [cw]*
Or suppose you want to send all the files in an entire directory tree, which can include any combination of text and binary files, and have the same directory tree replicated on the FTP server, even if it is on a different operating system:

  ftp put /recursive *
Now suppose that later, you want to refresh the same directory tree by uploading only those files that changed since last time:

  ftp put /recursive /update *
Suppose you want to send a text file written in (say) German to another computer that uses a different character set:

  ftp put /local-character-set:cp437 /server-character-set:latin1 Grüße.txt
Or suppose you want to continue uploading a very long file after a previous upload attempt was interrupted in the middle:

  ftp put /recover verylong.tar.gz
Or suppose you want to synchronize a local directory from a remote one, even when you keep getting cut off, no matter how many tries it takes, without transferring any file that does not need to be updated, without transferring any file more than once, and without retransmitting any part of a file that was already partially received:

mkdir somelocaldirectory
cd somelocaldirectory

while true {
    ftp open foo.bar.com /user:myname /password:secret
    if fail exit 1 Can't reach host
    if not \v(ftp_loggedin) exit 1 FTP login failed
    ftp cd blah/blah/somepath
    if fail exit 1 Directory change failed
    while true {
	ftp get /recover /update *      
	if success goto done
	if not \v(ftp_connected) break
    }
    ftp bye  
}
:done
Or suppose you want to . . .

Keep a record of your file transfers.
Keep incoming files from overwriting existing files of the same name.
Pass files through standard i/o filters as part of the transfer process.
Upload a file with its permissions intact.
Rename a group of files in a server directory.
All of this, and lots more, is easy to do with the Kermit FTP client, and it all can be automated.

*/
	22.4 Expect script to automate ftp upload
 
[Log in to get rid of this advertisement]
#!/usr/bin/expect
spawn ftp ftp.remotedomain.com
expect "Name (ftp.remotedomain:root):"
send "username\r" //provide the username here
expect "Password:"
send "password\r" //provide the password here
expect "ftp>"
send "lcd /var/log/httpd\r" //any desired location
expect "ftp>"
send "put *.acslog\r" //any file
expect "ftp>"
send "bye\r"
#interact

Hope the script is self explanatory. You need to have "expect" rpm installed on the server and you can run it as
#expect scriptname
or you can put the above command in a file and can make it a bash executable.

With little modifications you can use this script for automating ssh logging also.
If anyone want help on that just post it here I will post taht code also

	22.5
23. grep , tags: grep

	23.1 Searching Files Using UNIX grep
The grep program is a standard UNIX utility that searches through a set of files for an arbitrary text pattern, specified through a regular expression. Also check the man pages as well for egrep and fgrep. The MPE equivalents are MPEX and Magnet, both third-party products. By default, grep is case-sensitive (use -i to ignore case). By default, grep ignores the context of a string (use -w to match words only). By default, grep shows the lines that match (use -v to show those that don't match).


% grep BOB tmpfile	{search 'tmpfile' for 'BOB' anywhere in a line}
% grep -i -w blkptr * 	{search files in CWD for word blkptr, any case}
% grep run[- ]time *.txt	{find 'run time' or 'run-time' in all txt files}
% who | grep root 	{pipe who to grep, look for root}

Understanding Regular Expressions
Regular Expressions are a feature of UNIX. They describe a pattern to match, a sequence of characters, not words, within a line of text. Here is a quick summary of the special characters used in the grep tool and their meaning:

^ (Caret)	=	match expression at the start of a line, as in ^A.
$ (Question)	=	match expression at the end of a line, as in A$.
\ (Back Slash)	=	turn off the special meaning of the next character, as in \^.
[ ] (Brackets)	=	match any one of the enclosed characters, as in [aeiou]. Use Hyphen "-" for a range, as in [0-9].
[^ ]	=	match any one character except those enclosed in [ ], as in [^0-9].
. (Period)	=	match a single character of any value, except end of line.
* (Asterisk)	=	match zero or more of the preceding character or expression.
\{x,y\}	=	match x to y occurrences of the preceding.
\{x\}	=	match exactly x occurrences of the preceding.
\{x,\}	=	match x or more occurrences of the preceding.

As an MPE user, you may find regular expressions difficult to use at first. Please persevere, because they are used in many UNIX tools, from more to perl. Unfortunately, some tools use simple regular expressions and others use extended regular expressions and some extended features have been merged into simple tools, so that it looks as if every tool has its own syntax. Not only that, regular expressions use the same characters as shell wildcarding, but they are not used in exactly the same way. What do you expect of an operating system built by graduate students?

Since you usually type regular expressions within shell commands, it is good practice to enclose the regular expression in single quotes (') to stop the shell from expanding it before passing the argument to your search tool. Here are some examples using grep:

grep smug files	{search files for lines with 'smug'}
grep '^smug' files	{'smug' at the start of a line}
grep 'smug$' files	{'smug' at the end of a line}
grep '^smug$' files	{lines containing only 'smug'}
grep '\^s' files	{lines starting with '^s', "\" escapes the ^}
grep '[Ss]mug' files	{search for 'Smug' or 'smug'}
grep 'B[oO][bB]' files	{search for BOB, Bob, BOb or BoB }
grep '^$' files	{search for blank lines}
grep '[0-9][0-9]' file	{search for pairs of numeric digits}

Back Slash "\" is used to escape the next symbol, for example, turn off the special meaning that it has. To look for a Caret "^" at the start of a line, the expression is ^\^. Period "." matches any single character. So b.b will match "bob", "bib", "b-b", etc. Asterisk "*" does not mean the same thing in regular expressions as in wildcarding; it is a modifier that applies to the preceding single character, or expression such as [0-9]. An asterisk matches zero or more of what precedes it. Thus [A-Z]* matches any number of upper-case letters, including none, while [A-Z][A-Z]* matches one or more upper-case letters.

The vi editor uses \< \> to match characters at the beginning and/or end of a word boundary. A word boundary is either the edge of the line or any character except a letter, digit or underscore "_". To look for if, but skip stiff, the expression is \<if\>. For the same logic in grep, invoke it with the -w option. And remember that regular expressions are case-sensitive. If you don't care about the case, the expression to match "if" would be [Ii][Ff], where the characters in square brackets define a character set from which the pattern must match one character. Alternatively, you could also invoke grep with the -i option to ignore case.

Here are a few more examples of grep to show you what can be done:

grep '^From: ' /usr/mail/$USER	{list your mail}
grep '[a-zA-Z]'	{any line with at least one letter}
grep '[^a-zA-Z0-9]	{anything not a letter or number}
grep '[0-9]\{3\}-[0-9]\{4\}'	{999-9999, like phone numbers}
grep '^.$'	{lines with exactly one character}
grep '"smug"'	{'smug' within double quotes}
grep '"*smug"*'	{'smug', with or without quotes}
grep '^\.'	{any line that starts with a Period "."}
grep '^\.[a-z][a-z]'	{line start with "." and 2 lc letters}

	23.2 grep for whole word (like vim /\<word\>
grep -w "word" file

	23.3 egrep, tags egrep, extended regular expression

Basic Usage
egrep or grep -E
Run grep with extended regular expressions.
-i Ignore case (ie uppercase, lowercase letters).
-v Return all lines which don't match the pattern.
-w Select only matches that form whole words.
-c Print a count of matching lines.
Can be combined with the -v option to print a count of non matchine lines.
-l Print the name of each file which contains a match.
Normally used when grep is invoked with wildcards for the file argument.
-n Print the line number before each line that matches.
-r Recursive, read all files in given directory and subdirectories.

Regular Expressions
.  A single character
[abc] Range. ie any one of these characters
[^abc] Not range. A character that is not one of those enclosed.
(abc) Group these characters and remember for later.
\n Replace n with a number. Recall the charactes matched in that set of brackets.
May also be used to rename files or directories.
| The logical 'or' operation.
\ In front of a character, removes it's special meaning.

RE Multipliers
?  The preceding item is optional, it is matched zero or one times.
* The preceding item will be matched zero or more times.
+ The preceding item will be matched one or more times.
{n} The preceding item will be matched exactly n times.
{n,} The preceding item will be matched n or more times.
{n,m} The preceding item will be matched between n and m times.

RE Anchors
^ From the beginning of the line.
$ To the end of the line.
\< At the beginning of a word.
\> At the end of a word.
\b Match either the beginning or end of a word.

Some Examples
egrep 'mellon' myfile.txt
Print every line in myfile.txt containing the string 'mellon'.
egrep -n 'mellon' myfile.txt
Same as above but print a line number before each line.
egrep '(.)bb\1' myfile.txt
Find every line with 2 b's and the same character both before and after those b's.
egrep -l '[0-9]{8,}' /files/projectx/*
Print each file in the directory projectx which contains a number of 8 digits or more.
egrep '\b[a-z0-9._%+-]+@[a-z0-9.-]+\.[a-z]{2,4}\b' myfile.txt
Print every line of myfiles.txt containing an email address.
Note: this is just a simple email matching pattern. There is a miniscule number of email addresses it will not match.

	23.4 Grep examples
First create the following demo_file that will be used in the examples below to demonstrate grep command.

$ cat demo_file
THIS LINE IS THE 1ST UPPER CASE LINE IN THIS FILE.
this line is the 1st lower case line in this file.
This Line Has All Its First Character Of The Word With Upper Case.

Two lines above this line is empty.
And this is the last line.

		23.4.1.1. Search for the given string in a single file

The basic usage of grep command is to search for a specific string in the specified file as shown below.

Syntax:
grep "literal_string" filename

$ grep "this" demo_file
this line is the 1st lower case line in this file.
Two lines above this line is empty.
And this is the last line.

		23.4.1.1.2. Checking for the given string in multiple files.

Syntax:
grep "string" FILE_PATTERN

This is also a basic usage of grep command. For this example, let us copy the demo_file to demo_file1. The grep output will also include the file name in front of the line that matched the specific pattern as shown below. When the Linux shell sees the meta character, it does the expansion and gives all the files as input to grep.

$ cp demo_file demo_file1

$ grep "this" demo_*
demo_file:this line is the 1st lower case line in this file.
demo_file:Two lines above this line is empty.
demo_file:And this is the last line.
demo_file1:this line is the 1st lower case line in this file.
demo_file1:Two lines above this line is empty.
demo_file1:And this is the last line.

            23.4.1.1.3. Case insensitive search using grep -i

Syntax:
grep -i "string" FILE

This is also a basic usage of the grep. This searches for the given string/pattern case insensitively. So it matches all the words such as “the”, “THE” and “The” case insensitively as shown below.


 
$ grep -i "the" demo_file
THIS LINE IS THE 1ST UPPER CASE LINE IN THIS FILE.
this line is the 1st lower case line in this file.
This Line Has All Its First Character Of The Word With Upper Case.
And this is the last line.

            23.4.1.1.4. Match regular expression in files

Syntax:
grep "REGEX" filename

This is a very powerful feature, if you can use use regular expression effectively. In the following example, it searches for all the pattern that starts with “lines” and ends with “empty” with anything in-between. i.e To search “lines[anything in-between]empty” in the demo_file.

$ grep "lines.*empty" demo_file
Two lines above this line is empty.
From documentation of grep: A regular expression may be followed by one of several repetition operators:

? The preceding item is optional and matched at most once.
* The preceding item will be matched zero or more times.
+ The preceding item will be matched one or more times.
{n} The preceding item is matched exactly n times.
{n,} The preceding item is matched n or more times.
{,m} The preceding item is matched at most m times.
{n,m} The preceding item is matched at least n times, but not more than m times.
            23.4.1.1.5. Checking for full words, not for sub-strings using grep -w

If you want to search for a word, and to avoid it to match the substrings use -w option. Just doing out a normal search will show out all the lines.

The following example is the regular grep where it is searching for “is”. When you search for “is”, without any option it will show out “is”, “his”, “this” and everything which has the substring “is”.

$ grep -i "is" demo_file
THIS LINE IS THE 1ST UPPER CASE LINE IN THIS FILE.
this line is the 1st lower case line in this file.
This Line Has All Its First Character Of The Word With Upper Case.
Two lines above this line is empty.
And this is the last line.

The following example is the WORD grep where it is searching only for the word “is”. Please note that this output does not contain the line “This Line Has All Its First Character Of The Word With Upper Case”, even though “is” is there in the “This”, as the following is looking only for the word “is” and not for “this”.

$ grep -iw "is" demo_file
THIS LINE IS THE 1ST UPPER CASE LINE IN THIS FILE.
this line is the 1st lower case line in this file.
Two lines above this line is empty.
And this is the last line.
            23.4.1.1.6. Displaying lines before/after/around the match using grep -A, -B and -C

When doing a grep on a huge file, it may be useful to see some lines after the match. You might feel handy if grep can show you not only the matching lines but also the lines after/before/around the match.


Please create the following demo_text file for this example.

$ cat demo_text
|4. Vim Word Navigation

You may want to do several navigation in relation to the words, such as:

 * e - go to the end of the current word.
 * E - go to the end of the current WORD.
 * b - go to the previous (before) word.
 * B - go to the previous (before) WORD.
 * w - go to the next word.
 * W - go to the next WORD.

WORD - WORD consists of a sequence of non-blank characters, separated with white space.
word - word consists of a sequence of letters, digits and underscores.

Example to show the difference between WORD and word

 * 192.168.1.1 - single WORD
 * 192.168.1.1 - seven words.
            23.4.1.1.6.1 Display N lines after match
-A is the option which prints the specified N lines after the match as shown below.

Syntax:
grep -A <N> "string" FILENAME

The following example prints the matched line, along with the 3 lines after it.

$ grep -A 3 -i "example" demo_text
Example to show the difference between WORD and word

* 192.168.1.1 - single WORD
* 192.168.1.1 - seven words.
            23.4.1.1.6.2 Display N lines before match
-B is the option which prints the specified N lines before the match.

Syntax:
grep -B <N> "string" FILENAME

When you had option to show the N lines after match, you have the -B option for the opposite.

$ grep -B 2 "single WORD" demo_text
Example to show the difference between WORD and word

* 192.168.1.1 - single WORD
            23.4.1.1.6.3 Display N lines around match
-C is the option which prints the specified N lines before the match. In some occasion you might want the match to be appeared with the lines from both the side. This options shows N lines in both the side(before & after) of match.

$ grep -C 2 "Example" demo_text
word - word consists of a sequence of letters, digits and underscores.

Example to show the difference between WORD and word

* 192.168.1.1 - single WORD
            23.4.1.1.7. Highlighting the search using GREP_OPTIONS

As grep prints out lines from the file by the pattern / string you had given, if you wanted it to highlight which part matches the line, then you need to follow the following way.

When you do the following export you will get the highlighting of the matched searches. In the following example, it will highlight all the this when you set the GREP_OPTIONS environment variable as shown below.

$ export GREP_OPTIONS='--color=auto' GREP_COLOR='100;8'

$ grep this demo_file
this line is the 1st lower case line in this file.
Two lines above this line is empty.
And this is the last line.
            23.4.1.1.8. Searching in all files recursively using grep -r

When you want to search in all the files under the current directory and its sub directory. -r option is the one which you need to use. The following example will look for the string “ramesh” in all the files in the current directory and all it’s subdirectory.

$ grep -r "ramesh" *
            23.4.1.1.9. Invert match using grep -v

You had different options to show the lines matched, to show the lines before match, and to show the lines after match, and to highlight match. So definitely You’d also want the option -v to do invert match.

When you want to display the lines which does not matches the given string/pattern, use the option -v as shown below. This example will display all the lines that did not match the word “go”.

$ grep -v "go" demo_text
|4. Vim Word Navigation

You may want to do several navigation in relation to the words, such as:

WORD - WORD consists of a sequence of non-blank characters, separated with white space.
word - word consists of a sequence of letters, digits and underscores.

Example to show the difference between WORD and word

* 192.168.1.1 - single WORD
* 192.168.1.1 - seven words.
            23.4.1.1.10. display the lines which does not matches all the given pattern.

Syntax:
grep -v -e "pattern" -e "pattern"

$ cat test-file.txt
a
b
c
d

$ grep -v -e "a" -e "b" -e "c" test-file.txt
d
            23.4.1.1.11. Counting the number of matches using grep -c

When you want to count that how many lines matches the given pattern/string, then use the option -c.

Syntax:
grep -c "pattern" filename

$ grep -c "go" demo_text
6

When you want do find out how many lines matches the pattern

$ grep -c this demo_file
3

When you want do find out how many lines that does not match the pattern

$ grep -v -c this demo_file
4
            23.4.1.1.12. Display only the file names which matches the given pattern using grep -l

If you want the grep to show out only the file names which matched the given pattern, use the -l (lower-case L) option.

When you give multiple files to the grep as input, it displays the names of file which contains the text that matches the pattern, will be very handy when you try to find some notes in your whole directory structure.

$ grep -l this demo_*
demo_file
demo_file1
            23.4.1.1.13. Show only the matched string

By default grep will show the line which matches the given pattern/string, but if you want the grep to show out only the matched string of the pattern then use the -o option.

It might not be that much useful when you give the string straight forward. But it becomes very useful when you give a regex pattern and trying to see what it matches as

$ grep -o "is.*line" demo_file
is line is the 1st lower case line
is line
is is the last line
            23.4.1.1.14. Show the position of match in the line

When you want grep to show the position where it matches the pattern in the file, use the following options as

Syntax:
grep -o -b "pattern" file

$ cat temp-file.txt
12345
12345

$ grep -o -b "3" temp-file.txt
2:3
8:3

Note: The output of the grep command above is not the position in the line, it is byte offset of the whole file.

            23.4.1.1.15. Show line number while displaying the output using grep -n

To show the line number of file with the line matched. It does 1-based line numbering for each file. Use -n option to utilize this feature.

$ grep -n "go" demo_text
5: * e - go to the end of the current word.
6: * E - go to the end of the current WORD.
7: * b - go to the previous (before) word.
8: * B - go to the previous (before) WORD.
9: * w - go to the next word.
10: * W - go to the next WORD.

	23.5
24. FAQS

	24.1 howto kill defunct processes
Would anyone know the best procedure for killing "defunct" processes? I tried killing some but they reappear. I'm not sure how to locate their parent process and stop the "defunct" pid's from coming back.
  	
	
A process can't "come back". What happens is either 1) the process never really died, or 2) the parent process is intentionally monitoring it's children and actively spawning another process when the first one got killed. I suspect number one, as programs can handle a term signal without terminating. If it is number two, then the processes are very likely not defunct, as they are respawning other processes.
  	
	
You cannot kill a defunct process (a.k.a zombie) as it is already dead.
It doesn't take any resources so it's no big deal but if you really want it to disappear form the process table you need to have its parent procees reaping it.
"pstree" should give you the process hierarchy and "kill -1 <parent-pid>" is sometimes enough for the job.
  	



A couple of comments and the answer to your qestion:

a. If you want to kill a process first find out the pid. For example I want to kill "mythfrontend" process

tdec@amd:~> ps -C mythfrontend
PID TTY TIME CMD
11063 ? 00:01:05 mythfrontend
tdec@amd:~> kill -9 11063

b. Here a great tip from another user (Thxs Bill Dandreta):
Sometimes

kill -9 <pid>

will not kill a process. Run

ps -xal

the 4rd field is the parent process, kill all of a zombie's parents and the zombie dies!

Example

4 0 18581 31706 17 0 2664 1236 wait S ? 0:00 sh -c /usr/bin/gcc -fomit-frame-pointer -O -mfpmat
4 0 18582 18581 17 0 2064 828 wait S ? 0:00 /usr/i686-pc-linux-gnu/gcc-bin/3.3.6/gcc -fomit-fr
4 0 18583 18582 21 0 6684 3100 - R ? 0:00 /usr/lib/gcc-lib/i686-pc-linux-gnu/3.3.6/cc1 -quie

18581,18582,18583 are zombies -

kill -9 18581 18582 18583

has no effect.

kill -9 31706

removes the zombies.


In any case, try less extreme options before "kill -9". eg:
Code:

kill -1 31706
kill 31706
kill -2 31706

Check the zombies presence after each of these commands.
Use "kill -9" only if the zombies are still there.


	24.2 logrotate and syslog Troubleshoot

		24.2.1 CSCty31541 Log Rotation:Excess log file generated for Runtime, Mgmt & mgmtAudit log (blocker for testing?)
test fix:
[root@acs-yizaq-02 ~]# cat /etc/logrotate.d/acs-logrotate 
/opt/CSCOacs/logs/monit.log {
    rotate 9
    size=5M
    missingok
}

/opt/CSCOacs/logs/acsupgrade.log {
    rotate 9
    size=5M
    missingok
}

/opt/CSCOacs/mgmt/apache-tomcat-6.0.18/logs/catalina.out {
    rotate 9
    size=5M
    missingok
}

## yizaq: File was previously managed by both logrotate & log4j configuration
## I removed this duplicity and left only the log4j configuration
#/opt/CSCOacs/logs/ACSManagementAudit.log {
#    rotate 9
#    size=5M
#    missingok
#}

/opt/CSCOacs/logs/MonitoringAndReportingProcess.log {
    rotate 9
    size=5M
    missingok
}


/opt/CSCOacs/logs/ACSADAgent.log {
    rotate 9
    size=5M
    missingok
}

/opt/CSCOacs/db/dberr.log {
    rotate 9
    size=10M
    missingok
}

	--> logrotate settings
[root@acs-yizaq-02 ~]# cat /etc/cron.minute/logrotate 
#!/bin/sh

/usr/sbin/logrotate /etc/logrotate.conf
EXITVALUE=$?
if [ $EXITVALUE != 0 ]; then
    /usr/bin/logger -t logrotate "ALERT exited abnormally with [$EXITVALUE]"
fi
exit 0
[root@acs-yizaq-02 ~]# cat /etc/logrotate.conf
# see "man logrotate" for details
# rotate log files weekly
weekly

# keep 4 weeks worth of backlogs
rotate 4

# create new (empty) log files after rotating old ones
create

# uncomment this if you want your log files compressed
#compress

# RPM packages drop log rotation information into this directory
include /etc/logrotate.d

# no packages own wtmp -- we'll rotate them here
/var/log/wtmp {
    monthly
    create 0664 root utmp
    rotate 1
}

# system-specific logs may be also be configured here.


	-->
inflate:
[root@acs-yizaq-02 ~]# while true; do  echo aaaaaaaaaaaaaaaaaaaaaaaa >>  /opt/CSCOacs/logs/acsupgrade.log; done

monitor:
[root@acs-yizaq-02 ~]# while true; do  ls -lh /opt/CSCOacs/logs/acsupgrade.log*; sleep 60; date; done
-rw-r--r-- 1 root root 2.4K Mar 21 16:49 /opt/CSCOacs/logs/acsupgrade.log
Wed Mar 21 17:39:11 IST 2012
-rw-r--r-- 1 root root 2.4K Mar 21 16:49 /opt/CSCOacs/logs/acsupgrade.log
...
Wed Mar 21 17:41:11 IST 2012
-rw-r--r-- 1 root root 43M Mar 21 17:41 /opt/CSCOacs/logs/acsupgrade.log
Wed Mar 21 17:42:12 IST 2012
-rw-r--r-- 1 root root 79M Mar 21 17:42 /opt/CSCOacs/logs/acsupgrade.log
Wed Mar 21 17:43:12 IST 2012
-rw-r--r-- 1 root root 99M Mar 21 17:42 /opt/CSCOacs/logs/acsupgrade.log
Wed Mar 21 17:44:12 IST 2012
-rw-r--r-- 1 root root 99M Mar 21 17:42 /opt/CSCOacs/logs/acsupgrade.log
Wed Mar 21 17:45:12 IST 2012
-rw-r--r-- 1 root root 99M Mar 21 17:42 /opt/CSCOacs/logs/acsupgrade.log
Wed Mar 21 17:46:12 IST 2012
-rw-r--r-- 1 root root 99M Mar 21 17:42 /opt/CSCOacs/logs/acsupgrade.log
Wed Mar 21 17:47:12 IST 2012
-rw-r--r-- 1 root root 99M Mar 21 17:42 /opt/CSCOacs/logs/acsupgrade.log
Wed Mar 21 17:48:12 IST 2012
-rw-r--r-- 1 root root 99M Mar 21 17:42 /opt/CSCOacs/logs/acsupgrade.log
Wed Mar 21 17:49:12 IST 2012
-rw-r--r-- 1 root root 99M Mar 21 17:42 /opt/CSCOacs/logs/acsupgrade.log


	--> That sucks - logrotate not working. why?
[root@acs-yizaq-02 ~]# tail -f /var/log/messages 
Mar 21 17:07:27 acs-yizaq-02 SQLAnywhere(acssrvr): Starting checkpoint of "acsdb" (acs.db) at Wed Mar 21 2012 17:07
Mar 21 17:07:27 acs-yizaq-02 SQLAnywhere(acssrvr): Finished checkpoint of "acsdb" (acs.db) at Wed Mar 21 2012 17:07
Mar 21 17:15:01 acs-yizaq-02 logrotate: ALERT exited abnormally with [1]
Mar 21 17:27:28 acs-yizaq-02 SQLAnywhere(acssrvr): Starting checkpoint of "acsdb" (acs.db) at Wed Mar 21 2012 17:27
Mar 21 17:27:28 acs-yizaq-02 SQLAnywhere(acssrvr): Finished checkpoint of "acsdb" (acs.db) at Wed Mar 21 2012 17:27
Mar 21 17:30:01 acs-yizaq-02 logrotate: ALERT exited abnormally with [1]
Mar 21 17:30:46 acs-yizaq-02 sshd[6561]: Accepted keyboard-interactive/pam for root from 64.103.121.183 port 53913 ssh2
Mar 21 17:34:12 acs-yizaq-02 sshd[6630]: error: PAM: Authentication failure for root from 64.103.121.183
Mar 21 17:34:16 acs-yizaq-02 sshd[6630]: Accepted keyboard-interactive/pam for root from 64.103.121.183 port 53926 ssh2
Mar 21 17:45:01 acs-yizaq-02 logrotate: ALERT exited abnormally with [1]
Mar 21 17:47:29 acs-yizaq-02 SQLAnywhere(acssrvr): Starting checkpoint of "acsdb" (acs.db) at Wed Mar 21 2012 17:47
Mar 21 17:47:29 acs-yizaq-02 SQLAnywhere(acssrvr): Finished checkpoint of "acsdb" (acs.db) at Wed Mar 21 2012 17:47
Mar 21 17:48:48 acs-yizaq-02 SQLAnywhere(acsview): Starting checkpoint of "acsview" (acsview51.db) at Wed Mar 21 2012 17:48
Mar 21 17:48:55 acs-yizaq-02 SQLAnywhere(acsview): Finished checkpoint of "acsview" (acsview51.db) at Wed Mar 21 2012 17:48


[root@acs-yizaq-02 ~]# /usr/sbin/logrotate -v /etc/logrotate.conf
reading config file /etc/logrotate.conf
including /etc/logrotate.d
reading config file ADE
reading config info for /var/log/ade/ADE.log 
reading config info for /var/log/backup*.log 
reading config info for /var/log/restore*.log 
reading config file acs-logrotate
reading config info for /opt/CSCOacs/logs/monit.log 
reading config info for /opt/CSCOacs/logs/acsupgrade.log 
reading config info for /opt/CSCOacs/mgmt/apache-tomcat-6.0.18/logs/catalina.out 
reading config info for /opt/CSCOacs/logs/MonitoringAndReportingProcess.log 
reading config info for /opt/CSCOacs/logs/ACSADAgent.log 
reading config info for /opt/CSCOacs/db/dberr.log 
reading config file conman
reading config info for /var/log/conman/* 
olddir is now /var/log/conman.old/
reading config file mgetty
reading config info for /var/log/mgetty.log.tty[^.] /var/log/mgetty.log.tty[^.][^.] /var/log/mgetty.log.tty[^.][^.][^.] /var/log/mgetty.log.tty[^.][^.][^.][^.] /var/log/mgetty.log.tty[^.][^.][^.][^.][^.] /var/log/mgetty.log.tty[^.][^.][^.][^.][^.][^.] /var/log/mgetty.log.tty[^.][^.][^.][^.][^.][^.][^.] /var/log/mgetty.log.tty[^.][^.][^.][^.][^.][^.][^.][^.] /var/log/mgetty.log.tty[^.][^.][^.][^.][^.][^.][^.][^.][^.] /var/log/mgetty.log.tty[^.][^.][^.][^.][^.][^.][^.][^.][^.][^.] /var/log/mgetty.log.unknown /var/log/mgetty.callback 
reading config file named
reading config info for /var/log/named.log 
error: named:3 unknown user 'named'

[root@acs-yizaq-02 ~]# grep named /etc/logrotate.d/*
/etc/logrotate.d/named:/var/log/named.log {
/etc/logrotate.d/named:    create 0644 named named
/etc/logrotate.d/named:        /sbin/service named reload  2> /dev/null > /dev/null || true
[root@acs-yizaq-02 ~]# cat /etc/logrotate.d/named
/var/log/named.log {
    missingok
    create 0644 named named
    postrotate
        /sbin/service named reload  2> /dev/null > /dev/null || true
    endscript
}

	--> Remove named to fix 
[root@acs-yizaq-02 ~]# rm !$
rm /etc/logrotate.d/named
rm: remove regular file `/etc/logrotate.d/named'? y
[root@acs-yizaq-02 ~]# /usr/sbin/logrotate -v /etc/logrotate.conf
reading config file /etc/logrotate.conf
including /etc/logrotate.d
reading config file ADE
reading config info for /var/log/ade/ADE.log 
reading config info for /var/log/backup*.log 
reading config info for /var/log/restore*.log 
reading config file acs-logrotate
reading config info for /opt/CSCOacs/logs/monit.log 
reading config info for /opt/CSCOacs/logs/acsupgrade.log 
reading config info for /opt/CSCOacs/mgmt/apache-tomcat-6.0.18/logs/catalina.out 
reading config info for /opt/CSCOacs/logs/MonitoringAndReportingProcess.log 
reading config info for /opt/CSCOacs/logs/ACSADAgent.log 
reading config info for /opt/CSCOacs/db/dberr.log 
reading config file conman
reading config info for /var/log/conman/* 
olddir is now /var/log/conman.old/
reading config file mgetty
reading config info for /var/log/mgetty.log.tty[^.] /var/log/mgetty.log.tty[^.][^.] /var/log/mgetty.log.tty[^.][^.][^.] /var/log/mgetty.log.tty[^.][^.][^.][^.] /var/log/mgetty.log.tty[^.][^.][^.][^.][^.] /var/log/mgetty.log.tty[^.][^.][^.][^.][^.][^.] /var/log/mgetty.log.tty[^.][^.][^.][^.][^.][^.][^.] /var/log/mgetty.log.tty[^.][^.][^.][^.][^.][^.][^.][^.] /var/log/mgetty.log.tty[^.][^.][^.][^.][^.][^.][^.][^.][^.] /var/log/mgetty.log.tty[^.][^.][^.][^.][^.][^.][^.][^.][^.][^.] /var/log/mgetty.log.unknown /var/log/mgetty.callback 
reading config file psacct
reading config info for /var/account/pacct 
reading config file rpm
reading config info for /var/log/rpmpkgs 
reading config file squid
reading config info for /var/log/squid/access.log 
reading config info for /var/log/squid/cache.log 
reading config info for /var/log/squid/store.log 
reading config file syslog
reading config info for /var/log/messages /var/log/secure /var/log/maillog /var/log/spooler /var/log/boot.log /var/log/cron 
reading config file yum
reading config info for /var/log/yum.log 
reading config info for /var/log/wtmp 

Handling 19 logs

rotating pattern: /var/log/ade/ADE.log  5242880 bytes (10 rotations)
empty log files are rotated, old logs are removed
considering log /var/log/ade/ADE.log
  log does not need rotating

rotating pattern: /var/log/backup*.log  5242880 bytes (10 rotations)
empty log files are rotated, old logs are removed
considering log /var/log/backup*.log
  log /var/log/backup*.log does not exist -- skipping

rotating pattern: /var/log/restore*.log  5242880 bytes (10 rotations)
empty log files are rotated, old logs are removed
considering log /var/log/restore*.log
  log /var/log/restore*.log does not exist -- skipping

rotating pattern: /opt/CSCOacs/logs/monit.log  5242880 bytes (9 rotations)
empty log files are rotated, old logs are removed
considering log /opt/CSCOacs/logs/monit.log
  log does not need rotating

rotating pattern: /opt/CSCOacs/logs/acsupgrade.log  5242880 bytes (9 rotations)
empty log files are rotated, old logs are removed
considering log /opt/CSCOacs/logs/acsupgrade.log
  log needs rotating
rotating log /opt/CSCOacs/logs/acsupgrade.log, log->rotateCount is 9
renaming /opt/CSCOacs/logs/acsupgrade.log.9 to /opt/CSCOacs/logs/acsupgrade.log.10 (rotatecount 9, logstart 1, i 9), 
old log /opt/CSCOacs/logs/acsupgrade.log.9 does not exist
renaming /opt/CSCOacs/logs/acsupgrade.log.8 to /opt/CSCOacs/logs/acsupgrade.log.9 (rotatecount 9, logstart 1, i 8), 
old log /opt/CSCOacs/logs/acsupgrade.log.8 does not exist
renaming /opt/CSCOacs/logs/acsupgrade.log.7 to /opt/CSCOacs/logs/acsupgrade.log.8 (rotatecount 9, logstart 1, i 7), 
old log /opt/CSCOacs/logs/acsupgrade.log.7 does not exist
renaming /opt/CSCOacs/logs/acsupgrade.log.6 to /opt/CSCOacs/logs/acsupgrade.log.7 (rotatecount 9, logstart 1, i 6), 
old log /opt/CSCOacs/logs/acsupgrade.log.6 does not exist
renaming /opt/CSCOacs/logs/acsupgrade.log.5 to /opt/CSCOacs/logs/acsupgrade.log.6 (rotatecount 9, logstart 1, i 5), 
old log /opt/CSCOacs/logs/acsupgrade.log.5 does not exist
renaming /opt/CSCOacs/logs/acsupgrade.log.4 to /opt/CSCOacs/logs/acsupgrade.log.5 (rotatecount 9, logstart 1, i 4), 
old log /opt/CSCOacs/logs/acsupgrade.log.4 does not exist
renaming /opt/CSCOacs/logs/acsupgrade.log.3 to /opt/CSCOacs/logs/acsupgrade.log.4 (rotatecount 9, logstart 1, i 3), 
old log /opt/CSCOacs/logs/acsupgrade.log.3 does not exist
renaming /opt/CSCOacs/logs/acsupgrade.log.2 to /opt/CSCOacs/logs/acsupgrade.log.3 (rotatecount 9, logstart 1, i 2), 
old log /opt/CSCOacs/logs/acsupgrade.log.2 does not exist
renaming /opt/CSCOacs/logs/acsupgrade.log.1 to /opt/CSCOacs/logs/acsupgrade.log.2 (rotatecount 9, logstart 1, i 1), 
old log /opt/CSCOacs/logs/acsupgrade.log.1 does not exist
renaming /opt/CSCOacs/logs/acsupgrade.log.0 to /opt/CSCOacs/logs/acsupgrade.log.1 (rotatecount 9, logstart 1, i 0), 
old log /opt/CSCOacs/logs/acsupgrade.log.0 does not exist
log /opt/CSCOacs/logs/acsupgrade.log.10 doesn't exist -- won't try to dispose of it
renaming /opt/CSCOacs/logs/acsupgrade.log to /opt/CSCOacs/logs/acsupgrade.log.1
creating new log mode = 0644 uid = 0 gid = 0

rotating pattern: /opt/CSCOacs/mgmt/apache-tomcat-6.0.18/logs/catalina.out  5242880 bytes (9 rotations)
empty log files are rotated, old logs are removed
considering log /opt/CSCOacs/mgmt/apache-tomcat-6.0.18/logs/catalina.out
  log does not need rotating

rotating pattern: /opt/CSCOacs/logs/MonitoringAndReportingProcess.log  5242880 bytes (9 rotations)
empty log files are rotated, old logs are removed
considering log /opt/CSCOacs/logs/MonitoringAndReportingProcess.log
  log does not need rotating

rotating pattern: /opt/CSCOacs/logs/ACSADAgent.log  5242880 bytes (9 rotations)
empty log files are rotated, old logs are removed
considering log /opt/CSCOacs/logs/ACSADAgent.log
  log does not need rotating

rotating pattern: /opt/CSCOacs/db/dberr.log  10485760 bytes (9 rotations)
empty log files are rotated, old logs are removed
considering log /opt/CSCOacs/db/dberr.log
  log does not need rotating

rotating pattern: /var/log/conman/*  weekly (4 rotations)
olddir is /var/log/conman.old/, empty log files are not rotated, old logs are removed
considering log /var/log/conman/*
  log /var/log/conman/* does not exist -- skipping
not running postrotate script, since no logs were rotated

rotating pattern: /var/log/mgetty.log.tty[^.] /var/log/mgetty.log.tty[^.][^.] /var/log/mgetty.log.tty[^.][^.][^.] /var/log/mgetty.log.tty[^.][^.][^.][^.] /var/log/mgetty.log.tty[^.][^.][^.][^.][^.] /var/log/mgetty.log.tty[^.][^.][^.][^.][^.][^.] /var/log/mgetty.log.tty[^.][^.][^.][^.][^.][^.][^.] /var/log/mgetty.log.tty[^.][^.][^.][^.][^.][^.][^.][^.] /var/log/mgetty.log.tty[^.][^.][^.][^.][^.][^.][^.][^.][^.] /var/log/mgetty.log.tty[^.][^.][^.][^.][^.][^.][^.][^.][^.][^.] /var/log/mgetty.log.unknown /var/log/mgetty.callback  weekly (4 rotations)
empty log files are rotated, old logs are removed
considering log /var/log/mgetty.log.tty[^.]
  log /var/log/mgetty.log.tty[^.] does not exist -- skipping
considering log /var/log/mgetty.log.tty[^.][^.]
  log /var/log/mgetty.log.tty[^.][^.] does not exist -- skipping
considering log /var/log/mgetty.log.tty[^.][^.][^.]
  log /var/log/mgetty.log.tty[^.][^.][^.] does not exist -- skipping
considering log /var/log/mgetty.log.tty[^.][^.][^.][^.]
  log /var/log/mgetty.log.tty[^.][^.][^.][^.] does not exist -- skipping
considering log /var/log/mgetty.log.tty[^.][^.][^.][^.][^.]
  log /var/log/mgetty.log.tty[^.][^.][^.][^.][^.] does not exist -- skipping
considering log /var/log/mgetty.log.tty[^.][^.][^.][^.][^.][^.]
  log /var/log/mgetty.log.tty[^.][^.][^.][^.][^.][^.] does not exist -- skipping
considering log /var/log/mgetty.log.tty[^.][^.][^.][^.][^.][^.][^.]
  log /var/log/mgetty.log.tty[^.][^.][^.][^.][^.][^.][^.] does not exist -- skipping
considering log /var/log/mgetty.log.tty[^.][^.][^.][^.][^.][^.][^.][^.]
  log /var/log/mgetty.log.tty[^.][^.][^.][^.][^.][^.][^.][^.] does not exist -- skipping
considering log /var/log/mgetty.log.tty[^.][^.][^.][^.][^.][^.][^.][^.][^.]
  log /var/log/mgetty.log.tty[^.][^.][^.][^.][^.][^.][^.][^.][^.] does not exist -- skipping
considering log /var/log/mgetty.log.tty[^.][^.][^.][^.][^.][^.][^.][^.][^.][^.]
  log /var/log/mgetty.log.tty[^.][^.][^.][^.][^.][^.][^.][^.][^.][^.] does not exist -- skipping
considering log /var/log/mgetty.log.unknown
  log /var/log/mgetty.log.unknown does not exist -- skipping
considering log /var/log/mgetty.callback
  log /var/log/mgetty.callback does not exist -- skipping

rotating pattern: /var/account/pacct  after 1 days (31 rotations)
empty log files are not rotated, old logs are removed
considering log /var/account/pacct
  log does not need rotating
not running postrotate script, since no logs were rotated

rotating pattern: /var/log/rpmpkgs  weekly (4 rotations)
empty log files are not rotated, old logs are removed
considering log /var/log/rpmpkgs
  log does not need rotating

rotating pattern: /var/log/squid/access.log  weekly (5 rotations)
empty log files are not rotated, old logs are removed
considering log /var/log/squid/access.log
  log /var/log/squid/access.log does not exist -- skipping

rotating pattern: /var/log/squid/cache.log  weekly (5 rotations)
empty log files are not rotated, old logs are removed
considering log /var/log/squid/cache.log
  log /var/log/squid/cache.log does not exist -- skipping

rotating pattern: /var/log/squid/store.log  weekly (5 rotations)
empty log files are not rotated, old logs are removed
considering log /var/log/squid/store.log
  log /var/log/squid/store.log does not exist -- skipping
not running postrotate script, since no logs were rotated

rotating pattern: /var/log/messages /var/log/secure /var/log/maillog /var/log/spooler /var/log/boot.log /var/log/cron  weekly (4 rotations)
empty log files are rotated, old logs are removed
considering log /var/log/messages
  log does not need rotating
considering log /var/log/secure
  log does not need rotating
considering log /var/log/maillog
  log does not need rotating
considering log /var/log/spooler
  log does not need rotating
considering log /var/log/boot.log
  log does not need rotating
considering log /var/log/cron
  log does not need rotating
not running postrotate script, since no logs were rotated

rotating pattern: /var/log/yum.log  yearly (4 rotations)
empty log files are not rotated, old logs are removed
considering log /var/log/yum.log
  log does not need rotating

rotating pattern: /var/log/wtmp  monthly (1 rotations)
empty log files are rotated, old logs are removed
considering log /var/log/wtmp
  log does not need rotating

	--> after succesful run:
Wed Mar 21 17:58:12 IST 2012
-rw-r--r-- 1 root root   0 Mar 21 17:57 /opt/CSCOacs/logs/acsupgrade.log
-rw-r--r-- 1 root root 99M Mar 21 17:42 /opt/CSCOacs/logs/acsupgrade.log.1
Wed Mar 21 17:59:12 IST 2012
-rw-r--r-- 1 root root   0 Mar 21 17:57 /opt/CSCOacs/logs/acsupgrade.log
-rw-r--r-- 1 root root 99M Mar 21 17:42 /opt/CSCOacs/logs/acsupgrade.log.1
Wed Mar 21 18:00:12 IST 2012
-rw-r--r-- 1 root root   0 Mar 21 17:57 /opt/CSCOacs/logs/acsupgrade.log
-rw-r--r-- 1 root root 99M Mar 21 17:42 /opt/CSCOacs/logs/acsupgrade.log.1

even rerun doesn't help
[root@acs-yizaq-02 ~]# /usr/sbin/logrotate -v /etc/logrotate.conf
...
rotating pattern: /opt/CSCOacs/logs/acsupgrade.log  5242880 bytes (9 rotations)
empty log files are rotated, old logs are removed
considering log /opt/CSCOacs/logs/acsupgrade.log
  log does not need rotating
  ...

	--> crontab settings
[root@acs-yizaq-02 ~]# cat /etc/crontab 
SHELL=/bin/bash
PATH=/sbin:/bin:/usr/sbin:/usr/bin
MAILTO=root
HOME=/

# run-parts
01 * * * * root run-parts /etc/cron.hourly
02 4 * * * root run-parts /etc/cron.daily
22 4 * * 0 root run-parts /etc/cron.weekly
42 4 1 * * root run-parts /etc/cron.monthly
0,15,30,45  * * * * root run-parts /etc/cron.minute
[root@acs-yizaq-02 ~]# crontab -l
00 5 * * * /opt/CSCOacs/bin/check-view-compress.sh
00 5 * * * /opt/CSCOacs/bin/check-view-compress.sh

logrotate runs every 15 minuts.

To fix: 
	a. check named problem
	b. change to run logrotate every minute
(so that files don't pass threshold so fast)
crontab -e 
*/1 * * * * echo "job every minute"

	c. Make syslog close fd (so doesn't get stuck on writing to xxx.log.1)
[root@acs-yizaq-02 ~]# cat /etc/syslog.conf 
local0.info                                                                     /var/log/ade/ADE.log
*.info;mail.none;news.none;authpriv.none;cron.none;local0.none;local1.none      /var/log/messages
authpriv.*                                                                      /var/log/secure
mail.*                                                                          -/var/log/maillog
cron.*                                                                          /var/log/cron
*.emerg                                                                         *
uucp,news.crit                                                                  /var/log/spooler
local7.*                                                                        /var/log/boot.log

#Application acs Generated config
# Centrify DirectControl logging (automatically added when ACS installed)
auth.debug                         -/opt/CSCOacs/logs/ACSADAgent.log

# monit logging - local6
# centrify - auth.*
local6.*                         /opt/CSCOacs/logs/monit.log
local6.*;auth.*                  /var/log/ade/ADE.log

remove the - so that syslog will flush on each write

		24.2.2 logrotate keeps writing to xxx.log.1 file

Hi,

I seem to be having a problem with logrotate, or not understanding the
way logrotate works.

I have a log that is created from a redirection in a script ( ... >
mylog.log)

The thing is, that when this log gets rotated, the wrong file keeps
growing. Something like this:

0 nov 3 17:24 mylog.log
422 nov 3 17:43 mylog.log.1

Instead of mylog.log growing, now mylog.log.1 grows. And tomorrow, I
will have 2 zero size files, and mylog.log.2 will grow.

Is there any way to make this work without having to restart my
process?

----------------------------------------------------------------
Put a SIGHUP handler and generate a PID file in the script. On the HUP
close and reopen the logfile. Tell logrotate in postrotate processing to
"kill -HUP `cat /the/PIF/file`" the script.

----------------------------------------------------------------
Not if you get the SIGHUP handler right. do 'info bash' and search for
`trap' (exactly as typed).

try this:

somewhere that gets executed once

trap 'SIGED=1' SIGHUP

then in your main processing loop

if [ $SIGED -eq 1 ] ; then
SIGED=0
close the logfile
open the logfile
fi
----------------------------------------------------------------
Here is my understanding of what is happening: logrotate seems like its doing an "mv" on mylog.log to mylog.log.1 but your program is just continuing to write its output, but now to a file named mylog.log.1. What is needed is for the logging to continue, but to to mylog.log. To do this, your program doing the logging needs to close its current log output file (which is now mylog.log.1) and (re)open mylog.log and write its output to mylog.log. If your program was created to accept the HUP signal, then upon receipt of that signal, it should close its log file and reopen it, then this should fix the problem - thats what the "kill -HUP pid" stuff is about and that's what programs are supposed to do with a HUP (in addition to rereading config files). I'm referring to a C program here (and you have referred to your shell script) because I was trying to figure the same thing out, but for a C program.
----------------------------------------------------------------

		24.2.3 rsyslog and log rotate problem
 
I have an Ubuntu 10.0.4 LTS server running as my syslog server for all my devices that can use it.

in /etc/rsyslog.d/50-default.conf I added these lines at the top:

Code:
:msg, contains, "atl1-wichorus" /var/log/wic1.atl.log
:msg, contains, "atl2-wichorus" /var/log/wic2.atl.log
:msg, contains, "jax1-wichorus" /var/log/wic1.jax.log
:msg, contains, "jax2-wichorus" /var/log/wic2.jax.log
incoming messages are directed to the proper log files. The problem is when the files rotate - /var/log/wic1.atl.log becomes /var/log/wic1.atl.log.1 like it's supposed to, and a new 0-byte /var/log/wic1.atl.log is created. However, syslog writes new messages to the /var/log/wic1.atl.log.1 file. Since the new /var/log/wic1.atl.log is always 0 bytes, it never gets rotated again.

I have a file in /etc/logrotate.d that contains:

Code:
/var/log/wic*.log {
        daily
        missingok
        rotate 8
        compress
        delaycompress
        notifempty
        create 640 root adm
        endscript
        postrotate
                reload rsyslog >/dev/null 2>&1 || true

}
So these files only start writing to the new file if I restart syslog.

Code:
-rw-r-----  1 root      adm             0 2012-02-16 04:37 wic1.atl.log
-rw-r-----  1 syslog    adm    4936864679 2012-02-23 08:14 wic1.atl.log.1
-rw-r-----  1 syslog    adm      35695955 2012-02-15 11:59 wic1.atl.log.2.gz
-rw-r-----  1 syslog    adm      29216126 2012-02-14 08:31 wic1.atl.log.3.gz
-rw-r-----  1 syslog    adm     174011677 2012-02-13 09:13 wic1.atl.log.4.gz
-rw-r-----  1 syslog    adm       2426022 2012-02-08 11:01 wic1.atl.log.5.gz
-rw-r-----  1 syslog    adm      24951368 2012-02-02 11:08 wic1.atl.log.6.gz
I even added "/etc/init.d/rsyslog restart" to the logrotate script, and that didn't work either! It does when I do it on a command line.. What am I missing here?
Last edited by suprstar; 02-23-2012 at 07:17 AM.
 	
 02-23-2012, 11:37 AM	   #2
pafoo
LQ Newbie
 
Registered: Jul 2011
Location: Alabama
Distribution: Red Hat/Ubuntu/Solaris
Posts: 27

Rep: 
Take the * out of your logrotate conf file and specify each file to get rotated. See if that helps.
 	
 02-24-2012, 04:29 AM	   #3
Reuti
Member
Contributing Member
 
Registered: Dec 2004
Location: Marburg, Germany
Distribution: openSUSE 11.4
Posts: 940

Rep: 
The endscript is missing below the reload command (man logrotate). Also the “|| true” shouldn’t be necessary.
 	
1 members found this post helpful.
 02-26-2012, 06:09 AM	   #4
suprstar
Member
 
Registered: Aug 2010
Location: Atlanta
Distribution: ubuntu, redhat
Posts: 132
Blog Entries: 2

Original Poster 
Rep: 
Quote:
Originally Posted by Reuti  
The endscript is missing below the reload command (man logrotate). Also the “|| true” shouldn’t be necessary.
That was it, the 'endscript' was in the middle of the config, not at the end. Thanks!

		24.2.4 Run bash command for exactly X seconds time


			24.2.4.1 simplest approach
You could run the command as a background job (i.e. with "&"), use the bash variable for "pid of last command run," sleep for the requisite amount of time, then run kill with that pid.

[root@acs-yizaq-01 ~]# while true; do  echo aaaaaaaaaaaaaaaaaaaaaaaa ; done & sleep 1 ; kill $!

this approach elaborated:
The following script shows how to do this using background tasks. The first section kills a 60-second process after the 10-second limit. The second attempts to kill a process that's already exited. Keep in mind that, if you set your timeout really high, the process IDs may roll over and you'll kill the wrong process but this is more of a theoretical issue - the timeout would have to be very large and you would have to be starting a lot of processes.

#!/usr/bin/bash

sleep 60 &
pid=$!
sleep 10
kill -9 $pid

sleep 3 &
pid=$!
sleep 10
kill -9 $pid

Here's the output on my Cygwin box:

$ ./limit10
./limit10: line 9:  4492 Killed sleep 60
./limit10: line 11: kill: (4560) - No such process

If you want to only wait until the process has finished, you need to enter a loop and check. This is slightly less accurate since sleep 1 and the other commands will actually take more than one second (but not much more). Use this script to replace the second section above (the "echo $proc" and "date" commands are for debugging, I wouldn't expect to have them in the final solution).

#!/usr/bin/bash

date
sleep 3 &
pid=$!
((lim = 10))
while [[ $lim -gt 0 ]] ; do
    sleep 1
    proc=$(ps -ef | awk -v pid=$pid '$2==pid{print}{}')
    echo $proc
    ((lim = lim - 1))
    if [[ -z "$proc" ]] ; then
            ((lim = -9))
    fi
done
date
if [[ $lim -gt -9 ]] ; then
    kill -9 $pid
fi
date

It basically loops, checking if the process is still running every second. If not, it exits the loop with a special value to not try and kill the child. Otherwise it times out and does kill the child.

Here's the output for a sleep 3:

Mon Feb  9 11:10:37 WADT 2009
pax 4268 2476 con 11:10:37 /usr/bin/sleep
pax 4268 2476 con 11:10:37 /usr/bin/sleep
Mon Feb  9 11:10:41 WADT 2009
Mon Feb  9 11:10:41 WADT 2009

and a sleep 60:

Mon Feb  9 11:11:51 WADT 2009
pax 4176 2600 con 11:11:51 /usr/bin/sleep
pax 4176 2600 con 11:11:51 /usr/bin/sleep
pax 4176 2600 con 11:11:51 /usr/bin/sleep
pax 4176 2600 con 11:11:51 /usr/bin/sleep
pax 4176 2600 con 11:11:51 /usr/bin/sleep
pax 4176 2600 con 11:11:51 /usr/bin/sleep
pax 4176 2600 con 11:11:51 /usr/bin/sleep
pax 4176 2600 con 11:11:51 /usr/bin/sleep
pax 4176 2600 con 11:11:51 /usr/bin/sleep
pax 4176 2600 con 11:11:51 /usr/bin/sleep
Mon Feb  9 11:12:03 WADT 2009
Mon Feb  9 11:12:03 WADT 2009
./limit10: line 20:  4176 Killed sleep 60


			24.2.4.2 More sophisticated 
- c:\work\scripts\bash\timeout3.sh 

- You are probably looking for the timeout command in coreutils. Since it's a part of coreutils, it is technically a C solution, but it's still coreutils. info timeout for more details. Here's an example:

timeout 5 /path/to/slow/command with options

	
- In Mac you can install this via Macports or homebrew. – Ivan Z. Siu Jan 31 at 5:00
feedback
up vote 6 down vote
	

I prefer "timelimit", which has a package at least in debian.

http://devel.ringlet.net/sysutils/timelimit/

It is a bit nicer than the coreutils "timeout" because it prints something when killing the process, and it also sends SIGKILL after some time by default.


			24.2.4.3

		24.2.5 Limit find to only one file
find . -type f -name $l | sed -e '1p' 

		24.2.6 How to insert a date inside the filename with logrotate
I need to set logrotate to rotate logs files from an application running on the server. I need the date inside the filename.

I set dateext and also dateformat to add a - in the date. The result filename is:whatever.csv_2012-03-03

I would like the timestamp to be part of the filename keeping safe the extension; Whatever_2012-03-03.csv.

----------------------------------------------------------------
You should be able to keep the extension apart, e.g. whatever.2012-03-03.csv, with the following configuration:

whatever.csv {
  dateext
  dateformat %Y-%m-%d.
  extension csv
  ...
}
Note the dateext is deliberately empty.

----------------------------------------------------------------
To insert the date within the filename (and not as extension) of a file under Linux while rotating a file it is correct to use:

# Daily rotation
    daily

# We keep original file live
    copytruncate

# Rotation is 1 so we have always .1 as extension
    rotate 1

# If file is missing keep working
    missingok

   sharedscripts
   postrotate
           day=$(date +%Y-%m-%d)
           mv blabla.csv.1 /var/www/gamelogs/dir/blabla$day.csv
   endscript
}
Simple and work fine.
----------------------------------------------------------------



	24.3 how-to-list-just-directories-the-correct-way

Before that I had tried hard to use ls -d to list just directories but failed, therefore I wrote a bash script uses find to do that. It seems Ntropia provided a better solution and it is straight to the point, so let just treat the previous post as examples of find command.
The simplest way of list just directories
ls -d */ 
You can list the directories start with letter b
ls -d b*/ 
Further more list the subdirectories of the directories start with letter b
ls -d b*/*/ 
The outcome will be look like this

backup/10-5-2007/  backup/lunatic/              bin/gdc/
backup/ccbe/       backup/wplbe/                bt/çŽ‹åŠ›å® - æ”¹å˜è‡ªå·±/
backup/full/       bin/ffmpeg-0.4.9-p20051216/
Bare in mind the command lines above do not list out the hidden directories, to list hidden directories
ls -d .*/
Yes, you can also list with details, and with nice colors.
ls -d .*/ -l
Manual of ls should at lease gives a line of example on how to list just directories, ls capable of doing that but it seems to like easter egg for me.

	24.4 Linux: Find Out How Many File Descriptors Are Being Used
hile administrating a box, you may wanted to find out what a processes is doing and find out how many file descriptors (fd) are being used. You will surprised to find out that process does open all sort of files:
=> Actual log file

=> /dev files

=> UNIX Sockets

=> Network sockets

=> Library files /lib /lib64

=> Executables and other programs etc

In this quick post, I will explain how to to count how many file descriptors are currently in use on your Linux server system.
Step # 1 Find Out PID

To find out PID for mysqld process, enter:
# ps aux | grep mysqld
OR
# pidof mysqld
Output:

28290

Step # 2 List File Opened By a PID # 28290

Use the lsof command or /proc/$PID/ file system to display open fds (file descriptors), run:
# lsof -p 28290
# lsof -a -p 28290
OR
# cd /proc/28290/fd
# ls -l | less
You can count open file, enter:
# ls -l | wc -l
Tip: Count All Open File Handles

To count the number of open file handles of any sort, type the following command:
# lsof | wc -l
Sample outputs:

5436

List File Descriptors in Kernel Memory

Type the following command:
# sysctl fs.file-nr
Sample outputs:

fs.file-nr = 1020	0	70000

Where,

    1020 The number of allocated file handles.
    0 The number of unused-but-allocated file handles.
    70000 The system-wide maximum number of file handles.

You can use the following to find out or set the system-wide maximum number of file handles:
# sysctl fs.file-max
Sample outputs:

fs.file-max = 70000

See how to set the system-wide maximum number of file handles under Linux for more information.
More about /proc/PID/file & procfs File System

/proc (or procfs) is a pseudo-file system that it is dynamically generated after each reboot. It is used to access kernel information. procfs is also used by Solaris, BSD, AIX and other UNIX like operating systems. Now, you know how many file descriptors are being used by a process. You will find more interesting stuff in /proc/$PID/file directory:

    /proc/PID/cmdline : process arguments
    /proc/PID/cwd : process current working directory (symlink)
    /proc/PID/exe : path to actual process executable file (symlink)
    /proc/PID/environ : environment used by process
    /proc/PID/root : the root path as seen by the process. For most processes this will be a link to / unless the process is running in a chroot jail.
    /proc/PID/status : basic information about a process including its run state and memory usage.
    /proc/PID/task : hard links to any tasks that have been started by this (the parent) process.

See also: /proc related FAQ/Tips

/proc is an essentials file system for sys-admin work. Just browser through our previous article to get more information about /proc file system:

    /proc/filesystems: Find out what filesystems supported by kernel
    Howto: Linux detect or find out a dual-core cpu
    Linux display CPU information - number of CPUs and their speed
    How to Scan new LUNs on Linux with QLogic driver
    Linux command to gathers up information about a Linux system
    Linux increase the maximum number of open files or file descriptors
    How to display or show information about a Linux Kernel module or drivers
    Linux scan wireless card for information
    Linux disable or drop / block ping packets all together
    How do I find out if my server CPU can run a 64 bit kernel version (apps) or not?
    Display Linux kernel slab cache information in real time
    Making changes to /proc filesystem permanently
    Howto rebuilding a RAID array after a disk fails
    Find out if my server is capable of running para-virtualized guest ( PAE support )
    Linux configure Network Address Translation or NAT
    Howto: Linux see new fiber channel attached disk LUNs without rebooting
    I also recommend reading /proc file system related document, and lsof man page to get a better understanding about fd and files.

Note, Putting it all together to debug fd leakage problem, in this example, of a process named adclient
 $ netstat  -anp | grep `ps -ef | grep adclient | grep -v grep | awk '{print $2}'`
 $ lsof -ap  `ps -ef | grep adclient | grep -v grep | awk '{print $2}'`
 $ lsof -ap  `ps -ef | grep adclient | grep -v grep | awk '{print $2}'` | wc -l
 $ strace -p `ps -ef | grep adclient | grep -v grep | awk '{print $2}'` >& adclient_strace_log 
the file adclient_strace_log)
 $  lsof | wc -l
 $ sysctl fs.file-max
 $ ulimit -n

And in script form:
#!/bin/bash

#check adclient FD/port usage. yizaq - ACS R&D

ADC_PID=$( ps -ef | grep adclient | grep -v grep | awk '{print $2}' )
#echo "adclient pid is $ADC_PID "

echo "Gathering adclient FD/port usage"
echo "adclient port usage: "
netstat  -anp | grep $ADC_PID  
echo "--------------------" > adclient_fd_usage_report
echo "adclient port usage: " >> adclient_fd_usage_report
netstat  -anp | grep $ADC_PID  >> adclient_fd_usage_report

echo "--------------------" >> adclient_fd_usage_report
echo "adclient fd usage: "
lsof -ap $ADC_PID 
echo "adclient fd usage: " >> adclient_fd_usage_report
lsof -ap   $ADC_PID >> adclient_fd_usage_report


echo "--------------------" >> adclient_fd_usage_report
echo "adclient fd #: "
lsof -ap   $ADC_PID | wc -l
echo "adclient fd usage: " >> adclient_fd_usage_report
lsof -ap   $ADC_PID | wc -l >> adclient_fd_usage_report

echo "--------------------" >> adclient_fd_usage_report
echo "fd #: "
 lsof | wc -l
echo "fd #: " >> adclient_fd_usage_report
 lsof | wc -l >> adclient_fd_usage_report

echo "--------------------" >> adclient_fd_usage_report
echo "fd sys limit: "
sysctl fs.file-max
echo "fd sys limit: " >> adclient_fd_usage_report
sysctl fs.file-max >> adclient_fd_usage_report

echo "--------------------" >> adclient_fd_usage_report
echo "fd proc limit: "
ulimit -n
echo "fd proc limit: " >> adclient_fd_usage_report
ulimit -n >> adclient_fd_usage_report

echo "--------------------" >> adclient_fd_usage_report
echo "strace (plz wait 2 min): "
echo "strace (plz wait 2 min): " >> adclient_fd_usage_report
strace -p $ADC_PID >> adclient_fd_usage_report 2>&1 & sleep 60; kill $!
echo "--------------------" >> adclient_fd_usage_report

This command will check FD usage for all processes:
the following command will gather FD usage info on a machine:

cd /proc 
while true ; echo "checking for FDs, time is: "; date; do for proc in `ls  | grep '^[0-9]\+'`; do echo checking process $proc ; echo `ps -e | grep $proc | grep -v grep | awk '{print $4}'` using `ls -l ./$proc/fd/ | wc -l` fds ; echo --- ; done ; sleep 900 ; done   > /tmp/fds_report &

It will gather the info every 15 minutes.
Output ex:
“
checking for FDs, time is: 
Thu Feb 16 16:45:26 IST 2012
…
---
checking process 245
aio/1 using 1 fds
---
checking process 2497
syslogd using 12 fds
---
checking process 2501
klogd using 3 fds
---
checking process 2510
debugd using 57 fds 
…
“________

To stop it do
jobs 
kill %[job id] 

	24.5 Check the open FD limit for a given process in Linux
/proc/pid/limits

ulimit -n

The only interfaces provided by the Linux kernel to get resource limits are getrlimit() and /proc/pid/limits. getrlimit() can only get resource limits of the calling process. /proc/pid/limits allows you to get the resource limits of any process with the same user id, and is available on RHEL 5.2, RHEL 4.7, Ubuntu 9.04, and any distribution with a 2.6.24 or later kernel.

If you need to support older Linux systems then you will have to get the process itself to call getrlimit(). Of course the easiest way to do that is by modifying the program, or a library that it uses. If you are running the program then you could use LD_PRELOAD to load your own code into the program. If none of those are possible then you could attach to the process with gdb and have it execute the call within the process. You could also do the same thing yourself using ptrace() to attach to the process, insert the call in its memory, etc., however this is very complicated to get right and is not recommended.

With appropriate privileges, the other ways to do this would involve looking through kernel memory, loading a kernel module, or otherwise modifying the kernel, but I am assuming that these are out of the question

 On trying to find the resource limits of a running process on an old kernel

I had cause to try and get a core dump from a segfaulting process at work the other day, and I wanted to figure out if fiddling with /etc/security/limits.conf was going to do the trick (it didn't) or if I had to modify the initscript to include a ulimit -c unlimited call.

Of course, on a modern system (>= 2.6.24), one would just take a look at /proc/PID/limits and get on with life, but unfortunately the system in question was running 2.6.18, so more fiddling around was required.

I'd found something once before that told me how to do it with GDB, but all I could find this time around was a rather over-complicated Knol article, which made a bunch of assumptions (well mainly that the binary in question wasn't stripped). So with some help from the Knol article, I muddled through it.

Disclaimer: I don't profess to be an expert on system internals like this, so if this eats your first-born, don't come crying to me.

Firstly, you need to know that it's the getrlimit(2) system call that you want to be using, and then you need to figure out the number for the resource limit you want to retrieve. The man page for getrlimit(), tells you it's defined in /usr/include/sys/resource.h, but I've found that the actual useful bits end up being in /usr/include/bits/resource.h

I wanted the resource limit for the maximum core dump size, which is RLIMIT_CORE and has a value of 4.

Next, you need to know that the getrlimit() system call takes an integer and a pointer to an rlimit structure as arguments. We've just figured out the value for the integer, but we're also going to need to pass a pointer as the second parameter. A pointer to enough memory to hold an rlimit structure. Fortunately, the rlimit structure is pretty simple:

struct rlimit {
    rlim_t rlim_cur;  /* Soft limit */
    rlim_t rlim_max;  /* Hard limit (ceiling for rlim_cur) */
};

After a bit of grepping around in /usr/include, I determined that an rlim_t is essentially an unsigned long int, so we need to allocate a pointer big enough to hold two of them.

Note that if we had an unstripped binary, we could have saved a lot of faffing around by just going

print sizeof(struct rlimit)

in GDB (assuming that the binary has the getrlimit symbol in it)

The sure-fire way of figuring out how much memory we need for this pointer is to go

print sizeof(unsigned long int)

in GDB, and then double that (since we want two of them). On my system an unsigned long int is four bytes, so I'm going to want to allocate enough memory for 8 bytes.

Now it's time to attach GDB to the offending process and see what the resource limit currently is. (gdb -p PID)

(gdb) print malloc(8)
$1 = 152186904
(gdb) print getrlimit(4, $1)
$2 = 0
(gdb) x/2xw $1
0x9123018:	0x00000000	0x7fffffff
(gdb) quit

So in this particular case, we've retrieved the soft and hard limits of the RLIMIT_CORE resource limit, and you can see that the soft limit is zero, and the hard limit is unlimited. Note that the getrlimit() function returns an integer as its return code, which is what the $2 = 0 is above.

Now it's just a case of altering the the resource limits via the preferred mechanism, restarting the process and then repeating this GDB examination of the process to check they were changed successfully.

Circumstances where this can all fall down would appear to be ones where the getrlimit symbol isn't present in the binary, or the binary is compiled as a position-independent executable. I'd think that in the latter case, the system is probably modern enough to be running a kernel that supports directly examining the resource limits via the /proc filesystem.

	24.6 Tip: Redirecting Multiple Command Outputs
Let's imagine a simple script:

#!/bin/bash

echo 1
echo 2
echo 3

Simple enough. It produces three lines of output:

1
2
3

Now let's say we wanted to redirect the output of the commands to a file named foo.txt. We could change the script as follows:

#!/bin/bash

F=foo.txt

echo 1 >> $F
echo 2 >> $F
echo 3 >> $F

Again, pretty straightforward, but what if we wanted to pipe the output of all three echo commands into less? We would soon discover that this won't work:

#!/bin/bash

F=foo.txt

echo 1 | less
echo 2 | less
echo 3 | less

This causes less to be executed three times. Not what we want. We want a single instance of less to input the results of all three echo commands. There are four approaches to this:

Make A Separate Script
script1:

#!/bin/bash

echo 1
echo 2
echo 3

script2:

#!/bin/bash

script1 | less

By running script2, script1 is also executed and its output is piped into less. This works but it's a little clumsy.

Write A Shell Function
We could take the basic idea of the separate script and incorporate it into a single script by making script1 into a shell function:

#!/bin/bash

# shell function
run_echoes () {
   echo 1
   echo 2
   echo 3
}

# call shell function and redirect
run_echoes | less

This works too, but it's not the simplest way to do it.

Make A List
We could construct a compound command using {} characters to enclose a list of commands:

#!/bin/bash

{ echo 1; echo 2; echo 3; } | less

The {} characters allow us to group the three commands into a single output stream. Note that the spaces between the {} and the commands, as well as the trailing semicolon after the third echo, are required.

Launch A Subshell
Finally, we could do this:

#!/bin/bash

(echo 1; echo 2; echo 3) | less

Placing the list inside () creates a subshell, or another copy of bash and it executes the commands. This has the same result as enclosing the list of commands within {} but with more overhead. The real reason we would want to do this is if, instead of just redirecting the output, we wanted to put all three commands in the background:

#!/bin/bash

(echo 1; echo 2; echo 3) > foo.txt &

This doesn't make much sense for our echo commands (they execute too quickly to bother with), but if we have commands that take a long time to run, this technique can come in handy.

	24.7 Question BASH: How to Redirect Output to File, AND Still Have it on Screen

Hi,

Actually this is quite a straightforward question, but somehow i can't find the answer (maybe i'm searching the wrong places). The places i see all teach me how to redirect to file fullstop.

OK, the question...

I know that to redirect the output of a command to a file, i do this:

Code:

ls -l > filename

But what if i want to log the output to file, AND have it displayed on screen (as it would if it were not redirected) as well? Is it possible to do this?



TIA and Regards,
Edwin
  	
Old 02-07-2006, 09:43 AM 	  #2
homey
Senior Member
 
Registered: Oct 2003
Posts: 3,057

Rep: Reputation: 53
	
Try this

ls -al | tee file.txt
  	
	
Brilliant... thanks!



Regards,
Edwin
  	
Old 02-07-2006, 10:40 AM 	  #5
haertig
Senior Member
 
Registered: Nov 2004
Distribution: Debian, Ubuntu, Slackware, Slax, Knoppix, SysrescueCD
Posts: 1,328

Rep: Reputation: 51
	
If you want to capture error output (stderr) along with normal output (stdout):
Code:

ls -l 2>&1 | tee file.txt

  	
Old 02-07-2006, 11:50 PM 	  #6
edwin11
Member
 
Registered: Dec 2005
Distribution: Ubuntu 10.04 LTS
Posts: 119

Original Poster
Rep: Reputation: 15
	
Thanks,

Would the ordering between "| tee file.txt" and "2>&1" matter?

Asking as "ls -l > file.txt 2>&1 file.txt" is different from "ls -l 2>&1 > file.txt".



Regards,
Edwin
  	
Old 02-08-2006, 06:02 AM 	  #7
timmeke
Senior Member
 
Registered: Nov 2005
Location: Belgium
Distribution: Red Hat, Fedora
Posts: 1,515

Rep: Reputation: 59
	
Actually,
Code:

ls -l 2>&1 | tee file.txt

may not work as you expect it.
It is likely that you won't see the stderr and stdout messages in the order in which they were sent to your terminal, simply because stdout (file descriptor 1) is buffered and stderr (2) is not (or was that the other way around - I keep mixing that up). So you'll see the stderr messages first and then the stdout ones (or vice-versa).

If you want the stderr and stdout messages to appear in the right order, you'll need to use a subshell, like so:
(ls -l 2>&1) > file.txt
or
(ls -l 2>&1) | tee file.txt if you want it to appear on screen and in file.txt
This basically says that it should execute the ls -l command, sending stderr to stdout in a subshell. All output from the subshell then comes from the subshell's stdout, and you can capture it in a file or with "tee".
The subshell will only make sure that the stderr and stdout messages appear in the right order.
  	
Old 02-08-2006, 08:14 AM 	  #8
edwin11
Member
 
Registered: Dec 2005
Distribution: Ubuntu 10.04 LTS
Posts: 119

Original Poster
Rep: Reputation: 15
	
i just tried it out, using a simple self-written command.sh shell script which has a number of standard and error output.

Seems like

Code:

$ ./command.sh 2>&1 | tee command.log

works fine (even without the sub-shell). Both the terminal and the log file recorded output in the correct order. Of course, adding the parenthesis does no harm.

However,

Code:

$ ./command.sh | tee command.log 2>&1

(with or without parenthesis) displays all the error output followed by all the normal output, and logs only the normal output.


This behaviour seems to run opposite if i use "> command.log" instead of "| tee command.log".

Code:

$ ./command.sh > command.log 2>&1

logs both the normal and error output in the correct order.

Code:

$ (./command.sh > command.log) 2>&1

logs only the normal output and displays only the error output.

Code:

$ ./command.sh 2>&1 > command.log

also logs only the normal output and displays only the error output.

Code:

$ (./command.sh 2>&1) > command.log

logs both normal and error output in the correct order, and displays nothing.




Regards,
Edwin
  	
Old 02-08-2006, 10:30 AM 	  #9
haertig
Senior Member
 
Registered: Nov 2004
Distribution: Debian, Ubuntu, Slackware, Slax, Knoppix, SysrescueCD
Posts: 1,328

Rep: Reputation: 51
	
I have to admire you for running all these tests to try to learn. The three pieces I think you're missing in your analysis are "How many commands am I running?", "What order am I redirecting things in?" and "What does | do anyway?"

The first missing piece:
---

For example
Code:

./command.sh >file.txt 2>&1

is only running ONE command, namely "command.sh"

However
Code:

./command.sh 2>&1 | tee file.txt

is running TWO commands, namely "command.sh" and "tee"

When you are running more than one command, you have to remember that EACH command has it's own stdout and stderr. So depending on where you put that little "2>&1" thingy, for example, will determine whether you're redirecting stderr from "command.sh" or from "tee". The same goes for when you're running subshells. The subshell itself is a command so think about whether you're redirecting output from "command.sh" which is running within the subshell, or redirecting output from the subshell itself.

To the second missing piece:
---

Remember "1" = stdout and "2" = stderr. "1" is assumed if you don't provide a number.

Example: ls -l >file.txt is the same as ls -l 1>file.txt

An ampersand can be thought of as meaning: "to the same place as"

Example: ls -l >file.txt 2>&1 means "Send stdout to file.txt and send stderr to the same place as stdout

When we're talking ampersands, "to the same place as" has to also take into account "at this time"

Example
Code:

ls -l >file.txt 2>&1

will send both stdout and stderr to file.txt. Why? Because FIRST stdout was redirected to file.txt and THEN stderr was told to go to the same place as stdout.

On the other hand
Code:

ls -l 2>&1 >file.txt

will send stderr to your screen and stdout to file.txt. Why? Because stderr was redirected to the same place as stdout, but at this time stdout was still pointing to the console. stdout was not redirected to file.txt until AFTER stderr had been redirected. When we are talking AFTER in this example we are talking about placement on the command line. Things specified further to the right are thought of as coming after. So this example does nothing for stderr. stderr is indeed redirected to the same place as stdout, which is the console at the time of the redirection, but stderr was ALREADY pointing there in the first place - resulting in no perceivable change.

To the third missing piece:
---

The pipe symbol "|" can be thought of like a fancy redirection of stdout. It redirects stdout from the first command to the stdin of the second command. So ls -l | tee file.txt says "Take the stdout of ls and send it to the stdin of tee. "tee" is simply a program that takes its stdin and sends it to two places, stdout and also to a file.

The eagle-eyed observer will notice an apparent discrepancy for "|" ...

ls -l 2>&1 | tee file.txt will log BOTH stdout and stderr from ls to file.txt. But at this time, when stderr was redirected to the same place as stdout, stdout had not yet been redirected to tee via the "|". Hmmm ... gotcha! Just one of those things. Never fails - when you try to rationalize your way through something logically ... you run into an exception! This is one of those things were experience tells me "I know it works this way", but when you press me with the "why?" question, I have to hem and haw and say "Because that's the way it is!"

---

Now that you head is spining with this cryptic "explanation", maybe it will start making more sense why you're seeing what you're seeing in your tests.

[edit]Corrected a few of my spelling errors![/edit]
Last edited by haertig; 02-08-2006 at 10:33 AM.
  	
1 members found this post helpful.
Old 02-09-2006, 07:40 AM 	  #10
edwin11
Member
 
Registered: Dec 2005
Distribution: Ubuntu 10.04 LTS
Posts: 119

Original Poster
Rep: Reputation: 15
	
Thanks haertig for the explanation! The results of the tests that i ran certainly makes sense now.

As for

Code:

ls -l 2>&1 | tee file.txt

Can we say that:
First, the stderr is redirected to where the stdout goes.
Then we send whatever "comes out" of where stdout normally "comes out", and send it to tee.
And since at that point, both stdout and stderr "comes out" of that place, both are piped to tee, which both displays and sends it to file.
?



Thanks and Regards,
Edwin
  	
Old 02-09-2006, 09:25 AM 	  #11
haertig
Senior Member
 
Registered: Nov 2004
Distribution: Debian, Ubuntu, Slackware, Slax, Knoppix, SysrescueCD
Posts: 1,328

Rep: Reputation: 51
	
Quote:
Originally Posted by edwin11
Can we say that:
First, the stderr is redirected to where the stdout goes.
Then we send whatever "comes out" of where stdout normally "comes out", and send it to tee.
And since at that point, both stdout and stderr "comes out" of that place, both are piped to tee, which both displays and sends it to file.
Yes, I think we can say this!
  	
Old 02-27-2007, 11:31 AM 	  #12
na5m
Member
 
Registered: Jan 2005
Location: california
Distribution: O.A.M. (Overmonitoring Address Matrix) Release 2.2 with 2120 Patch
Posts: 37

Rep: Reputation: 17
	
ampersand
Actually, I don't think that the ampersand strictly means "to the same place as". I think that the ampersand is necessary so that "1" isn't interpreted as a file.
The code
Code:

ls 1>1

will redirect stdout to a file named "1"
Whereas
Code:

ls 1>&1

will redirect stdout to stdout.

ADDITION:
Try
Code:

ls 1>&1>&1>&1>&1

Last edited by na5m; 02-27-2007 at 11:39 AM.
  	
1 members found this post helpful.
Old 11-22-2010, 08:08 AM 	  #13
doru
Member
 
Registered: Sep 2008
Distribution: Ubuntu 8.04 LTS Server
Posts: 83

Rep: Reputation: 16
	
ampersand
Quote:
Originally Posted by na5m View Post
Actually, I don't think that the ampersand strictly means "to the same place as". I think that the ampersand is necessary so that "1" isn't interpreted as a file.
The code
Code:

ls 1>1

will redirect stdout to a file named "1"
Whereas
Code:

ls 1>&1

will redirect stdout to stdout. :)
Good point, see REDIRECTION in bash man. As you can see, the manual is far from clear on this:

Quote:
The general format for redirecting output is:

[n]>word
[...]
>&word
[...] is semantically equiva‐
lent to

>word 2>&1
[...]
The operator

[n]>&word

is used similarly to duplicate output file descriptors. (That is: "If word expands to one or
more digits, the file descriptor denoted by n is made to be a copy of
that file descriptor.", doru's note) If n is not
specified, the standard output (file descriptor 1) is used. If the
digits in word do not specify a file descriptor open for output, a re‐
direction error occurs. As a special case, if n is omitted, and word
does not expand to one or more digits, the standard output and standard
error are redirected as described previously.
Last edited by doru; 11-22-2010 at 11:12 AM. Reason: completion
  	
Old 11-22-2010, 09:27 AM 	  #14
catkin
LQ 5k Club
 
Registered: Dec 2008
Location: No fixed abode
Distribution: Slackware 13.37, Debian Squeeze
Posts: 8,105
Blog Entries: 25

Rep: Reputation: 1040Reputation: 1040Reputation: 1040Reputation: 1040Reputation: 1040Reputation: 1040Reputation: 1040Reputation: 1040
	
Quote:
Originally Posted by haertig View Post
The eagle-eyed observer will notice an apparent discrepancy for "|" ...

ls -l 2>&1 | tee file.txt will log BOTH stdout and stderr from ls to file.txt. But at this time, when stderr was redirected to the same place as stdout, stdout had not yet been redirected to tee via the "|". Hmmm ... gotcha! Just one of those things. Never fails - when you try to rationalize your way through something logically ... you run into an exception! This is one of those things were experience tells me "I know it works this way", but when you press me with the "why?" question, I have to hem and haw and say "Because that's the way it is!"
It makes sense in the light of the shell's operation. The "|" is actioned in "3. Parses the tokens into simple and compound commands" and the ">"s are actioned in "5. Performs any necessary redirections ..." so, in this case, the ls command's stdout is already configured for redirection to the tee command by the time the ">"s are processed.
  	
1 members found this post helpful.
Old 11-22-2010, 11:26 AM 	  #15
doru
Member
 
Registered: Sep 2008
Distribution: Ubuntu 8.04 LTS Server
Posts: 83

Rep: Reputation: 16
	
bash man
Quote:
Originally Posted by catkin View Post
It makes sense in the light of the shell's operation. The "|" is actioned in "3. Parses the tokens into simple and compound commands" and the ">"s are actioned in "5. Performs any necessary redirections ..." so, in this case, the ls command's stdout is already configured for redirection to the tee command by the time the ">"s are processed.
Thank you. My Ubuntu 10.04 bash man does not really say that.

	24.8 Use find to test whether file found and exists

clearly when path is known use -e and -f:

if [ -e "/abcd/efgh/xxxx" ] ; then
   echo "file exist"
else
   echo "file not exist"
fi



    -e: Returns true value if file exists
    -f: Return true value if file exists and regular file

When path not known use find + -z:
if [ ! -z `find /abcd/efgh -name "xxxx" 2>/dev/null` ] ; then
    some files found
else
    no files
fi

	24.9 How to determine the Boost version on a system?
- If you want to figure it out manually (rather than in-code), the go to the include directory, and open up version.hpp.
$ locate boost
$ cat /usr/include/boost/version.hpp 
//  Boost version.hpp configuration header file  ------------------------------//

//  (C) Copyright John maddock 1999. Distributed under the Boost
//  Software License, Version 1.0. (See accompanying file
//  LICENSE_1_0.txt or copy at http://www.boost.org/LICENSE_1_0.txt)

//  See http://www.boost.org/libs/config for documentation

#ifndef BOOST_VERSION_HPP
#define BOOST_VERSION_HPP

//
//  Caution, this is the only boost header that is guarenteed
//  to change with every boost release, including this header
//  will cause a recompile every time a new boost version is
//  released.
//
//  BOOST_VERSION % 100 is the sub-minor version
//  BOOST_VERSION / 100 % 1000 is the minor version
//  BOOST_VERSION / 100000 is the major version

#define BOOST_VERSION 103200

//
//  BOOST_LIB_VERSION must be defined to be the same as BOOST_VERSION
//  but as a *string* in the form "x_y" where x is the major version
//  number and y is the minor version number.  This is used by 
//  <config/auto_link.hpp> to select which library version to link to.

#define BOOST_LIB_VERSION "1_32"

#endif

- code
#include <boost/version.hpp>
#include <iostream>
#include <iomanip>

int main()
{
    std::cout << "Boost version: " << std::hex
              << ((BOOST_VERSION >> 20) & 0xF)
              << "."
              << ((BOOST_VERSION >> 8) & 0xFFF)
              << "."
              << (BOOST_VERSION & 0xFF)
              << std::endl;
    return 0;
}


- another way
std::cout << "Using Boost "     
          << BOOST_VERSION / 100000     << "."  // major version
          << BOOST_VERSION / 100 % 1000 << "."  // minior version
          << BOOST_VERSION % 100                // patch level
          << std::endl;
- Install boost: # yum install boost

	24.10 Sort du output in Human-readable format

Sort du output in Human-readable format

bash$ for i in $(echo -e 'G\nM\nK'); do du -hsx /* 2>/dev/null | grep '[0-9]'$i | sort -rn; done

— by jasembo on April 14, 2012, 4:02 a.m.
Explanation

    echo -e prints G for Gigabytes, M for Megabytes and K for Kilobytes in a line each.
    2>/dev/null send stderr to /dev/null
    sort -rn sorts in reverse numerical order. Largest first

Alternative one-liners
Sort du output in Human-readable format

bash$ for i in G M K; do du -hsx * | grep "[0-9]$i\b" | sort -nr; done 2>/dev/null

— by openiduser3 on April 14, 2012, 9:06 a.m.
Explanation

    The reason to use a for loop is to sort results with G or M or K values separately, otherwise sort -n would just sort everything by the numbers regardless of G M K suffix.
    grep "[0-9]$i\b" matches lines containing a digit followed by G or M or K followed by a "word boundary"


    */

	24.11 How to: Mount an ISO image under Linux
by NIX CRAFT on MARCH 29, 2004 · 58 COMMENTS· LAST UPDATED DECEMBER 11, 2008
in DEBIAN LINUX, FILE SYSTEM, HOWTO

An ISO image is an archive file (disk image) of an optical disc using a conventional ISO (International Organization for Standardization) format. ISO image files typically have a file extension of .ISO. The name "ISO" is taken from the ISO 9660 file system used with CD-ROM media, but an ISO image can also contain UDF file system because UDF is backward-compatible to ISO 9660.

You can mount an ISO images via the loop device under Linux. It is possible to specify transfer functions (for encryption/decryption or other purposes) using loop device.

But, how do you mount an ISO image under Linux? You need to use mount command as follows:

Procedure to mount ISO images under Linux

1) You must login as a root user, if not root user then switch to root user using following command:
$ su -

2) Create the directory i.e. mount point:
# mkdir -p /mnt/disk

3) Use mount command as follows to mount iso file called disk1.iso:
# mount -o loop disk1.iso /mnt/disk

4) Change directory to list files stored inside an ISO image:
# cd /mnt/disk
# ls -l

More about loop device

A loop device is a pseudo-device that makes a file accessible as a block device. Loop devices are often used for CD ISO images and floppy disc images. Mounting a file containing a filesystem via such a loop mount makes the files within that filesystem accessible. They appear in the mount point directory using above commands.

	24.12 How big is the pipe buffer?
------------------------------------------------------------------------------------------------------------------------

The capacity of a pipe buffer varies across systems (and can even vary on the same system). I am not sure there is a quick, easy, and cross platform way to just lookup the capacity of a pipe.

Mac OS X, for example, uses a capacity of 16384 bytes by default, but can switch to 65336 byte capacities if large write are made to the pipe, or will switch to a capacity of a single system page if too much kernel memory is already being used by pipe buffers (see xnu/bsd/sys/pipe.h, and xnu/bsd/kern/sys_pipe.c; since these are from FreeBSD, the same behavior may happen there, too).

One Linux pipe(7) man page says that pipe capacity is 65536 bytes since Linux 2.6.11 and a single system page prior to that (e.g. 4096 bytes on (32-bit) x86 systems). The code (include/linux/pipe_fs_i.h, and fs/pipe.c) seems to use 16 system pages (i.e. 64 KiB if a system page is 4 KiB), but the buffer for each pipe can be adjusted via a fcntl on the pipe (up to a maximum capacity which defaults to 1048576 bytes, but can be changed via /proc/sys/fs/pipe-max-size)).

Here is a little bash/*perl* combination that I used to test the pipe capacity on my system:

#!/bin/bash
test $# -ge 1 || { echo "usage: $0 write-size [wait-time]"; exit 1; }
test $# -ge 2 || set -- "$@" 1
bytes_written=$(
{
    exec 3>&1
    {
        perl -e '
            $size = $ARGV[0];
            $block = q(a) x $size;
            $num_written = 0;
            sub report { print STDERR $num_written * $size, qq(\n); }
            report; while (defined syswrite STDOUT, $block) {
                $num_written++; report;
            }
        ' "$1" 2>&3
    } | (sleep "$2"; exec 0<&-);
} | tail -1
)
printf "write size: %10d; bytes successfully before error: %d\n" \
    "$1" "$bytes_written"
Here is what I found running it with various write sizes on a Mac OS X 10.6.7 system (note the change for writes larger than 16KiB):

% /bin/bash -c 'for p in {0..18}; do /tmp/ts.sh $((2 ** $p)) 0.5; done'
write size:          1; bytes successfully before error: 16384
write size:          2; bytes successfully before error: 16384
write size:          4; bytes successfully before error: 16384
write size:          8; bytes successfully before error: 16384
write size:         16; bytes successfully before error: 16384
write size:         32; bytes successfully before error: 16384
write size:         64; bytes successfully before error: 16384
write size:        128; bytes successfully before error: 16384
write size:        256; bytes successfully before error: 16384
write size:        512; bytes successfully before error: 16384
write size:       1024; bytes successfully before error: 16384
write size:       2048; bytes successfully before error: 16384
write size:       4096; bytes successfully before error: 16384
write size:       8192; bytes successfully before error: 16384
write size:      16384; bytes successfully before error: 16384
write size:      32768; bytes successfully before error: 65536
write size:      65536; bytes successfully before error: 65536
write size:     131072; bytes successfully before error: 0
write size:     262144; bytes successfully before error: 0
Note: The PIPE_BUF value defined in the C header files (and the pathconf value for _PC_PIPE_BUF), does not specify the capacity of pipes, but the maximum number of bytes that can be written atomically (see POSIX write(2)).

Quote from include/linux/pipe_fs_i.h:

/* Differs from PIPE_BUF in that PIPE_SIZE is the length of the actual
   memory allocation, whereas PIPE_BUF makes atomicity guarantees.  */

------------------------------------------------------------------------------------------------------------------------
In bash, ulimit -p tells you.

$ ulimit -p
8

$ ulimit -a | grep pipe
pipe size            (512 bytes, -p) 8
so on my system it's 8 * 512 = 4096 bytes.

If you're not using bash, you can use either PIPE_BUF from <limits.h>, e.g.:

/usr/include/linux/limits.h: #define PIPE_BUF 4096
Or pathconf, e.g. using Python:

>>> os.pathconf('.', os.pathconf_names['PC_PIPE_BUF'])
4096
shareimprove this answer
edited Apr 24 '11 at 23:46

answered Apr 24 '11 at 23:29

Mikel
19.2k54168
2	 	
Since ulimit is a shell built-in, it matters which shell you do it from -- this appears to be bash. I tried it in zsh and it didn't recognize -p or output anything about pipe size with -a –  Michael Mrozek♦ Apr 24 '11 at 23:32 
  	 	
Updated to be bash specific. I guess zsh users should look at limits.h? –  Mikel Apr 24 '11 at 23:38
  	 	
Added pathconf and example Python program. –  Mikel Apr 24 '11 at 23:48
4	 	
Neither PIPE_BUF nor its pathconf equivalent are the same as the buffer capacity of a pipe (e.g. PATH_BUF is 512 on Mac OS X 10.6.7 (and bash -c 'ulimit -p' gives 1), but most pipes can actually buffer either 16Ki or 64Ki bytes). –  Chris Johnsen Apr 25 '11 at 5:34 
3	 	
PIPE_BUF is the largest size for which writes are guaranteed to be atomic. Your pipe size is probably 64k. See here: stackoverflow.com/questions/4624071/… –  stephenbez May 30 '13 at 17:54

------------------------------------------------------------------------------------------------------------------------

up vote
11
down vote
this shell-line can show pipe buffer size too:

M=0; while true; do dd if=/dev/zero bs=1k count=1 2>/dev/null; \
       M=$(($M+1)); echo -en "\r$M KB" 1>&2; done | sleep 999
(sending 1k chunks to blocked pipe until buffer full) ...some test outputs:

64K (intel-debian), 32K (aix-ppc), 64K (jslinux bellard.org)               ...Ctrl+C.
shareimprove this answer
edited Apr 17 '13 at 23:16

answered Apr 17 '13 at 22:52

Omer Merdan
16517
3	 	
Very nice! (dd if=/dev/zero bs=1 | sleep 999) & then wait a second and killall -SIGUSR1 dd gives 65536 bytes (66 kB) copied, 5.4987 s, 11.9 kB/s - same as your solution, but at 1 byte resolution ;) –  frostschutz Apr 17 '13 at 23:21
------------------------------------------------------------------------------------------------------------------------
[root@cow2 ~]# M=0; while true; do dd if=/dev/zero bs=1k count=1 2>/dev/null; \
>        M=$(($M+1)); echo -en "\r$M KB" 1>&2; done | sleep 999
64 KB
^C
[root@cow2 ~]# M=0; while true; do dd if=/dev/zero bs=1k count=1 2>/dev/null;        M=$(($M+1)); echo -en "\r$M KB" 1>&2; done | sleep 999
64 KB
^C
[root@cow2 ~]# (dd if=/dev/zero bs=1 | sleep 999) &
[1] 17147
[root@cow2 ~]# killall -SIGUSR1 dd
65537+0 records in
65536+0 records out
65536 bytes (66 kB) copied, 7.71049 s, 8.5 kB/s


	24.13 Get exit status of process that's piped to another
I have two processes foo and bar, connected with a pipe:

$ foo | bar
bar always exits 0; I'm interested in the exit code of foo. Is there any way to get at it?
------------------------------------------------------------------------------------------------------------------------
There are 3 common ways of doing this:

Pipefail
The first way is to set the pipefail option (ksh, zsh or bash). This is the simplest and what it does is basically set the exit status $? to the exit code of the last program to exit non-zero (or zero if all exited successfully).

# false | true; echo $?
0
# set -o pipefail
# false | true; echo $?
1
$PIPESTATUS
Bash also has an array variable called $PIPESTATUS ($pipestatus in zsh) which contains the exit status of all the programs in the last pipeline.

# true | true; echo "${PIPESTATUS[@]}"
0 0
# false | true; echo "${PIPESTATUS[@]}"
1 0
# false | true; echo "${PIPESTATUS[0]}"
1
# true | false; echo "${PIPESTATUS[@]}"
0 1
You can use the 3rd command example to get the specific value in the pipeline that you need.

Separate executions
This is the most unwieldy of the solutions. Run each command separately and capture the status

# OUTPUT="$(echo foo)"
# STATUS_ECHO="$?"
# printf '%s' "$OUTPUT" | grep -iq "bar"
# STATUS_GREP="$?"
# echo "$STATUS_ECHO $STATUS_GREP"
0 1
------------------------------------------------------------------------------------------------------------------------
If you are using bash, you can use the PIPESTATUS array variable to get the exit status of each element of the pipeline.

$ false | true
$ echo "${PIPESTATUS[0]} ${PIPESTATUS[1]}"
1 0
------------------------------------------------------------------------------------------------------------------------

	24.14 Find which user is running a process, also get user name from ID
[root@yizaq-ise6 ~]# ps aux | grep lwsmd
318       7033  0.0  0.1 1826892 18264 ?       Sl   Sep10   0:01 /opt/pbis/sbin/lwsmd --start-as-daemon
root     27261  0.0  0.0 103212   840 pts/0    S+   18:52   0:00 grep lwsmd
[root@yizaq-ise6 ~]# cat /etc/passwd | grep 318
iseadagent:x:318:300::/home/iseadagent:/bin/bash

	24.15
	24.16
	24.17
	24.18
	24.19
	24.20
	24.21
	24.22
	24.23
	24.24
	24.25
	24.26
	24.27
	24.28
	24.29
	24.30
	24.31
	24.32
	24.33
	24.34
	24.35
	24.36
	24.37
	24.38
	24.39
	24.40
	24.41
	24.42
	24.43
	24.44
	24.45
	24.46
	24.47
	24.48
	24.49
	24.50
	24.51
	24.52
	24.53
	24.54
	24.55
	24.56
	24.57
	24.58
	24.59
	24.60
	24.61
	24.62
	24.63

25. NFS

	25.1 http://www.troubleshooters.com/linux/nfs.htm
Contents:

Executive Summary:
Directories and IP addresses used in these examples
Get the Daemons Running
Configure the NFS Server
Mounting an NFS Share on a Client
Gotchas
1: Check the Daemons on the Server
2: Eyeball the Syntax
3: Carefully read error messages and develop a symptom description
4: If it mounts but can't access, check permissions, gid's and uid's
5: If there are still problems, disable firewalls or log firewalls
6: If there are still problems, investigate the server's DNS, host name resolution, etc
Summary
Executive Summary:

NFS is the best thing since sliced bread. It stands for Network File System. NFS is a file and directory sharing mechanism native to Unix and Linux.

NFS is conceptually simple. On the server (the box that is allowing others to use its disk space), you place a line in /etc/exports to enable its use by clients. This is called sharing. For instance, to share /home/myself for both read and write on subnet 192.168.100, netmask 255.255.255.0, place the following line in /etc/exports on the server:
/home/myself 192.168.100.0/24(rw)
To share it read only, change the (rw)to (ro).

On the client wanting to access the shared directory, use the mount command to access the share:
mkdir /mnt/test
mount -t nfs -o rw 192.168.100.85:/home/myself /mnt/test
The preceding must be performed by user root, or else as a sudo command. Another alternative is to place the mount in /etc/fstab. That will be discussed later in this document.

As mentioned, NFS is conceptually simple, but in practice you'll encounter some truly nasty gotchas:
Non-running NFS or portmap daemons
Differing uid's for the same usernames
NFS hostile firewall
Defective DNS causes timeouts
To minimize troubleshooting time, quickcheck for each of these problems. Each of these gotchas is explained in detail in this document.
Directories and IP addresses used in these examples

The following settings are used in examples on this page:
Server (computer donating disk space) Settings
FQDN hostname = myserver.domain.cxm
IP address = 192.168.100.85
Netmask = 255.255.255.0
RedHat ISO container directory = /scratch/rh8iso
Mandrake RPM container directory = /scratch/mand9iso
Client (computer using the donated disk space) settings
FDQN hostname = mydesk.domain.cxm
IP address = 192.168.100.2
Netmask = 255.255.255.0
Please make note of these settings so that you're not confused in the examples.
Get the Daemons Running

You can't run NFS without the server's portmap and NFS daemons running. This article discusses how to set them to run at boot, and how to check that they're currently running. First you'll use the chkconfig program to set the portmap, nfs and mountd daemons to run at boot. Then you'll check whether these daemons are running. Finally, whether they're running or not, you'll restart these daemons.
Checking the portmap Daemon

In order to run NFS, the portmap daemon must run. Check for automatic running at boot with the following command:

[root@myserver root]# chkconfig --list portmap
portmap         0:off   1:off   2:off   3:on    4:on    5:on    6:off
[root@myserver root]#

Note that in the preceding example, runlevels 3, 4, and 5 say "on". That means that at boot, for runlevels 3, 4 and 5, the portmap daemon is started automatically. If either 3, 4 or 5 say "off", turn them on with the following command:
chkconfig portmap on
Now check that the portmapper is really running, using the ps and grep commands:

[root@myserver root]# ps ax | grep portmap
 3171 ?        S      0:00 portmap
 4255 pts/0    S      0:00 grep portmap
You have new mail in /var/spool/mail/root
[root@myserver root]#

The preceding shows the portmap daemon running at process number 3171.
Checking the NFS Daemon

Next perform the exact same steps for the NFS daemon. Check for automatic run at boot:

[root@myserver root]# chkconfig --list nfs
nfs             0:off   1:off   2:on    3:on    4:on    5:on    6:off
[root@myserver root]#

If either of runlevels 3, 4 or 5 say "off", turn all 3 on with the following command:
chkconfig nfs on
And check that the NFS and the mountd daemons are running as follows:
ps ax | grep nfs
ps ax | grep mountd
You might get several different different nfs daemons -- that's OK.
Restarting the Daemons

When learning or troubleshooting, there's nothing so wonderful as a known state. On that theory, restart the daemons before proceeding. Or if you want a truly known state, reboot. Always restart the portmap daemon BEFORE restarting the NFS daemon. Here's how you restart the two daemons:
service portmap restart
service nfs restart
You'll see messages on the screen indicating if the startups were successful. If they are not, troubleshoot. If they are, continue.

Note that the mountd daemon is started by restarting nfs.

You don't need to restart these daemons every time. Now that you've enabled the daemons on reboot, you can safely assume they're running (unless there's an NFS problem -- then don't make this assumption). From now on the only time you need to restart the NFS daemon is when you change /etc/exports. Theoretically you should never need to restart the portmap daemon unless there's a problem.
Summary

NFS requires a running portmap daemon and NFS daemon. Use chkconfig to make sure these daemons run at boot, make sure they're running, and the first time, restart the portmap daemon and then the NFS daemon just to be sure you've achieved a known state.
Configure the NFS Server

Configuring an NFS server is as simple as placing a line in the /etc/exports file. That line has three pieces of information:
The directory to be shared (exported)
The computer, NIS group, hostname, domain name or subnet allowed to access that directory
Any options, such as ro or rw, or several other options
There's one line for each directory being shared. The general syntax is:
directory_being_shared subnet_allowed_to_access(options)
Here's an example:
/home/myself 192.168.100.0/24(ro)
In the preceding example, the directory being shared is /home/myself, the subnet is 192.168.100.0/24, and the options are ro(read only). The subnet can also be a single host, in which case there would be an IP address with no bitmask (the /24 in the preceding example). Or it can be an NIS netgroup, in which case the IP address is replaced with @groupname. You can use a wildcard such as ? or * to replace part of a Fully Qualified Domain Name (FQDN). An example would be *.accounting.domain.cxm. Do not use wildcards in IP addresses, as they are intermittent in IP addresses.

There are two kinds of options: General options and User ID Mapping options. Read on...
General Options

Many options can go in the parentheses. If more than one, they are delimited by commas. Here are the common options:
Option
What it does
Comment
ro
Read Only
The share cannot be written. This is the default.
rw
Read Write
The share can be written.
secure
Ports under 1024
Requires that requests originate on a port less  than IPPORT_RESERVED (1024). This is the default.
insecure
Negation of secure

async
Reply before disk write
Replies to requests before the data is written to disk. This improves performance, but results in lost data if the server goes down.
sync	Reply only after disk write	Replies to the NFS request only after all data has been written to disk. This is much safer than async, and is the default in all nfs-utils versions after 1.0.0.
no_wdelay
Write disk as soon as possible
NFS has an optimization algorithm that delays disk writes if NFS deduces a likelihood of a related write request soon arriving. This saves disk writes and can speed performance.
BUT...
If NFS deduced wrongly, this behavior causes delay in every request, in which case this delay should be eliminated. That's what the no_wdelay option does -- it eliminates the delay. In general, no_wdelay is recommended when most NFS requests are small and unrelated.
wdelay	Negation of no_wdelay
This is the default.
nohide
Reveal nested directories
Normally, if a server exports two filesystems one of which is mounted on the other, then  the  client  will  have  to mount  both filesystems explicitly to get access to them.  If it just mounts the parent, it will see an empty  directory  at  the place where the other filesystem is mounted.  That filesystem is "hidden".

Setting the nohide option on a filesystem causes it  not  to  be hidden,  and  an appropriately authorised client will be able to move from the parent to that  filesystem  without  noticing  the change.

However,  some  NFS clients do not cope well with this situation as, for instance, it is then possible for two files in  the  one apparent filesystem to have the same inode number.

The  nohide  option  is  currently only effective on single host exports.  It does not work reliably with  netgroup,  subnet,  or wildcard exports.

This option can be very useful in some situations, but it should be used with due care, and only after confirming that the client system copes with the situation effectively.
hide
Negation of nohide
This is the default
subtree_check
Verify requested file is in exported tree
This is the default. Every file request is checked to make sure that the requested file is in an exported subdirectory. If this option is turned off, the only verification is that the file is in an exported filesystem.
no_subtree_check	Negation of subtree_check
Occasionally, subtree checking can produce problems when a requested file is renamed while the client has the file open. If many such situations are anticipated, it might be better to set no_subtree_check. One such situation might be the export of the /home filesystem. Most other situations are best handed with subtree_check.
secure_locks
Require authorization for lock requests
This is the default. Require authorization of all locking requests.
insecure_locks
Negation of secure_locks
Some NFS clients don't send credentials with lock requests, and hence work incorrectly with secure_locks., in which case you can only lock world-readable files. If you have such clients, either replace them with better ones, or use the insecure_locks option.
auth_nlm
Synonym for secure_locks

no_auth_nlm
Synonym for secure_locks

User ID Mapping Options

In an ideal world, the user and group of the requesting client would determine the permissions of the data returned. We don't live in an ideal world. Two real-world problems intervene:
You might not trust the root user of a client with root access to the server's files.
The same username on client and server might have different numerical ID's
Problem 1 is conceptually simple. John Q. Programmer is given a test machine for which he has root access. In no way does that mean that John Q. Programmer should be able to alter root owned files on the server. Therefore NFS offers root squashing, a feature that maps uid 0 (root) to the anonymous (nfsnobody) uid, which defaults to -2 (65534 on 16 bit numbers).

So when John Q. Programmer mounts the share, he can access only what the anonymous user and group can access. That means files that are world readable or writeable, or files that belong to either user nfsnobody or group nfsnobodyand allow access by the user or group. One way to do this is to export a chmod 777 directory (booooooo). A better way is to export a directory belonging to user nfsnobody or group nfsnobody, and permissioning accordingly. Now root users from other boxes can write files in that directory, and read the files they write, but they can't read or write files created by root on the server itself.

Now that you know what root squashing is, how do you enable or disable it on a per-share basis? If you want to enable root squashing, that's simple, because it's the default. If you want to disable it, so that root on any box operates as root within the mounted share, disable it with the no_root_squash option as follows:
/data/foxwood 192.168.100.0/24(rw,no_root_squash)
If, for documentation purposes or to guard against a future change in the default, you'd like to explicitly specify root squashing, use the root_squash option.

Perhaps you'd like to change the default anonymous user or group on a per-share basis. That way the client's root user can access files within the share as a specific user, let's say user myself. No problem. Use the anonuid or anongid option. The following example uses the anongid option to access the share as group myself., assuming that on the server group myself has gid 655:
/data/wekiva 192.168.100.0/24(rw,anongid=655)
The preceding makes the client's root user group 655 (which happens to be group myself on share /data/wekiva. Files created by the client's root user are user and group 655, but files modified by the client's root are group 655, and a different user.

Now imagine that instead of mapping incoming client root requests to the anonymous user or group, you want ALL incoming NFS requests to be mapped to the anonomous user or the anonymous group. To accomplish that you use the all_squashoption, as follows:
/data/altamonte 192.168.100.0/24(rw,all_squash)
You can combine the all_squash option with the anonuidandanongid options to make directories accessible as if the incoming request was from that user or that group. The one problem with that is that, for NFS purposes, it makes the share world readable and/or world writeable, at least to the extent of which hosts are allowed to mount the share.

We'll get into this subject a little bit more when discussing the Gotcha concerning different user and group id numbers.

The following table lists the User ID Mapping Options:


Option
What it does
Comment
 root_squash
Convert incoming requests from user root to the anonymous uid and gid.
This is the default.
no_root_squash
Negation of root_squash

anonuid	Set anonymous user id to a specific id
The id is a number, not a name. This number can be obtained by this command on the server:
grep myself /etc/passwd
Where myself is the username whose uid you want to find.
anongid	Set anonymous group id to a specific id	The id is a number, not a name. This number can be obtained by this command on the server:
grep myself /etc/group
Where myself is the name of the group whose uid you want to find.
all_squash
Convert incoming requests, from ALL users, to the anonymous uid and gid.	Remember that this gives all incoming users the same set of rights to the share. This may not be what you want.

Mounting an NFS Share on a Client

Mounting an NFS share on a client can be simple. At its simplest it might look like this:
mount -t nfs -o ro 192.168.100.85:/data/altamonte /mnt/test
The English translation of the preceding is this: mount type (-t) nfs with options (-o) read only (ro) server 192.168.100.85's directory /data/altamonteat mount point /mnt/test. What usually changes is the comma delimited list of options (-o). For instance, NFS typically performs better with rsize=8192and wsize=8192. These are the read and write buffer sizes, and it's been found that in general 8192 performs better than the default 4096. Thehard option keeps the request alive even if the server goes down, whereas the softoption enables the mount to time out if the server goes down. The hardoption has the advantage that whenever the server comes back up, the file activity continues where it left off.

Besides these and a few other NFS specific options, there are filesystem independent options such as async/sync/dirsync, atime/noatime, auto/noauto, defaults,dev/nodev, exec/noexec, _netdev, remount, ro, rw, suid/nosuid, user/nouser.
Option
Action
Default?
Comment
Negation
option
async	All I/O done asynchronously
Y
Better performance, more possiblity of corruption when things crash. Do not use when the same file is being modified by different users.
sync
sync	All I/O done synchronously	N
Less likelihood of corruption, less likelihood of overwrite by other users.
async
dirsync	All I/O to directories done synchronously
N


atime	Update inode access time for each  access.
Y

noatime
auto	Automatic mounting.
Y
Can be mounted with the -a option. Mounted at boot time.	noauto
defaults	Shorthand for default options.

rw,suid,dev,exec,auto,nouser,async.

dev	Device
Y
Interpret character or block special devices on the  file system.
nodev
exec	Permit execution of binaries.
Y

noexec
_netdev	Device requires network.

The  device holding the filesystem requires network access. Do not mount until the network has been enabled.

remount	Remount a mounted system.

Used to change the mount flags, especially to toggle between rw and ro.

ro	Allow only read access.
N
Used to protect the mounted filesystem from writes. Even if the filesystem is writeable by the user, and is exported writeable, this still protects it.
rw
rw	Allow both read and write.
Y
Allow writing to the filesystem, assuming that the system is writeable by the user and has been exported writeable.
ro
suid	Allow set-user-identifier and/or set-group-identifier bits to take effect.
Y

nosuid
user	Allow mounting by ordinary user.
N
When used in /etc/fstab, this allows mounting by an ordinary user. Only the user performing the mount can unmount it.
nouser
users
Allow mounting and dismounting by arbitrary user.	N
When used in /etc/fstab, this allows mounting by an ordinary user. Any user can unmount it at any time, regardless of who initially mounted it.	

/etc/fstab syntax

Like any other mount, NFS mounting can be done in /etc/fstab. The advantages to placing it in /etc/fstab are:
It can be mounted automatically (auto) either with mount -a or on boot.
It can easily be configured to be mountable by ordinary users (user or users).
The mount is documented in /etc/fstab.
The disadvantages to placing a mount in /etc/fstab are:
/etc/fstab can become cluttered by too many mounts.
The mountpoint cannot be used for different filesystems.
The following example shows an NFS mount: 192.168.100.85:/home/myself   /mnt/test  nfs  users,noauto,rw   0   0
The preceding is a typical example. Just like other /etc/fstab mounts, NFS mounts in /etc/fstab have 6 columns, listed in order as follows:
The filesystem to be mounted (192.168.100.85:/home/myself)
The mountpoint (/mnt/test)
The type of the filesystem (nfs)
The options (users,noauto,rw)
Frequency to be dumped (a backup method)  (0)
Order in which to be fsck'ed at boot time.  (0). The root filesystem should have a value of 1 so it gets fsck'ed first. Others should have 2 or more so they get fsck'ed later. A value of 0 means don't perform the fsck at all.
Summary

The server exports a share, but to use it the client must mount that share. The mount is performed with a mount command, like this:
mount -t nfs -o rw 192.168.100.85:/data/altamonte /mnt/test
That same mount can be performed in /etc/fstab with the following syntax: 192.168.100.85:/data/altamonte   /mnt/test   nfs   rw   0   0
There are many mount options that can be used, and those are listed in this article.
Gotchas

If you've worked with NFS, you know it's not that simple. Often times the mount fails, times out, or takes so long as to discourage use. Sometimes the mount succeeds but the data is inaccessible. These problems can be a bear to troubleshoot.

To make troubleshooting easier this article lists the usual causes of NFS failure, ways to quickly check whether these problems are the cause, and methods to overcome these problems. Here are the typical causes of NFS problems:
The portmap or nfs daemons are not running
Syntax error on client mount command or server /etc/exports
A space between the mount point and the (rw) causes the (rw) to be ignored.
Problems with permissions, uid's and gid's
Firewalls filtering packets necessary for NFS. The offending firewall is typically on the server, but it could also be on the client.
Bad DNS on server (including /etc/resolv.conf on the server).
!! WARNING !!

Always restart the nfs service after making a change to /etc/exports. Otherwise your changes will not be recognized, leading you down a long and winding dead end.

Cause category
Symptom
The portmap or nfs daemons are not running	Typically, failure to mount
Syntax error on client mount command or server's /etc/exports
Typically, failure to mount or failure to write enable. A space between the mount point and the (rw) causes the share to be read-only -- a frustrating and hard to diagnose problem.
Problems with permissions, uid's and gid's	Mounts OK, but access to the data is impossible or not as specified
Firewalls filtering packets necessary for NFS	Mount failures, timeouts, excessively slow mounts, or intermittent mounts
Bad DNS on server	Mount failures, timeouts, excessively slow mounts, or intermittent mounts
Here's your predefined diagnostic:
Check the daemons on the server
Eyeball the syntax of the client mount command and the server /etc/exports. Pay particular attention that the mountpoint is NOT separated from the parenthasized options list, because a space between the mountpoint and the opening paren causes the options to be ignored.
Carefully read error messages and develop a symptom description
If the symptom involves successful mounts but you can't correctly access the data, check permissions, gid's and uid's. Correct as necessary.
If there are still problems, disable firewalls or log firewalls. 
If there are still problems, investigate the server's DNS, host name resolution, etc.
For maximum diagnostic speed, quickly check that the portmap and nfs daemons are running on the server. If not, investigate why not. Next, eyeball the syntax on the client's mount command and the server's /etc/exports file. Look for not only bad syntax, but wrong information such as wrong IP addresses, wrong filesystem directories, and wrong mountpoints. If you find bad syntax, correct it. These two steps should take no more than 3 minutes, and will find the root cause in many cases.

Next, carefully read the error message, and formulate a symptom description. Try to determine whether the mount has succeeded. If the mount succeeded but you can't access the data, it's likely a problem with permissions, uid's or gid's. Investigate that. If the mount succeeds but it's slow, investigate firewalls and DNS. A healthy NFS system should mount instantaneously. By the time you lift your finger off the Enter key, the mount should have been completed. If it takes more than one second, there's a problem that bears investigation.

The hardest problems are those in which you experience mount failures, timeouts, excessively slow mounts, or intermittent mounts. In such situations, it's likely either a firewall problem or a server DNS problem. Investigate those.

Each of these problem categories is discussed in an article later in this document.
1: Check the Daemons on the Server

This will take you all of a minute. Perform the following 2 commands on the server:
ps ax | grep portmap
ps ax | grep nfs
If either shows nothing (or if it shows just the grep command), that server is not running. Investigate why. Start by seeing if it's even set to run at boot:
/sbin/chkconfig --list portmap
/sbin/chkconfig --list nfs
Each command will output a line showing the run levels at which the command is on. If either one is not on at any runlevel between 3 and 5 inclusive, turn it on with one or both of these commands:
/sbin/chkconfig portmap on
/sbin/chkconfig nfs on
The preceding commands set it to fire at boot, but do not run the daemon. You must run them manually:
service portmap restart
service nfs restart
Always restart the portmap daemon before restarting the nfs daemon, because NFS needs the portmapper to function. If either of those commands fails or produces an error message, investigate.

IMPORTANT NOTE: Even if the daemons were both running when you investigated, restart them both anyway. First, you might see an error message. Second, it's always nice to achieve a known state. Restarting these two daemons should take a minute. That one minute is a tiny price to pay for the peace of mind you achieve knowing that there's no undiscovered problem with the daemons.

If NFS fails to start, investigate the syntax in /etc/exports, and possibly comment out everything in that file, and try another restart. If that changes the symptom, divide and conquer. If restarting NFS takes a huge amount of time, investigate the server's DNS.
2: Eyeball the Syntax

If the daemons work, eyeball the syntax of the mount command on the client and the /etc/exports file on the server. Obviously, if you use the wrong syntax (or wrong IP addresses or directories) in your mount command, the mount fails. You needn't take a great deal of time -- just verify that the syntax is correct and you're using the correct IP addresses, directories and mount points. Correct as necessary, and retest.

Pay SPECIAL attention to make sure there is no space between the mountpoint and the opening paren of the options list. A space between them causes the options to be ignored -- clearly not what you want. If you can't figure out why a mount is read-only, even though the client mount command specifies read-write and the server's directory is clearly read-write with the correct user and group (not a number, but an actual name), suspect this intervening space.
!! WARNING !!

Always restart the nfs service after making a change to /etc/exports. Otherwise your changes will not be recognized, leading you down a long and winding dead end.
3: Carefully read error messages and develop a symptom description

The first two steps were general maintenance -- educated guesses designed to yield quick solutions. If they didn't work, it's time to buckle down and troubleshoot. The first step is to read the error message, and learn more about it. You might want to check the system logs (start with /var/log/messages) in case relevent messages were written.

Try several mounts and umounts, and note exactly what the malfunction looks like:
Does the mount produce an error message?
Does the mount time out?
Does the mount appear to hang forever (more than 5 minutes)?
Does the mount appear to succeed, but the data can't be seen, read or written as expected?
Does the symptom change over time, or with reboots?
The more you learn and record about the symptom, the better your chances of quickly and accurately solving the problem.
4: If it mounts but can't access, check permissions, gid's and uid's

Generally speaking, the permissions on the server don't affect the mounting or unmounting of the NFS share. But they very much affect whether such a share can be seen, executed, read or written. Often the cause is obvious. If the directory is owned by root, permissioned 700, it obviously can't be read and written by user myself.  This type of problem is easy to diagnose and fix.

Tougher are root squashing problems. You access an NFS share as user root, and yet you can't see the mounted share or its contents. You need to remember this is probably happening because on the server you're operating not as root, but as the anonomous user. A quick test can be done by changing the server's export to export to a no_root_squash and single IP address (for security). If the problem goes away, it's a root squashing problem. Either access it as non-root, or change the ownership of the directory and contents to the anonomous gid or uid.

By far the toughest problems are caused by non-matching uid's and gid's. Let's say you share your home directory on the server, and you log in as yourself on the client and mount that share. It mounts ok (we'll assume you used su -c or sudo to mount it), but you can't read the data -- permission denied!

That's crazy. The directory you're sharing is owned by myself, and you're logged into the client as myself, and yet you don't have permission to read. What's up?

It turns out that under the hood, NFS requests contain numeric uid's and gid's, but not actual usernames or groupnames. What that means is that if user myself is uid 555 on the server, but uid 600 on the client, you're trying to access files owned by uid 555 when you're uid 600. That means your only rights to the mounted material are permissions granted to "other" -- not to "user" or "group".

The best solution to this problem is to create a system in which all boxes on your network have the same uid for each username and the same gid for each groupname. This can be accomplished either by attention to detail, by using NIS to assign users and groups, or by using some other authentication scheme yielding global users and groups.

If you cannot have a single uid for all instances of a username, suboptimal steps must be taken. In some instances you could make the directory and files world-readable, thereby enabling all users to read it. It could also be made world-writeable, but that's always a bad idea. It could be mounted all_squash with a specific anonuid and/or a specific anongid to cure the problem, but once again, at least from the NFS viewpoint, that's equivalent to making it world readable or writeable.

If you have problems accessing mounts, always check the gid's and uid's on both sides and make sure they match. If they don't, find a way of fixing it. Sometimes it's as simple as editing /etc/passwd and /etc/group to change the numeric ID's on one or both sides. Remember that if you do that, you need to perform the proper chown command on any files that were owned or grouped by the owner and/or group that you renumbered. A dead giveaway are files that are listed with numbers rather than names for group and user.
5: If there are still problems, disable firewalls or log firewalls

Many supposed NFS problems are really problems with the firewall. In order for your NFS server to successfully serve NFS shares, its firewall must enable the following:
ICMP Type 3 packets
Port 111, the Portmap daemon
Port 2049, NFS
The port(s) assigned to the mountd daemon
The easiest way to see whether your problem resides in the firewall is to completely open up the client and server firewalls and anything in between. For details on how to manipulate iptables see the May 2003 Linux Productivity Magazine.

Note that opening up firewalls is appropriate only if you're disconnected from the Internet, or if you're in a very un-hostile environment. Even so, you should open up the firewalls for a very short time (less than 5 minutes). If in doubt, instead of opening the firewalls, insert logging statements in IPTables to show what packets are being rejected during NFS mounts, and take action to enable those ports. For details on IPTables diagnostic logging, see the May 2003 Linux Productivity Magazine.

The mountd daemon ports are especially problematic, because they're normally assigned by the portmap daemon, and vary from NFS restart to NFS restart. The /etc/rc.d/init.d/nfs script can be changed to nail down the mountd daemon to a specific port, which then enables you to pinhole a specific port. The A Somewhat Practical Server Firewall article in the May 2003 Linux Productivity Magazine. explains how to do this.

If for some reason you don't want to nail down the port, your only other alternatives are to create a firewall enabling a huge range of ports in the 30000's, or to create a master NFS restart script which does the following:
Use the rcpinfo program to find all ports used by mountd.
Issue iptables commands to find the rule numbers for those ports.
Issue iptables commands to delete all rules on those ports.
Restart NFS
Use the rcpinfo program to find all ports used by mountd.
Issue iptables commands to insert rules for those ports where the rules for those ports used to be.
One technique that might make that easier is to create a user defined chain just to hold mountd rules. In that case you'd simply empty that chain, restart NFS, use rpcinfo to find the port numbers, and add the proper rules using the iptables -A command.

It bears repeating that the May 2003 Linux Productivity Magazine details how to create an NFS friendly firewall.
6: If there are still problems, investigate the server's DNS, host name resolution, etc

Bad forward and reverse name resolution can mess up any server app, including NFS. Like other apps, bad DNS most often results in very slow performance or timeouts. Be sure to check your /etc/resolv.conf and make sure you're querying the correct DNS server. Check your DNS server with DNSwalk or DNS lint or another suitable utility.
Summary

NFS is wonderful. It's a convenient and lightning fast way to use a network. Although it's not particularly secure, its security can be beefed up with firewalls. Its security can also be strengthened by authentication schemes.

Although conceptually simple, NFS often requires overcoming troubleshooting challenges before a working system is achieved. Here's a handy predefined diagnostic:
Check the daemons on the server
Eyeball the syntax of the client mount command and the server /etc/exports
Carefully read error messages and develop a symptom description
If the symptom involves successful mounts but you can't correctly access the data, check permissions, gid's and uid's. Correct as necessary.
If there are still problems, disable firewalls or log firewalls. 
If there are still problems, investigate the server's DNS, host name resolution, etc.
If you suspect firewall problems are stopping your NFS, see the May 2003 Linux Productivity Magazine , which details IPTables and how to create an NFS-friendly firewall.
	25.2

26. Linux system install

	26.1 Install from ISO

		26.1.1  Using kickstart

			26.1.1.1  How Kickstart Works http://kickstart-tools.sourceforge.net/howkickstartworks.html
This document will describe the pieces involved in the kickstart process and how they work together.

    What is Kickstart?
    The Bootdisk
    The Kickstart Server
    Postinstall
    Making better use of KickStart

What is Kickstart?
Due to the need for automated installation, Red Hat has created the kickstart installation method. With this method, a system administrator can create a single file containing the answers to all the questions that would normally be asked during a typical Red Hat interactive Linux installation.

Kickstart files can be kept on single server system, and read by individual computers during the installation. The kickstart installation method is powerful enough that often a single kickstart file can be used to install Red Hat Linux on multiple machines, making it ideal for network and system administrators.

Kickstart lets you automate most of a Red Hat Linux installation, including:

    Language selection
    Network configuration
    Keyboard selection
    Boot loader installation (LILO)
    Disk partitioning
    Mouse Selection
    X Window System configuration

Kickstart installations can only be performed using a local CD-ROM, a local hard drive, or NFS installation methods. FTP and HTTP installations cannot be automated using kickstart mode. (Starting with RedHat 7.0 and higher, FTP and HTTP are now supported)

To use kickstart mode, you must first create a kickstart file (ks.cfg), and make it available to the Red Hat Linux installation program.

Where to put a KickStart File:

A KickStart file must be placed in one of two locations.

    On a network
    On a bootdisk

To do a network-based KickStart installation, you must have a BOOTP/DHCP server on your network, and it must include configuration information for the machine on which you are attempting to install Red Hat Linux. The BOOTP/DHCP server will be used to give the client its networking information as well as the location of the KickStart file.

To perform a diskette-based KickStart installation, the KickStart file must be named ks.cfg, and reside in the boot disk's top-level directory. Note that the RedHat Linux boot disks are in MS-DOS format, making it easy to copy the KickStart file under Linux using the mcopy command.

In the case of the KickStart-Tools package the we have put together it was decited that we would go with locating the ks.cfg file on the floppy bootdisk.
Even if we put the ks.cfg on the network somewhere, the client would still need to use a boot floppy to load the kernel so that it can ask the nework for the ks.cfg file. Since we have to use a boot floppy anyway, we figured it would be alot easier to just put the ks.cfg file on the floppy and not worry about changing the BOOTP/DHCP servers at all. As of RedHat Linux 7.0 there is now support for booting over the network using PXIE. However many of our systems don't support PXIE so we decided to still stay with the floppy bootdisk method.

( Go to top )


The Bootdisk

The Bootdisk accomplishes several things. It contains the Linux kernel which is booted so that it can run the KickStart process.
It also contains a configuration files that is used to tell KickStart what to do.
By default the "Redhat" bootdisk will do a regular interactive install.

If at the boot prompt you specify: linux ks=floppy the installer will switch into KickStart mode and install linux using the configuration recorded in the ks.cfg file on the floppy.

To simplify the creation of the boot disk a cgi was put together so that it would ask the user the pertinent questions, then create the ks.cfg file for them, and then create a batch/shell script that can be used to create the boot floppy.

Specifically, the batch file or shell script will create a basic boot floppy and then copy across 3 files. They are: syslinux.cfg, boot.msg, and the ks.cfg.

    syslinux.cfg is the file that is read before the kernel loads. It is similar in functionality to lilo.conf. We have modified syslinux.cfg so that it runs linux ks=floppy by default if you press enter at the boot prompt. If you want to do a regular interactive install using the floppy then you can just type linux at the boot prompt.
    boot.msg contains the text that is displayed at the boot prompt.
    ks.cfg is the KickStart configuration file.


The kernel that is on the standard bootnet.img bootdisk image has been designed to work specifically for desktop PC's. It will not work with notebooks that require PCMCIA services in order to get onto the network. for that you need to use pcmcia.img.
The various bootdisk images can be found the images directory on the RedHat installation CD.


( Go to top )
The KickStart Server

Once the bootdisk lauches the kernel, it looks in the ks.cfg file to figure out where it is going to get it's root file system.
This is specified in the NFS line in ks.cfg. For example, here is the default nfs line:

nfs --server KickStart.cisco.com --dir /export/ftp/pub/linux/redhat/linux/7.0/en/os/i386

It specify's the NFS server as well as the NFS export to be mounted from that server.

The directory /export/ftp/pub/linux/redhat/linux/7.0/en/os/i386 contains the following files and directories.

    COPYING
    README
    RELEASE-NOTES
    RPM-GPG-KEY
    RedHat/
    autorun*
    boot.cat*
    doc/
    dosutils/
    images/
    ls-lR.gz
    misc/


The only one that has to exist is the RedHat directory. Here is what is inside the RedHat directory and what each directory is for.

    RPMS/         Location of the RPMS that get installed during KickStart.
    base/         Location of RPMS database and comps file.
    i386          File used to identify this directory is for the i386 architecture.
    instimage/    Location of the files that get mounted as the root file system during KickStart.

The above layout is taken from what you would see on a RedHat Linux 6.2 CD. Starting with Redhat Linux 7.1 there is no longer an instimage directory. The instimage directory is now located in a file which is kept in the base directory.

The NFS server must be exported to the world so that KickStarts that are using DHCP address's (and therefore are not in the netgroup file) will be able to mount the share point. That is only if you are using netgroups to restrict access.




Postinstall

The end of the ks.cfg file contains a %post section. This is where the postinstall commands are kept.
If we put all the postinstall commands here, the ks.cfg file would become rather large and might not fit on the floppy disk anymore.

So instead we install an RPM during the KickStart and in the %post section of the ks.cfg file we run a script in the RPM and pass it various options. The RPM is called postinstall and contains all the various things we want done during the postinstall. The last thing that the postinstall script does is removes the postinstall package.




Making better use of kickstart

Being able to do kickstarts is OK, but there are a few problems.

    1. You may not want all the RPMS that RedHat provides
    2. You may want to add your own RPMS.
    3. If you list all the specific rpms you want in the %packages section of the ks.cfg then the ks.cfg file won't fit on boot floppy.
    4. If you put complex postinstall commands in the ks.cfg file then the file might not fit on the boot floppy.
    5. There are updates available for some packages. You don't won't to install old ones that would then need to be upgraded.

So, what if you could choose exactly the RPM's that were available for Kickstart? What if you could choose the grouping of RPM's? What if you could pull out old RPM's and replace them with newer versions?

Well, all of this is possible if you know how the inside of kickstart works and how to edit the files that kickstart uses. To make that much easier we have created some tools that do all of that for you.

Let me explain a few things about how the inside of kickstart works so that you understand what these new tools are doing for you.

The first thing you may want to do is replace some RPM's from the RPMS directory with your own versions or updated versions or whatever. So you cd into the RPMS directory that is described above. You remove a few RPM's and add a few.
If you tried to Kickstart now, it would fail because Kickstart knows exactly which RPM's are in the RPMS directory and expects  specific versions of RPM's to be there. It keeps track of all the RPM headers for each RPM in a file called hdlist in the base directory. Therefore, you need to generate a new hdlist. This is done by running a command that comes with anaconda-runtime called genhdlist . You also need to edit the comps file. The comps file is the file that lists all the packages that should get installed based on a particular profile. For example, when you do an interactive install and select Networked Workstation, that is a package group listed in the comps file. If you want to create your own package groups or make some new packages part of an existing package group then you would list those packages in the comps file. The comps file is located in the base directory.

One thing to note about this file is that the package names listed in it do not include the version number. This means that you could not use this file as an authorative list of packages installed by kickstart since you would only know the package name and not the package version.

Now that you understand the basics of how to make changes to kickstart, you can forget it. Build_ks takes care of building the installer based on a configuration file that list all the rpms and what groups they should be in. It takes care of putting the correct RPM's in the RPMS directory. In also creates a new comps file and hdlist.  

What you need to do now is find out how to define a distribution .

		26.1.2

	26.2

27. Ubuntu 

	27.1 Configuring

		27.1.1 Install guide for VNC
Virtual Network Computing (VNC) is a way to share the desktop of one computer (server) with a remote computer (client). The client's keyboard and mouse input are transmitted over a network and control the operation of the server. The server's graphics are transmitted back over the network. Except for some latency issues, it will appear as if you are working directly on the machine you are sitting at. However, all programs and commands are performed on the server side.

Difficulty:
    Moderately Easy

Instructions

        1

        Open a Terminal window. The Terminal window can be found under "Start" > "Utilities."
        2

        Download vnc4server and xinetd:

        sudo apt-get install vnc4server xinetd
        3

        Start the vnc4server:

        vnc4server

        You will be prompted for a password. This password will be used to log into the VNC session.

        After providing a password, you will get output that looks like:

        New 'laptop:3 (laptop)' desktop is laptop:3

        Take notice of the number after the colon (:), in this case it is "3."

        Starting the vnc4server will cause a .vnc directory to be placed in your home directory.
        4

        Stop the vnc4server

        vnc4server -kill :3

        Replace the number "3" with the number from Step 3.
        5

        Open the .vnc/xstartup file for editing:

        gedit ~/.vnc/xstartup

        The file will look like:

        #!/bin/sh

        # Uncomment the following two lines for normal desktop:

        #unset SESSION_MANAGER

        #exec /etc/X11/xinit/xinitrc

        [ -x /etc/vnc/xstartup ] && exec /etc/vnc/xstartup

        [ -r $HOME/.Xresources ] && xrdb $HOME/.Xresources

        xsetroot -solid grey

        vncconfig -iconic &

        xterm -geometry 80x24+10+10 -ls -title "$VNCDESKTOP Desktop" &

        twm &
        6

        Now, Change the .vnc/xstartup file and save it.

        Uncomment the lines that start with unset and exec. Comment out the lines that start with xsetroot, vncconfig, xterm and twm.

        The final file should look like:

        #!/bin/sh

        # Uncomment the following two lines for normal desktop:

        unset SESSION_MANAGER

        exec /etc/X11/xinit/xinitrc

        [ -x /etc/vnc/xstartup ] && exec /etc/vnc/xstartup

        [ -r $HOME/.Xresources ] && xrdb $HOME/.Xresources

        #xsetroot -solid grey

        #vncconfig -iconic &

        #xterm -geometry 80x24+10+10 -ls -title "$VNCDESKTOP Desktop" &

        #twm &
        7

        Change the permissions on the /etc/X11/xinit/xinitrc file to make it executable:

        sudo chmod 755 /etc/X11/xinit/xinitrc
        8

        Start the vnc4server:

        vnc4server

        Again, remember the number after the colon (:).
        9

        Log into your remote desktop to check the configuration:

        vncviewer localhost:3


Read more: How to Install a VNC Server in Ubuntu | eHow.com http://www.ehow.com/how_5089245_install-vnc-server-ubuntu.html#ixzz1ceDEfTvE

		27.1.2 Change Ubuntu Server from DHCP to a Static IP Address
If the Ubuntu Server installer has set your server to use DHCP, you will want to change it to a static IP address so that people can actually use it.

Changing this setting without a GUI will require some text editing, but that’s classic linux, right?

Let’s open up the /etc/network/interfaces file. I’m going to use vi, but you can choose a different editor

sudo vi /etc/network/interfaces

For the primary interface, which is usually eth0, you will see these lines:

auto eth0
iface eth0 inet dhcp

As you can see, it’s using DHCP right now. We are going to change dhcp to static, and then there are a number of options that should be added below it. Obviously you’d customize this to your network.

auto eth0
iface eth0 inet static
        address 192.168.1.100
        netmask 255.255.255.0
        network 192.168.1.0
        broadcast 192.168.1.255
        gateway 192.168.1.1

Now we’ll need to add in the DNS settings by editing the resolv.conf file:

sudo vi /etc/resolv.conf

On the line ‘name server xxx.xxx.xxx.xxx’ replace the x with the IP of your name server. (You can do ifconfig /all to find out what they are)

You need to also remove the dhcp client for this to stick (thanks to Peter for noticing). You might need to remove dhcp-client3 instead.

sudo apt-get remove dhcp-client
Now we’ll just need to restart the networking components:

sudo /etc/init.d/networking restart

Ping www.google.com. If you get a response, name resolution is working(unless of course if google is in your hosts file).


		27.1.3 Login as root

Don't. Use sudo instead


		27.1.4 Disable firewall


			27.1.4.1 Disable / Turn Off Firewall in Ubuntu Linux Server
How do I disable firewall in Ubuntu Linux server edition?

You can type the following command to see if firewall is active or not (open a terminal or ssh session and type the following command):
$ sudo iptables -L -n

Sample outputs:

Chain INPUT (policy ACCEPT)
target     prot opt source               destination
ACCEPT     udp  --  0.0.0.0/0            0.0.0.0/0           udp dpt:53
ACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0           tcp dpt:53
ACCEPT     udp  --  0.0.0.0/0            0.0.0.0/0           udp dpt:67
ACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0           tcp dpt:67
Chain FORWARD (policy ACCEPT)
target     prot opt source               destination
ACCEPT     all  --  0.0.0.0/0            192.168.122.0/24    state RELATED,ESTABLISHED
ACCEPT     all  --  192.168.122.0/24     0.0.0.0/0
ACCEPT     all  --  0.0.0.0/0            0.0.0.0/0
REJECT     all  --  0.0.0.0/0            0.0.0.0/0           reject-with icmp-port-unreachable
REJECT     all  --  0.0.0.0/0            0.0.0.0/0           reject-with icmp-port-unreachable
Chain OUTPUT (policy ACCEPT)
target     prot opt source               destination
You can save existing firewall rules as follows:
$ sudo iptables-save > firewall.rules

Finally, type the following commands to stop firewall:
$ sudo iptables -X
$ sudo iptables -t nat -F
$ sudo iptables -t nat -X
$ sudo iptables -t mangle -F
$ sudo iptables -t mangle -X
$ sudo iptables -P INPUT ACCEPT
$ sudo iptables -P FORWARD ACCEPT
$ sudo iptables -P OUTPUT ACCEPT

A Note About ufw

The latest version of Ubuntu comes with ufw (now it is the default firewall configuration tool for Ubuntu). It is developed to ease iptables firewall configuration, ufw provides a user friendly way to create an IPv4 or IPv6 host-based firewall. To disable ufw, enter:
$ sudo ufw disable

You can also use GUI tool to enable or disable Firewall under Ubuntu Linux desktop edition by visiting System > Administration > Firewall configuration option:


Fig.01: Disabling Firewall In Ubuntu Linux

Simply unselect "Enabled" option to turn off the firewall.

			27.1.4.2


		27.1.5
	27.2 Troubleshooting 


		27.2.1  How To Fix “Problem with MergeList /var/lib/apt/lists” Error In Ubuntu 11.04
While using the package manager or trying to install applications through Terminal, it is possible to get a nasty error which is something like this :

E:Encountered a section with no Package: header,

E:Problem with MergeList /var/lib/apt/lists /us.archive.ubuntu.com_ubuntu_dists_natty_main_binary-i386_Packages,

E:The package lists or status file could not be parsed or opened.

This  will prevent from  installing or upgrading any application in  Ubuntu 11.04.  Fortunately, the fix is simple for this.

From the Terminal type the following commands :

sudo rm /var/lib/apt/lists/* -vf
and

sudo apt-get update
This  will delete the older entries and download the latest ones after which the error should no longer be there.

[ In fact, to avoid remembering these commands, they can be put in a shell script and only that script needs to be executed. Check out this post on how to do that.]

Cheers.



*/
		27.2.2

	27.3 packages

id=__install_istapd_ubuntu__
		27.3.1 istapd
yosi@ubuntu11:~$ sudo apt-get install isatapd
[sudo] password for yosi: 
Reading package lists... Done
Building dependency tree       
Reading state information... Done
E: Unable to locate package isatapd

To fix:
cd /var/lib/apt
sudo mv lists lists.old
sudo mkdir -p lists/partial
sudo apt-get update

[yosi@ubuntu11:/var/lib/apt$ sudo apt-get install isatapd
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following NEW packages will be installed:
  isatapd
0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded.
Need to get 18.5 kB of archives.
After this operation, 168 kB of additional disk space will be used.
Get:1 http://us.archive.ubuntu.com/ubuntu/ oneiric/universe isatapd i386 0.9.6-1 [18.5 kB]
Get:2 http://us.archive.ubuntu.com/ubuntu/ oneiric/universe isatapd i386 0.9.6-1 [18.5 kB]
Fetched 18.5 kB in 2min 0s (153 B/s)
Selecting previously deselected package isatapd.
(Reading database ... 125368 files and directories currently installed.)
Unpacking isatapd (from .../isatapd_0.9.6-1_i386.deb) ...
Processing triggers for man-db ...
Processing triggers for ureadahead ...
Setting up isatapd (0.9.6-1) ...
 * Starting ISATAP client isatapd                                                                                                    [ OK ] 
yosi@ubuntu11:/var/lib/apt$ ps -ef | grep isatapd
root     25202     1  0 01:15 ?        00:00:00 /usr/sbin/isatapd --daemon --pid /var/run/isatapd.pid --mtu 1280 isatap
nobody   25212 25202  0 01:15 ?        00:00:00 /usr/sbin/isatapd --daemon --pid /var/run/isatapd.pid --mtu 1280 isatap

		27.3.2

	27.4

28. Logging

	28.1 Logging, Log File Rotation, and Syslog Tutorial
http://content.hccfl.edu/pollock/aunix2/logging.htm

Logging Overview:

To identify problems and trends, and to trouble-shoot them, requires observing events over a period of time (historical monitoring).  Since it is generally impossible to observe all events as they occur, most daemons record important events to files known as log files. 

Early services managed their own files.  Today most (but not all!) can use the logging daemon syslog to collect, filter, store, alert, and forward logging data.  Syslog has the added benefit of somewhat standardizing log file formats, making it much easier to examine log data with various standard tools.  However some daemons may need to have their default logging configuration changed to have them take advantage of syslog (or indeed, provide any log data at all).

An important part of a system administrator's job is to regularly check various log files.  Be sure to learn where your hosts keep their log files as the directory is different for different flavors of *nix.

For most distributions of Linux, you should examine various log files in /var/log, especially the main (default) log file messages.  Use the dmesg command (and /var/log/boot.log) to see boot problems, hardware issues (and identification).  Copy /tmp/install.log to someplace safe.  (Must do this on first boot as /tmp gets erased on reboot.)  The secure log is also very important to monitor.

Other log files include audit/* (for SE Linux and related log messages), wtmp is a log of who logged in and when (This is a binary file, so view with last command and manage with the Linux sessreg command).  utmp is a binary file (not a log) of who's logged in now.  Two related files may exist (create these as empty files to use): btmp (a log of failed login attempts) and lastlog (not a log file but a sparse file — examine it with ls -l and du), which shows the last login per user id (view with finger and lastlog commands).

				*/



Note that most network devices (routers, switches, printers) today can produce syslog data.  You should examine these log files too.

Collect logging data from all important sources, including network devices, printers, workstations, and Windows servers.

Log files need to be examined or they are useless.  However it would be foolish to try to read all log data, all the time.  Usually summaries of the data are sufficient to alert you to potential problems, at which time you would then examine the relevant log entries.  This can be partially automated with log alerting and parsing tools (such as logwatch, logcheck, swatch, logsurfer, and SEC), GUI tools to examine and mange log files, and standard text processing tools such as grep, tail, tail -f, and less  (For example:  grep service logFile |grep date |less.)

Use SNMP/RMON to monitor network devices and networks.

Most hosts today ship with a basic syslog daemon but a number of replacement versions (that are compatible) include many new features.  Some of these replacements are:

syslog-ng (the most popular replacement currently)
module syslog
SDSC-syslog
nsyslog
New syslog IETF standards (RFCs) for syslog are being developed, to address security issues.

Windows (and Macintosh) System Logs:

Non Unix/Linux systems also maintain log files, but usually not in syslog format.  Windows systems uses binary event logs, that automatically overwrite themselves (!) when full.  However binary the format is publicly available and a number of Perl and other tools exists to convert these to text.

Windows logs are consistent across all Windows versions and services (e.g., Event ID 529 always means a failed login).  And as event logging is built into the OS it is generally more secure than syslog.

Windows provides no mechanism to forward events to a central loghost.  Instead there are a number of third party tools for this such as Kiwi syslog for Windows, EventReporter, Snare for Windows, and even roll-your-own with Perl module Win32::EventLog.

The Windows event log is really 3 logs: the system log, the security log, and the application log.  (Think of these as 3 syslog facilities.)  Each log is stored in a separate file in ...\system32\conf\SysEvent.Evt, ...\SecEvent.Evt, and ...\AppEvent.Evt.

Applications must register themselves to be able to use the event log service (see registry key HKEY_LOCAL_MACHINE\System\CurrentControlSet\Services\Eventlog\Application).

System and service event logging is controlled by the Windows Audit Policy (Control Panel→Administrative Tools→Local Security Policy→Audit Policy).

Windows provides logevent (equivalent to the Unix/Linux logger command line tool) to create event log messages.

For older Macintosh systems (OS9 and earlier) you can use the syslog compatible netlogger tool.

Other Logging Issues:

IDS and NIDS aren't enough!  Don't rely on these tools exclusively.  The alerts they generate are often meaningless without log data to see if the unusual event has resulted in a failure or a security breach.  Only log data can tell you that.
Common IDSs include file integrity checkers such as tripwire, osiris, and samhain.  Snort is a common NIDS scanner.

When managing a network you will have many hosts and devices that generate log data.  It is usually best to funnel all the log data to a single host.  This loghost is often a dedicated host just for logging and should be secured by removing all unused services and access, and generally hardened.
With loghosts listening for both local and remote syslog data, make sure you open up the syslog port in all firewalls between you and the remote host: UDP/514.  Note UDP is not secure and you should only allow selected hosts to use that port.
Configure your services to use syslog.  Many services ship with logging off by default, or attempt to use a custom log file.  Each service should send all messages to syslog.  If this is too much data you can configure syslog to filter it out.  This means you don't need to continually remember how to configure every service when you need to change its log-level.  Just configure syslog for any service.
When initially installing a new server (or updating one) it will pay to crank up the amount of log data collected for that service until you are confident it is working correctly.
Send all data to a single central loghost.  If your network is spread out geographically, each location can have a local loghost and these in turn send data to the central loghost.  These relay loghosts should also store the log data, giving you a backup in case the central loghost fails or if a network connection goes down.
One problem with a central loghost is that the default syslog daemon (but not all newer versions) only reports the last hop IP address in the log, so the central loghost will lose the IP address of the original host that generated the message.  A good solution is to use a more modern syslog replacement, such as syslog-ng.
Loghosts are attacked by hackers so it is often worth the work to build a stealth loghost.  Such a host has one NIC connected to an inner network and is used for SSH connections by the administrator or to relay log data.  The other NIC is on the LAN from which you are collecting the log data.  This NIC is unnumbered and set in promiscuous mode.  You use various tools (netcat, a.k.a. nc) to monitor the network for UDP log data sent to a fictitious address.  The various hosts on the LAN will try to send their log data to that fictitious host.
Log data is usually send via UDP.  Modern syslog replacements (and soon syslog itself) will be able to use TCP, SSH/SSL/TLS encryption, and digitally signed log messages, to provide extra security.
Log files contain sensitive information, so:
Set the file permissions accordingly!  This includes the files and (usually) syslog's /dev/log device (and similar ones in chroot jails).
When a log file is closed (when rotated for instance) consider digitally signing and/or encrypting the log file to prevent tampering and/or unauthorized access.
When possible, don't collect data you don't need! The best way to keep data private is not to store it at all.  For example turn down the log level (amount of detail) produced after a daemon is setup completely.
Only keep data as long as you need to.  Most industry have standard limits on how long to keep different types of data.  Follow these guidelines.
Of the data you do keep, decide which data can be blinded, which data should be encrypted, and which data can be safely left open:
Blinding data means that it is destroyed, but in a way that makes it unique.  A hash (one-way) function is a good technique for this.
Blinding is a useful way to store personal data such as credit card numbers, phone numbers, addresses, customer ID numbers, etc.  Note that blinded data can still be used as a primary key for a database table, as a hash of a unique value is still unique.
If possible randomize the order of such data, to hide the sequence of data (not a good idea for system log files, but useful for, say, financial transaction logs).
Blinded data can't be recovered to its original form.  So if there is a requirement for sensitive (private) data that must be recoverable use encryption instead of data blinding.
Consider using digital signatures for each log entry.  (Some syslog replacements do this already, and syslog will soon).  A different key should be used to digitally sign the whole log, either after every entry or after the log file is closed/rotated.  This is an example of a dual control that prevents a single person working alone from falsifying data (e.g., hiding financial transactions to embezzle funds).
Without special hardware support encryption and digital signatures can take a long time.  This can cause a busy log server to fall behind, eventually losing log entries or crashing altogether!  Some modern syslog replacements report such lost log entry statistics, so you can test your system under a simulated load to make sure you won't lose valuable data.
Note that blinding, encrypting, and not keeping data are general techniques, not well suited to system log files.  However it still pays to consider these issues.

Log data can grow to fill even large disks quickly.  You must make sure large log files won't crash your system or result in some DOS (when sending log information across your network).  (See log rotation below.)  Consider various data reduction techniques: summarizing repetitive log entries, discarding low utility log messages, and data compression.
Old log data, at least some of it, has lasting value for baselining and auditing.  Consider archiving old log data, or filtering the logs and archiving important events and summary data (e.g., 208 SSH logins successful in past day).  One useful design would be to have old log data sent to a database.  Then you could easily get reports and make queries using SQL.
Syslog doesn't restrict which hosts can send log data or what data is sent.  This make all log data unreliable.  Hackers can easily generate fake log data and send it to your loghost.  This can reduce the evidentiary value of your data.  This can be partially mitigated using a careful firewall configuration, but this issue is better addressed by using one of the modern syslog replacements.
Some syslog replacements use TCP instead of UDP and support IPSec or SSL tunnels for the transport.  (This makes stealth loghosts both impossible and unnecessary.)  Digitally signing log entries can also reduce fake log entries (but not if the evil-doer has access to the remote system and can cause it to generate the correctly signed log entries!)
Newer syslog protocols use syslog-sign and syslog-reliable for safer transport and sender authentication, and are supported by some syslog replacements.
If you're not going to replace your current syslog, consider using SSH or SSL tunnels (see stunnel) for transport of syslog data.  Set up tunnel on each remote host so data send to localhost port 9999 (or any unprivileged port) gets automatically forwarded to the loghost via the secure tunnel:
On client:     nc -l -u -p syslog | nc locahost 9999
On loghost:    nc -l -p 999 | nc localhost -u syslog
Syslog time-stamps don't contain the year, so make sure the archived log files' names include the year.
Syslog time-stamps don't contain time zone information.  When data is sent to a central loghost from a wide geographical area, it is important to know the time zone of the original host that generated the log message.  Consider using one of the newer syslog replacements that does provide this information, or otherwise take care of this issue when performing data reduction and data parsing.
Accurate network-wide time is vital to correlating log events and for preserving their value as an audit trail and evidence in court.  Use NTP to synchronize your hosts and network devices to the same time.  If an external time server is not feasible for some reason, pick one (well-connected) host to act as your or4ganizatin's time server.  For about $100 you can install a GPS or radio-controlled clock.
Syslog Overview:

What gets logged by syslogd and where it goes is controlled by /etc/syslog.conf.  In the past (and to a small extent today), servers had hard-coded filenames to use for their log files.  This is a very inflexible scheme, and log files would wind up scattered all over the disk.  A modern system uses syslog to centralize logging.  A single configuration file can control what gets logged (and what gets ignored), and where the log messages should go. 

Here's how it works:  A developer uses the syslog API function (or uses the logger program in shell scripts) to send log messages to syslogd.  The information passed to syslogd includes the source of the log message (called a facility) and the priority of the log message.

syslogd then matches the facility and priority against selectors (combinations of facilities and priorities) in its configuration file.  For the selector(s) that match the messages is sent to the corresponding destination(s).

Note that many PAM modules send log messages to syslogd.  Also, some systems have a separate log daemon for the kernel, klogd, that you may need to configure.

Most log files go under /var/log directory.  Besides the more specific log files, there is a general system log file usually called messages.  Other important log files to monitor include: boot.log, dmesg (also the dmesg command), maillog, secure, wtmp (examine with the last command), and yum.log (or whatever system you use instead, to see what was installed recently).

Log files contain sensitive information!  You must protect these files by setting permissions carefully!

Syslog.conf Syntax:

Aside from blank lines and comment lines, syslog.conf has lines with two parts:

The selector says what messages to log
The action says what to do with them (i.e., where the message goes)
Each log message is matched against the selectors.  For each matching selector, the associated action is done.

Syslog Selectors — Facilities and Priorities:

The source of a log message is referred to as a facility.  For example any email related program that sends a log message uses the mail facility no matter what the name of the program actually was.

There is no way to define your own facilities but there are many predefined ones:

auth (Security events get logged with this)
authpriv (user access messages use this)
cron
daemon (other daemon programs without a facility of their own)
kern (kernel messages)
lpr
mail
mark (used by syslogd to produce timestamps in log files)
news
syslog
user (for user programs)
uucp
local0 – local7 (any use; RH uses local7 for boot messages)
* (for all)
Note that syslog trusts the software to use the correct facility when sending a log message.

Due to the limited number of facilities available, it is inevitable that some services will wind up using the same facility for their log messages.  Syslog allows programs to supply an identifying string, known as a tag, that syslog will prepend to each line of the log messages.  This permits easy selection using grep or other tools, to filter only the log messages of interest.

Priorities:

The priority is one of the following eight levels, which are ranked in order from high to low priority:

emerg
alert
crit
err
warning
notice
info
debug
When specifying a priority, that and all higher ones are selected too.  A selector is one or more facilities (separated by commas), a dot, then the priority.  More complex selectors are possible too; one such is shown below.)  Some example selectors:

mail.*       mail facility, any priority
mail.debug   mail facility, debug or higher priority (same as *)
mail,news.*  all messages from mail or news
auth.warning all security messages of warning or higher priority
*.info       all messages from any facility except debug msgs
*.=info      any facility, info msgs only (and not higher)
*.!err       any facility, pri <= err only
*.!=alert    any facility, any priority except alert
*.info;mail,news,authpriv.none
             all msgs with info or higher priority except
             mail, news, and authpriv
That last one is tricky.  Using multiple selectors on a single line this way allows you to specify a general category first, then for the matching log messages you can specify exceptions.  Always go from most general selector to most specific or your setup may not log what you think it should!

Syslog Actions — files, users, pipes:

Log messages don't only have to go to files, you can direct them to user terminals, run them through other programs (with a pipe, to email, pager, or just a log file analyzer), or send them to another host running syslogd.

(This last is handy if you have a network of computers you must monitor.  Besides consolidating many log files, there is great security in using a remote log server that has no other services on it.  This is because when a server is hacked the attacker usually destroys the log files.  This scheme protects against disk crashes too.)

Here's the syntax for the actions:

/complete/path/of/some/file
/dev/console 
(This is a link to the system console)
-/complete/path/of/some/file 
(Don't flush file each time; better performance but risks loss of some log info.)
username1[,username2 ...]
* 
(all logged in users)
@remotehost 
(e.g., @log.hcc.com; start the remote syslogd with -r option.)
|/path/to/named/pipe 
(To send output to a command you must create a named pipe, say /var/lib/cmd.pipe with the mkfifo command.  Then start the command with cmd </var/lib/cmd.pipe.)
Using logger:

   logger [-p facility.priority] [-t tag] message
The default selector is user.info, and the default tag is logger.

Log File Rotation:

One problem with log files is that over time they grow.  When a system is experiencing problems the log files can grow very large, very quickly.  Periodically trimming or removing log files is necessary.  This is known as log file rotation and is a service usually run via cron.

The most popular scheme is to rename a log file log as log.1 and to start a new log file.  Next time, log.1 is renamed to log.2, log is renamed to log.1, and a new log file is started.  This continues for N previous files.

Since dealing with log file rotation is a common problem most Unix systems have a standard way to deal with it.  On Solaris9 you have /usr/sbin/logadm (/usr/lib/newsyslog on earlier Solaris).

On Linux you have the logrotate command.  This command runs via the cron facility.  You can set your log rotation policy for any log file by editing the file logrotate.conf, or editing files in the /etc/logrotate.d directory.  Here's a sample logrotate.conf file:

#Global settings:

# rotate log files weekly
weekly
# keep 4 weeks worth of backlogs
rotate 4
# Create new (empty) log files after rotating old ones
create 0644 root root

# Per log file settings:

/var/log/cups/*_log {
    missingok
    notifempty
    errors root
    postrotate
        /etc/init.d/cups condrestart >/dev/null 2>&1 || true
    endscript
}
Links:


*/

29.   Featured Articles

	29.1 20 Linux System Monitoring Tools Every SysAdmin Should Know 
Need to monitor Linux server performance? Try these built-in command and a few add-on tools. Most Linux distributions are equipped with tons of monitoring. These tools provide metrics which can be used to get information about system activities. You can use these tools to find the possible causes of a performance problem. The commands discussed below are some of the most basic commands when it comes to system analysis and debugging server issues such as:

    Finding out bottlenecks.
    Disk (storage) bottlenecks.
    CPU and memory bottlenecks.
    Network bottlenecks.

		29.1.1 #1: top - Process Activity Command

The top program provides a dynamic real-time view of a running system i.e. actual process activity. By default, it displays the most CPU-intensive tasks running on the server and updates the list every five seconds.
Fig.01: Linux top command

Fig.01: Linux top command
Commonly Used Hot Keys

The top command provides several useful hot keys:
Hot Key	Usage
t	Displays summary information off and on.
m	Displays memory information off and on.
A	Sorts the display by top consumers of various system resources. Useful for quick identification of performance-hungry tasks on a system.
f	Enters an interactive configuration screen for top. Helpful for setting up top for a specific task.
o	Enables you to interactively select the ordering within top.
r	Issues renice command.
k	Issues kill command.
z	Turn on or off color/mono


=> Related: How do I Find Out Linux CPU Utilization?

		29.1.2 #2: vmstat - System Activity, Hardware and System Information

The command vmstat reports information about processes, memory, paging, block IO, traps, and cpu activity.
# vmstat 3
Sample Outputs:

procs -----------memory---------- ---swap-- -----io---- --system-- -----cpu------
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 0  0      0 2540988 529188 5130400    0    0     2    32    4    2  4  1 96  0  0
 1  0      0 2540988 522188 5130400    0    0     0   720 1199  665  1  0 99  0  0
 0  0      0 2540956 522188 5130400    0    0     0     0 1151 1569  4  1 95  0  0
 0  0      0 2540956 522188 5130500    0    0     0     6 1117  439  1  0 99  0  0
 0  0      0 2540940 522188 5130512    0    0     0   536 1189  932  1  0 98  0  0
 0  0      0 2538444 522188 5130588    0    0     0     0 1187 1417  4  1 96  0  0
 0  0      0 2490060 522188 5130640    0    0     0    18 1253 1123  5  1 94  0  0

Display Memory Utilization Slabinfo

# vmstat -m
Get Information About Active / Inactive Memory Pages

# vmstat -a
=> Related: How do I find out Linux Resource utilization to detect system bottlenecks?

		29.1.3 #3: w - Find Out Who Is Logged on And What They Are Doing

w command displays information about the users currently on the machine, and their processes.
# w username
# w vivek
Sample Outputs:

 17:58:47 up 5 days, 20:28,  2 users,  load average: 0.36, 0.26, 0.24
USER     TTY      FROM              LOGIN@   IDLE   JCPU   PCPU WHAT
root     pts/0    10.1.3.145       14:55    5.00s  0.04s  0.02s vim /etc/resolv.conf
root     pts/1    10.1.3.145       17:43    0.00s  0.03s  0.00s w

		29.1.4 #4: uptime - Tell How Long The System Has Been Running

The uptime command can be used to see how long the server has been running. The current time, how long the system has been running, how many users are currently logged on, and the system load averages for the past 1, 5, and 15 minutes.
# uptime
Output:

 18:02:41 up 41 days, 23:42,  1 user,  load average: 0.00, 0.00, 0.00

1 can be considered as optimal load value. The load can change from system to system. For a single CPU system 1 - 3 and SMP systems 6-10 load value might be acceptable.

		29.1.5 #5: ps - Displays The Processes

ps command will report a snapshot of the current processes. To select all processes use the -A or -e option:
# ps -A
Sample Outputs:

  PID TTY          TIME CMD
    1 ?        00:00:02 init
    2 ?        00:00:02 migration/0
    3 ?        00:00:01 ksoftirqd/0
    4 ?        00:00:00 watchdog/0
    5 ?        00:00:00 migration/1
    6 ?        00:00:15 ksoftirqd/1
....
.....
 4881 ?        00:53:28 java
 4885 tty1     00:00:00 mingetty
 4886 tty2     00:00:00 mingetty
 4887 tty3     00:00:00 mingetty
 4888 tty4     00:00:00 mingetty
 4891 tty5     00:00:00 mingetty
 4892 tty6     00:00:00 mingetty
 4893 ttyS1    00:00:00 agetty
12853 ?        00:00:00 cifsoplockd
12854 ?        00:00:00 cifsdnotifyd
14231 ?        00:10:34 lighttpd
14232 ?        00:00:00 php-cgi
54981 pts/0    00:00:00 vim
55465 ?        00:00:00 php-cgi
55546 ?        00:00:00 bind9-snmp-stat
55704 pts/1    00:00:00 ps

ps is just like top but provides more information.
Show Long Format Output

# ps -Al
To turn on extra full mode (it will show command line arguments passed to process):
# ps -AlF
To See Threads ( LWP and NLWP)

# ps -AlFH
To See Threads After Processes

# ps -AlLm
Print All Process On The Server

# ps ax
# ps axu
Print A Process Tree

# ps -ejH
# ps axjf
# pstree
Print Security Information

# ps -eo euser,ruser,suser,fuser,f,comm,label
# ps axZ
# ps -eM
See Every Process Running As User Vivek

# ps -U vivek -u vivek u
Set Output In a User-Defined Format

# ps -eo pid,tid,class,rtprio,ni,pri,psr,pcpu,stat,wchan:14,comm
# ps axo stat,euid,ruid,tty,tpgid,sess,pgrp,ppid,pid,pcpu,comm
# ps -eopid,tt,user,fname,tmout,f,wchan
Display Only The Process IDs of Lighttpd

# ps -C lighttpd -o pid=
OR
# pgrep lighttpd
OR
# pgrep -u vivek php-cgi
Display The Name of PID 55977

# ps -p 55977 -o comm=
Find Out The Top 10 Memory Consuming Process

# ps -auxf | sort -nr -k 4 | head -10
Find Out top 10 CPU Consuming Process

# ps -auxf | sort -nr -k 3 | head -10

		29.1.6 #6: free - Memory Usage

The command free displays the total amount of free and used physical and swap memory in the system, as well as the buffers used by the kernel.
# free
Sample Output:

            total       used       free     shared    buffers     cached
Mem:      12302896    9739664    2563232          0     523124    5154740
-/+ buffers/cache:    4061800    8241096
Swap:      1052248          0    1052248

=> Related: :

    Linux Find Out Virtual Memory PAGESIZE
    Linux Limit CPU Usage Per Process
    How much RAM does my Ubuntu / Fedora Linux desktop PC have?

		29.1.7 #7: iostat - Average CPU Load, Disk Activity

The command iostat report Central Processing Unit (CPU) statistics and input/output statistics for devices, partitions and network filesystems (NFS).
# iostat
Sample Outputs:

| Linux 2.6.18-128.1.14.el5 (www03.nixcraft.in) 	06/26/2009
| avg-cpu:  %user   %nice %system %iowait  %steal   %idle
|            3.50    0.09    0.51    0.03    0.00   95.86
| Device:            tps   Blk_read/s   Blk_wrtn/s   Blk_read   Blk_wrtn
| sda              22.04        31.88       512.03   16193351  260102868
| sda1              0.00         0.00         0.00       2166        180
| sda2             22.04        31.87       512.03   16189010  260102688
| sda3              0.00         0.00         0.00       1615          0

=> Related: : Linux Track NFS Directory / Disk I/O Stats

		29.1.8 #8: sar - Collect and Report System Activity

The sar command is used to collect, report, and save system activity information. To see network counter, enter:
# sar -n DEV | more
To display the network counters from the 24th:
# sar -n DEV -f /var/log/sa/sa24 | more
You can also display real time usage using sar:
# sar 4 5
Sample Outputs:

Linux 2.6.18-128.1.14.el5 (www03.nixcraft.in) 		06/26/2009
06:45:12 PM       CPU     %user     %nice   %system   %iowait    %steal     %idle
06:45:16 PM       all      2.00      0.00      0.22      0.00      0.00     97.78
06:45:20 PM       all      2.07      0.00      0.38      0.03      0.00     97.52
06:45:24 PM       all      0.94      0.00      0.28      0.00      0.00     98.78
06:45:28 PM       all      1.56      0.00      0.22      0.00      0.00     98.22
06:45:32 PM       all      3.53      0.00      0.25      0.03      0.00     96.19
Average:          all      2.02      0.00      0.27      0.01      0.00     97.70

=> Related: : How to collect Linux system utilization data into a file

		29.1.9 #9: mpstat - Multiprocessor Usage

The mpstat command displays activities for each available processor, processor 0 being the first one. mpstat -P ALL to display average CPU utilization per processor:
# mpstat -P ALL
Sample Output:

Linux 2.6.18-128.1.14.el5 (www03.nixcraft.in)	 	06/26/2009
06:48:11 PM  CPU   %user   %nice    %sys %iowait    %irq   %soft  %steal   %idle    intr/s
06:48:11 PM  all    3.50    0.09    0.34    0.03    0.01    0.17    0.00   95.86   1218.04
06:48:11 PM    0    3.44    0.08    0.31    0.02    0.00    0.12    0.00   96.04   1000.31
06:48:11 PM    1    3.10    0.08    0.32    0.09    0.02    0.11    0.00   96.28     34.93
06:48:11 PM    2    4.16    0.11    0.36    0.02    0.00    0.11    0.00   95.25      0.00
06:48:11 PM    3    3.77    0.11    0.38    0.03    0.01    0.24    0.00   95.46     44.80
06:48:11 PM    4    2.96    0.07    0.29    0.04    0.02    0.10    0.00   96.52     25.91
06:48:11 PM    5    3.26    0.08    0.28    0.03    0.01    0.10    0.00   96.23     14.98
06:48:11 PM    6    4.00    0.10    0.34    0.01    0.00    0.13    0.00   95.42      3.75
06:48:11 PM    7    3.30    0.11    0.39    0.03    0.01    0.46    0.00   95.69     76.89

=> Related: : Linux display each multiple SMP CPU processors utilization individually.

		29.1.10 #10: pmap - Process Memory Usage

The command pmap report memory map of a process. Use this command to find out causes of memory bottlenecks.
# pmap -d PID
To display process memory information for pid # 47394, enter:
# pmap -d 47394
Sample Outputs:

47394:   /usr/bin/php-cgi
Address           Kbytes Mode  Offset           Device    Mapping
0000000000400000    2584 r-x-- 0000000000000000 008:00002 php-cgi
0000000000886000     140 rw--- 0000000000286000 008:00002 php-cgi
00000000008a9000      52 rw--- 00000000008a9000 000:00000   [ anon ]
0000000000aa8000      76 rw--- 00000000002a8000 008:00002 php-cgi
000000000f678000    1980 rw--- 000000000f678000 000:00000   [ anon ]
000000314a600000     112 r-x-- 0000000000000000 008:00002 ld-2.5.so
000000314a81b000       4 r---- 000000000001b000 008:00002 ld-2.5.so
000000314a81c000       4 rw--- 000000000001c000 008:00002 ld-2.5.so
000000314aa00000    1328 r-x-- 0000000000000000 008:00002 libc-2.5.so
000000314ab4c000    2048 ----- 000000000014c000 008:00002 libc-2.5.so
.....
......
..
00002af8d48fd000       4 rw--- 0000000000006000 008:00002 xsl.so
00002af8d490c000      40 r-x-- 0000000000000000 008:00002 libnss_files-2.5.so
00002af8d4916000    2044 ----- 000000000000a000 008:00002 libnss_files-2.5.so
00002af8d4b15000       4 r---- 0000000000009000 008:00002 libnss_files-2.5.so
00002af8d4b16000       4 rw--- 000000000000a000 008:00002 libnss_files-2.5.so
00002af8d4b17000  768000 rw-s- 0000000000000000 000:00009 zero (deleted)
00007fffc95fe000      84 rw--- 00007ffffffea000 000:00000   [ stack ]
ffffffffff600000    8192 ----- 0000000000000000 000:00000   [ anon ]
mapped: 933712K    writeable/private: 4304K    shared: 768000K

The last line is very important:

    mapped: 933712K total amount of memory mapped to files
    writeable/private: 4304K the amount of private address space
    shared: 768000K the amount of address space this process is sharing with others

=> Related: : Linux find the memory used by a program / process using pmap command

		29.1.11 #11 and #12: netstat and ss - Network Statistics

The command netstat displays network connections, routing tables, interface statistics, masquerade connections, and multicast memberships. ss command is used to dump socket statistics. It allows showing information similar to netstat. See the following resources about ss and netstat commands:

    ss: Display Linux TCP / UDP Network and Socket Information
    Get Detailed Information About Particular IP address Connections Using netstat Command

		29.1.12 #13: iptraf - Real-time Network Statistics

The iptraf command is interactive colorful IP LAN monitor. It is an ncurses-based IP LAN monitor that generates various network statistics including TCP info, UDP counts, ICMP and OSPF information, Ethernet load info, node stats, IP checksum errors, and others. It can provide the following info in easy to read format:

    Network traffic statistics by TCP connection
    IP traffic statistics by network interface
    Network traffic statistics by protocol
    Network traffic statistics by TCP/UDP port and by packet size
    Network traffic statistics by Layer2 address

Fig.02: General interface statistics: IP traffic statistics by network interface

Fig.02: General interface statistics: IP traffic statistics by network interface
Fig.03 Network traffic statistics by TCP connection

Fig.03 Network traffic statistics by TCP connection

		29.1.13 #14: tcpdump - Detailed Network Traffic Analysis

The tcpdump is simple command that dump traffic on a network. However, you need good understanding of TCP/IP protocol to utilize this tool. For.e.g to display traffic info about DNS, enter:
# tcpdump -i eth1 'udp port 53'
To display all IPv4 HTTP packets to and from port 80, i.e. print only packets that contain data, not, for example, SYN and FIN packets and ACK-only packets, enter:
# tcpdump 'tcp port 80 and (((ip[2:2] - ((ip[0]&0xf)<<2)) - ((tcp[12]&0xf0)>>2)) != 0)'
To display all FTP session to 202.54.1.5, enter:
# tcpdump -i eth1 'dst 202.54.1.5 and (port 21 or 20'
To display all HTTP session to 192.168.1.5:
# tcpdump -ni eth0 'dst 192.168.1.5 and tcp and port http'
Use wireshark to view detailed information about files, enter:
# tcpdump -n -i eth1 -s 0 -w output.txt src or dst port 80

		29.1.14 #15: strace - System Calls

Trace system calls and signals. This is useful for debugging webserver and other server problems. See how to use to trace the process and see What it is doing.

		29.1.15 #16: /Proc file system - Various Kernel Statistics

/proc file system provides detailed information about various hardware devices and other Linux kernel information. See Linux kernel /proc documentations for further details. Common /proc examples:
# cat /proc/cpuinfo
# cat /proc/meminfo
# cat /proc/zoneinfo
# cat /proc/mounts

17#: Nagios - Server And Network Monitoring

Nagios is a popular open source computer system and network monitoring application software. You can easily monitor all your hosts, network equipment and services. It can send alert when things go wrong and again when they get better. FAN is "Fully Automated Nagios". FAN goals are to provide a Nagios installation including most tools provided by the Nagios Community. FAN provides a CDRom image in the standard ISO format, making it easy to easilly install a Nagios server. Added to this, a wide bunch of tools are including to the distribution, in order to improve the user experience around Nagios.

18#: Cacti - Web-based Monitoring Tool

Cacti is a complete network graphing solution designed to harness the power of RRDTool's data storage and graphing functionality. Cacti provides a fast poller, advanced graph templating, multiple data acquisition methods, and user management features out of the box. All of this is wrapped in an intuitive, easy to use interface that makes sense for LAN-sized installations up to complex networks with hundreds of devices. It can provide data about network, CPU, memory, logged in users, Apache, DNS servers and much more. See how to install and configure Cacti network graphing tool under CentOS / RHEL.

		29.1.16 #19: KDE System Guard - Real-time Systems Reporting and Graphing

KSysguard is a network enabled task and system monitor application for KDE desktop. This tool can be run over ssh session. It provides lots of features such as a client/server architecture that enables monitoring of local and remote hosts. The graphical front end uses so-called sensors to retrieve the information it displays. A sensor can return simple values or more complex information like tables. For each type of information, one or more displays are provided. Displays are organized in worksheets that can be saved and loaded independently from each other. So, KSysguard is not only a simple task manager but also a very powerful tool to control large server farms.
Fig.05 KDE System Guard

Fig.05 KDE System Guard {Image credit: Wikipedia}

See the KSysguard handbook for detailed usage.

		29.1.17 #20: Gnome System Monitor - Real-time Systems Reporting and Graphing

The System Monitor application enables you to display basic system information and monitor system processes, usage of system resources, and file systems. You can also use System Monitor to modify the behavior of your system. Although not as powerful as the KDE System Guard, it provides the basic information which may be useful for new users:

    Displays various basic information about the computer's hardware and software.
    Linux Kernel version
    GNOME version
    Hardware
    Installed memory
    Processors and speeds
    System Status
    Currently available disk space
    Processes
    Memory and swap space
    Network usage
    File Systems
    Lists all mounted filesystems along with basic information about each.

Fig.06 The Gnome System Monitor application

Fig.06 The Gnome System Monitor application
Bonus: Additional Tools

A few more tools:

    nmap - scan your server for open ports.
    lsof - list open files, network connections and much more.
    ntop web based tool - ntop is the best tool to see network usage in a way similar to what top command does for processes i.e. it is network traffic monitoring software. You can see network status, protocol wise distribution of traffic for UDP, TCP, DNS, HTTP and other protocols.
    Conky - Another good monitoring tool for the X Window System. It is highly configurable and is able to monitor many system variables including the status of the CPU, memory, swap space, disk storage, temperatures, processes, network interfaces, battery power, system messages, e-mail inboxes etc.
    GKrellM - It can be used to monitor the status of CPUs, main memory, hard disks, network interfaces, local and remote mailboxes, and many other things.
    vnstat - vnStat is a console-based network traffic monitor. It keeps a log of hourly, daily and monthly network traffic for the selected interface(s).
    htop - htop is an enhanced version of top, the interactive process viewer, which can display the list of processes in a tree form.
    mtr - mtr combines the functionality of the traceroute and ping programs in a single network diagnostic tool.


    29.2 Iptables
    
	29.2.1 Linux: 20 Iptables Examples For New SysAdmins

by Vivek Gite on December 13, 2011 ? 33 comments? Last updated March 20, 2012

Linux comes with a host based firewall called Netfilter. According to the official project site:

    netfilter is a set of hooks inside the Linux kernel that allows kernel modules to register callback functions with the network stack. A registered callback function is then called back for every packet that traverses the respective hook within the network stack.

This Linux based firewall is controlled by the program called iptables to handles filtering for IPv4, and ip6tables handles filtering for IPv6. I strongly recommend that you first read our quick tutorial that explains how to configure a host-based firewall called Netfilter (iptables) under CentOS / RHEL / Fedora / Redhat Enterprise Linux. This post list most common iptables solutions required by a new Linux user to secure his or her Linux operating system from intruders.
IPTABLES Rules Example

    Most of the actions listed in this post are written with the assumption that they will be executed by the root user running the bash or any other modern shell. Do not type commands on remote system as it will disconnect your access.
    For demonstration purpose I've used RHEL 6.x, but the following command should work with any modern Linux distro.
    This is NOT a tutorial on how to set iptables. See tutorial here. It is a quick cheat sheet to common iptables commands.

		29.2.1.1 #1: Displaying the Status of Your Firewall

Type the following command as root:
# iptables -L -n -v
Sample outputs:

Chain INPUT (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination
Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination
Chain OUTPUT (policy ACCEPT 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination

Above output indicates that the firewall is not active. The following sample shows an active firewall:
# iptables -L -n -v
Sample outputs:

Chain INPUT (policy DROP 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination
    0     0 DROP       all  --  *      *       0.0.0.0/0            0.0.0.0/0           state INVALID
  394 43586 ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0           state RELATED,ESTABLISHED
   93 17292 ACCEPT     all  --  br0    *       0.0.0.0/0            0.0.0.0/0
    1   142 ACCEPT     all  --  lo     *       0.0.0.0/0            0.0.0.0/0
Chain FORWARD (policy DROP 0 packets, 0 bytes)
 pkts bytes target     prot opt in     out     source               destination
    0     0 ACCEPT     all  --  br0    br0     0.0.0.0/0            0.0.0.0/0
    0     0 DROP       all  --  *      *       0.0.0.0/0            0.0.0.0/0           state INVALID
    0     0 TCPMSS     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0           tcp flags:0x06/0x02 TCPMSS clamp to PMTU
    0     0 ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0           state RELATED,ESTABLISHED
    0     0 wanin      all  --  vlan2  *       0.0.0.0/0            0.0.0.0/0
    0     0 wanout     all  --  *      vlan2   0.0.0.0/0            0.0.0.0/0
    0     0 ACCEPT     all  --  br0    *       0.0.0.0/0            0.0.0.0/0
Chain OUTPUT (policy ACCEPT 425 packets, 113K bytes)
 pkts bytes target     prot opt in     out     source               destination
Chain wanin (1 references)
 pkts bytes target     prot opt in     out     source               destination
Chain wanout (1 references)
 pkts bytes target     prot opt in     out     source               destination

Where,

    -L : List rules.
    -v : Display detailed information. This option makes the list command show the interface name, the rule options, and the TOS masks. The packet and byte counters are also listed, with the suffix 'K', 'M' or 'G' for 1000, 1,000,000 and 1,000,000,000 multipliers respectively.
    -n : Display IP address and port in numeric format. Do not use DNS to resolve names. This will speed up listing.

#1.1: To inspect firewall with line numbers, enter:

# iptables -n -L -v --line-numbers
Sample outputs:

Chain INPUT (policy DROP)
num  target     prot opt source               destination
1    DROP       all  --  0.0.0.0/0            0.0.0.0/0           state INVALID
2    ACCEPT     all  --  0.0.0.0/0            0.0.0.0/0           state RELATED,ESTABLISHED
3    ACCEPT     all  --  0.0.0.0/0            0.0.0.0/0
4    ACCEPT     all  --  0.0.0.0/0            0.0.0.0/0
Chain FORWARD (policy DROP)
num  target     prot opt source               destination
1    ACCEPT     all  --  0.0.0.0/0            0.0.0.0/0
2    DROP       all  --  0.0.0.0/0            0.0.0.0/0           state INVALID
3    TCPMSS     tcp  --  0.0.0.0/0            0.0.0.0/0           tcp flags:0x06/0x02 TCPMSS clamp to PMTU
4    ACCEPT     all  --  0.0.0.0/0            0.0.0.0/0           state RELATED,ESTABLISHED
5    wanin      all  --  0.0.0.0/0            0.0.0.0/0
6    wanout     all  --  0.0.0.0/0            0.0.0.0/0
7    ACCEPT     all  --  0.0.0.0/0            0.0.0.0/0
Chain OUTPUT (policy ACCEPT)
num  target     prot opt source               destination
Chain wanin (1 references)
num  target     prot opt source               destination
Chain wanout (1 references)
num  target     prot opt source               destination

You can use line numbers to delete or insert new rules into the firewall.
#1.2: To display INPUT or OUTPUT chain rules, enter:

# iptables -L INPUT -n -v
# iptables -L OUTPUT -n -v --line-numbers

		29.2.1.2 #2: Stop / Start / Restart the Firewall

If you are using CentOS / RHEL / Fedora Linux, enter:
# service iptables stop
# service iptables start
# service iptables restart
You can use the iptables command itself to stop the firewall and delete all rules:
# iptables -F
# iptables -X
# iptables -t nat -F
# iptables -t nat -X
# iptables -t mangle -F
# iptables -t mangle -X
# iptables -P INPUT ACCEPT
# iptables -P OUTPUT ACCEPT
# iptables -P FORWARD ACCEPT
Where,

    -F : Deleting (flushing) all the rules.
    -X : Delete chain.
    -t table_name : Select table (called nat or mangle) and delete/flush rules.
    -P : Set the default policy (such as DROP, REJECT, or ACCEPT).

		29.2.1.3 #3: Delete Firewall Rules

To display line number along with other information for existing rules, enter:
# iptables -L INPUT -n --line-numbers
# iptables -L OUTPUT -n --line-numbers
# iptables -L OUTPUT -n --line-numbers | less
# iptables -L OUTPUT -n --line-numbers | grep 202.54.1.1
You will get the list of IP. Look at the number on the left, then use number to delete it. For example delete line number 4, enter:
# iptables -D INPUT 4
OR find source IP 202.54.1.1 and delete from rule:
# iptables -D INPUT -s 202.54.1.1 -j DROP
Where,

    -D : Delete one or more rules from the selected chain

		29.2.1.4
#4: Insert Firewall Rules

To insert one or more rules in the selected chain as the given rule number use the following syntax. First find out line numbers, enter:
# iptables -L INPUT -n --line-numbers
Sample outputs:

Chain INPUT (policy DROP)
num  target     prot opt source               destination
1    DROP       all  --  202.54.1.1           0.0.0.0/0
2    ACCEPT     all  --  0.0.0.0/0            0.0.0.0/0           state NEW,ESTABLISHED 

To insert rule between 1 and 2, enter:
# iptables -I INPUT 2 -s 202.54.1.2 -j DROP
To view updated rules, enter:
# iptables -L INPUT -n --line-numbers
Sample outputs:

Chain INPUT (policy DROP)
num  target     prot opt source               destination
1    DROP       all  --  202.54.1.1           0.0.0.0/0
2    DROP       all  --  202.54.1.2           0.0.0.0/0
3    ACCEPT     all  --  0.0.0.0/0            0.0.0.0/0           state NEW,ESTABLISHED

		29.2.1.5 #4: Insert Firewall Rules

To insert one or more rules in the selected chain as the given rule number use the following syntax. First find out line numbers, enter:
# iptables -L INPUT -n --line-numbers
Sample outputs:

Chain INPUT (policy DROP)
num  target     prot opt source               destination
1

		29.2.1.6 #5: Save Firewall Rules

To save firewall rules under CentOS / RHEL / Fedora Linux, enter:
# service iptables save
In this example, drop an IP and save firewall rules:
# iptables -A INPUT -s 202.5.4.1 -j DROP
# service iptables save
For all other distros use the iptables-save command:
# iptables-save > /root/my.active.firewall.rules
# cat /root/my.active.firewall.rules


		29.2.1.7 #6: Restore Firewall Rules

To restore firewall rules form a file called /root/my.active.firewall.rules, enter:
# iptables-restore < /root/my.active.firewall.rules
To restore firewall rules under CentOS / RHEL / Fedora Linux, enter:
# service iptables restart


		29.2.1.8 #7: Set the Default Firewall Policies

To drop all traffic:
# iptables -P INPUT DROP
# iptables -P OUTPUT DROP
# iptables -P FORWARD DROP
# iptables -L -v -n
#### you will not able to connect anywhere as all traffic is dropped ###
# ping cyberciti.biz
# wget http://www.kernel.org/pub/linux/kernel/v3.0/testing/linux-3.2-rc5.tar.bz2
#7.1: Only Block Incoming Traffic

To drop all incoming / forwarded packets, but allow outgoing traffic, enter:
# iptables -P INPUT DROP
# iptables -P FORWARD DROP
# iptables -P OUTPUT ACCEPT
# iptables -A INPUT -m state --state NEW,ESTABLISHED -j ACCEPT
# iptables -L -v -n
### *** now ping and wget should work *** ###
# ping cyberciti.biz
# wget http://www.kernel.org/pub/linux/kernel/v3.0/testing/linux-3.2-rc5.tar.bz2

		29.2.1.9 #8:Drop Private Network Address On Public Interface

IP spoofing is nothing but to stop the following IPv4 address ranges for private networks on your public interfaces. Packets with non-routable source addresses should be rejected using the following syntax:
# iptables -A INPUT -i eth1 -s 192.168.0.0/24 -j DROP
# iptables -A INPUT -i eth1 -s 10.0.0.0/8 -j DROP
#8.1: IPv4 Address Ranges For Private Networks (make sure you block them on public interface)

|    10.0.0.0/8 -j (A)
|    172.16.0.0/12 (B)
|    192.168.0.0/16 (C)
|    224.0.0.0/4 (MULTICAST D)
|    240.0.0.0/5 (E)
|    127.0.0.0/8 (LOOPBACK)

		29.2.1.10 #9: Blocking an IP Address (BLOCK IP)

To block an attackers ip address called 1.2.3.4, enter:
# iptables -A INPUT -s 1.2.3.4 -j DROP
# iptables -A INPUT -s 192.168.0.0/24 -j DROP

		29.2.1.11 #10: Block Incoming Port Requests (BLOCK PORT)

To block all service requests on port 80, enter:
# iptables -A INPUT -p tcp --dport 80 -j DROP
# iptables -A INPUT -i eth1 -p tcp --dport 80 -j DROP

To block port 80 only for an ip address 1.2.3.4, enter:
# iptables -A INPUT -p tcp -s 1.2.3.4 --dport 80 -j DROP
# iptables -A INPUT -i eth1 -p tcp -s 192.168.1.0/24 --dport 80 -j DROP

		29.2.1.12 #11: Block Outgoing IP Address

To block outgoing traffic to a particular host or domain such as cyberciti.biz, enter:
# host -t a cyberciti.biz
Sample outputs:

cyberciti.biz has address 75.126.153.206

Note down its ip address and type the following to block all outgoing traffic to 75.126.153.206:
# iptables -A OUTPUT -d 75.126.153.206 -j DROP
You can use a subnet as follows:
# iptables -A OUTPUT -d 192.168.1.0/24 -j DROP
# iptables -A OUTPUT -o eth1 -d 192.168.1.0/24 -j DROP
#11.1: Example - Block Facebook.com Domain

First, find out all ip address of facebook.com, enter:
# host -t a www.facebook.com
Sample outputs:

www.facebook.com has address 69.171.228.40

Find CIDR for 69.171.228.40, enter:
# whois 69.171.228.40 | grep CIDR
Sample outputs:

CIDR:           69.171.224.0/19

To prevent outgoing access to www.facebook.com, enter:
# iptables -A OUTPUT -p tcp -d 69.171.224.0/19 -j DROP
You can also use domain name, enter:
# iptables -A OUTPUT -p tcp -d www.facebook.com -j DROP
# iptables -A OUTPUT -p tcp -d facebook.com -j DROP

From the iptables man page:

    ... specifying any name to be resolved with a remote query such as DNS (e.g., facebook.com is a really bad idea), a network IP address (with /mask), or a plain IP address ...

		29.2.1.13 #12: Log and Drop Packets

Type the following to log and block IP spoofing on public interface called eth1
# iptables -A INPUT -i eth1 -s 10.0.0.0/8 -j LOG --log-prefix "IP_SPOOF A: "
# iptables -A INPUT -i eth1 -s 10.0.0.0/8 -j DROP
By default everything is logged to /var/log/messages file.
# tail -f /var/log/messages
# grep --color 'IP SPOOF' /var/log/messages

		29.2.1.14 #13: Log and Drop Packets with Limited Number of Log Entries

The -m limit module can limit the number of log entries created per time. This is used to prevent flooding your log file. To log and drop spoofing per 5 minutes, in bursts of at most 7 entries .
# iptables -A INPUT -i eth1 -s 10.0.0.0/8 -m limit --limit 5/m --limit-burst 7 -j LOG --log-prefix "IP_SPOOF A: "
# iptables -A INPUT -i eth1 -s 10.0.0.0/8 -j DROP


		29.2.1.15 #14: Drop or Accept Traffic From Mac Address

Use the following syntax:
# iptables -A INPUT -m mac --mac-source 00:0F:EA:91:04:08 -j DROP
## *only accept traffic for TCP port # 8080 from mac 00:0F:EA:91:04:07 * ##
# iptables -A INPUT -p tcp --destination-port 22 -m mac --mac-source 00:0F:EA:91:04:07 -j ACCEPT

		29.2.1.16 #15: Block or Allow ICMP Ping Request

Type the following command to block ICMP ping requests:
# iptables -A INPUT -p icmp --icmp-type echo-request -j DROP
# iptables -A INPUT -i eth1 -p icmp --icmp-type echo-request -j DROP
Ping responses can also be limited to certain networks or hosts:
# iptables -A INPUT -s 192.168.1.0/24 -p icmp --icmp-type echo-request -j ACCEPT
The following only accepts limited type of ICMP requests:
### ** assumed that default INPUT policy set to DROP ** #############
iptables -A INPUT -p icmp --icmp-type echo-reply -j ACCEPT
iptables -A INPUT -p icmp --icmp-type destination-unreachable -j ACCEPT
iptables -A INPUT -p icmp --icmp-type time-exceeded -j ACCEPT
## ** all our server to respond to pings ** ##
iptables -A INPUT -p icmp --icmp-type echo-request -j ACCEPT

		29.2.1.17 #16: Open Range of Ports

Use the following syntax to open a range of ports:
iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 7000:7010 -j ACCEPT


		29.2.1.18 #17: Open Range of IP Addresses

Use the following syntax to open a range of IP address:
## only accept connection to tcp port 80 (Apache) if ip is between 192.168.1.100 and 192.168.1.200 ##
iptables -A INPUT -p tcp --destination-port 80 -m iprange --src-range 192.168.1.100-192.168.1.200 -j ACCEPT

## nat example ##
iptables -t nat -A POSTROUTING -j SNAT --to-source 192.168.1.20-192.168.1.25

		29.2.1.19 #18: Established Connections and Restaring The Firewall

When you restart the iptables service it will drop established connections as it unload modules from the system under RHEL / Fedora / CentOS Linux. Edit, /etc/sysconfig/iptables-config and set IPTABLES_MODULES_UNLOAD as follows:

IPTABLES_MODULES_UNLOAD = no

		29.2.1.20 #19: Help Iptables Flooding My Server Screen

Use the crit log level to send messages to a log file instead of console:
iptables -A INPUT -s 1.2.3.4 -p tcp --destination-port 80 -j LOG --log-level crit

		29.2.1.21 #20: Block or Open Common Ports

The following shows syntax for opening and closing common TCP and UDP ports:

 
Replace ACCEPT with DROP to block port:
## open port ssh tcp port 22 ##
iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 22 -j ACCEPT
iptables -A INPUT -s 192.168.1.0/24 -m state --state NEW -p tcp --dport 22 -j ACCEPT
 
## open cups (printing service) udp/tcp port 631 for LAN users ##
iptables -A INPUT -s 192.168.1.0/24 -p udp -m udp --dport 631 -j ACCEPT
iptables -A INPUT -s 192.168.1.0/24 -p tcp -m tcp --dport 631 -j ACCEPT
 
## allow time sync via NTP for lan users (open udp port 123) ##
iptables -A INPUT -s 192.168.1.0/24 -m state --state NEW -p udp --dport 123 -j ACCEPT
 
## open tcp port 25 (smtp) for all ##
iptables -A INPUT -m state --state NEW -p tcp --dport 25 -j ACCEPT
 
# open dns server ports for all ##
iptables -A INPUT -m state --state NEW -p udp --dport 53 -j ACCEPT
iptables -A INPUT -m state --state NEW -p tcp --dport 53 -j ACCEPT
 
## open http/https (Apache) server port to all ##
iptables -A INPUT -m state --state NEW -p tcp --dport 80 -j ACCEPT
iptables -A INPUT -m state --state NEW -p tcp --dport 443 -j ACCEPT
 
## open tcp port 110 (pop3) for all ##
iptables -A INPUT -m state --state NEW -p tcp --dport 110 -j ACCEPT
 
## open tcp port 143 (imap) for all ##
iptables -A INPUT -m state --state NEW -p tcp --dport 143 -j ACCEPT
 
## open access to Samba file server for lan users only ##
iptables -A INPUT -s 192.168.1.0/24 -m state --state NEW -p tcp --dport 137 -j ACCEPT
iptables -A INPUT -s 192.168.1.0/24 -m state --state NEW -p tcp --dport 138 -j ACCEPT
iptables -A INPUT -s 192.168.1.0/24 -m state --state NEW -p tcp --dport 139 -j ACCEPT
iptables -A INPUT -s 192.168.1.0/24 -m state --state NEW -p tcp --dport 445 -j ACCEPT
 
## open access to proxy server for lan users only ##
iptables -A INPUT -s 192.168.1.0/24 -m state --state NEW -p tcp --dport 3128 -j ACCEPT
 
## open access to mysql server for lan users only ##
iptables -I INPUT -p tcp --dport 3306 -j ACCEPT
 

		29.2.1.22 #21: Restrict the Number of Parallel Connections To a Server Per Client IP

You can use connlimit module to put such restrictions. To allow 3 ssh connections per client host, enter:
# iptables -A INPUT -p tcp --syn --dport 22 -m connlimit --connlimit-above 3 -j REJECT

Set HTTP requests to 20:
# iptables -p tcp --syn --dport 80 -m connlimit --connlimit-above 20 --connlimit-mask 24 -j DROP
Where,

    --connlimit-above 3 : Match if the number of existing connections is above 3.
    --connlimit-mask 24 : Group hosts using the prefix length. For IPv4, this must be a number between (including) 0 and 32.

		29.2.1.23 #22: HowTO: Use iptables Like a Pro

For more information about iptables, please see the manual page by typing man iptables from the command line:
$ man iptables
You can see the help using the following syntax too:
# iptables -h
To see help with specific commands and targets, enter:
# iptables -j DROP -h
#22.1: Testing Your Firewall

Find out if ports are open or not, enter:
# netstat -tulpn
Find out if tcp port 80 open or not, enter:
# netstat -tulpn | grep :80
If port 80 is not open, start the Apache, enter:
# service httpd start
Make sure iptables allowing access to the port 80:
# iptables -L INPUT -v -n | grep 80
Otherwise open port 80 using the iptables for all users:
# iptables -A INPUT -m state --state NEW -p tcp --dport 80 -j ACCEPT
# service iptables save
Use the telnet command to see if firewall allows to connect to port 80:
$ telnet www.cyberciti.biz 80
Sample outputs:

Trying 75.126.153.206...
Connected to www.cyberciti.biz.
Escape character is '^]'.
^]
telnet> quit
Connection closed.

You can use nmap to probe your own server using the following syntax:
$ nmap -sS -p 80 www.cyberciti.biz
Sample outputs:

Starting Nmap 5.00 ( http://nmap.org ) at 2011-12-13 13:19 IST
Interesting ports on www.cyberciti.biz (75.126.153.206):
PORT   STATE SERVICE
80/tcp open  http
Nmap done: 1 IP address (1 host up) scanned in 1.00 seconds

I also recommend you install and use sniffer such as tcpdupm and ngrep to test your firewall settings.
Conclusion:

This post only list basic rules for new Linux users. You can create and build more complex rules. This requires good understanding of TCP/IP, Linux kernel tuning via sysctl.conf, and good knowledge of your own setup. Stay tuned for next topics:

    Stateful packet inspection.
    Using connection tracking helpers.
    Network address translation.
    Layer 2 filtering.
    Firewall testing tools.
    Dealing with VPNs, DNS, Web, Proxy, and other protocols.

	29.2.2 My examples

		29.2.2.1 Block incoming traffic from given server
- ex, block AD DC
a. Get DC IP address

[root@yizaq-ise2 ~]# nslookup r1.dom
Server:		10.56.53.79
Address:	10.56.53.79#53

Name:	r1.dom
Address: 10.56.53.111
Name:	r1.dom
Address: 10.56.53.76

[root@yizaq-ise2 ~]# nslookup 
> set type=SRV
>  _ldap._tcp.dc._msdcs.r1.dom 
Server:		10.56.53.79
Address:	10.56.53.79#53

_ldap._tcp.dc._msdcs.r1.dom	service = 0 100 389 r1dc01.r1.dom.
> exit

[root@yizaq-ise2 ~]# nslookup r1dc01.r1.dom
Server:		10.56.53.79
Address:	10.56.53.79#53

Name:	r1dc01.r1.dom
Address: 10.56.53.76

b. block DC

[root@cow2 ~]# iptables -I INPUT -s 10.56.53.79 -j DROP
[root@cow2 ~]# iptables -L INPUT -n --line-numbers

c. to unblock
[root@yizaq-ise2 ~]#  iptables -L INPUT -n --line-numbers
Chain INPUT (policy DROP)
num  target     prot opt source               destination         
1    DROP       all  --  10.56.50.66          0.0.0.0/0           
2    RATELIMIT  all  --  0.0.0.0/0            0.0.0.0/0           
3    ACCEPT     all  --  0.0.0.0/0            0.0.0.0/0           state RELATED,ESTABLISHED 

# iptables -D INPUT 1

		29.2.2.2
	29.2.3
    29.3
30. Performance Optimization, tools and Analysis


	30.1  Tools

		30.1.1  uptime
[root@acs-yizaq-01 ~]# uptime 
 16:11:14 up 4 days, 5 min,  1 user,  load average: 0.01, 0.03, 0.00
In particular, the load average represents the average number of tasks that could be run over a period of 1, 5, and 15 minutes. Runnable tasks are those that either are currently running or those that can run but are waiting for a processor to be available. 

load should be less than # of CPUs as determined by next command:

		30.1.2  cat /proc/cpuinfo 
[root@acs-yizaq-01 ~]# cat /proc/cpuinfo 
processor       : 0
vendor_id       : GenuineIntel
cpu family      : 6
model           : 44
model name      : Intel(R) Xeon(R) CPU           E5620  @ 2.40GHz
stepping        : 2
cpu MHz         : 2400.085
cache size      : 12288 KB
fdiv_bug        : no
hlt_bug         : no
f00f_bug        : no
coma_bug        : no
fpu             : yes
fpu_exception   : yes
cpuid level     : 11
wp              : yes
flags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss nx rdtscp lm constant_tsc ida nonstop_tsc pni cx16 popcnt lahf_lm [8]
bogomips        : 4800.17

processor       : 1
vendor_id       : GenuineIntel
cpu family      : 6
model           : 44
model name      : Intel(R) Xeon(R) CPU           E5620  @ 2.40GHz
stepping        : 2
cpu MHz         : 2400.085
cache size      : 12288 KB
fdiv_bug        : no
hlt_bug         : no
f00f_bug        : no
coma_bug        : no
fpu             : yes
fpu_exception   : yes
cpuid level     : 11
wp              : yes
flags           : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss nx rdtscp lm constant_tsc ida nonstop_tsc pni cx16 popcnt lahf_lm [8]
bogomips        : 4922.79


		30.1.3 vmstat

vmstat is a real-time performance monitoring tool. The vmstat command provides data that can be used to help find unusual system activity, such as high page faults or excessive context switches, which can degrade system performance. The frequency of data display is a user-specified parameter. A sample of the vmstat output is as follows:
Code View: Scroll / Show All

							procs -----------memory---------- ---swap-- -----io---- --system-- ----cpu----
							r   b   swpd  free   buff  cache   si   so    bi    bo   in    cs  us sy id wa
							18  8    0 5626196   3008 122788    0    0 330403   454 2575  4090 91  8  1  0
							18 15    0 5625132   3008 122828    0    0 328767   322 2544  4264 91  8  0  0
							17 12    0 5622004   3008 122828    0    0 327956   130 2406  3998 92  8  0  0
							22  2    0 5621644   3008 122828    0    0 327892   689 2445  4077 92  8  0  0
							23  5    0 5621616   3008 122868    0    0 323171   407 2339  4037 92  8  1  0
							21 14    0 5621868   3008 122868    0    0 323663    23 2418  4160 91  9  0  0
							22 10    0 5625216   3008 122868    0    0 328828   153 2934  4518 90  9  1  0
						

					  


vmstat produces the following information:

    The procs section reports the number of running processes (r) and blocked processes (b) at the time of reporting. You can use the information in this section to check whether the number of running and blocked processes is similar to what you expected. If it isn't, you can check a number of things: the parameters of the applications and kernel, system scheduler and I/O scheduler, the distribution of processes among available processors, and so on.

    The memory section reports the amount of memory being swapped out (swpd), free memory (free), buffer cache for I/O data structures (buff), and cached memory for files read from the disk (cache) in kilobytes. The value of swpd reflects kswapd activities.

    The swap section returns the amount of memory swapped in (si) from disk and swapped out (so) to disk, in kilobytes per second. Note that so reflects the kswapd activity as data is swapped out to the swap area. However, si reflects page fault activities as pages are swapped back to the physical memory.

    The io section reports the number of blocks read in (bi) from the devices and blocks written out (bo) to the devices in kilobytes per second. Pay particular attention to these two fields when running I/O-intensive applications.

    The system section reports the number of interrupts (in) and context switches (cs) per second.

    The cpu section reports the percentage of total CPU time in terms of user (us), system (sy), true idleness (id), and waiting for I/O completion (wa). Perhaps the most common metric is CPU utilization. If wa is large, examine the I/O subsystem—you may conclude, for example, that more I/O controllers and disks are needed to reduce the I/O wait time.

For I/O-intensive workloads, you can monitor bi and bo for the transfer rate and in for the interrupt rate. You can monitor swpd, si, and so to see whether the system is swapping. If so, you can check on the swapping rate. Perhaps the most common metric is CPU utilization and the monitoring of us, sy, id, and wa. If wa is large, you need to examine the I/O subsystem. You might come to the conclusion that more I/O controllers and disks are needed to reduce the I/O wait time.

Like uptime, when vmstat is run without options, it reports a single snapshot of the system. If you run uptime, followed by vmstat, you can get a quick snapshot of how busy the system is and some indication of what the processor is doing with the percentage breakdown of user, system, idle, and waiting on I/O times. In addition, vmstat provides an instantaneous count of the number of runnable processes. Note that uptime provides another view of the number of runnable processes across three time periods: 1 minute, 5 minutes, and 15 minutes. So, if the load average from uptime remains above 1 for any period of time, the number of runnable tasks reported by vmstat should also be near 1.

Because vmstat can also provide information at regular, repeated intervals, you can get a more dynamic view of the system with the following command:

$ vmstat 5 10


This command outputs vmstat information every 5 seconds for 10 iterations. Again, the output should generally show about one runnable task in each line of output if the load average has been 1 for the past 1/5/10 minutes per the output of uptime. Do not be surprised to see peaks of 5, 7, or even 20 in the output of vmstat! Remember that the load average is a calculated average as opposed to an instantaneous snapshot. Both views have their benefits to system performance analysis.

vmstat also provides a rough view of how much time the processor is spending in the operating system, how much time is spent in user application space (including libraries provided as part of the application or operating system), how much time is idle, and how much time the operating system is blocked waiting on I/O. Various workloads have different utilization patterns, so there are no definite rules as to what values are good or bad. In these cases, it is good to have developed a baseline for your workload to help you recognize how the workload has changed as the response time or system load increases. vmstat helps identify which tools can help drill down into any workload degradations or provide clues as to where tuning or reconfiguration can improve workload performance.

The remainder of this section explores a few examples. Imagine a scenario where users are reporting poor response times with a workload. Examining the load average via uptime shows the load average to be very low, possibly even well below the baseline times. Further, vmstat shows that the number of runnable jobs was very low and that the system was relatively idle from the percentage of CPU idle time. An analyst might be interpret these results as a sign that some key process had exited or was blocking waiting on some event that wasn't completing. For instance, some applications use a form of semaphore technique to dispatch work and wait for completion. Perhaps the work was dispatched to a back-end server or other application and that application has for some reason stopped processing all activity. As a result, the application closest to the user is blocked, not runnable, waiting on some semaphore to notify it of completion before it can return information to the user. This might cause the administrator to focus attention on the server application to see why it is unable to complete the requests being queued for it.

In another scenario, suppose that the load average is showing a load in excess of 1, possibly even a full point higher on average than established baselines. In addition, vmstat shows one or two processes always runnable, but the percentage of user time is nearly 100% over an extended period of time. Another tool might be necessary to find out what process or processes are using up 100% of the CPU time—for instance, ps(1) or top(1). ps(1) provides a listing of all processes that currently exist, or some selected subset of processes based on its options. top(1) (or gtop(1)) provides a constantly updating view of the most active processes, where most active can be defined as those processes that are using the most processor time. This data might help identify a runaway process that is doing nothing useful on the system. If vmstat(1) had reported that the processes were mostly running in user space, the administrator might want to connect a debugger such as gdb(1) to the process and use breakpoints, tracing, or other debugging means to understand what the application was doing. If vmstat had reported that most of the time was being consumed as “system” time, other tools such as strace(1) might be used to find out what system calls were being made. If vmstat(1) had reported that a large percentage of time was being spent waiting for I/O completion, tools such sar(1) could be used to see what devices were being used and also provide some possible insights into which applications or file systems were in use, whether the system was swapping or paging, and so on.

vmstat(1) provides some simple insights into the current state of the system. In this section, we looked at the areas that primarily interact with the CPU utilization. However, vmstat(1) also provides some insight into memory utilization, basic swapping activity, and I/O activity. Later sections in this chapter look at more of these areas in detail.

		30.1.4 top and gtop

top and gtop are very useful tools for understanding the tasks and processes that contribute to the high-level information provided by vmstat or uptime. They can show which processes are active and which ones over time are consuming the most processing time or memory.

The top command provides an updating overview of all running processes and the system load. top provides information on CPU load, memory usage, and usage per process, as detailed in the snapshot that follows. Note that it also provides load average snapshots much like uptime(1) does; however, top also provides a breakout of the number of processes that have been created but that are currently sleeping, and the number of processes that are running. “Sleeping” tasks are those that are blocked waiting on some activity, such as a key press from a user at a keyboard, data from a pipe or socket, requests from another host (such as a web server waiting for someone to request content), and so on. top(1) also shows load average for each processor independently, which can help identify any imbalances in scheduling tasks. By default, the output of top is refreshed frequently, and tasks are sorted by percentage of CPU consumption. Other sorting options are possible, such as cumulative CPU consumption or percentage of memory consumption.
Code View: Scroll / Show All

							4:52pm  up  5:08,  3 users,  load average: 2.77, 5.81, 3.15
							37 processes: 36 sleeping, 1 running, 0 zombie, 0 stopped
							CPU0 states:  5.1% user, 53.1% system,  0.0% nice, 41.1% idle
							CPU1 states:  5.0% user, 52.4% system,  0.0% nice, 41.4% idle
							Mem:   511480K av,   43036K used,  468444K free,       0K shrd,    2196K
							Swap:  263992K av,       0K used,  263992K free                 21432K
							PID USER   PRI  NI  SIZE  RSS SHARE STAT %CPU %MEM  TIME COMMAND
							1026 root    2   0   488  488   372  S    1.1  0.0   0:00 chat_s
							7490 root   11   0  1012 1012   816  R    0.5  0.1   0:00 top
							3 root   19  19     0    0     0  SWN  0.3  0.0   0:00 ksoftirqd_C
							4 root   19  19     0    0     0  SWN  0.1  0.0   0:00 ksoftirqd_C
							1 root    9   0   536  536   468  S    0.0  0.1   0:04 init
							2 root    9   0     0    0     0  SW   0.0  0.0   0:00 keventd
							5 root    9   0     0    0     0  SW   0.0  0.0   0:00 kswapd
							6 root    9   0     0    0     0  SW   0.0  0.0   0:00 bdflush
							7 root    9   0     0    0     0  SW   0.0  0.0   0:00 kupdated
							9 root    9   0     0    0     0  SW   0.0  0.0   0:00 scsi_eh_0
							10 root    9   0     0    0     0  SW   0.0  0.0   0:00 khubd
							331 root    9   0   844  844   712  S    0.0  0.1   0:00 syslogd
							341 root    9   0  1236 1236   464  S    0.0  0.2   0:00 klogd
							356 rpc     9   0   628  628   536  S    0.0  0.1   0:00 portmap
						

					  


top includes the following information in its output:

    Line 1 shows the system uptime, including the current time, how long the system has been up since the last reboot, the current number of users, and three load average numbers. The load average numbers represent the average number of processors ready to run during the previous 1, 5, and 15 minutes.

    Line 2 includes the process statistics, including the total number of processes running at the time of the last top screen update. This line also includes the number of sleeping, running, zombie, and stopped processes.

    Lines 3 and 4 display CPU statistics for individual CPUs, including the percentage of CPU time used by the user, system, niced, and idle processes.

    Line 5 provides memory statistics, including total memory available, used memory, free memory, shared memory by different processes, and memory used for buffers.

    Line 6 shows virtual memory or swap statistics, including total available swap space, used swap space, free swap space, and cached swap space.

    The remaining lines show statistics on the individual processes. Some of the more useful top parameters are as follows:

    d	Delay between updates to the data.
    p	Display only the processes specified. Up to 20 processes can be specified.
    S	Display a summary of time being spent by the process and its children. Also displays dead time.
    I	Do not report idle processes.
    H	Show all threads by process.
    N	Number of times to produce the reports.


top also has a dynamic mode to change report information. Activate dynamic mode by pressing the f key. By further pressing the j key, you can add a new column to show the CPU last used by an executing process. This additional information is particularly useful for understanding the process behavior in an SMP system.

This section barely scratches the surface of the types of information that top or gtop provide. For more information on top, see the corresponding man page; for more information on gtop, see the built-in help command.


		30.1.5 sar

sar is part of the sysstat package. sar collects and reports a wide range of system activity in the operating system, including CPU utilization, the rate of context switches and interrupts, the rate of paging in and paging out, the shared memory usage, the buffer usage, and network usage. sar(1) is useful because it constantly collects and logs system activity information in a set of log files, which makes it possible to evaluate performance problems both prior to the reporting of a performance regression event as well as after the event. sar can often be used to pinpoint the time of the event and can also be used to identify specific changes in the system's behavior. sar can also output information with a shorter interval or a fixed number of intervals, much like vmstat. Based on the values in the count and interval parameters, the sar tool writes information the specified number of times spaced at the specified intervals in seconds. In addition, sar can provide averages for a number of data points that it collects. The following example provides statistics on a four-way SMP system by collecting data every 5 seconds:
CPU Utilization

								11:09:13   CPU  %user  %nice  %system %iowait  %idle
								11:09:18   all   0.00   0.00     4.70   52.45  42.85
								11:09:18     0   0.00   0.00     5.80   57.00  37.20
								11:09:18     1   0.00   0.00     4.80   49.40  45.80
								11:09:18     2   0.00   0.00     6.00   62.20  31.80
								11:09:18     3   0.00   0.00     2.40   41.12  56.49
								11:09:23   all   0.00   0.00     3.75   47.30  48.95
								11:09:23     0   0.00   0.00     5.39   37.33  57.29
								11:09:23     1   0.00   0.00     2.80   41.80  55.40
								11:09:23     2   0.00   0.00     5.40   41.60  53.00
								11:09:23     3   0.00   0.00     1.40   68.60  30.00
								. . .
								Average:   all   0.00   0.00     4.22   16.40  79.38
								Average:     0   0.00   0.00     8.32   24.33  67.35
								Average:     1   0.00   0.00     2.12   14.35  83.53
								Average:     2   0.01   0.00     4.16   12.07  83.76
								Average:     3   0.00   0.00     2.29   14.85  82.86
							


One component of CPU consumption by the system is the networking and disk servicing routines. As the operating system generates I/O, the corresponding device subsystems respond by signaling the completion of those I/O requests with hardware interrupts. The operating system counts each of these interrupts; the output can help you visualize the rate of networking and disk I/O activity. sar(1) provides this input. With baselines, it is possible to track the rate of system interrupts, which can be another source of system overhead or an indicator of possible changes to system performance. The –I SUM option can generate the following information, including the total number of interrupts per second. The –I ALL option can provide similar information for each interrupt source (not shown).
Interrupt Rate

								10:53:53         INTR    intr/s
								10:53:58          sum   4477.60
								10:54:03          sum   6422.80
								10:54:08          sum   6407.20
								10:54:13          sum   6111.40
								10:54:18          sum   6095.40
								10:54:23          sum   6104.81
								10:54:28          sum   6149.80
								. . .
								Average:          sum   4416.53
							


A per-CPU view of the interrupt distribution on an SMP machine is available through the sar –A command (the following example is an excerpt from the full output). Note that the IRQ values of the system are 0, 1, 2, 9, 12, 14, 17, 18, 21, 23, 24, and 25. Due to the limited width of the page, interrupts for 9, 12, 14, and 17 have been clipped away.
Interrupt Distribution
Code View: Scroll / Show All

								10:53:53  CPU  i000/s i001/s i002/s ... i018/s  i021/s i023/s i024/s  i025/s
								10:53:58    0 1000.20   0.00   0.00 ...  0.40    0.00    0.00   3.00    0.00
								10:53:58    1    0.00   0.00   0.00 ...  0.00    0.00    0.00   0.00 2320.00
								10:53:58    2    0.00   0.00   0.00 ...  0.00 1156.00    0.00   0.00    0.00
								10:53:58    3    0.00   0.00   0.00 ...  0.00    0.00    0.00   0.00    0.00
								Average:    0  999.94   0.00   0.00 ...  1.20  590.99    0.00   3.73    0.00
								Average:    1    0.00   0.00   0.00 ...  0.00    0.00    0.00   0.00  926.61
								Average:    2    0.00   0.00   0.00 ...  0.00  466.51    0.00   0.00 1427.48
								Average:    3    0.00   0.00   0.00 ...  0.00    0.00    0.00   0.00    0.00
							

					  


The study of interrupt distribution might reveal an imbalance in interrupt processing. The next step should be an examination of the scheduler. One way to tackle the problem is to bind IRQ processing to a specific processor or a number of processors by setting up an affinity for a particular device's interrupt (or IRQ) to a particular CPU or set of CPUs. For example, if 0x0001 is echoed to /proc/irq/ID, where ID corresponds to a device, only CPU 0 will process IRQ for this device. If 0x000f is echoed to /proc/irq/ID, CPU 0 through CPU 3 will be used to process IRQ for this device. For some workloads, this technique can reduce contention on certain heavily used processors. This technique allows I/O interrupts to be processed more efficiently; the I/O performance should increase accordingly.


		30.1.6 Memory Utilization tools

Workloads have a tendency to consume all available memory. Linux provides reasonably efficient access to physical memory and provides access to potentially huge amounts of “virtual” memory. Virtual memory is usually little more than a capability of an operating system to offload less frequently used data to disk storage while presenting the illusion that the system has an enormous amount of physical memory. Unfortunately, the price for offloading memory can be ten or a hundred times more expensive in terms of application latency. Those high latencies can impact application response times dramatically if the memory that is paged out to disk is the wrong memory, or if the application's active memory footprint is larger than the size of physical memory.

			30.1.6.1 /proc/meminfo and  /proc/slabinfo


Many performance problems are caused by insufficient memory, which triggers system swapping. Thus, it is useful to have tools that monitor memory utilization—for example, how the kernel memory is consumed per process or per thread, and how the memory is consumed by the kernel data structures along with their counts and sizes. As with CPU utilization, understanding how both the system and individual processes are behaving is key to tracking down any performance problems caused by memory shortages.
/proc/meminfo and /proc/slabinfo

Linux provides facilities to monitor the utilization of overall system memory resources under the /proc file system—namely, /proc/meminfo and /proc/slabinfo. These two files capture the state of the physical memory. A partial display of /proc/meminfo is as follows:

							MemTotal:      8282420 kB
							MemFree:       7942396 kB
							Buffers:         46992 kB
							Cached:         191936 kB
							SwapCached:          0 kB
							HighTotal:     7470784 kB
							HighFree:      7232384 kB
							LowTotal:       811636 kB
							LowFree:        710012 kB
							SwapTotal:      618492 kB
							SwapFree:       618492 kB
							Mapped:          36008 kB
							Slab:            36652 kB
						


MemTotal gives the total amount of physical memory of the system, whereas MemFree gives the total amount of unused memory.

Buffers corresponds to the buffer cache for I/O operations. Cached corresponds to the memory for reading files from the disk.

SwapCached represents the amount of cache memory that has been swapped out in the swap space.

SwapTotal represents the amount of disk memory for swapping purposes. If an IA32-based system has more than 1GB of physical memory, HighTotal is nonzero.

HighTotal corresponds to memory greater than ~860MB of the physical memory.

LowTotal is the memory used by the kernel. Mapped corresponds to the files that are memory-mapped.

Slab corresponds to the memory used for the kernel data structures. By capturing /proc/meminfo periodically, you can establish a pattern of memory utilization. With the aid of simple scripts and graphics tools, the pattern can be also summarized visually.

To understand kernel memory consumption, examine /proc/slabinfo. A partial display of /proc/slabinfo is as follows:

							tcp_bind_bucket       56    224     32    2    2    1
							tcp_open_request      16     58     64    1    1    1
							inet_peer_cache        0      0     64    0    0    1
							secpath_cache          0      0     32    0    0    1
							flow_cache             0      0     64    0    0    1
						


The first column lists the names of the kernel data structures. To further describe tcp_bind_bucket, there is a total of 224 tcp_bind_bucket objects, 56 of which are active. Each data structure takes up 32 bytes. There are two pages that have at least one active object, and there is a total of two allocated pages. Moreover, one page is allocated for each slab. This information highlights certain data structures that merit more focus, such as those with larger counts or sizes. Thus, by capturing meminfo and slabinfo together, you can begin to understand what elements of the operating system are consuming the most memory. If the values of LowFree or HighFree are relatively small (or smaller than usual), that might indicate that the system is running with more requests for memory than usual, which may lead to a reduction in overall performance or application response times.
ps

To find out how the memory is used within a particular process, use ps for an overview of memory used per process:

			30.1.6.2 ps memory info
							$ ps aux
							USER PID %CPU %MEM   VSZ  RSS TTY STAT START TIME COMMAND
							root   1  0.0  0.0  1528  528 ?   S    15:24 0:00 init [2]
							root   2  0.0  0.0     0    0 ?   SN   15:24 0:00 [ksoftirqd/0]
							root   3  0.0  0.0     0    0 ?   S<   15:24 0:00 [events/0]
							root   4  0.0  0.0     0    0 ?   S<   15:24 0:00 [khelper]
							root   5  0.0  0.0     0    0 ?   S<   15:24 0:00 [kacpid]
							root  48  0.0  0.0     0    0 ?   S<   15:24 0:00 [kblockd/0]
							root  63  0.0  0.0     0    0 ?   S    15:24 0:00 [pdflush]
							root  64  0.0  0.0     0    0 ?   S    15:24 0:00 [pdflush]
						


The output of the ps aux command shows the total percentage of system memory that each process consumes, as well as its virtual memory footprint (VSZ) and the amount of physical memory that the process is currently using (RSS). You can also use top(1) to sort the process listing interactively to see which processes are consuming the most memory and how that consumption changes as the system runs.

			30.1.6.3 /proc/pid/maps
After you have identified a few processes of interest, you can look into the specific allocations of memory that the process is using by looking at the layout of the processes' virtual address space. /proc/pid/maps, where pid is the process ID of a particular process as found through ps(1) or top(1), contains all mappings of the processes' address spaces and their sizes. Each map shows the address range that is allocated, the permissions on the page, and the location of the backing store associated with that address range (if any). /proc/pid/maps is not a performance tool per se; however, it provides insight into how memory is allocated. For example, for performance purposes, you can confirm whether a certain amount of shared memory is allocated between 1GB and 2GB in the virtual address space. The preceding map can be used to examine its utilization.

The following output is for process ID 3162:
Code View: Scroll / Show All

							$ cat /proc/3162/maps
							08048000-08056000 r-xp 00000000 03:05 33015   /usr/lib/gnome-applets/battstat-applet-2
							08056000-08058000 rw-p 0000d000 03:05 33015   /usr/lib/gnome-applets/battstat-applet-2
							08058000-08163000 rw-p 08058000 00:00 0
							40000000-40016000 r-xp 00000000 03:02 40006   /lib/ld-2.3.2.so
							40016000-40017000 rw-p 00015000 03:02 40006   /lib/ld-2.3.2.so
							40017000-40018000 rw-p 40017000 00:00 0
							40018000-4001a000 r-xp 00000000 03:05 578493  /usr/X11R6/lib/X11/locale/lib/common/xlcDef.so.2
							4001a000-4001b000 rw-p 00001000 03:05 578493  /usr/X11R6/lib/X11/locale/lib/common/xlcDef.so.2
							4001b000-4001d000 r-xp 00000000 03:05 128867  /usr/lib/gconv/ISO8859-1.so
							4001d000-4001e000 rw-p 00001000 03:05 128867  /usr/lib/gconv/ISO8859-1.so
						4001f000-40023000 r-xp 00000000 03:05 514375  /usr/lib/gtk-2.0/2.4.0/loaders/libpixbufloader-png.so
						40023000-40024000 rw-p 00003000 03:05 514375  /usr/lib/gtk-2.0/2.4.0/loaders/libpixbufloader-png.so
							40025000-40031000 r-xp 00000000 03:05 337881  /usr/lib/libpanel-applet-2.so.0.0.19
							40031000-40032000 rw-p 0000c000 03:05 337881  /usr/lib/libpanel-applet-2.so.0.0.19
							40032000-400d2000 r-xp 00000000 03:05 337625  /usr/lib/libgnomeui-2.so.0.600.1
							400d2000-400d6000 rw-p 0009f000 03:05 337625  /usr/lib/libgnomeui-2.so.0.600.1
							400d6000-400d7000 rw-p 400d6000 00:00 0
							400d7000-400df000 r-xp 00000000 03:05 53      /usr/X11R6/lib/libSM.so.6.0
							400df000-400e0000 rw-p 00007000 03:05 53      /usr/X11R6/lib/libSM.so.6.0
							400e0000-400f4000 r-xp 00000000 03:05 51      /usr/X11R6/lib/libICE.so.6.3
							400f4000-400f5000 rw-p 00013000 03:05 51      /usr/X11R6/lib/libICE.so.6.3
						

					  
			30.1.6.4 vmstat

vmstat was introduced in the section on CPU utilization. However, its primary purpose is to monitor memory availability and swapping activity, and it provides an overview of I/O activity. vmstat can be used to help find unusual system activity, such as high page faults or excessive context switches, that can lead to a degradation in system performance. A sample of the vmstat output is as follows:
Code View: Scroll / Show All

							procs -----------memory---------- ---swap-- -----io---- --system-- ----cpu----
							r   b   swpd   free   buff  cache  si   so    bi    bo  in    cs us sy id wa
							18  8      0 5626196   3008 122788  0   0  330403  454 2575  4090 91  8  1  0
							18 15      0 5625132   3008 122828  0   0  328767  322 2544  4264 91  8  0  0
							17 12      0 5622004   3008 122828  0   0  327956  130 2406  3998 92  8  0  0
							22  2      0 5621644   3008 122828  0   0  327892  689 2445  4077 92  8  0  0
							23  5      0 5621616   3008 122868  0   0  323171  407 2339  4037 92  8  1  0
							21 14      0 5621868   3008 122868  0   0  323663   23 2418  4160 91  9  0  0
							22 10      0 5625216   3008 122868  0   0  328828  153 2934  4518 90  9  1  0
						

					  


The memory-related data reported by vmstat includes the following:

    memory reports the amount of memory being swapped out (swpd), free memory (free), buffer cache for I/O data structures (buff), and cached memory for files read from the disk (cache) in kilobytes.

    swap, in kilobytes per second, is the amount of memory swapped in (si) from disk and swapped out (so) to disk.

    io reports the number of blocks read in (bi) from the devices and blocks written out (bo) to the devices in kilobytes per second.

For I/O-intensive workloads, you can monitor bi and bo for the transfer rate, and in for the interrupt rate. You can monitor swpd, si, and so to see whether the system is swapping. If so, you can check on the swapping rate. Perhaps the most common metric is CPU utilization and the monitoring of us, sy, id, and wa. If wa is large, you need to examine the I/O subsystem. You might come to the conclusion that more I/O controllers and disks are needed to reduce the I/O wait time.

		30.1.7 I/O Utilization

Although the overall processor speeds, memory sizes, and I/O speeds continue to increase, I/O throughput and latency are still orders of magnitude slower than equivalent memory access. Additionally, because many workloads have a substantial I/O component, I/O can easily become a significant bottleneck to overall throughput and overall application response times. For I/O-intensive applications, the performance analyst must be able to access tools that help provide insights into the operations of the I/O subsystem.

This section initially looks at disk I/O. A future section looks at networking I/O because the tools to measure throughput and latency for each are different.

For disk I/O, performance is often evaluated in terms of throughput and latency. Disk drives tend to be able to handle large sequential transfers much better than small random transfers. Large sequential transfers allow optimizations such as read ahead or write behind, allow the storage system to reduce head movement and perform full track writes when possible. However, many applications rely on the capability to access data in disparate, often unpredictable locations on the media. As a result, the I/O patterns of various workloads are often a mix of sequential and random I/O, with varying sizes of block transfers.

When evaluating I/O performance on a system, the performance analyst needs to keep in mind several things. The first, and perhaps most obvious (although it is often forgotten), is that I/O performance cannot exceed the performance of the underlying hardware. Although we will not go into much detail on this aspect, it is very helpful when analyzing I/O throughput and latency to understand the system's underlying limitations. For instance, the performance aspects of the storage device, the I/O buses connecting the storage devices (for example, SCSI and Fibre Channel), any limitations imposed by the storage fabric (such as a Fibre Channel Switch), the host bus adapters, the systems interconnect bus (for example, PCI, PCI-X, and Infiniband), and in some cases the system's architecture (such as how host bus adapters perform DMA, how memory is interleaved, and NUMA connectivity) all contribute to the overall performance analysis of the I/O subsystem. In more complex system configurations, it may be helpful to maintain a list of “speeds and feeds” for the system to help distinguish between hardware bottlenecks and software bottlenecks.

To understand software bottlenecks and related performance impacts, the two major considerations are overall I/O throughput and latency of any individual I/O requests. Ideally, a system wants to optimize the data transfer rate to and from the media. However, because the latency of an individual request can be extremely long compared to the speed of the processor, applications can effectively stall waiting for I/O. Take, for example, an application that reads a block of data that provides information on how to access the next block of data, and so on. If the system or application is unable to optimize this pattern, performance is limited to the combined latencies of the I/O subsystem. One common solution to this problem is to perform many similar operations like this simultaneously. Multitasking—the capability of many tasks to run in parallel—allows an application or operating system to schedule many long latency I/O requests simultaneously, even when each application might spend a large portion of the time blocked on an individual request. As a result, the overall efficiency of the I/O subsystem, or the total I/O throughput, might approach the capacity of the underlying I/O subsystem. And, although it is always a goal of the operating system and the individual applications to optimize for overall system throughput, doing so at the expense of end-user response times is not usually an acceptable trade-off.

The underlying I/O subsystem latencies can be severely impacted by the incoming pattern of data transfer requests. For instance, if disk I/O requests alternately ask for a block of I/O at the “beginning” and “end” of the disk media, the physical disk arm may need to make relatively slow adjustments to position the disk head over the selected disk block. This type of access would obviously slow down all accesses to a given device, thereby reducing the number of I/O operations that could be completed over a period of time. Further, such a bizarre access pattern would likely reduce not just the number of devices per logical drive, but also the overall I/O transfer rate.

Another solution to multitasking is to ensure that the data requests from the applications and operating system are well distributed among the disks connected to the system. Distributing the I/O requests to multiple disks effects a level of parallelism that further reduces the performance impacts of the individual latencies of disk drives. Redistributing the application data among a number of disk devices often requires a solid understanding of the workload as well as an understanding of the data access patterns of that workload.

Although system monitoring tools do not provide the capability to track each I/O that a particular application issues, there are tools that allow a performance analyst to monitor the total number of I/Os processed by the system, the number of I/O operations per logical disk drive, and the overall I/O transfer rate. The two primary tools discussed in the next sections are iostat(1) and sar(1). You can use these tools to understand what the I/O bottlenecks are, what disks or interconnects are underutilized, and what the latencies are from a system perspective (as opposed to an application perspective).

Before exploring the specific tools, keep in mind that there are many techniques for increasing I/O performance. These techniques include purely hardware-related solutions (such as using disk drives that have higher revolutions per minute, thereby providing lower I/O latencies, larger disk cache sizes, or I/O controller cache sizes). They also include improved data transfer rates for both reads and writes, and/ increases in I/O bus speeds or I/O fabric speeds, which both increase data transfer rates and reduce I/O latencies. Also, some disk drives and disk storage subsystems provide multiported logical or physical disks, allowing parallel I/O from a single disk, which again increases potential I/O throughput. Additionally, hardware and software RAID (Redundant Array of Independent Disks) were designed to increase access parallelism by striping data across multiple disk drives.

The tools discussed in the following sections provide data that can be useful in considering hardware and software solutions as well as improvements in data layout.

			30.1.7.1 iostat

The iostat command monitors system I/O activities by observing how long the physical disks are active in relation to their average transfer rates. The iostat command generates reports that can be used to change system configuration to better balance the I/O load among physical disks. iostat(1) also provides CPU utilization that can sometimes be useful in comparing directly against the I/O activities. If no display interval is given, iostat gives out I/O information since the system was last booted. If a display interval is given, the first set of output represents total activity since boot time, and subsequent displays only show the delta activities. The following display corresponds to copying files from /dev/sdo7 to /dev/sds7, /dev/sdp7 to /dev/sdt7, and /dev/sdr7 to /dev/sdu7:

							avg-cpu:  %user   %nice    %sys %iowait   %idle
						|	0.21    0.00    0.80    2.07   96.92
						|	Device:  tps Blk_read/s Blk_wrtn/s   Blk_read   Blk_wrtn
						|	sdx     0.00       0.00      0.00         32          0
						|	sdw     0.00       0.00      0.00         32          0
						|	sdv     0.00       0.00      0.00         32          0
						|	sdu     2.49       0.05   1443.46       2778   79552392
						|	sdt     4.94       0.10   2871.73       5322  158268008
						|	sds     4.95       0.10   2860.91       5330  157671720
						|	sdr    30.20    1518.55      0.42   83690898      23288
						|	sdq    60.25    2902.76      0.92  159978258      50896
						|	sdp     0.00       0.01      0.00        378         24
						|	sdo    59.49    2883.87      0.90  158937034      49520
						


iostat reports CPU utilization similar to how it is provided by the top tool. It splits the CPU time into user, nice, system, I/O wait, and system idle. It is followed by the disk utilization report. The disk header is followed by lines of disk statistics, where each line reports the activities of one logical disk that is configured. The tps column represents the number of I/O requests that are issued to the logical disk. However, the size of the I/O requests is not given. Blk_read/s and Blk_wrtn/s represent the amount of data read from and written to the logical drive in a number of blocks per second. Again, the block size is not given. Finally, Blk_read and Blk_wrtn correspond to the amount of data read from and written to the logical drive in a number of blocks per second, without specifying the block's size.

The –k option displays statistics in kilobytes, the -p option gets per-partition statistics, and the -x option gets information such as average wait time and average service time. In the data, the count on Blk_read of sdo is very close to the count of Blk_wrtn of sds, as data is copied from sdo7 to sds7. In addition, the rate of reads is slightly higher than the rate of writes. This report can highlight disk I/O bottlenecks, if any, and helps database designers lay out data to achieve higher access parallelism.

			30.1.7.2 sar

sar is included in the sysstat package. sar collects and reports a wide range of system activities in the operating system. Activities include I/O operations, CPU utilization, the rate of context switches and interrupts, the rate of paging in and paging out, and the use of shared memory, buffer, and network. Based on the values in the count and interval parameters, sar writes information the specified number of times spaced at the specified intervals in seconds. For example, the command sar –b 3 12 reports disk usage every 3 seconds for a total of 12 seconds. In addition, at the end of data collection, average statistics are given. sar is a very option-rich tool. The remainder of this section discusses a few features of the tool:

    sar displays I/O statistics similar to iostat. sar provides the total number of I/O operations (tps), which is further split into read operations (rtps) and write operations (wtps). sar also provides the rates of read and write operations under bread/s and bwrtn/s. The following data is collected every 2 seconds for a total of 18 seconds. At the end of the data collection, the averages of five fields are given. However, operations for individual logical drives are not given.

    									12:59:15       tps     rtps     wtps   bread/s   bwrtn/s
    									12:59:17     37.50    37.50     0.00    396.00      0.00
    									12:59:19     66.50    66.50     0.00  16140.00      0.00
    									12:59:21    268.50   268.50     0.00  66560.00      0.00
    									12:59:23    333.50   261.50    72.00  64548.00   9620.00
    									12:59:25    153.50    40.50   113.00   9728.00  27984.00
    									12:59:27    133.00     5.00   128.00   1024.00  31744.00
    									12:59:29    119.50     7.50   112.00   1536.00  27776.00
    									12:59:31    133.00     5.00   128.00   1024.00  31744.00
    									Average:    155.63    86.50    69.13  20119.50  16108.50
    								

    sar provides data on CPU utilization for individual processors as well as for the whole system. This particular feature is especially useful in multiprocessor environments. If some processors do more work than others, the display clearly shows. You can then check whether the imbalanced use of processors is from, for example, the applications or the scheduler of the kernel. The following data is collected from a four-way SMP system every 5 seconds:

    									11:09:13   CPU  %user  %nice   %system   %iowait  %idle
    									11:09:18   all   0.00   0.00      4.70     52.45  42.85
    									11:09:18     0   0.00   0.00      5.80     57.00  37.20
    									11:09:18     1   0.00   0.00      4.80     49.40  45.80
    									11:09:18     2   0.00   0.00      6.00     62.20  31.80
    									11:09:18     3   0.00   0.00      2.40     41.12  56.49
    									11:09:23   all   0.00   0.00      3.75     47.30  48.95
    									11:09:23     0   0.00   0.00      5.39     37.33  57.29
    									11:09:23     1   0.00   0.00      2.80     41.80  55.40
    									11:09:23     2   0.00   0.00      5.40     41.60  53.00
    									11:09:23     3   0.00   0.00      1.40     68.60  30.00
    									. . .
    									Average:   all   0.00   0.00      4.22     16.40  79.38
    									Average:     0   0.00   0.00      8.32     24.33  67.35
    									Average:     1   0.00   0.00      2.12     14.35  83.53
    									Average:     2   0.01   0.00      4.16     12.07  83.76
    									Average:     3   0.00   0.00      2.29     14.85  82.86
    								

    sar also provides interrupt information among the processors:
    Code View: Scroll / Show All

    									10:53:53  CPU  i000/s  i001/s  i002/s  i003/s  i004/s  i005/s  i006/s  i007/s
    									10:53:58    0 1000.20    0.00    0.00    0.40    0.00    0.00    3.00    0.00
    									10:53:58    1    0.00    0.00    0.00    0.00    0.00    0.00    0.00 2320.00
    									10:53:58    2    0.00    0.00    0.00    0.00 1156.00    0.00    0.00    0.00
    									10:53:58    3    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00
    									Average:    0  999.94    0.00    0.00    1.20  590.99    0.00    3.73    0.00
    									Average:    1    0.00    0.00    0.00    0.00    0.00    0.00    0.00  926.61
    									Average:    2    0.00    0.00    0.00    0.00  466.51    0.00    0.00 1427.48
    									Average:    3    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00
    								

    					  

The study of interrupt distribution reveals an imbalance of interrupt processing. One method to tackle this imbalance is to affinitize IRQ processing to a specific processor or to a number of processors. For example, if 0x0001 is echoed to /proc/irq/ID, where ID corresponds to a device, only CPU 0 will process IRQ for this device. If 0x000f is echoed to /proc/irq/ID, CPU 0 through CPU 3 will be used to process IRQ for this device. For some workloads, this technique can reduce contention on certain heavily used processors. This technique allows I/O interrupts to be processed more efficiently, causing the I/O performance to increase accordingly.

		30.1.8 Network Utilization

Computer networking is a major computer discipline by itself as it has become ubiquitous. The government, private companies, and the mass media heavily rely on the Internet to function. The World Wide Web, e-mail, instant messaging, and so on have made the world much smaller, putting dispersed countries much closer than ever before. Search engines are very popular in getting information to your fingertips in a fraction of a second. eBusiness has taken business to the next level, where people, without leaving the place of their comfort, can shop, bank, trade stocks, play games with remote partners, collaborate on work, and so on. All these are made possible through advances in computer networking such as high-speed and high-bandwidth networks. Moreover, these advances facilitate new computer infrastructures such as cluster networks, storage networks, and multitiered setups. Tanenbaum has very good coverage of major topics in this field, such as the TCP/IP protocol suite, circuit and packet switching, wireless communications, security, and voice and data transmission.

Linux not only offers many of the powerful network capabilities that other major operating systems provide, but it also surpasses them through additional features such as masquerading. The Linux kernel supports several networking protocols such as TCP/IP, IPX (Internetwork Packet Exchange), and AppleTalk DDP, and it supports features such as packet forwarding, firewall operations, proxy, masquerading, tunneling, and aliasing.

Many network monitoring tools available in Linux help you evaluate the performance of any Linux network. Some of these tools can also be used to troubleshoot network problems along with monitoring performance. The Linux kernel makes a large amount of networking system information available to the user, helping you monitor the health of the network and detect problems in configuration, runtime, and performance.

This section explores only some of the tools that are readily available in most major Linux distributions. In this section, we look at the network tools netstat, nfsstat, tcpdump, ethtool, snmp, ifport, ifconfig, route, arp, ping, traceroute, host, and nslookup.

System and network administrators use some of these tools every day. Tools such as ping, route, arp, traceroute, ethtool, and tcpdump are used to determine network problems. These tools can be described as follows:

    The command ping ipaddress/hostname shows if a computer is operating and if network connections are intact. ping uses the Internet Control Message Protocol (ICMP) Echo function. A small packet is sent through the network for a given IP address. If a reply to the packet is received, the computer network connection is alive. It also tells how many hops lie between the source computer and the destination computer.

    The route command can be used to display the route table, add a route, delete a route to the table, and flush all the routes.

    The arp command is useful if ping does not work—that is, if the network connection is not alive—to determine the root cause of the problem. The arp -a command can be used to make sure that the hardware address is correctly associated with the right system. The other options available with this command include flushing the arp cache, adding to the cache.

    The IRRTToolset (Internet Routing Register Toolset) makes routing information more convenient and useful for network engineers by providing tools for automated router configuration, routing policies analysis, and maintenance.

    ifconfig determines a host's media access control address. If another host with a duplicate IP address exists on the network, the arp cache may have had the media access control address for the other computer placed in it, in which case, the arp command can be used to delete the saved address in the cache and add the correct address to the arp cache.

    traceroute tracks one of the possible routing pathways. It can measure the time taken to travel between each hop (router) and identify the hop's address as the packet travels through the network.

    ethtool queries and changes the settings of an Ethernet device. The devices are assigned a number for identification, such as eth0, eth1… ethn for n Ethernet devices in the system. This tool uses this device name to query/change the settings.

    tcpdump sniffs network packets. It captures all the packets that are seen at the computer. It can be used for network monitoring, protocol debugging, and data acquisition. tcpdump puts the NIC into promiscuous mode in order to capture all the packets going through the wire. Numerous options exist to filter the output down to only those packets of interest. The drawback with tcpdump is that the buffer can overflow and wrap around. tcpdump on high-bandwidth networks tends to drop packets—that is, tcpdump cannot keep up with the rate of the packets.

    ethereal is another network sniffing tool similar to tcpdump. ethereal can read capture files from tcpdump.

    host is a tool used to retrieve the host name for a given IP address from the Domain Name System. This tool is much more flexible than nslookup and is suited for use in shell scripts.

    Some of the network security tools that are available on Linux include tools such as snort (a network intrusion detection system), dsniff (a suite of powerful network auditing and penetrating-testing tools), and SAINT (Security Administrator's Integrated Network Tool).

Network Statistics

The netstat utility, available in the net-tools package, displays a large amount of information related to the networking subsystem.

netstat is one of the most frequently used tools for monitoring network connections on a Linux server. netstat displays a list of active sockets for each network protocol, such as TCP and UDP. It also provides information about network routes and cumulative statistics for network interfaces, including the number of incoming and outgoing packets and the number of packet collisions. The netstat output that follows shows a number of network protocol statistics and routing information, such as Internet protocol (IP), transport control protocol (TCP), and user datagram protocol (UDP). From the statistics, you can tell whether the number of packets received is higher or lower than expected. This tool can easily be used to investigate performance degradation between kernels.

Without any arguments, netstat displays a list of the existing network sockets and their connection information. All protocol families are displayed, including UNIX domain sockets. The following are typical lines from sample output:
Code View: Scroll / Show All

							$ netstat
							Active Internet connections (servers and established)
							Proto Recv-Q Send-Q Local Address     Foreign Address     State
							tcp        0      0 *:32768          *:*                LISTEN
							tcp        0      0 *:smux           *:*                LISTEN
							tcp        0      0 *:9099           *:*                LISTEN
							tcp        0      0 *:sunrpc         *:*                LISTEN
							tcp        0      0 *:x11            *:*                LISTEN
							tcp        0      0 *:http           *:*                LISTEN
							tcp        0      0 *:ftp            *:*                LISTEN
							tcp        0      0 *:ssh            *:*                LISTEN
							tcp        0      0 *:telnet         *:*                LISTEN
							tcp        0      0 nethostA:smtp    *:*                LISTEN
							tcp        0      0 nethostA:32974   nethostB:ssh       ESTABLISHED
							tcp        0      0 nethostA:32996   nethostB:ssh       ESTABLISHED
							tcp        0      0 nethostA:33002   64.233.161.99:http ESTABLISHED
							tcp        0      0 nethostA:33005   nethostB:ftp       ESTABLISHED
							udp        0      0 *:32768          *:*
							udp        0      0 *:snmp           *:*
							udp        0      0 *:sunrpc         *:*
							Active UNIX domain sockets (servers and established)
							Proto RefCnt Flags       Type       State         I-Node Path
							unix  2      [ ACC ]     STREAM     LISTENING     2012   /dev/gpmctl
							unix  2      [ ACC ]     STREAM     LISTENING     159792 /tmp/ksocket-nivedita/kdeinit-:0
							unix  2      [ ACC ]     STREAM     LISTENING     2210   /tmp/.X11-unix/X0
							unix  2      [ ACC ]     STREAM     LISTENING     79840  /tmp/.ICE-unix/dcop15789-1077867386
						

					  


The first column indicates the protocol family of the socket, which is commonly either tcp (transport control protocol), udp (user datagram protocol), or unix (UNIX domain socket). The second and third columns indicate the amount of data, in bytes, that is currently present in receive and send socket queues. The next columns list the local and remote address and port information. The last column displays the protocol state that the socket is currently in.

The IP addresses are normally translated into host names (nethostA, nethostB) unless the -n flag is provided to netstat.

To display only select address families, their corresponding flags can be provided. For example, netstat --tcp or -t displays only the TCP sockets present. A full listing of the flags for the individual families is available in the netstat man page.

The asterisk (*) indicates a wildcard. For the local address, this is typical of listener processes, which listen on all the local interfaces. Remote host address and port information is displayed when the socket has made a connection to a remote host and is in established state. You see ssh, http, and ftp connections in progress in the preceding display.
Displaying Interface Information

This information is identical to that displayed by the ifconfig command. It is a listing of the statistics provided by the interface. These include the MTU (maximum transmission unit) and counts of packets received and sent that were successful, erroneous in some way, dropped, or overflowed.
Code View: Scroll / Show All

$ netstat –i

Kernel Interface table
							Iface   MTU Met   RX-OK RX-ERR RX-DRP RX-OVR   TX-OK TX-ERR TX-DRP TX-OVR Flg
							eth0   1500 0     21941      0     0      0   11998      0     0      0 BMRU
							lo    16436 0       795      0     0      0     795      0     0      0 LRU
						

					  


TCP/IP Protocol Statistics

The Linux kernel supports the statistics counters specified in RFC 2012 as part of the Simple Network Management Protocol (SNMP) Management Information Base (MIB). It also implements a large number of counters that are Linux-specific and capture network protocol events, primarily TCP.

The netstat utility displays most, but not all, of the counters present in the kernel. To see the full list of the events being counted, view the content of the /proc/net/snmp and /proc/net/netstat files. The former contains the RFC 2012 counters, and the latter contains the extended Linux-specific MIB. The following is a sample listing of SNMP counters produced by the netstat –s command:
Code View: Scroll / Show All

							netstat -s
							Ip:
							662968 total packets received
							0 forwarded
							0 incoming packets discarded
							659592 incoming packets delivered
							162297 requests sent out
							Tcp:
							5721 active connections openings
							39 passive connection openings
							0 failed connection attempts
							0 connection resets received
							1 connections established
							136759 segments received
							152791 segments send out
							20660 segments retransmited
							3 bad segments received.
							1165 resets sent
							Udp:
							14031 packets received
							15 packets to unknown port received.
							0 packet receive errors
							7519 packets sent
						

					  


Moreover, network communication involves heavy interrupt processing. Thus, in conjunction with netstat, vmstat can be used to capture the number of interrupts, and sar can be used to determine the spread of interrupt processing.
nfsstat

Network File System (NFS) is a technique to incorporate a file system from a remote machine into the local file system—that is, NFS uses the same read and write interface to access data remotely as the one used locally. nfsstat is a simple tool that prints NFS kernel statistics. nfsstat prints the counts of NFS API calls during a workload. In the following example, the server is running an I/O workload. Output from nfsstat shows the counts of reads and writes, which can be used for debugging purposes. The counts of reads and writes can also be used to understand performance issues.

							Server nfs v3:
							null       getattr    setattr    lookup     access     readlink
							0       0% 8       0% 0       0% 6      0% 43      0% 0       0%
							read       write      create     mkdir      symlink    mknod
							262242 44% 328004  55% 2       0% 0       0% 0       0% 0       0%
							remove     rmdir      rename     link       readdir    readdirplus
							3       0% 0       0% 0       0% 0       0% 0       0% 0      0%
							fsstat     fsinfo     pathconf   commit
							1       0% 1       0% 0       0% 2586    0%
						
		30.1.9 perf stat
https://man7.org/linux/man-pages/man1/perf-stat.1.html

performance analysis tool
|   examples:
|      $ perf stat — make
|   
|              Performance counter stats for 'make':
|   
|                 83723.452481      task-clock:u (msec)       #    1.004 CPUs utilized
|                            0      context-switches:u        #    0.000 K/sec
|                            0      cpu-migrations:u          #    0.000 K/sec
|                    3,228,188      page-faults:u             #    0.039 M/sec
|              229,570,665,834      cycles:u                  #    2.742 GHz
|              313,163,853,778      instructions:u            #    1.36  insn per cycle
|               69,704,684,856      branches:u                #  832.559 M/sec
|                2,078,861,393      branch-misses:u           #    2.98% of all branches
|   
|              83.409183620 seconds time elapsed
|   
|              74.684747000 seconds user
|               8.739217000 seconds sys

		    30.1.9.1 user perf test to troubelshoot cache misses
https://www.reddit.com/r/C_Programming/comments/hhpqh5/why_does_using_a_mutex_increase_execution_speed/

Q.
TL;DR - Why does using a mutex in the code below increase the execution speed?

Code

#include <stdio.h>
#include <pthread.h>
#include <time.h>
#include <string.h>
#include <stdbool.h>

int counter = 0;
bool use_lock = false;
pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;

void* increment_counter(void* arg) {
    if (use_lock)
        pthread_mutex_lock(&lock);

    for (int i = 0; i < 1e6; i++) {
        counter++;
    }

    if (use_lock)
        pthread_mutex_unlock(&lock);

    return NULL;
}

int main(int argc, char** argv) {
    if (argc != 2) {
        printf("Usage:\n"
               "To run using a mutex: ./main 1\n"
               "To run without using a mutex: ./main 0\n");
        return 0;
    }
    if (strcmp(argv[1], "1") == 0) {
        use_lock = true;
    }

    clock_t start, end;
    pthread_t p1, p2;

    start = clock();
    pthread_create(&p1, NULL, increment_counter, NULL);
    pthread_create(&p2, NULL, increment_counter, NULL);
    pthread_join(p1, NULL);
    pthread_join(p2, NULL);
    end = clock();

    printf("counter: %d\n", counter);
    printf("elapsed time: %fs\n", ((double)(end - start)) / CLOCKS_PER_SEC);
    return 0;
}
Build

Built on Ubuntu 18.04 with gcc version 7.5.0

$ gcc main.c -o main -pthread
Run

Here are the typical results after running each case many times:

$./main 0
counter: 1240895
elapsed time: 0.017318s

$./main 1
counter: 2000000
elapsed time: 0.007553s
Question

What makes ./main 1 consistently faster than ./main 0?

A.

Could be cache contention. If the cache line containing counter is continually bouncing between cores access to it will be slower.

Look at the L1D cache misses, for instance:

|  $ perf stat -e L1-dcache-loads,L1-dcache-load-misses ./main 0
|  counter: 1057748
|  elapsed time: 0.015411s
|  
|   Performance counter stats for './main 0':
|  
|           6,534,727      L1-dcache-loads:u                                             (58.37%)
|             537,865      L1-dcache-load-misses:u   #    8.23% of all L1-dcache hits    (66.33%)
|  
|         0.010624321 seconds time elapsed
|  
|         0.017009000 seconds user
|         0.001016000 seconds sys


|   $ perf stat -e L1-dcache-loads,L1-dcache-load-misses ./main 1
|   counter: 2000000
|   elapsed time: 0.010670s
|   
|    Performance counter stats for './main 1':
|   
|            6,298,272      L1-dcache-loads:u                                             (61.02%)
|                9,410      L1-dcache-load-misses:u   #    0.15% of all L1-dcache hits    (65.89%)
|   
|          0.011877574 seconds time elapsed
|   
|          0.009174000 seconds user
|          0.002786000 seconds sys
|   		30.1.10
	30.2

31. Services

	31.1 Configuring services in Linux.


		31.1.1 Overview of services in Linux.

Services are programs (called daemons) that once started run continuously in the background and are ready for input or monitor changes in your computer and respond to them. For example 
the Apache server has a daemon called httpd (the d is for daemon) that listens on port 80 on your computer and when it receives a request for a page it sends the 
appropriate data back to the client machine.
Many services are required to run all the time however many can be safely turned of for both security reasons as running 
unnecessary services opens more doors into your computer, but also for performance reasons. It may not make much difference but your computer should boot slightly faster the less services 
it has to start on boot.
Even though I use my laptop as a development machine running Apache, PHP and MySQL and I am also connected to a home network I choose to start Apache, MySQL and 
Samba manually as required. This page explains how to start and stop services and how to control which services start automatically at boot time.

		31.1.2 chkconfig and service commands.
There are 2 commands used to control services:

/sbin/chkconfig - this controls which services are set to start on boot, by their nature these setting are saved and are applied at next boot. Changing these settings will not start 
the service immediately, it will just flag them to be started from the next boot.
/sbin/service - this controls the starting and stopping of services during a session, these setting are not saved. If you start Apache this way but it is not set to start 
on boot using the above method then it will continue to run but on next boot will not start automatically.

		31.1.3 GUI tool - system-config-services.
First of all we need to know which services are running and which are stopped. There are two ways of doing this, the graphical way is to open up system-config-services. 
If you look in the start menu it should be under Desktop > System > Administration > Services. You will be asked for your root password and then be presented 
with a screen like below. You can also start it from a terminal by typing 'system-config-services'.
system-config-services screenshot
From here you are able to tell which services are set to start on boot, those denoted with a tick in the checkbox, and also which are currently running by looking 
at the information in the bottom right panel. You can also start, stop and restart services from here. Note that if you start a service it will not be set to start on next boot 
unless you click the checkbox and then save your settings. Note that this graphical front end uses both chkconfig and service commands since you are able to control both in session starting 
and starting at next boot.
This is a good place to start looking at services but like many things in Linux it's actually easier to control services from the command line.

		31.1.4 Commands

			31.1.4.1 Finding which services start on boot using chkconfig
To get this information we use the chkconfig command, if you wish to see the status of all services then type:
$ /sbin/chkconfig --list
NetworkManager  0:off   1:off   2:off   3:off   4:off   5:on    6:off
NetworkManagerDispatcher        0:off   1:off   2:off   3:off   4:off   5:on    6:off
acpid           0:off   1:off   2:off   3:on    4:on    5:on    6:off
anacron         0:off   1:off   2:on    3:on    4:on    5:on    6:off
apmd            0:off   1:off   2:on    3:on    4:on    5:on    6:off
atd             0:off   1:off   2:off   3:on    4:on    5:on    6:off
ati-fglrx       0:off   1:off   2:on    3:on    4:on    5:on    6:off
auditd          0:off   1:off   2:on    3:on    4:on    5:on    6:off
autofs          0:off   1:off   2:off   3:on    4:on    5:on    6:off
bluetooth       0:off   1:off   2:on    3:on    4:on    5:off   6:off
cpuspeed        0:off   1:on    2:on    3:on    4:on    5:on    6:off
......................
As you can see this returns a long list of all the services, each column refers to a different run level. In most cases we boot into level 5 so this is the column 
we are most interested in (see below for the meaning of the different run levels). 
The on and off refer to whether the service is set to start on boot, it does not tell us whether the service is currently running.
Run levels.


0: Halt
1: Single User Mode
2: Basic Multi-user Mode
3: Full Multi-user without X
4: Not Used
5: Mutli-user with X
6: Reboot

This table just shows the meaning of different run levels. As you can see we normally boot into run level 5, multi-user with X.
To query the status of just one service we can use grep to filter the returned data. Here we use chkconfig but only want to see the Apache service.
$ /sbin/chkconfig --list|grep httpd
httpd           0:off   1:off   2:off   3:off   4:off   5:off   6:off

So this shows that Apache is not set to start on boot in all run levels.

			31.1.4.2 View only services not started on boot.
We can use grep again to see only those services set to start on boot for a particular run level. Here we see services not set to start in run level 5.
$ /sbin/chkconfig --list|grep -v 5:on
bluetooth       0:off   1:off   2:on    3:on    4:on    5:off   6:off
cups-config-daemon      0:off   1:off   2:off   3:on    4:on    5:off   6:off
dc_client       0:off   1:off   2:off   3:off   4:off   5:off   6:off
dc_server       0:off   1:off   2:off   3:off   4:off   5:off   6:off
dhcdbd          0:off   1:off   2:off   3:off   4:off   5:off   6:off
diskdump        0:off   1:off   2:off   3:off   4:off   5:off   6:off
httpd           0:off   1:off   2:off   3:off   4:off   5:off   6:off
irda            0:off   1:off   2:off   3:off   4:off   5:off   6:off
lirc            0:off   1:off   2:off   3:off   4:off   5:off   6:off
mDNSResponder   0:off   1:off   2:off   3:on    4:on    5:off   6:off
.........................

This command uses the -v option for grep, meaning non matching lines. What this line does is get all services from chkconfig but then only shows the lines that do not include the term 5:on,
 i.e. it will only display lines where 5:off occurs meaning the service is not set to start on boot. You could replace '5:on' with '5:off' to get all services set to start on boot or just ':off' 
 would return all lines where the service is set to start in all run levels.


			31.1.4.3 Changing whether a service starts on boot.
If you want to alter which services get started at boot time then we use chkconfig with different arguments. For example to set Apache to start on boot in run level 5:
$ /sbin/chkconfig --level 5 httpd on
To then stop it Apache starting on boot replace 'on' with 'off'.
$ /sbin/chkconfig --level 5 httpd off
If you wish to set more run levels just replace '5' with the run level required without spaces, e.g. '345'.So you can see that it's simple to configure which services start on boot, 
the next step is to manually start a service when required. This way we can turn off many services not actually required for the normal operation of the computer and just start 
them when the need arises.


			31.1.4.4 Starting and stopping services using the service command.
To start a service simply use the service command, this example uses Apache (httpd service) as an example but the command is the same for any service.
$ /sbin/service httpd start
Starting httpd:                                            [  OK  ]

Stopping a service is just as easy
$ /sbin/service httpd stop
Stopping httpd:                                            [  OK  ]

As you may have guessed restarting just uses restart in place of start or stop. Note that this will start a service for this session but after rebooting this 
service will not automatically restart. In my case this is what I want, many times I start my laptop to check email but have no need for Apache, MySQL, Samba and 
so on. I find it much easier just to start them as required. It's also very easy to write a simple bash script that starts Apache and MySQL from one command.


			31.1.4.5
Get all running services.
To see a list of all the currently running services we can use grep again to filter the data for the word 'running'. Conversely we could use 'stopped' to see a list of all stopped services.
$ /sbin/service --status-all|grep running
acpid (pid 2146) is running...
anacron (pid 2236) is running...
atd (pid 2243) is running...
Init-Script is running.
auditd (pid 1944) is running...
cpuspeed (pid 1880) is running...
crond (pid 2206) is running...
cupsd (pid 2156) is running...
..........................

If we just use /sbin/service --status-all then we get a list of all services whether they are running or not.
If we just want to query the status of one service then do the following. In this case I'm getting the status of the Samba (smb) service.
$ /sbin/service smb status
smbd is stopped
nmbd is stopped

Which services do I need to run?
As I said earlier I tend to turn off as many services as I can while still being able to use my computer for day to day things. For this reason I've included the list off all services 
on my computer and whether they are set to start at boot or not. Also note that even though I have a laptop I have turned off pcmcia since I 
do not have any need for it, but if you use a pcmcia network card for example then you will need this service running.
Remember that it's easy to test this for yourself, since if you turn off a service and lose some functionality you can just turn it on again 
without doing any harm. This list of my running services allows me to do all basic activities including internet browsing and email. I do use my laptop as 
a web server running Apache and MySQL but I just start them as required, the same goes for SSH sessions. In this way I get a faster boot up time, 
increased security from less services open to attack and fewer ports open.

		31.1.5
	31.2 Automatically load services (and scripts) after reboot


		31.2.1 Get To Know Linux: The /etc/init.d Directory

If you use Linux you most likely have heard of the init.d directory. But what exactly does this directory do? It ultimately does one thing but it does that one thing for your entire system, so init.d is very important. The init.d directory contains a number of start/stop scripts for various services on your system. Everything from acpid to x11-common is controlled from this directory. Of course it’s not exactly that simple.

If you look at the /etc directory you will find directories that are in the form rc#.d (Where # is a number reflects a specific initialization level – from 0 to 6). Within each of these directories is a number of other scripts that control processes. These scripts will either begin with a “K” or an “S”. All “K” scripts are run before “S” scripts. And depending upon where the scripts are located will determine when the scripts initiate. Between the directories the system services work together like a well-oiled machine. But there are times when you need to start or stop a process cleanly and without using the kill or killall commands. That is where the /etc/init.d directory comes in handy.

Now if you are using a distribution like Fedora you might find this directory in /etc/rc.d/init.d. Regardless of location, it serves the same purpose.

In order to control any of the scripts in init.d manually you have to have root (or sudo) access. Each script will be run as a command and the structure of the command will look like:

/etc/init.d/command OPTION

Where command is the actual command to run and OPTION can be one of the following:

    start
    stop
    reload
    restart
    force-reload

Most often you will use either start, stop, or restart. So if you want to stop your network you can issue the command:

/etc/init.d/networking stop

Or if you make a change to your network and need to restart it, you could do so with the following command:

/etc/init.d/networking restart

Some of the more common init scripts in this directory are:

    networking
    samba
    apache2
    ftpd
    sshd
    dovecot
    mysql

Of course there may be more often-used scripts in your directory – it depends upon what you have installed. The above list was taken from a Ubuntu Server 8.10 installation so a standard desktop installation would have a few less networking-type scripts.

But what about /etc/rc.local

There is a third option that I used to use quite a bit. This option is the /etc/rc.local script. This file runs after all other init level scripts have run, so it’s safe to put various commands that you want to have issued upon startup. Many times I will place mounting instructions for things like nfs in this script. This is also a good place to place “troubleshooting” scripts in. For instance, once I had a machine that, for some reason, samba seemed to not want to start. Even afer checking to make sure the Samba daemon was setup to initialize at boot up. So instead of spending all of my time up front with this I simply placed the line:

/etc/init.d/samba start

in the /etc/rc.local script and Samba worked like a charm. Eventually I would come back and trouble shoot this issue.

Final Thoughts

Linux is flexible. Linux is so flexible there is almost, inevitably, numerous ways to solve a single problem. Starting a system service is one such issue. With the help of the /etc/init.d system (as well as /etc/rc.local) you can pretty much rest assured your service will start.

		31.2.2 Linux Init Process / PC Boot Procedure

			31.2.2.1 PC Boot and Linux Init Process:

    BIOS: The Basic Input/Output System is the lowest level interface between the computer and peripherals.
    The BIOS performs integrity checks on memory and seeks instructions on the Master Boor Record (MBR) on the floppy drive or hard drive.
    The MBR points to the boot loader (GRUB or LILO: Linux boot loader).
    Boot loader (GRUB or LILO) will then ask for the OS label which will identify which kernel to run and where it is located (hard drive and partition specified). The installation process requires to creation/identification of partitions and where to install the OS. GRUB/LILO are also configured during this process. The boot loader then loads the Linux operating system.
        See the YoLinux tutorial on creating a boot disk for more information on GRUB and LILO and also to learn how to put the MBR and boot loader on a floppy for system recovery.
    The first thing the kernel does is to execute init program. Init is the root/parent of all processes executing on Linux.
    The first processes that init starts is a script /etc/rc.d/rc.sysinit
    Based on the appropriate run-level, scripts are executed to start various processes to run the system and make it functional.

			31.2.2.2 The Linux Init Processes:

The init process is the last step in the boot procedure and identified by process id "1". Init is responsible for starting system processes as defined in the /etc/inittab file. Init typically will start multiple instances of "getty" which waits for console logins which spawn one's user shell process. Upon shutdown, init controls the sequence and processes for shutdown. The init process is never shut down. It is a user process and not a kernel system process although it does run as root.

System Processes:

    Process ID 	Description
    0 	The Scheduler
    1 	The init process
    2 	kflushd
    3 	kupdate
    4 	kpiod
    5 	kswapd
    6 	mdrecoveryd

Init config file (Red Hat 7.3-9.0, Fedora Core 1-3): /etc/inittab

    # Author:       Miquel van Smoorenburg, 
    #               Modified for RHS Linux by Marc Ewing and Donnie Barnes

    id:3:initdefault:

    # System initialization.
    si::sysinit:/etc/rc.d/rc.sysinit

    l0:0:wait:/etc/rc.d/rc 0
    l1:1:wait:/etc/rc.d/rc 1
    l2:2:wait:/etc/rc.d/rc 2
    l3:3:wait:/etc/rc.d/rc 3
    l4:4:wait:/etc/rc.d/rc 4
    l5:5:wait:/etc/rc.d/rc 5
    l6:6:wait:/etc/rc.d/rc 6

    # Things to run in every runlevel.  This line is only in Red Hat 7.X Used to flush disk buffers.
    ud::once:/sbin/update

    # Trap CTRL-ALT-DELETE
    ca::ctrlaltdel:/sbin/shutdown -t3 -r now

    # When our UPS tells us power has failed, schedule a shutdown for 2 minutes from now.
    pf::powerfail:/sbin/shutdown -f -h +2 "Power Failure; System Shutting Down"

    # If power was restored before the shutdown kicked in, cancel it.
    pr:12345:powerokwait:/sbin/shutdown -c "Power Restored; Shutdown Canceled"

    # Run gettys in standard runlevels
    1:2345:respawn:/sbin/mingetty tty1
    2:2345:respawn:/sbin/mingetty tty2
    3:2345:respawn:/sbin/mingetty tty3
    4:2345:respawn:/sbin/mingetty tty4
    5:2345:respawn:/sbin/mingetty tty5
    6:2345:respawn:/sbin/mingetty tty6

    # Run xdm in runlevel 5
    x:5:respawn:/etc/X11/prefdm -nodaemon

Note that this config file directs the init process to run the shell script /etc/rc.d/rc.sysinit. This script should be used as is and NOT changed. Extend rc.local and NOT this script. This will (not in exact order):

    Run /sbin/initlog
    Run devfs to generate/manage system devices
    Run network scripts: /etc/sysconfig/network
    Start graphical boot (If so configured): rhgb
    Start console terminals, load keymap, system fonts and print console greeting: mingetty, setsysfonts
    The various virtual console sessions can be viewed with the key-stroke:
        RHEL6: ctrl-alt-F2 through F7. F1 is reserved for the GUI screen invoked in run level 5.
        Older systems: ctrl-alt-F1 through F6. F7 is reserved for the GUI screen invoked in run level 5.
    Mount /proc and start device controllers.
    Done with boot configuration for root drive. (initrd) Unmount root drive.
    Re-mount root file system as read/write
    Direct kernel to load kernel parameters and modules: sysctl, depmod, modprobe
    Set up clock: /etc/sysconfig/clock
    Perform disk operations based on fsck configuration
    Check/mount/check/enable quotas non-root file systems: fsck, mount, quotacheck, quotaon
    Initialize logical volume management: vgscan, /etc/lvmtab
    Activate syslog, write to log files: dmesg
    Configure sound: sndconfig
    Activate PAM
    Activate swapping: swapon

Local system boot processes can be placed in file: /etc/rc.d/rc.local

The system will then boot to the runlevel set by the directive initdefault.

Also see:

    init man page
    inittab man page

			31.2.2.3 Linux init Run Levels:

The Linux boot process has six states of operation of which "0" is the shutdown state and "3" and above are fully operational with all essential processes running for user interaction. Upon system boot the LINUX system /sbin/init program starts other processes by performing the following:

    Init will bring up the machine by starting processes as defined in the /etc/inittab file.

    The computer will be booted to the runlevel as defined by the initdefault directive in the /etc/inittab file.

            id:5:initdefault:
            

    In this example a runlevel of "5" is chosen. Runlevel "5" will boot the system into GUI mode using XDM and X-Windows. Booting to runlevel "3" (often called console mode) is often used by servers which do not need a graphical user interface. If booted to init level "3" one can promote the run level with the command [root prompt]# init 5. See the more detailed explanation of run levels below.

    The inittab file will allow you to capture key sequences (ctrl-alt-del), start dial in internet connections etc.

    One of these process started by init is /sbin/rc. This script runs a series of scripts in the directories /etc/rc.d/rc0.d/, /etc/rc.d/rc1.d/, /etc/rc.d/rc2.d/, etc

    Scripts in these directories are executed for each boot state of operation until it becomes fully operational. Scripts beginning with S denote startup scripts while scripts beginning with K denote shutdown (kill) scripts. Numbers follow these letters to denote the order of execution. (lowest to highest)

Runlevel "3" will boot to text or console mode and "5" will boot to the graphical login mode ( "4" for slackware)

    Runlevel 	Scripts Directory
    (Red Hat/Fedora Core) 	State
    0 	/etc/rc.d/rc0.d/ 	shutdown/halt system
    1 	/etc/rc.d/rc1.d/ 	Single user mode
    2 	/etc/rc.d/rc2.d/ 	Multiuser with no network services exported
    3 	/etc/rc.d/rc3.d/ 	Default text/console only start. Full multiuser
    4 	/etc/rc.d/rc4.d/ 	Reserved for local use. Also X-windows (Slackware/BSD)
    5 	/etc/rc.d/rc5.d/ 	XDM X-windows GUI mode (Redhat/System V)
    6 	/etc/rc.d/rc6.d/ 	Reboot
    s or S 	
    	Single user/Maintenance mode (Slackware)
    M 	
    	Multiuser mode (Slackware)

One may switch init levels by issuing the init command with the appropriate runlevel. Use the command "init #" where # is one of s,S,0,1,3,5,6. The command telinit does the same.

The scripts for a given run level are run during boot and shutdown. The scripts are found in the directory /etc/rc.d/rc#.d/ where the symbol # represents the run level. i.e. the run level "3" will run all the scripts in the directory /etc/rc.d/rc3.d/ which start with the letter "S" during system boot. This starts the background processes required by the system. During shutdown all scripts in the directory which begin with the letter "K" will be executed. This system provides an orderly way to bring the system to different states for production and maintenance modes.

If you installed all demons (background processes), Linux will run them all. To avoid slowing down your machine, remove unneeded services from the start-up procedure. You can start/stop individual demons by running service init scripts located in the /etc/init.d/ directory:

    /etc/rc.d/init.d/ (Red Hat/Fedora) Also /etc/init.d/ which is linked to /etc/rc.d/init.d/
    /etc/init.d/ (S.u.s.e.)
    /etc/init.d/ (Ubuntu / Debian)

and issuing the command and either the start, stop, status, restart or reload option i.e. to stop the web server:

    /etc/init.d/httpd stop

Use the command ps -aux to view all process on your machine.

TIP: List state and run level of all services which can be started by init: chkconfig --list
or
service --status-all | grep running (Red Hat/Fedora Core based systems)

GUI tool: /usr/X11R6/bin/tksysv

			31.2.2.4 Run Level Commands:

    Shutdown:
        init 0
        shutdown -h now
            -a: Use file /etc/shutdown.allow
            -c: Cancel scheduled shutdown.
        halt -p
            -p: Turn power off after shutdown.
        poweroff
    Reboot:
        init 6
        shutdown -r now
        reboot
    Enter single user mode:
        init 1

			31.2.2.5 Init Script Activation:

Adding a script to the /etc/rc.d/rc#.d/ directory with either an S or K prefix, adds the script to the boot or shutdown process. The scripts are run in numerical order. S20abc is run before S30xyz. The extensibility to the boot and shutdown procedures of the operating system is one of the strengths of UNIX. The orderly sequential initiation of processes can be coordinated for dependent processes. The orderly shutdown of processes is often required of complex programs such as databases. This is how it is done. Individual processes may be monitored, shutdown and started at any time using these scripts. i.e. /etc/rc.d/rc2.d/httpd start. The modifiers start, stop or status may be used.

The start/stop/status scripts actually reside in the directory:

    /etc/rc.d/init.d/ (Red Hat/Fedora) Also /etc/init.d/ which is linked to /etc/rc.d/init.d/
    /etc/init.d/ (S.u.s.e. and Ubuntu / Debian)

and are linked to the appropriate directories. These links may be created or destroyed using the chkconfig command. i.e. chkconfig --del httpd will remove the web server from the startup and shutdown process. Inversely chkconfig --add httpd will add it to the startup/shutdown process by generating links from the script in /etc/rc.d/init.d/ to the appropriate /etc/rc.d/rc#.d/ directory. For more information see the LINUX manual page on init.

Basic services include:

System Service 	Description
anacron 	Run jobs which were scheduled for execution while computer was turned off. Catch up with system duties.
arpwatch 	Keeps track of IP address to MAC address pairings
atd 	Run scheduled batch jobs.
autofs 	automounts file systems on demand.
bluetooth, pand, hidd, dund 	Bluetooth netwoork support.
crond 	Job sheduler for periodic tasks.
gpm 	Allows console terminal cut and paste. (Non X-window consoles)
https 	Apache web server.
iptables 	Firewall rules interface to kernel.
keytable 	Loads selected keyboard map as set in /etc/sysconfig/keyboard
kudzu 	New hardware probe/detection during system boot.
lpd or cups 	Network printer services.
microcode_ctl 	Uploads microcode to kernel and ultimately to the Intle Pentium processor. (Hardware specific.)
mysqld 	Database services
named 	DNS name services (Bind)
network 	Active network services during system boot. Required for network connectivity.
nfs 	Network file system. Unix file sharing services. Also uses services: nfslock, portmap, rpcgssd, rpcidmapd, rpcsvcgssd
nscd 	Password and group lookup services for use with network authentication (NIS, LDAP,...).
ntpd 	Network Time Protocol time synchronization services.
random 	Random number generation tool used for encryption.
rawdevices 	Enables raw IO. Useful for Oracle and software which utilizes this for high speed disk access.
smb 	SAMBA: MS/Windows PC file sharing services
syslog 	System log file facility.
ypbind 	NIS file sharing/authentication infrastructure service.
yppasswd 	NIS file sharing/authentication infrastructure service.
ypserv 	NIS file sharing/authentication infrastructure service.
xfs 	X-Windows font server.

Recommended basic services: anacron, ard, autofs, crond, gpm, iptables, keytable, kudzu, microcode_ctl (Intel32 hardware only), network, random. syslog
Graphics Workstation - add: xfs
File Server for PC clients - add: smb
Print Server - add: lpd or cups (hplip - HP Linux Imaging and Printing)
File server Linux/Unix clients - add: nfs, netfs, nfslock, portmap, ypbind, yppasswd, ypserv; NFSv4 add: rpcgssd, rpcidmapd, rpcsvcgssd
Web Server - add: httpd, tux, xinetdi, sshd

GUI configuration tools:
GUI tools can help you configure the appropriate services to start and provide a description of each service available:

    Fedora/RHEL: /usr/bin/system-config-services
    (Also /usr/sbin/serviceconf)
    Red Hat 8.0/9.0: /usr/bin/redhat-config-services
    Ubuntu / Debian:
        bum (Boot Up Manager) (GUI image)
        /usr/bin/services-admin (GUI image)
        /usr/sbin/sysv-rc-conf (console program - see below)

Red Hat / Fedora Core GUI: system-config-services (and redhat-config-services)

Red Hat/Fedora Core text console services selection tool: /usr/sbin/ntsysv

Debian/Ubuntu: sysv-rc-conf
(Install: aptget install sysv-rc-conf)

			31.2.2.6 Init Script:

A single copy of the script is located in the directory: /etc/rc.d/init.d/script-name (Red Hat/Fedora) or /etc/init.d/script-name (Ubuntu / Debian).
Use the command chkconfig to generate soft links to the appropriate directories for the various run levels.

    #!/bin/sh
    #
    # Startup script for program
    #
    # chkconfig: 345 85 15     - start or stop process definition within the boot process
    # description: Description of program
    # processname: process-name
    # pidfile: /var/run/process-name.pid

    # Source function library.      This creates the operating environment for the process to be started
    . /etc/rc.d/init.d/functions

    case "$1" in
      start)
            echo -n "Starting  process-name: "
            daemon  process-name                 - Starts only one process of a given name.
            echo
            touch /var/lock/subsys/process-name
            ;;
      stop)
            echo -n "Shutting down process-name: "
            killproc process-name
            echo
            rm -f /var/lock/subsys/process-name
            rm -f /var/run/process-name.pid      - Only if process generates this file
            ;;
      status)
            status process-name
            ;;
      restart)
            $0 stop
            $0 start
            ;;
      reload)
            echo -n "Reloading process-name: "
            killproc process-name -HUP
            echo
            ;;
      *)
            echo "Usage: $0 {start|stop|restart|reload|status}"
            exit 1
    esac

    exit 0

The bash script functions daemon, killproc and status can all be found in the script /etc/rc.d/init.d/functions. (Red Hat/Fedora distributions)

The script must be executable to work. (chmod +x script-name).

The script may be used to start and stop processes. i.e.:

    /etc/rc.d/init.d/httpd restart
    (Ubuntu / Debian / S.u.s.e.: /etc/init.d/apache2 restart)
    /etc/rc.d/init.d/httpd stop
    /etc/rc.d/init.d/httpd start

OR use the Red Hat/Fedora core based service command:

    service httpd restart
    service httpd stop
    service httpd start

Note that two lines in the script enable the chkconfig command to control the script for the boot and shutdown process.

    # chkconfig: 345 85 15 
    # description: Description of program
              

When added to the boot process using the "chkconfig --add script-name" command the start order/priority will be set to 80 while the stop/shutdown order will be set to 15. The process will be added to runlevels 3, 4 and 5. This is enabled by generating links from the location of the script (/etc/rc.d/init.d/) to the directory for the appropriate run level: /etc/rc.d/rc#.d/. The file name in the run level directory will reflect if it is used for boot (starts with an "S") or shutdown (starts with a "K")

			31.2.2.7 chkconfig:

The (Red Hat/Fedora/IRIX) chkconfig command generates and breaks links between the directory /etc/rc.d/init.d/ and the appropriate run level directory: /etc/rc.d/rc[0-6].d/ to control boot process initiation and process shutdown.

     chkconfig [--level ]  on | off | reset >
     chkconfig --list
     chkconfig --list
     chkconfig --add 

     chkconfig --del 
     chkconfig --level 0123456  off
      

Examples:

    chkconfig --level 345 httpd on - forces apache to be invoked for run levels 3, 4 and 5.
    chkconfig --add httpd - Start the web server daemon upon system boot.
    chkconfig --del sendmail - Do not start the sendmail daemon upon system boot.
    chkconfig --list - List all services and init levels.
    chkconfig --list | grep on - List all services to be started upon system boot.

Using chkconfig to administer xinetd processes.

    chkconfig wu-ftpd on - Turn on FTP service managed by xinetd.
    chkconfig ipop3 off - Turn off POP3 service managed by xinetd.

This will reconfigure the appropriate xinetd file (in directory /etc/xinetd.d/) and restart the xinetdprocess.

Also see: chkconfig - Linux man page.

			31.2.2.8 Related Commands:

    service 	Display status of system services.
    Example: service --status-all
    Help: service --help

Also see: service - Linux man page. 

		31.2.3
	31.3


32. HowTO

	32.1  HowTo: Debug Crashed Linux Application Core Files Like A Pro
Core dumps are often used to diagnose or debug errors in Linux or UNIX programs. Core dumps can serve as useful debugging aids for sys admins to find out why Application like Lighttpd, Apache, PHP-CGI or any other program crashed. Many vendors and open source project author requests a core file to troubleshoot a program. A core file is generated when an application program abnormally terminates due to bug, operating system security protection schema, or program simply try to write beyond the area of memory it has allocated, and so on. This article explains how to turn on core file support and track down bugs in programs.
Turn On Core File Creation Support

By default most Linux distributions turn off core file creation (at least this is true for RHEL, CentOS, Fedora and Suse Linux). You need to use the ulimit command to configure core files.
See The Current Core File Limits

Type the following command:
# ulimit -c
Sample outputs:

0

The output 0 (zero) means core file is not created.
Change Core File Limits

In this example, set the size limit of core files to 75000 bytes:
# ulimit -c 75000
HowTo: Enable Core File Dumps For Application Crashes And Segmentation Faults

Edit /etc/profile file and find line that read as follows to make persistent configuration:

ulimit -S -c 0 > /dev/null 2>&1

Update it as follows:

ulimit -c unlimited >/dev/null 2>&1

Save and close the file. Edit /etc/sysctl.conf, enter:
# vi /etc/sysctl.conf
Append the following lines:

kernel.core_uses_pid = 1
kernel.core_pattern = /tmp/core-%e-%s-%u-%g-%p-%t
fs.suid_dumpable = 2

Save and close the file. Where,

    kernel.core_uses_pid = 1 - Appends the coring processes PID to the core file name.
    fs.suid_dumpable = 2 - Make sure you get core dumps for setuid programs.
    kernel.core_pattern = /tmp/core-%e-%s-%u-%g-%p-%t - When the application terminates abnormally, a core file should appear in the /tmp. The kernel.core_pattern sysctl controls exact location of core file. You can define the core file name with the following template whih can contain % specifiers which are substituted by the following values when a core file is created:
        %% - A single % character
        %p - PID of dumped process
        %u - real UID of dumped process
        %g - real GID of dumped process
        %s - number of signal causing dump
        %t - time of dump (seconds since 0:00h, 1 Jan 1970)
        %h - hostname (same as ’nodename’ returned by uname(2))
        %e - executable filename

Finally, enable debugging for all apps, enter (Redhat and friends specific):
# echo "DAEMON_COREFILE_LIMIT='unlimited'" >> /etc/sysconfig/init
Reload the settings in /etc/sysctl.conf by running the following command:
# sysctl -p
How Do I Enable Core Dumping For Specific Deamon?

To enable core dumping for specific deamons, add the following line in the /etc/sysconfig/daemon-file file. In this example, edit /etc/init.d/lighttped and add line as follows:

DAEMON_COREFILE_LIMIT='unlimited'

Please note that DAEMON_COREFILE_LIMIT is Redhat specific, for all other distro add configuration as follows:

ulimit -c unlimited >/dev/null 2>&1
echo /tmp/core-%e-%s-%u-%g-%p-%t > /proc/sys/kernel/core_pattern

Save and close the file. Restart / reload lighttpd:
# /etc/init.d/lighttpd restart
# su - lighttpd
$ ulimit -c
Sample outputs:

unlimited

Now, you can send core files to vendor or software writes.
How Do I Read Core Files?

You need use the gdb command as follows:
$ gdb /path/to/application /path/to/corefile
See the gdb command man page for more information.
strace command

System administrators, diagnosticians and trouble-shooters will find it invaluable for solving problems with programs for which the source is not readily available since they do not need to be recompiled in order to trace them. This is also useful to submit bug reports to open source developers. See how to use the strace command under Linux to debug the problems.
Recommended readings:

    Debugging Tip: Trace the Process and See What It is Doing with strace
    The Art of Debugging with GDB, DDD, and Eclipse
    man pages core(5), strace, and bash

Stay tunned for gdb tutorial which will explains how to use generated core file to track down problem.


	32.2 30 Handy Bash Shell Aliases For Linux / Unix / Mac OS X

by nixCraft on June 11, 2012 · 59 comments· Last updated November 23, 2012

An alias is nothing but shortcut to commands. The alias command allows user to launch any command or group of commands (including options and filenames) by entering a single word. Use alias command to display list of all defined aliases. You can add user defined aliases to ~/.bashrc file. You can cut down typing time with these aliases, work smartly, and increase productivity at the command prompt.
More about aliases

The general syntax for the alias command for the bash shell is as follows.
Task: List aliases

Type the following command:

 
alias
 

Sample outputs:

alias ..='cd ..'
alias amazonbackup='s3backup'
alias apt-get='sudo apt-get'
...

By default alias command shows a list of aliases that are defined for the current user.
Task: Define / create an alias (bash syntax)

To create the alias use the following syntax:

 
alias name=value
alias name='command'
alias name='command arg1 arg2'
alias name='/path/to/script'
alias name='/path/to/script.pl arg1'
 

In this example, create the alias c for the commonly used clear command, which clears the screen, by typing the following command and then pressing the ENTER key:

 
alias c='clear'
 

Then, to clear the screen, instead of typing clear, you would only have to type the letter 'c' and press the [ENTER] key:

 
c
 

Task: Disable an alias temporarily (bash syntax)

An alias can be disabled temporarily using the following syntax:

 
## path/to/full/command
/usr/bin/clear
## call alias with a backslash ##
\c
 

Task: Remove an alias (bash syntax)

You need to use the command called unalias to remove aliases. Its syntax is as follows:

 
unalias aliasname
 

In this example, remove the alias c which was created in an earlier example:

 
unalias c
 

You also need to delete the alias from the ~/.bashrc file using a text editor (see next section).
Task: Make aliases permanent (bash syntax)

The alias c remains in effect only during the current login session. Once you logs out or reboot the system the alias c will be gone. To avoid this problem, add alias to your ~/.bashrc file, enter:

 
vi ~/.bashrc
 

The alias c for the current user can be made permanent by entering the following line:

 
alias c='clear'
 

Save and close the file. System-wide aliases (i.e. aliases for all users) can be put in the /etc/bashrc file. Please note that the alias command is built into a various shells including ksh, tcsh/csh, ash, bash and others.
A note about privileged access

You can add code as follows in ~/.bashrc:

 
# if user is not root, pass all commands via sudo #
if [ $UID -ne 0 ]; then
    alias reboot='sudo reboot'
    alias update='sudo apt-get upgrade'
fi
 

A note about os specific aliases

You can add code as follows in ~/.bashrc using the case statement:

 
### Get os name via uname ###
_myos="$(uname)"
 
### add alias as per os using $_myos ###
case $_myos in
   Linux) alias foo='/path/to/linux/bin/foo';;
   FreeBSD|OpenBSD) alias foo='/path/to/bsd/bin/foo' ;;
   SunOS) alias foo='/path/to/sunos/bin/foo' ;;
   *) ;;
esac
 

30 uses for aliases

You can define various types aliases as follows to save time and increase productivity.
#1: Control ls command output

The ls command lists directory contents and you can colorize the output:

 
## Colorize the ls output ##
alias ls='ls --color=auto'
 
## Use a long listing format ##
alias ll='ls -la'
 
## Show hidden files ##
alias l.='ls -d .* --color=auto'
 

#2: Control cd command behavior

 
## get rid of command not found ##
alias cd..='cd ..'
 
## a quick way to get out of current directory ##
alias ..='cd ..'
alias ...='cd ../../../'
alias ....='cd ../../../../'
alias .....='cd ../../../../'
alias .4='cd ../../../../'
alias .5='cd ../../../../..'
 

#3: Control grep command output

grep command is a command-line utility for searching plain-text files for lines matching a regular expression:

 
## Colorize the grep command output for ease of use (good for log files)##
alias grep='grep --color=auto'
alias egrep='egrep --color=auto'
alias fgrep='fgrep --color=auto'
 

#4: Start calculator with math support

 
alias bc='bc -l'
 

#4: Generate sha1 digest

 
alias sha1='openssl sha1'
 

#5: Create parent directories on demand

mkdir command is used to create a directory:

 
alias mkdir='mkdir -pv'
 

#6: Colorize diff output

You can compare files line by line using diff and use a tool called colordiff to colorize diff output:

 
# install  colordiff package :)
alias diff='colordiff'
 

#7: Make mount command output pretty and human readable format

 
alias mount='mount |column -t'
 

#8: Command short cuts to save time

 
# handy short cuts #
alias h='history'
alias j='jobs -l'
 

#9: Create a new set of commands

 
alias path='echo -e ${PATH//:/\\n}'
alias now='date +"%T'
alias nowtime=now
alias nowdate='date +"%d-%m-%Y"'
 

#10: Set vim as default

 
alias vi=vim
alias svi='sudo vi'
alias vis='vim "+set si"'
alias edit='vim'
 

#11: Control output of networking tool called ping

 
# Stop after sending count ECHO_REQUEST packets #
alias ping='ping -c 5'
# Do not wait interval 1 second, go fast #
alias fastping='ping -c 100 -s.2'
 

#12: Show open ports

Use netstat command to quickly list all TCP/UDP port on the server:

 
alias ports='netstat -tulanp'
 

#13: Wakeup sleeping servers

Wake-on-LAN (WOL) is an Ethernet networking standard that allows a server to be turned on by a network message. You can quickly wakeup nas devices and server using the following aliases:

 
## replace mac with your actual server mac address #
alias wakeupnas01='/usr/bin/wakeonlan 00:11:32:11:15:FC'
alias wakeupnas02='/usr/bin/wakeonlan 00:11:32:11:15:FD'
alias wakeupnas03='/usr/bin/wakeonlan 00:11:32:11:15:FE'
 

#14: Control firewall (iptables) output

Netfilter is a host-based firewall for Linux operating systems. It is included as part of the Linux distribution and it is activated by default. This post list most common iptables solutions required by a new Linux user to secure his or her Linux operating system from intruders.

 
## shortcut  for iptables and pass it via sudo#
alias ipt='sudo /sbin/iptables'
 
# display all rules #
alias iptlist='sudo /sbin/iptables -L -n -v --line-numbers'
alias iptlistin='sudo /sbin/iptables -L INPUT -n -v --line-numbers'
alias iptlistout='sudo /sbin/iptables -L OUTPUT -n -v --line-numbers'
alias iptlistfw='sudo /sbin/iptables -L FORWARD -n -v --line-numbers'
alias firewall=iptlist
 

#15: Debug web server / cdn problems with curl

 
# get web server headers #
alias header='curl -I'
 
# find out if remote server supports gzip / mod_deflate or not #
alias headerc='curl -I --compress'
 

#16: Add safety nets

 
# do not delete / or prompt if deleting more than 3 files at a time #
alias rm='rm -I --preserve-root'
 
# confirmation #
alias mv='mv -i'
alias cp='cp -i'
alias ln='ln -i'
 
# Parenting changing perms on / #
alias chown='chown --preserve-root'
alias chmod='chmod --preserve-root'
alias chgrp='chgrp --preserve-root'
 

#17: Update Debian Linux server

apt-get command is used for installing packages over the internet (ftp or http). You can also upgrade all packages in a single operations:

 
# distro specific  - Debian / Ubuntu and friends #
# install with apt-get
alias apt-get="sudo apt-get"
alias updatey="sudo apt-get --yes"
 
# update on one command 
alias update='sudo apt-get update && sudo apt-get upgrade'
 

#18: Update RHEL / CentOS / Fedora Linux server

yum command is a package management tool for RHEL / CentOS / Fedora Linux and friends:

 
## distrp specifc RHEL/CentOS ##
alias update='yum update'
alias updatey='yum -y update'
 

#19: Tune sudo and su

 
# become root #
alias root='sudo -i'
alias su='sudo -i'
 

#20: Pass halt/reboot via sudo

shutdown command bring the Linux / Unix system down:

 
# reboot / halt / poweroff
alias reboot='sudo /sbin/reboot'
alias poweroff='sudo /sbin/poweroff'
alias halt='sudo /sbin/halt'
alias shutdown='sudo /sbin/shutdown'
 

#21: Control web servers

 
# also pass it via sudo so whoever is admin can reload it without calling you #
alias nginxreload='sudo /usr/local/nginx/sbin/nginx -s reload'
alias nginxtest='sudo /usr/local/nginx/sbin/nginx -t'
alias lightyload='sudo /etc/init.d/lighttpd reload'
alias lightytest='sudo /usr/sbin/lighttpd -f /etc/lighttpd/lighttpd.conf -t'
alias httpdreload='sudo /usr/sbin/apachectl -k graceful'
alias httpdtest='sudo /usr/sbin/apachectl -t && /usr/sbin/apachectl -t -D DUMP_VHOSTS'
 

#22: Alias into our backup stuff

 
# if cron fails or if you want backup on demand just run these commands # 
# again pass it via sudo so whoever is in admin group can start the job #
# Backup scripts #
alias backup='sudo /home/scripts/admin/scripts/backup/wrapper.backup.sh --type local --taget /raid1/backups'
alias nasbackup='sudo /home/scripts/admin/scripts/backup/wrapper.backup.sh --type nas --target nas01'
alias s3backup='sudo /home/scripts/admin/scripts/backup/wrapper.backup.sh --type nas --target nas01 --auth /home/scripts/admin/.authdata/amazon.keys'
alias rsnapshothourly='sudo /home/scripts/admin/scripts/backup/wrapper.rsnapshot.sh --type remote --target nas03 --auth /home/scripts/admin/.authdata/ssh.keys --config /home/scripts/admin/scripts/backup/config/adsl.conf'
alias rsnapshotdaily='sudo  /home/scripts/admin/scripts/backup/wrapper.rsnapshot.sh --type remote --target nas03 --auth /home/scripts/admin/.authdata/ssh.keys  --config /home/scripts/admin/scripts/backup/config/adsl.conf'
alias rsnapshotweekly='sudo /home/scripts/admin/scripts/backup/wrapper.rsnapshot.sh --type remote --target nas03 --auth /home/scripts/admin/.authdata/ssh.keys  --config /home/scripts/admin/scripts/backup/config/adsl.conf'
alias rsnapshotmonthly='sudo /home/scripts/admin/scripts/backup/wrapper.rsnapshot.sh --type remote --target nas03 --auth /home/scripts/admin/.authdata/ssh.keys  --config /home/scripts/admin/scripts/backup/config/adsl.conf'
alias amazonbackup=s3backup
 

#23: Desktop specific - play avi/mp3 files on demand

 
## play video files in a current directory ##
# cd ~/Download/movie-name 
# playavi or vlc 
alias playavi='mplayer *.avi'
alias vlc='vlc *.avi'
 
# play all music files from the current directory #
alias playwave='for i in *.wav; do mplayer "$i"; done'
alias playogg='for i in *.ogg; do mplayer "$i"; done'
alias playmp3='for i in *.mp3; do mplayer "$i"; done'
 
# play files from nas devices #
alias nplaywave='for i in /nas/multimedia/wave/*.wav; do mplayer "$i"; done'
alias nplayogg='for i in /nas/multimedia/ogg/*.ogg; do mplayer "$i"; done'
alias nplaymp3='for i in /nas/multimedia/mp3/*.mp3; do mplayer "$i"; done'
 
# shuffle mp3/ogg etc by default #
alias music='mplayer --shuffle *'
 

#24: Set default interfaces for sys admin related commands

vnstat is console-based network traffic monitor. dnstop is console tool to analyze DNS traffic. tcptrack and iftop commands displays information about TCP/UDP connections it sees on a network interface and display bandwidth usage on an interface by host respectively.

 
## All of our servers eth1 is connected to the Internets via vlan / router etc  ##
alias dnstop='dnstop -l 5  eth1'
alias vnstat='vnstat -i eth1'
alias iftop='iftop -i eth1'
alias tcpdump='tcpdump -i eth1'
alias ethtool='ethtool eth1'
 
# work on wlan0 by default #
# Only useful for laptop as all servers are without wireless interface
alias iwconfig='iwconfig wlan0'
 

#25: Get system memory, cpu usage, and gpu memory info quickly

 
## pass options to free ## 
alias meminfo='free -m -l -t'
 
## get top process eating memory
alias psmem='ps auxf | sort -nr -k 4'
alias psmem10='ps auxf | sort -nr -k 4 | head -10'
 
## get top process eating cpu ##
alias pscpu='ps auxf | sort -nr -k 3'
alias pscpu10='ps auxf | sort -nr -k 3 | head -10'
 
## Get server cpu info ##
alias cpuinfo='lscpu'
 
## older system use /proc/cpuinfo ##
##alias cpuinfo='less /proc/cpuinfo' ##
 
## get GPU ram on desktop / laptop## 
alias gpumeminfo='grep -i --color memory /var/log/Xorg.0.log'
 

#26: Control Home Router

The curl command can be used to reboot Linksys routers.

 
# Reboot my home Linksys WAG160N / WAG54 / WAG320 / WAG120N Router / Gateway from *nix.
alias rebootlinksys="curl -u 'admin:my-super-password' 'http://192.168.1.2/setup.cgi?todo=reboot'"
 
# Reboot tomato based Asus NT16 wireless bridge 
alias reboottomato="ssh admin@192.168.1.1 /sbin/reboot"
 

#27 Resume wget by default

The GNU Wget is a free utility for non-interactive download of files from the Web. It supports HTTP, HTTPS, and FTP protocols, and it can resume downloads too:

 
## this one saved by butt so many times ##
alias wget='wget -c'
 

#28 Use different browser for testing website

 
## this one saved by butt so many times ##
alias ff4='/opt/firefox4/firefox'
alias ff13='/opt/firefox13/firefox'
alias chrome='/opt/google/chrome/chrome'
alias opera='/opt/opera/opera'
 
#default ff 
alias ff=ff13
 
#my default browser 
alias browser=chrome
 

#29: A note about ssh alias

Do not create ssh alias, instead use ~/.ssh/config OpenSSH SSH client configuration files. It offers more option. An example:

 
Host server10
  Hostname 1.2.3.4
  IdentityFile ~/backups/.ssh/id_dsa
  user foobar
  Port 30000
  ForwardX11Trusted yes
  TCPKeepAlive yes
 

You can now connect to peer1 using the following syntax:
$ ssh server10
#30: It's your turn to share...

 
## set some other defaults ##
alias df='df -H'
alias du='du -ch'
 
# top is atop, just like vi is vim
alias top='atop'
 
## nfsrestart  - must be root  ##
## refresh nfs mount / cache etc for Apache ##
alias nfsrestart='sync && sleep 2 && /etc/init.d/httpd stop && umount netapp2:/exports/http && sleep 2 && mount -o rw,sync,rsize=32768,wsize=32768,intr,hard,proto=tcp,fsc natapp2:/exports /http/var/www/html &&  /etc/init.d/httpd start'
 
## Memcached server status  ##
alias mcdstats='/usr/bin/memcached-tool 10.10.27.11:11211 stats'
alias mcdshow='/usr/bin/memcached-tool 10.10.27.11:11211 display'
 
## quickly flush out memcached server ##
alias flushmcd='echo "flush_all" | nc 10.10.27.11 11211'
 
## Remove assets quickly from Akamai / Amazon cdn ##
alias cdndel='/home/scripts/admin/cdn/purge_cdn_cache --profile akamai'
alias amzcdndel='/home/scripts/admin/cdn/purge_cdn_cache --profile amazon'
 
## supply list of urls via file or stdin
alias cdnmdel='/home/scripts/admin/cdn/purge_cdn_cache --profile akamai --stdin'
alias amzcdnmdel='/home/scripts/admin/cdn/purge_cdn_cache --profile amazon --stdin'
 

Conclusion

This post summaries several types of uses for *nix bash aliases:

    Setting default options for a command (e.g. set eth0 as default option - alias ethtool='ethtool eth0' ).
    Correcting typos (cd.. will act as cd .. via alias cd..='cd ..').
    Reducing the amount of typing.
    Setting the default path of a command that exists in several versions on a system (e.g. GNU/grep is located at /usr/local/bin/grep and Unix grep is located at /bin/grep. To use GNU grep use alias grep='/usr/local/bin/grep' ).
    Adding the safety nets to Unix by making commands interactive by setting default options. (e.g. rm, mv, and other commands).
    Compatibility by creating commands for older operating systems such as MS-DOS or other Unix like operating systems (e.g. alias del=rm ).

I've shared my aliases that I used over the years to reduce the need for repetitive command line typing. If you know and use any other bash/ksh/csh aliases that can reduce typing, share below in the comments.


	32.3 Top 30 Nmap Command Examples For Sys/Network Admins

by nixCraft on November 26, 2012 · 8 comments· Last updated December 11, 2012

in Command Line Hacks, Howto, Networking, Security

Nmap is short for Network Mapper. It is an open source security tool for network exploration, security scanning and auditing. However, nmap command comes with lots of options that can make the utility more robust and difficult to follow for new users.

The purpose of this post is to introduce a user to the nmap command line tool to scan a host and/or network, so to find out the possible vulnerable points in the hosts. You will also learn how to use Nmap for offensive and defensive purposes.
nmap in action

nmap in action
More about nmap

From the man page:

    Nmap ("Network Mapper") is an open source tool for network exploration and security auditing. It was designed to rapidly scan large networks, although it works fine against single hosts. Nmap uses raw IP packets in novel ways to determine what hosts are available on the network, what services (application name and version) those hosts are offering, what operating systems (and OS versions) they are running, what type of packet filters/firewalls are in use, and dozens of other characteristics. While Nmap is commonly used for security audits, many systems and network administrators find it useful for routine tasks such as network inventory, managing service upgrade schedules, and monitoring host or service uptime.

It was originally written by Gordon Lyon and it can answer the following questions easily:

    What computers did you find running on the local network?
    What IP addresses did you find running on the local network?
    What is the operating system of your target machine?
    Find out what ports are open on the machine that you just scanned?
    Find out if the system is infected with malware or virus.
    Search for unauthorized servers or network service on your network.
    Find and remove computers which don't meet the organization's minimum level of security.

Sample setup (LAB)

Port scanning may be illegal in some jurisdictions. So setup a lab as follows:

                              +---------+
        +---------+           | Network |         +--------+
        | server1 |-----------+ swtich  +---------|server2 |
        +---------+           | (sw0)   |         +--------+
                              +----+----+
                                   |
                                   |
                         +---------+----------+
                         | wks01 Linux/OSX    |
                         +--------------------+

Where,

    wks01 is your computer either running Linux/OS X or Unix like operating system. It is used for scanning your local network. The nmap command must be installed on this computer.
    server1 can be powered by Linux / Unix / MS-Windows operating systems. This is an unpatched server. Feel free to install a few services such as a web-server, file server and so on.
    server2 can be powered by Linux / Unix / MS-Windows operating systems. This is a fully patched server with firewall. Again, feel free to install few services such as a web-server, file server and so on.
    All three systems are connected via switch.

How do I install nmap?

See:

    Debian / Ubuntu Linux: Install nmap Software For Scanning Network
    CentOS / RHEL: Install nmap Network Security Scanner
    OpenBSD: Install nmap Network Security Scanner

#1: Scan a single host or an IP address (IPv4)

### Scan a single ip address ###
nmap 192.168.1.1
 
## Scan a host name ###
nmap server1.cyberciti.biz
 
## Scan a host name with more info###
nmap -v server1.cyberciti.biz
 

Sample outputs:
Fig.01: nmap output

Fig.01: nmap output

#2: Scan multiple IP address or subnet (IPv4)

nmap 192.168.1.1 192.168.1.2 192.168.1.3
## works with same subnet i.e. 192.168.1.0/24
nmap 192.168.1.1,2,3

You can scan a range of IP address too:

nmap 192.168.1.1-20

You can scan a range of IP address using a wildcard:

nmap 192.168.1.*

Finally, you scan an entire subnet:

nmap 192.168.1.0/24

#3: Read list of hosts/networks from a file (IPv4)

The -iL option allows you to read the list of target systems using a text file. This is useful to scan a large number of hosts/networks. Create a text file as follows:
cat > /tmp/test.txt
Sample outputs:

server1.cyberciti.biz
| 192.168.1.0/24
| 192.168.1.1/24
| 10.1.2.3
localhost

The syntax is:

nmap -iL /tmp/test.txt

#4: Excluding hosts/networks (IPv4)

When scanning a large number of hosts/networks you can exclude hosts from a scan:

nmap 192.168.1.0/24 --exclude 192.168.1.5
nmap 192.168.1.0/24 --exclude 192.168.1.5,192.168.1.254

OR exclude list from a file called /tmp/exclude.txt

nmap -iL /tmp/scanlist.txt --excludefile /tmp/exclude.txt

#5: Turn on OS and version detection scanning script (IPv4)

nmap -A 192.168.1.254
nmap -v -A 192.168.1.1
nmap -A -iL /tmp/scanlist.txt 

#6: Find out if a host/network is protected by a firewall

nmap -sA 192.168.1.254
nmap -sA server1.cyberciti.biz

#7: Scan a host when protected by the firewall

nmap -PN 192.168.1.1
nmap -PN server1.cyberciti.biz

#8: Scan an IPv6 host/address

The -6 option enable IPv6 scanning. The syntax is:

nmap -6 IPv6-Address-Here
nmap -6 server1.cyberciti.biz
nmap -6 2607:f0d0:1002:51::4
nmap -v A -6 2607:f0d0:1002:51::4

#9: Scan a network and find out which servers and devices are up and running

This is known as host discovery or ping scan:

nmap -sP 192.168.1.0/24

Sample outputs:

Host 192.168.1.1 is up (0.00035s latency).
MAC Address: BC:AE:C5:C3:16:93 (Unknown)
Host 192.168.1.2 is up (0.0038s latency).
MAC Address: 74:44:01:40:57:FB (Unknown)
Host 192.168.1.5 is up.
Host nas03 (192.168.1.12) is up (0.0091s latency).
MAC Address: 00:11:32:11:15:FC (Synology Incorporated)
Nmap done: 256 IP addresses (4 hosts up) scanned in 2.80 second

#10: How do I perform a fast scan?

nmap -F 192.168.1.1

#11: Display the reason a port is in a particular state

nmap --reason 192.168.1.1
nmap --reason server1.cyberciti.biz

#12: Only show open (or possibly open) ports

nmap --open 192.168.1.1
nmap --open server1.cyberciti.biz

#13: Show all packets sent and received

nmap --packet-trace 192.168.1.1
nmap --packet-trace server1.cyberciti.biz

14#: Show host interfaces and routes

This is useful for debugging (ip command or route command or netstat command like output using nmap)

nmap --iflist

Sample outputs:

| Starting Nmap 5.00 ( http://nmap.org ) at 2012-11-27 02:01 IST
| ************************INTERFACES************************
| DEV    (SHORT)  IP/MASK          TYPE        UP MAC
| lo     (lo)     127.0.0.1/8      loopback    up
| eth0   (eth0)   192.168.1.5/24   ethernet    up B8:AC:6F:65:31:E5
| vmnet1 (vmnet1) 192.168.121.1/24 ethernet    up 00:50:56:C0:00:01
| vmnet8 (vmnet8) 192.168.179.1/24 ethernet    up 00:50:56:C0:00:08
| ppp0   (ppp0)   10.1.19.69/32    point2point up
|  
| **************************ROUTES**************************
| DST/MASK         DEV    GATEWAY
| 10.0.31.178/32   ppp0
| 209.133.67.35/32 eth0   192.168.1.2
| 192.168.1.0/0    eth0
| 192.168.121.0/0  vmnet1
| 192.168.179.0/0  vmnet8
| 169.254.0.0/0    eth0
| 10.0.0.0/0       ppp0
| 0.0.0.0/0        eth0   192.168.1.2
 

#15: How do I scan specific ports?

map -p [port] hostName
## Scan port 80
nmap -p 80 192.168.1.1
 
## Scan TCP port 80
nmap -p T:80 192.168.1.1
 
## Scan UDP port 53
nmap -p U:53 192.168.1.1
 
## Scan two ports ##
nmap -p 80,443 192.168.1.1
 
## Scan port ranges ##
nmap -p 80-200 192.168.1.1
 
## Combine all options ##
nmap -p U:53,111,137,T:21-25,80,139,8080 192.168.1.1
nmap -p U:53,111,137,T:21-25,80,139,8080 server1.cyberciti.biz
nmap -v -sU -sT -p U:53,111,137,T:21-25,80,139,8080 192.168.1.254
 
## Scan all ports with * wildcard ##
nmap -p "*" 192.168.1.1
 
## Scan top ports i.e. scan $number most common ports ##
nmap --top-ports 5 192.168.1.1
nmap --top-ports 10 192.168.1.1
 

Sample outputs:

Starting Nmap 5.00 ( http://nmap.org ) at 2012-11-27 01:23 IST
Interesting ports on 192.168.1.1:
PORT     STATE  SERVICE
21/tcp   closed ftp
22/tcp   open   ssh
23/tcp   closed telnet
25/tcp   closed smtp
80/tcp   open   http
110/tcp  closed pop3
139/tcp  closed netbios-ssn
443/tcp  closed https
445/tcp  closed microsoft-ds
3389/tcp closed ms-term-serv
MAC Address: BC:AE:C5:C3:16:93 (Unknown)
 
Nmap done: 1 IP address (1 host up) scanned in 0.51 seconds
 

#16: The fastest way to scan all your devices/computers for open ports ever

nmap -T5 192.168.1.0/24

#17: How do I detect remote operating system?

You can identify a remote host apps and OS using the -O option:

 
nmap -O 192.168.1.1
nmap -O  --osscan-guess 192.168.1.1
nmap -v -O --osscan-guess 192.168.1.1

Sample outputs:

Starting Nmap 5.00 ( http://nmap.org ) at 2012-11-27 01:29 IST
NSE: Loaded 0 scripts for scanning.
Initiating ARP Ping Scan at 01:29
Scanning 192.168.1.1 [1 port]
Completed ARP Ping Scan at 01:29, 0.01s elapsed (1 total hosts)
Initiating Parallel DNS resolution of 1 host. at 01:29
Completed Parallel DNS resolution of 1 host. at 01:29, 0.22s elapsed
Initiating SYN Stealth Scan at 01:29
Scanning 192.168.1.1 [1000 ports]
Discovered open port 80/tcp on 192.168.1.1
Discovered open port 22/tcp on 192.168.1.1
Completed SYN Stealth Scan at 01:29, 0.16s elapsed (1000 total ports)
Initiating OS detection (try #1) against 192.168.1.1
Retrying OS detection (try #2) against 192.168.1.1
Retrying OS detection (try #3) against 192.168.1.1
Retrying OS detection (try #4) against 192.168.1.1
Retrying OS detection (try #5) against 192.168.1.1
Host 192.168.1.1 is up (0.00049s latency).
Interesting ports on 192.168.1.1:
Not shown: 998 closed ports
PORT   STATE SERVICE
22/tcp open  ssh
80/tcp open  http
MAC Address: BC:AE:C5:C3:16:93 (Unknown)
Device type: WAP|general purpose|router|printer|broadband router
Running (JUST GUESSING) : Linksys Linux 2.4.X (95%), Linux 2.4.X|2.6.X (94%), MikroTik RouterOS 3.X (92%), Lexmark embedded (90%), Enterasys embedded (89%), D-Link Linux 2.4.X (89%), Netgear Linux 2.4.X (89%)
Aggressive OS guesses: OpenWrt White Russian 0.9 (Linux 2.4.30) (95%), OpenWrt 0.9 - 7.09 (Linux 2.4.30 - 2.4.34) (94%), OpenWrt Kamikaze 7.09 (Linux 2.6.22) (94%), Linux 2.4.21 - 2.4.31 (likely embedded) (92%), Linux 2.6.15 - 2.6.23 (embedded) (92%), Linux 2.6.15 - 2.6.24 (92%), MikroTik RouterOS 3.0beta5 (92%), MikroTik RouterOS 3.17 (92%), Linux 2.6.24 (91%), Linux 2.6.22 (90%)
No exact OS matches for host (If you know what OS is running on it, see http://nmap.org/submit/ ).
TCP/IP fingerprint:
OS:SCAN(V=5.00%D=11/27%OT=22%CT=1%CU=30609%PV=Y%DS=1%G=Y%M=BCAEC5%TM=50B3CA
OS:4B%P=x86_64-unknown-linux-gnu)SEQ(SP=C8%GCD=1%ISR=CB%TI=Z%CI=Z%II=I%TS=7
OS:)OPS(O1=M2300ST11NW2%O2=M2300ST11NW2%O3=M2300NNT11NW2%O4=M2300ST11NW2%O5
OS:=M2300ST11NW2%O6=M2300ST11)WIN(W1=45E8%W2=45E8%W3=45E8%W4=45E8%W5=45E8%W
OS:6=45E8)ECN(R=Y%DF=Y%T=40%W=4600%O=M2300NNSNW2%CC=N%Q=)T1(R=Y%DF=Y%T=40%S
OS:=O%A=S+%F=AS%RD=0%Q=)T2(R=N)T3(R=N)T4(R=Y%DF=Y%T=40%W=0%S=A%A=Z%F=R%O=%R
OS:D=0%Q=)T5(R=Y%DF=Y%T=40%W=0%S=Z%A=S+%F=AR%O=%RD=0%Q=)T6(R=Y%DF=Y%T=40%W=
OS:0%S=A%A=Z%F=R%O=%RD=0%Q=)T7(R=N)U1(R=Y%DF=N%T=40%IPL=164%UN=0%RIPL=G%RID
OS:=G%RIPCK=G%RUCK=G%RUD=G)IE(R=Y%DFI=N%T=40%CD=S)
Uptime guess: 12.990 days (since Wed Nov 14 01:44:40 2012)
Network Distance: 1 hop
TCP Sequence Prediction: Difficulty=200 (Good luck!)
IP ID Sequence Generation: All zeros
Read data files from: /usr/share/nmap
OS detection performed. Please report any incorrect results at http://nmap.org/submit/ .
Nmap done: 1 IP address (1 host up) scanned in 12.38 seconds
           Raw packets sent: 1126 (53.832KB) | Rcvd: 1066 (46.100KB)

See also: Fingerprinting a web-server and a dns server command line tools for more information.
#18: How do I detect remote services (server / daemon) version numbers?

nmap -sV 192.168.1.1

Sample outputs:

Starting Nmap 5.00 ( http://nmap.org ) at 2012-11-27 01:34 IST
Interesting ports on 192.168.1.1:
Not shown: 998 closed ports
PORT   STATE SERVICE VERSION
22/tcp open  ssh     Dropbear sshd 0.52 (protocol 2.0)
80/tcp open  http?
1 service unrecognized despite returning data.

#19: Scan a host using TCP ACK (PA) and TCP Syn (PS) ping

If firewall is blocking standard ICMP pings, try the following host discovery methods:

nmap -PS 192.168.1.1
nmap -PS 80,21,443 192.168.1.1
nmap -PA 192.168.1.1
nmap -PA 80,21,200-512 192.168.1.1

#20: Scan a host using IP protocol ping

nmap -PO 192.168.1.1

#21: Scan a host using UDP ping

This scan bypasses firewalls and filters that only screen TCP:

nmap -PU 192.168.1.1
nmap -PU 2000.2001 192.168.1.1

#22: Find out the most commonly used TCP ports using TCP SYN Scan

 
### Stealthy scan ###
nmap -sS 192.168.1.1
 
### Find out the most commonly used TCP ports using  TCP connect scan (warning: no stealth scan)
###  OS Fingerprinting ###
nmap -sT 192.168.1.1
 
### Find out the most commonly used TCP ports using TCP ACK scan
nmap -sA 192.168.1.1
 
### Find out the most commonly used TCP ports using TCP Window scan
nmap -sW 192.168.1.1
 
### Find out the most commonly used TCP ports using TCP Maimon scan
nmap -sM 192.168.1.1
 

#23: Scan a host for UDP services (UDP scan)

Most popular services on the Internet run over the TCP protocol. DNS, SNMP, and DHCP are three of the most common UDP services. Use the following syntax to find out UDP services:

nmap -sU nas03
nmap -sU 192.168.1.1

Sample outputs:

 
Starting Nmap 5.00 ( http://nmap.org ) at 2012-11-27 00:52 IST
Stats: 0:05:29 elapsed; 0 hosts completed (1 up), 1 undergoing UDP Scan
UDP Scan Timing: About 32.49% done; ETC: 01:09 (0:11:26 remaining)
Interesting ports on nas03 (192.168.1.12):
Not shown: 995 closed ports
PORT     STATE         SERVICE
111/udp  open|filtered rpcbind
123/udp  open|filtered ntp
161/udp  open|filtered snmp
2049/udp open|filtered nfs
5353/udp open|filtered zeroconf
MAC Address: 00:11:32:11:15:FC (Synology Incorporated)
 
Nmap done: 1 IP address (1 host up) scanned in 1099.55 seconds
 

#24: Scan for IP protocol

This type of scan allows you to determine which IP protocols (TCP, ICMP, IGMP, etc.) are supported by target machines:

nmap -sO 192.168.1.1

#25: Scan a firewall for security weakness

The following scan types exploit a subtle loophole in the TCP and good for testing security of common attacks:

 
## TCP Null Scan to fool a firewall to generate a response ##
## Does not set any bits (TCP flag header is 0) ##
nmap -sN 192.168.1.254
 
## TCP Fin scan to check firewall ##
## Sets just the TCP FIN bit ##
nmap -sF 192.168.1.254
 
## TCP Xmas scan to check firewall ##
## Sets the FIN, PSH, and URG flags, lighting the packet up like a Christmas tree ##
nmap -sX 192.168.1.254
 

See how to block Xmas packkets, syn-floods and other conman attacks with iptables.
#26: Scan a firewall for packets fragments

The -f option causes the requested scan (including ping scans) to use tiny fragmented IP packets. The idea is to split up the TCP header over
several packets to make it harder for packet filters, intrusion detection systems, and other annoyances to detect what you are doing.

nmap -f 192.168.1.1
nmap -f fw2.nixcraft.net.in
nmap -f 15 fw2.nixcraft.net.in
## Set your own offset size with the --mtu option ##
nmap --mtu 32 192.168.1.1

#27: Cloak a scan with decoys

The -D option it appear to the remote host that the host(s) you specify as decoys are scanning the target network too. Thus their IDS might report 5-10 port scans from unique IP addresses, but they won't know which IP was scanning them and which were innocent decoys:

nmap -n -Ddecoy-ip1,decoy-ip2,your-own-ip,decoy-ip3,decoy-ip4 remote-host-ip
nmap -n -D192.168.1.5,10.5.1.2,172.1.2.4,3.4.2.1 192.168.1.5

#28: Scan a firewall for MAC address spoofing

 
### Spoof your MAC address ##
nmap --spoof-mac MAC-ADDRESS-HERE 192.168.1.1
 
### Add other options ###
nmap -v -sT -PN --spoof-mac MAC-ADDRESS-HERE 192.168.1.1
 
 
### Use a random MAC address ###
### The number 0, means nmap chooses a completely random MAC address ###
nmap -v -sT -PN --spoof-mac 0 192.168.1.1
 

#29: How do I save output to a text file?

The syntax is:

nmap 192.168.1.1 > output.txt
nmap -oN /path/to/filename 192.168.1.1
nmap -oN output.txt 192.168.1.1

#30: Not a fan of command line tools?

Try zenmap the official network mapper front end:

    Zenmap is the official Nmap Security Scanner GUI. It is a multi-platform (Linux, Windows, Mac OS X, BSD, etc.) free and open source application which aims to make Nmap easy for beginners to use while providing advanced features for experienced Nmap users. Frequently used scans can be saved as profiles to make them easy to run repeatedly. A command creator allows interactive creation of Nmap command lines. Scan results can be saved and viewed later. Saved scan results can be compared with one another to see how they differ. The results of recent scans are stored in a searchable database.

You can install zenmap using the following apt-get command:
$ sudo apt-get install zenmap
Sample outputs:

[sudo] password for vivek:
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following NEW packages will be installed:
  zenmap
0 upgraded, 1 newly installed, 0 to remove and 11 not upgraded.
Need to get 616 kB of archives.
After this operation, 1,827 kB of additional disk space will be used.
Get:1 http://debian.osuosl.org/debian/ squeeze/main zenmap amd64 5.00-3 [616 kB]
Fetched 616 kB in 3s (199 kB/s)
Selecting previously deselected package zenmap.
(Reading database ... 281105 files and directories currently installed.)
Unpacking zenmap (from .../zenmap_5.00-3_amd64.deb) ...
Processing triggers for desktop-file-utils ...
Processing triggers for gnome-menus ...
Processing triggers for man-db ...
Setting up zenmap (5.00-3) ...
Processing triggers for python-central ...

Type the following command to start zenmap:
$ sudo zenmap
Sample outputs


	32.4
33. My examples and snippets

	33.1 use xargs for multiple commands and handle empty results

find -type f -print0 | xargs -0 -I '{}' sh -c 'echo search in {}; sed  -ne '290p' | grep cd' | tee  ~/res

-0 If there are blank spaces or characters (including newlines) many commands will not work. This option take cares of file names with blank space.
-I Replace occurrences of replace-str in the initial-arguments with names read from standard input. Also, unquoted blanks do not terminate input items; instead the separator is the newline character.
- sh -c , run multiple commands in new shell

	33.2 search all files for pattern in specific line only

[yizaq@cow1:Tue Apr 23:1146:149:/trunk/ise_dippindots_br/cpm]$ find -type f -print0 | xargs -0 -I '{}' sh -c 'if sed  -ne '290p' {} | grep cd; then echo found match in file {}; fi' | tee  ~/res
- nscd fixes (BZ#4074)
found match in file ./install/isos/ipep/target/isostage/Server/glibc-common-2.5-107.i386.rpm
...
cd ${Adagent_Path}/kerberos/lib64 || log 2 "error, directory ${Adagent_Path}/kerberos/lib64 doesn't exist."
found match in file ./install/rpms/common/src/CSCOcpm-common_postinstall.sh


	33.3

34. Shared objects

	34.1 ld-linux(8) - Linux man page

Name

ld.so, ld-linux.so* - dynamic linker/loader

Synopsis

The dynamic linker can be run either indirectly by running some dynamically linked program or library (in which case no command-line options to the dynamic linker can be passed and, in the ELF case, the dynamic linker which is stored in the .interp section of the program is executed) or directly by running:

/lib/ld-linux.so.* [OPTIONS] [PROGRAM [ARGUMENTS]]

		34.1.1 Description


 
The programs ld.so and ld-linux.so* find and load the shared libraries needed by a program, prepare the program to run, and then run it.

Linux binaries require dynamic linking (linking at run time) unless the -static option was given to ld(1) during compilation.

The program ld.so handles a.out binaries, a format used long ago; ld-linux.so* handles ELF (/lib/ld-linux.so.1 for libc5, /lib/ld-linux.so.2 for glibc2), which everybody has been using for years now. Otherwise both have the same behavior, and use the same support files and programs ldd(1), ldconfig(8) and /etc/ld.so.conf.

When resolving library dependencies, the dynamic linker first inspects each dependency string to see if it contains a slash (this can occur if a library pathname containing slashes was specified at link time). If a slash is found, then the dependency string is interpreted as a (relative or absolute) pathname, and the library is loaded using that pathname.

		34.1.2 Shared object load order 
If a library dependency does not contain a slash, then it is searched for in the following order:

o
(ELF only) Using the directories specified in the DT_RPATH dynamic section attribute of the binary if present and DT_RUNPATH attribute does not exist. Use of DT_RPATH is deprecated.

o

Using the environment variable LD_LIBRARY_PATH. Except if the executable is a set-user-ID/set-group-ID binary, in which case it is ignored.

o

(ELF only) Using the directories specified in the DT_RUNPATH dynamic section attribute of the binary if present.

o

From the cache file /etc/ld.so.cache, which contains a compiled list of candidate libraries previously found in the augmented library path. If, however, the binary was linked with the -z nodeflib linker option, libraries in the default library paths are skipped. Libraries installed in hardware capability directories (see below) are preferred to other libraries.

o

In the default path /lib, and then /usr/lib. If the binary was linked with the -z nodeflib linker option, this step is skipped.

$ORIGIN and rpath

ld.so understands the string $ORIGIN (or equivalently ${ORIGIN}) in an rpath specification (DT_RPATH or DT_RUNPATH) to mean the directory containing the application executable. Thus, an application located in somedir/app could be compiled with gcc -Wl,-rpath,'$ORIGIN/../lib' so that it finds an associated shared library in somedir/lib no matter where somedir is located in the directory hierarchy. This facilitates the creation of "turn-key" applications that do not need to be installed into special directories, but can instead be unpacked into any directory and still find their own shared libraries.
Options

		34.1.3 Options
--list
List all dependencies and how they are resolved.

--verify
Verify that program is dynamically linked and this dynamic linker can handle it.
--library-path PATH
Use PATH instead of LD_LIBRARY_PATH environment variable setting (see below).
--inhibit-rpath LIST
Ignore RPATH and RUNPATH information in object names in LIST. This option is ignored if ld.so is set-user-ID or set-group-ID.
--audit LIST
Use objects named in LIST as auditors.
Hardware Capabilities

Some libraries are compiled using hardware-specific instructions which do not exist on every CPU. Such libraries should be installed in directories whose names define the required hardware capabilities, such as /usr/lib/sse2/. The dynamic linker checks these directories against the hardware of the machine and selects the most suitable version of a given library. Hardware capability directories can be cascaded to combine CPU features. The list of supported hardware capability names depends on the CPU. The following names are currently recognized:

Alpha
ev4, ev5, ev56, ev6, ev67

MIPS

loongson2e, loongson2f, octeon, octeon2

PowerPC
4xxmac, altivec, arch_2_05, arch_2_06, booke, cellbe, dfp, efpdouble, efpsingle, fpu, ic_snoop, mmu, notb, pa6t, power4, power5, power5+, power6x, ppc32, ppc601, ppc64, smt, spe, ucache, vsx
SPARC
flush, muldiv, stbar, swap, ultra3, v9, v9v, v9v2

s390

dfp, eimm, esan3, etf3enh, g5, highgprs, hpage, ldisp, msa, stfle, z900, z990, z9-109, z10, zarch

x86 (32-bit only)
acpi, apic, clflush, cmov, cx8, dts, fxsr, ht, i386, i486, i586, i686, mca, mmx, mtrr, pat, pbe, pge, pn, pse36, sep, ss, sse, sse2, tm
Environment

There are four important environment variables.

LD_BIND_NOW
(libc5; glibc since 2.1.1) If set to a nonempty string, causes the dynamic linker to resolve all symbols at program startup instead of deferring function call resolution to the point when they are first referenced. This is useful when using a debugger.
LD_LIBRARY_PATH
A colon-separated list of directories in which to search for ELF libraries at execution-time. Similar to the PATH environment variable. Ignored in set-user-ID and set-group-ID programs.
LD_PRELOAD
A list of additional, user-specified, ELF shared libraries to be loaded before all others. The items of the list can be separated by spaces or colons. This can be used to selectively override functions in other shared libraries. The libraries are searched for using the rules given under DESCRIPTION. For set-user-ID/set-group-ID ELF binaries, preload pathnames containing slashes are ignored, and libraries in the standard search directories are loaded only if the set-user-ID permission bit is enabled on the library file.
LD_TRACE_LOADED_OBJECTS
(ELF only) If set to a nonempty string, causes the program to list its dynamic library dependencies, as if run by ldd(1), instead of running normally.
Then there are lots of more or less obscure variables, many obsolete or only for internal use.
LD_AOUT_LIBRARY_PATH
(libc5) Version of LD_LIBRARY_PATH for a.out binaries only. Old versions of ld-linux.so.1 also supported LD_ELF_LIBRARY_PATH.
LD_AOUT_PRELOAD
(libc5) Version of LD_PRELOAD for a.out binaries only. Old versions of ld-linux.so.1 also supported LD_ELF_PRELOAD.
LD_AUDIT
(glibc since 2.4) A colon-separated list of user-specified, ELF shared objects to be loaded before all others in a separate linker namespace (i.e., one that does not intrude upon the normal symbol bindings that would occur in the process). These libraries can be used to audit the operation of the dynamic linker. LD_AUDIT is ignored for set-user-ID/set-group-ID binaries.
The dynamic linker will notify the audit libraries at so-called auditing checkpoints-for example, loading a new library, resolving a symbol, or calling a symbol from another shared object-by calling an appropriate function within the audit library. For details, see rtld-audit(7). The auditing interface is largely compatible with that provided on Solaris, as described in its Linker and Libraries Guide, in the chapter Runtime Linker Auditing Interface.

LD_BIND_NOT
(glibc since 2.1.95) Do not update the GOT (global offset table) and PLT (procedure linkage table) after resolving a symbol.
LD_DEBUG
(glibc since 2.1) Output verbose debugging information about the dynamic linker. If set to all prints all debugging information it has, if set to help prints a help message about which categories can be specified in this environment variable. Since glibc 2.3.4, LD_DEBUG is ignored for set-user-ID/set-group-ID binaries.
LD_DEBUG_OUTPUT
(glibc since 2.1) File where LD_DEBUG output should be fed into, default is standard output. LD_DEBUG_OUTPUT is ignored for set-user-ID/set-group-ID binaries.
LD_DYNAMIC_WEAK
(glibc since 2.1.91) Allow weak symbols to be overridden (reverting to old glibc behavior). For security reasons, since glibc 2.3.4, LD_DYNAMIC_WEAK is ignored for set-user-ID/set-group-ID binaries.
LD_HWCAP_MASK
(glibc since 2.1) Mask for hardware capabilities.
LD_KEEPDIR
(a.out only)(libc5) Don't ignore the directory in the names of a.out libraries to be loaded. Use of this option is strongly discouraged.
LD_NOWARN
(a.out only)(libc5) Suppress warnings about a.out libraries with incompatible minor version numbers.
LD_ORIGIN_PATH
(glibc since 2.1) Path where the binary is found (for non-set-user-ID programs). For security reasons, since glibc 2.4, LD_ORIGIN_PATH is ignored for set-user-ID/set-group-ID binaries.
LD_POINTER_GUARD
(glibc since 2.4) Set to 0 to disable pointer guarding. Any other value enables pointer guarding, which is also the default. Pointer guarding is a security mechanism whereby some pointers to code stored in writable program memory (return addresses saved by setjmp(3) or function pointers used by various glibc internals) are mangled semi-randomly to make it more difficult for an attacker to hijack the pointers for use in the event of a buffer overrun or stack-smashing attack.
LD_PROFILE
(glibc since 2.1) Shared object to be profiled, specified either as a pathname or a soname. Profiling output is written to the file whose name is: "$LD_PROFILE_OUTPUT/$LD_PROFILE.profile".
LD_PROFILE_OUTPUT
(glibc since 2.1) Directory where LD_PROFILE output should be written. If this variable is not defined, or is defined as an empty string, then the default is /var/tmp. LD_PROFILE_OUTPUT is ignored for set-user-ID and set-group-ID programs, which always use /var/profile.
LD_SHOW_AUXV
(glibc since 2.1) Show auxiliary array passed up from the kernel. For security reasons, since glibc 2.3.5, LD_SHOW_AUXV is ignored for set-user-ID/set-group-ID binaries.
LD_USE_LOAD_BIAS
By default (i.e., if this variable is not defined) executables and prelinked shared objects will honor base addresses of their dependent libraries and (nonprelinked) position-independent executables (PIEs) and other shared objects will not honor them. If LD_USE_LOAD_BIAS is defined wit the value, both executables and PIEs will honor the base addresses. If LD_USE_LOAD_BIAS is defined with the value 0, neither executables nor PIEs will honor the base addresses. This variable is ignored by set-user-ID and set-group-ID programs.
LD_VERBOSE
(glibc since 2.1) If set to a nonempty string, output symbol versioning information about the program if LD_TRACE_LOADED_OBJECTS variable has been set.
LD_WARN
(ELF only)(glibc since 2.1.3) If set to a nonempty string, warn about unresolved symbols.
LDD_ARGV0
(libc5) argv[0] to be used by ldd(1) when none is present.
Files

/lib/ld.so

a.out dynamic linker/loader
/lib/ld-linux.so.{1,2}
ELF dynamic linker/loader
/etc/ld.so.cache
File containing a compiled list of directories in which to search for libraries and an ordered list of candidate libraries.
/etc/ld.so.preload
File containing a whitespace-separated list of ELF shared libraries to be loaded before the program.
lib*.so*
shared libraries
Notes

The ld.so functionality is available for executables compiled using libc version 4.4.3 or greater. ELF functionality is available since Linux 1.1.52 and libc5.

See Also

ldd(1), sln(1), getauxval(3), rtld-audit(7), ldconfig(8)

		34.1.4
	34.2
35. Connectivity

	35.1 VNC 

		35.1.1 GUI problem - can't open gnome session
fix:
vi ~/.vnc//xstartup
put content:
#!/bin/sh

# Uncomment the following two lines for normal desktop:
#unset SESSION_MANAGER
#exec /etc/X11/xinit/xinitrc

[ -x /etc/vnc/xstartup ] && exec /etc/vnc/xstartup
[ -r $HOME/.Xresources ] && xrdb $HOME/.Xresources
xsetroot -solid grey
vncconfig -nowin &
# xterm -geometry 80x24+10+10 -ls -title "$VNCDESKTOP Desktop" &
startkde &
# twm &

Then restart server:
[yizaq@pmbu-dev-vm58:Mon Nov 25:1002:7:~]$ vncserver -kill :4
Killing Xvnc process ID 10547
[yizaq@pmbu-dev-vm58:Mon Nov 25:1003:8:~]$ vnc_mac 

New 'pmbu-dev-vm58:4 (yizaq)' desktop is pmbu-dev-vm58:4


		35.1.2 Login via SSH but show GUI on VNC
YIZAQ-M-W1ZV:SearchEngine yizaq$ ssh yizaq@yizaq-dev01.cisco.com
Last login: Mon Feb 15 21:08:36 2016 from 10.56.224.228
Cisco Linux 5.50-5Client Kickstarted on: Mon Nov 17 16:30:04 CET 2014.
-bash-3.2$ export DISPLAY=":2.0"

or export DISPLAY=yizaq-dev01:2


		35.1.3 tmux

		    35.1.3.1 What is it


TMUX – ALTERNATIVE TO VNC ??
JANUARY 8, 2014 SUDKS	LEAVE A COMMENT
Its a very good alternative to vnc if your daily work is more to do with commands, scripts on switches, routers or linux kinda environment.

This is very lightweight application and thats one reason which makes it very much usable across geographies with lesser bandwidth.

An intro into it….

tmux – instead of vnc —Install tmux on a remote linux machine.
just login to any linux servers say via ssh, hope tmux command exist.
then.. just run tmux new-session -d -s “some_name”
-d is for not using the current terminal where you are running the tmux. -s is for the name
tmux –help or man tmux is another key to know all commands.. some basics here…
ctrl+b and then type any of the following commands [ remember its not ctrl+b+c . its ctrl+b and then just c ]
c – for create new terminal
f – find the termnial
, – for renaming
: – takes other commands
n – next window
p – previous window
<- -> - move windows
split-windows – this is used after typing ctrl+b  :  this splits the screen
w – lists the windows
d – detach
To attach back .. you can use the following command from the same server.
tmux attach-session -t “some_name” – same as the old name.
It looks something like this….

-bash-4.1$ tmux new-session -d -s  "csi-pmbu17-lnx"
-bash-4.1$ tmux attach-session -t "csi-pmbu17-lnx"

Working flow:
a. Create session: tmux new -s "yosi1"
work, make windows etc. 
detach (ctrl-b d)
logout, disconnect

b. login to machine
tmux list-sessions
[yizaq@csi-pmbu17-lnx:Tue Sep 06:93:7:~]$ tmux list-sessions
csi-pmbu17-lnx: 3 windows (created Mon Aug 22 17:19:34 2016) [249x73]
yosi1: 1 windows (created Tue Sep  6 16:19:46 2016) [249x73] (attached)

c. Resume work
[yizaq@csi-pmbu17-lnx:Tue Sep 06:94:8:~]$ tmux attach-session -t "yosi1"
or
[yizaq@csi-pmbu17-lnx:Tue Sep 06:95:9:~]$ tmux attach -t "yosi1"

		    35.1.3.2 install from source
https://linoxide.com/how-tos/install-tmux-manage-multiple-linux-terminals/

Method 1: Binary package

On Debian / Ubuntu based

$ sudo apt-get install tmux
On RedHat / CentOS based

# yum install tmux
Method 2 : Compiling the source


 
If your Linux repositories does not provide the latest one, you can install the latest one manually. At this article is written, the latest version of tmux is 1.8. To get the latest one, you can download it from tmux website, compile it anda install it. Here are the steps on Linux CentOS 6.4.

a. Download the source file and requirement package

$ wget http://downloads.sourceforge.net/tmux/tmux-1.8.tar.gz
not working. fix.
[212680136@G9VK2GH2E:Wed Nov 15:~/temp:]$ wget http://downloads.sourceforge.net/tmux/tmux-1.8.tar.gz --no-check-certificate

Still not working. fix
[212680136@G9VK2GH2E:Wed Nov 15:~/temp:]$ git clone https://github.com/tmux/tmux.git^C
[212680136@G9VK2GH2E:Wed Nov 15:~/temp:]$ tar cvfz tmux.tarz tmux
[212680136@G9VK2GH2E:Wed Nov 15:~/temp:]$ scp tmux.tarz ${USER_UNX}@${DEVM3}:/home/${USER_UNX}/
tmux.tarz                                                                                                                                                                                                  100% 9802KB   1.1MB/s   00:09    
$ wget https://github.com/downloads/libevent/libevent/libevent-2.0.21-stable.tar.gz
b. Extract the files

tar zxvf tmux-18.tar.gz
tar zxfv libevent-2.0.21-stable.tar.gz
c. Install pre-requisites application

yum install gcc kernel-devel make ncurses-devel y
Those application is needed to compile the tmux source file.

d. Install libevent2 source

# cd libevent-2.0.21-stable
# ./configure --prefix=/usr/local
# make && make install
e. Install tmux source

# cd tmux-1.8
de680136@ctds64-1:/home/de680136/temp/tmux> export LDFLAGS='-L/home/de680136/.local -Wl,-rpath=/home/de680136/.local/'
 ./configure --prefix=/home/de680136/.local 
# make && make install
f. Add tmux into path environment

# cd ~
export PATH=$PATH:/usr/local/bin
Run tmux

After the installation is finish, then type tmux on your console to run tmux.

Tmux default

A new session is now started. At the bottom line, you will find a bar contains some information. Here’s how to read it :

[0] 0:bash* : is the window number and the window name
pungki@dev-machine : is the username and the hostname
01:30 : tell us the current time
12-Jan-14 : tell us the current date
Splitting tmux vertically

To split tmux vertically, just press (Ctrl-b) + % . Then the screen will be separated vertically.

Tmux vertically split

Splitting tmux horizontally

To split tmux horizontally, press (Ctrl-b) + “ . Then the sceen will be separated horizontally.

Tmux horizontally split

But of course we can mix them. Here’s another example.

Tmux mix split

Moving between panes

Of course you will need move between panes. Otherwise, there is no use to create panes. By default, Linux console does not support mouse. So we need to know how to move between panes manually. There are some ways move between panes. Here’s a list how to to that.

Move Left : (Ctrl-b) + Left arrow OR (Ctrl-b) + {
Move Right : (Ctrl-b) + Right arrow OR (Ctrl-b) + }
Move Up : (Ctrl-b) + + Up arrow
Move Down : (Ctrl-b) + Down arrow
Move to the next pane : (Ctrl-b) + o
Show number for each panes and press the number : (Ctrl-b) + b q + pane number. For example : (Ctrl-b) + b q + 1 will move you to pane number 1
Tmux window number

Resizing panes

You may want to resize panes to fit your need. Here’s a list how to do that :

(Ctrl-b) + : then type resize-pane -D (Resizes the current pane down)
(Ctrl-b) + : then type resize-pane -U (Resizes the current pane upward)
(Ctrl-b) + : then type resize-pane -L (Resizes the current pane left)
(Ctrl-b) + : then type resize-pane -R (Resizes the current pane right)
(Ctrl-b) + : then type resize-pane -D 5 (Resizes the current pane down by 5 cells)
(Ctrl-b) + : then type resize-pane -U 5 (Resizes the current pane upward by 5 cells)
(Ctrl-b) + : then type resize-pane -L 5 (Resizes the current pane left by 5 cells)
(Ctrl-b) + : then type resize-pane -R 5 (Resizes the current pane right by 5 cells)
(Ctrl-b) + : then type resize-pane -t 2 5 (Resizes the pane with the id of 2 down by 5 cells)
(Ctrl-b) + : then type resize-pane -t -L 5 (Resizes the pane with the id of 2 left by 5 cells)
Please note that you need to press the colon sign (:) after pressing Ctrl-b

From the screenshot above, we try to resize pane number 1. If the active pane is pane number 1 then we can press (Ctrl-b) + : resize pane -D 13 to make it down for 13 cells.

Tmux resize

Zoom panes

This feature is new at 1.8 version. You can now zoomed a pane without need to detach or break it into independent window. At the active pane, press (Ctrl-b) + z to zoom the pane. Pressing it again will bring the zoomed pane back.

Make a pane into window

If you wish to exclude your pane into window, then you can do this by pressing (Ctrl-b) + : then type break pane. Then your pane will become an independent window. Next, you are recommended to give it a name in order to make it easy to remember.

Closing panes

To close a pane, simply type exit from the pane. Then it will close.

$ exit
Detach and Re-attach Tmux

One of Tmux advantage is you can detach tmux without losing anything you are working on it. Then you can re-attach Tmux with the condition exactly before you detach it. This is very useful if you have to move between computers and don’t want to lose anything.

To detach it, use keystroke (Ctrl-b) + d

While re-attach the same window can be executed by typing tmux attach on your console.

$ tmux attach
If you - let say - remote your Linux machine from Windows client using putty, you still can use tmux. Here’s a sample of Putty client run Tmux.

Tmux using putty

Tmux is different with Terminator. Terminator is a local application which run on Linux terminal as a terminal multiplexerl. We can’t run Terminator on a remote machine because it’s a local application.

Create more window

We are sure that you won't mess your window with a lot of panes. 3 - 5 panes are may the maximum panes on a single window. More than 5 panes, can make you uncomfortable. To solve this situation, we can make more tmux window. To create new window, you can press (Ctrl-b) + c . Then you will see at the status bar, an info like this [0] 0:bash - 1:bash*

Tmus status bar

Rename a window name

To rename a window name, press Ctrl-b + , . (press Ctrl-b and press comma sign). Then provide the name and press Enter to confirm it.

Tmux renaming window

Tmux renaming windows success

An asterisk sign (*) means the current window.

Moving between Window

To move between window, we can use :

(Ctrl-b) + n : Move to next window
(Ctrl-b) + p : Move to previous window
(Ctrl-b) + w : Interactively choose the window (useful if you have more than 2 window)
Interactively choost the window
Tmux move windows interactively

Closing Window

If you want to close a window, simply press (Ctrl-b) + &

Configuring Tmux

Tmux is highly configurable. You can edit tmux.conf file to do this. If you don’t have the file, you can create it. For system wide, you can put the tmux.conf in /etc folder. Or put it on ~/.tmux.conf for user spesific settings. Here some example of tmux.conf content. Here are some example of tmux.conf configuration content.

Change the Prefix Key

By default, tmux prefix is Ctrl-b. Every command in tmux, must begin with Ctrl-b keystroke. If you don’t like it, you can change it. Let say you want to change it into Ctrl-a. Just put this line into your tmux.conf :

unbind C-b
set -g prefix C-a
Save the file and re-run tmux. Please note, that in order to make tmux.conf changes works, you need to exit all tmux sessions before. If you detach a tmux session, and re-attach it, the changes will not work.

Change the splitting panes

As mentioned above, tmux use % sign and “ sign to split panes. You may don’t like the combination. To change it, just put this line into tmux.conf

unbind %
bind h split-window -v
unbind ‘ ” ’
bind v split-window -h
The above configuration will change :

% sign into h letter for vertically split
“ sign into v letter for horizontally split
Change the status bar looks

# Status bar theme
set -g status-bg black
set -g status-fg white

# Highlight and Notify
set-window-option -g window-status-current-bg red
setw -g monitor-activity on
set -g visual-activity on
The above configuration will highlight the active window with red color.

Change the numbering system of panes and windows

By default, the numbering system of panes and windows are start from 0. If you want to start it from 1, you can put this line on your .tmux.conf file.

# start with window 1 (instead of 0)
set -g base-index 1

# start with pane 1
set -g pane-base-index 1
There are still a lot of configuration that can be made for tmux. More parameters can be found inside tmux manual section Options.

		    35.1.3.3

	35.2 screen, similar tool to tmux
Linux Screen Tips
We use screen daily, so as we keep adding new tips when we find good ones.
Installing Screen with Yum
Starting Linux Screen
Control Command
Creating Windows
Switching Between Windows
Detaching From Screen
Reattach to a Screen
Logging Output
Getting Alerts
Locking Screen
Stopping Screen
Video Tutorial
Installing Screen with Yum
Chances are that you already have screen on your system. On most Red Hat and CentOS distributions you can find Linux screen  in /usr/bin/screen. To see if screen is in your path, you can use the which command:
[root@office ~]# which screen
/usr/bin/screen
If you do not have screen, then you can install it easily from an RPM or the package file for your system. For example, on CentOS you can install screen with yum:
[root@office ~]# yum install screen
...
Complete!
As you probably already have Linux screen or can use an RPM, I am not going to cover the building of screen from source. Lets get on to how to use screen.
Starting Linux Screen
Screen is started from the command line just like any other command:
[root@office ~]# screen
You are now inside of a window within screen. This functions just like a normal shell except for a few special characters.
Control Command
Command: “Ctrl-a”
Screen uses the command “Ctrl-a” that’s the control key and a lowercase “a”  as a signal to send commands to screen instead of the shell.
For example, “Ctrl-a” then “?”. You should now have the screen help page.

Screen key bindings, page 1 of 4.
 
Command key:  ^A   Literal ^A:  a
 
break      ^B b          only       Q
clear      C             other      ^A
colon      :             pow_break  B
copy       ^[ [          pow_detach D
detach     ^D d          prev       ^P p ^?
digraph    ^V            readbuf    &amp;amp;amp;lt;
displays   *             redisplay  ^L l
fit        F             removebuf  =
flow       ^F f          reset      Z
focus      ^I            screen     ^C c
hardcopy   h             select     '
help       ?             silence    _
Key bindings are the commands the screen accepts after you hit “Ctrl-a”. You can reconfigure these keys to your liking using a .screenrc file, but I just use the defaults.
Creating Windows
Command: “Ctrl-a” “c”.
To create a new window, you just use “Ctrl-a” “c”.
This will create a new window for you with your default prompt.  Your old window is still active.
For example, I can be running top and then open a new window to do other things. Top stays running! It is still there. To try this for yourself, start up screen and then run top. (Note: I have truncated some screens to save space.)
Start top

top - 09:10:33 up 35 days, 17:26,  1 user,  load averag
Tasks: 131 total,   1 running, 130 sleeping,   0 stoppe
Cpu(s):  0.4%us,  0.2%sy,  0.0%ni, 99.4%id,  0.0%wa,  0
Mem:  12302040k total,  6363652k used,  5938388k free,
Swap:  1052248k total,       12k used,  1052236k free,
Now open a new window with: “Ctrl-a” “c”
Your top window is still running you just have to switch back to it.
Switching Between Windows
Command: “Ctrl-a” “n”
Screen allows you to move forward and back. In the example above, you could use “Ctrl-a “n” to get back to top. This command switches you to the next window.
The windows work like a carousel and will loop back around to your first window.
You can create several windows and toggle through them with “Ctrl-a” “n” for the next window or “Ctrl-a” “p” for the previous window.
Each process will keep running until you kill that window.
Detaching From Screen
Command: “Ctrl-a” “d”
Detaching is the most powerful part of screen.  Screen allows you to detach from a window and reattach later.
If your network connection fails, screen will automatically detach your session! 
You can detach from the window using “Ctrl-a” “d”.
This will drop you into your shell.
All screen windows are still there and you can re-attach to them later.
This is great when you are using rsync for server migration.
 
Reattach to Screen
If your connection drops or you have detached from a screen, you can re-attach by just running:
1
[jeffh@office ~]$ screen -r
This will re-attach to your screen.
However, if you have multiple screens you may get this:

[jeffh@office ~]$ screen -r
There are several suitable screens on:
31917.pts-5.office      (Detached)
31844.pts-0.office      (Detached)
Type "screen [-d] -r [pid.]tty.host" to resume one of them.
If you get this, just specify the screen you want.
1
[jeffh@office ~]$ screen -r  31844.pts-0.office
Logging Your Screen Output
As a consultant, I find it important to keep track of what I do to someone’s server. Fortunately, screen makes this easy.
Using “Ctrl-a” “H”, creates a running log of the session.
Screen will keep appending data to the file through multiple sessions. Using the log function is very useful for capturing what you have done, especially if you are making a lot of changes. If something goes awry, you can look back through your logs.
Getting Alerts
Screen can monitor a window for activity or inactivity. This is great if you are downloading large files, compiling, or waiting for output.
If you are waiting for output from a long running program, you can use “Ctrl-a” “M” to look for activity. Screen will then flash an alert at the bottom of the page when output is registered on that screen.
I use this when running a command that takes a long time to return data. I can just fire up the command, switch to another window and not have to keep switching back to check the status.
You can also monitor for inactivity. Why use this?
If you are downloading a large file or compiling a program, you can be notified when there is no more output. This is a great signal to when that job is done. To monitor for silence or no output use “Ctrl-A” “_”.
Locking Your Screen Session
If you need to step away from your computer for a minute, you can lock your screen session using “Ctrl-a” “x”.  This will require a password to access the session again.

Screen used by root &amp;amp;amp;lt;jeffh&amp;amp;amp;gt;.
Password:
Stopping Screen
When you are done with your work, I recommend you stop the session instead of saving it for later.  To stop screen you can usually just type exit from your shell. This will close that screen window.  You have to close all screen windows to terminate the session.
You should get a message about screen being terminated once you close all windows.

[screen is terminating]
Alternatively, you can use “Ctrl-a” “k”.  You should get a message if you want to kill the screen.

reattach session
screen -d -r
As screen -r says, there is one screen, but it is attached. To resume it on your current terminal, you have to detach it from the other one first: screen -d -r 27863, see manpage -d.

	35.3 My screen commands
a. new session: screen -S sessionname 
b. reattach: screen -r
c. detach+reattach: screen -d -r
d. list sessions: screen -ls
e. list windows and switch: ctrl+a+"
f. list windows : ctrl+a+w
g. ~/.screenrc, note ctrl-a mapped to ctrl-b
[de680136@ctds64-1:Thu Nov 16:~:]$ cat .screenrc 
# ===============================================================
# ESCAPE - the COMMAND CHARACTER
# ===============================================================
escape ^Bb  # suggested binding for emacs users

# ===============================================================
# BINDINGS
# ===============================================================
bindkey "^[Od" prev  # change window with ctrl-left
bindkey "^[Oc" next  # change window with ctrl-right

# ===============================================================
# VARIABLES - Number values
# ===============================================================
defscrollback       3000          # default: 100

# ===============================================================
# VARIABLES - Paths and Files (esp. programs)
# ===============================================================
#
# shell:  Default process started in screen's windows.
# Makes it possible to use a different shell inside screen
# than is set as the default login shell.  Halleluja! :-)
shell               bash


startup_message off

term screen-256color
setenv LC_CTYPE en_US.UTF-8

# ===============================================================
# status line setup
# ===============================================================
#hardstatus off
hardstatus alwayslastline
#hardstatus string '%{= kG}[ %{G}%H %{g}][%= %{= kw}%?%-Lw%?%{r}(%{W}%n*%f%t%?(%u)%?%{r})%{w}%?%+Lw%?%?%= %{g}][%{B} %m-%d %{W} %c %{g}]'
hardstatus string '%{= kG}[ %{G}%H %{g}][%{= kW}%?%-w%?%{= MK}%n*%t%{= kW} %?%+w%?%= %{g}][%{B} %m-%d %{W} %c %{g}]'
#hardstatus string "%{.kW}%-w%{.W}%n %t%{-}%{=b kw}%?%+w%? %=%c %d/%m/%Y" #B&W & date&time
#hardstatus alwayslastline '%{= G}[ %{G}%H %{g}][%= %{= w}%?%-Lw%?%{= R}%n*%f %t%?%{= R}(%u)%?%{= w}%+Lw%?%= %{= g}][ %{y}Load: %l %{g}][%{B}%Y-%m-%d %{W}%c:%s %{g}]'


windowlist string "%4n %h%=%f"

# Enable mouse scrolling and scroll bar history scrolling
termcapinfo xterm* ti@:te@

	35.4 Screen status line
summary:
hardstatus string "%{= KW} %H [%`] %{= Kw}|%{-} %-Lw%{= bW}%n%f %t%{-}%+Lw %=%C%a %Y-%M-%d"
backtick 0 30 30 sh -c 'screen -ls | grep --color=no -o "$PPID[^[:space:]]*"'

linux screen status line
http://www.kilobitspersecond.com/2014/02/10/understanding-gnu-screens-hardstatus-strings/
My current development setup revolves mainly around Vim and GNU Screen. I use Screen only to keep sessions running between work days or in case I get disconnected, but lately I’ve been tempted to try using different windows inside Screen. In order to make this easier, I wanted one of those status lines that shows you all your windows as “tabs”.
Configuring this status line (the “hardware status line” or, as I’ll call it, “hardstatus”) is done with a single, often long string of characters in ~/.screenrc that at first can look entirely baffling:
hardstatus string "%{= KW} %H [%`] %{= Kw}|%{-} %-Lw%{= bW}%n%f %t%{-}%+Lw %=%C%a %Y-%M-%d"
Exactly.
To my dismay, almost everything I can find about hardstatus through Google are just dumps of other people’s strings, with little to no explanation about why they do what they do – it’s easy to imagine that the people who post them hardly know why they do what they do, either. GNU’s official documentation isn’t terribly helpful.
After finally deciphering a lot of what goes on in these strings, I wanted to spell it out to anybody else who might be hunting around for half a clue about this voodoo. There are (more than?) a few things I haven’t covered here, of course – truncation and conditionals, namely – but this should be enough to get you started.
Below is my current hardstatus string, with comments on each little unit inside it.
hardstatus string "%{= KW} %H [%`] %{= Kw}|%{-} %-Lw%{= bW}%n%f %t%{-}%+Lw %=%C%a %Y-%M-%d"
#
# http://www.gnu.org/software/screen/manual/html_node/String-Escapes.html
#
# %{= wK} : set colors to bright white (W) on bright black (K) and keep current text styles (=)
# %H      : hostname
# [       : opening bracket character
# %`      : print output of 'backtick' command (defined elsewhere in .screenrc)
# ]       : closing bracket character
# %{= wW} : set colors to white (w) on bright black (K) and keep current text styles (=)
# |       : bar character
# ${-}    : restore colors to previous colors / undo last color change
# %-Lw    : list windows before current window (L [optional] = "include flags")
# %{= bW} : set colors to bright white (W) on blue (b) and keep current text styles (=)
# %f      : window flags
# %t      : window title
# %{-}    : restore colors to previous colors / undo last color change
# %+Lw    : list windows after current window (L [optional] = "include flags")
# %=      : expand to fill all space (used here to make remaining content flush right)
# %C      : current time (12-hr; 24-hr is %c)
# %a      : am/pm (lowercase; uppercase is %A)
# %Y      : current year
# -       : hyphen character
# %m      : current month (0-padded; %M for "Jan" etc.)
# -       : hyphen character
# %d      : current date (0-padded)
This results in something like this:
hardstatus
Let’s take a closer look at the types of things that these units are doing.
Text Styles
Anything inside {curly braces} changes the way the text in hardstatus looks, and nothing more. I’ll get to this later, so for now, let’s just strip out these strings, making the rest of the code somewhat easier to understand. I recommend you do the same when studying anybody else’s hardstatus strings.
 %H [%`] | %-Lw%n%f %t%+Lw %=%C%a %Y-%M-%d
Windows
Anything with a w (or W) in it deals with displaying the Screen window titles themselves. Somewhere in the middle of most hardstatus strings will be something like this:
%-Lw%n%f %t%+Lw
Bookending this line are the strings %-Lw and %+Lw. These mean, simply, “Print the (previous/next) windows”: %-w for the previous ones, %+w for the next ones, and L to indicate that we want those windows’ flags to appear as well (although this is optional).
If we take these away, we’re left with this:
%n%f %t
This is the formatting of the title for our active window: Number, Flags, (space,) and Title. Simple!
System Info
Doing away with our window listing, we’re left with this:
 %H [%`] | %=%C%a %Y-%M-%d
Let’s get rid of the last half of it right away, the stuff that was to the right of the window list.  %C%a %Y-%M-%d is just the time and date, and %= just tells Screen to shove everything after it to the right edge of the terminal (more or less: this string “pushes” text away until it meets the screen edge or another %=).
Now all that remains is what was to the left of the window list:
 %H [%`] |
%H is the hostname of the server that we’re connected to; the square brackets are literally square brackets that get printed onto the screen, as is the vertical bar. (Anything that isn’t preceded by % will be printed out as-is.)
The backtick character, %`, prints the output of whatever you’ve assigned to the backtick command. In my case (in ~/.screenrc):
backtick 0 30 30 sh -c 'screen -ls | grep --color=no -o "$PPID[^[:space:]]*"'
This prints the name of my current Screen session. I have no idea why this does what it does; I confess I nicked it from Super User. My understanding is that later versions of Screen can or will be able to do this with a simple escape character similar to those above.
Text Styles (Again)
As I said, anything inside {curly braces} affects the way the text looks – this includes formatting (like bold, underline, etc) and color. Here’s an example:
%{+bu wW}
The first half of the string inside the braces (+bu) tells hardstatus to display the following text as bold and underlined. More styling codes are available and are listed in GNU’s documentation. The + indicates that we want to add this property; if we subsequently wanted to unbold text, we would do something like this: %{-b}.
The second half of the string (wW) tells hardstatus which colors to use, the first character being the background color and the second being the foreground color. In this case, we’re printing “bright white” on “white” (which is really more of a light gray). Again, more codes are available in GNU’s documentation.
To reset text styles to their previous state (before the most recent change), all you need is  %{-}.

	35.5
36. Cookbook

	36.1 Diff 

		36.1.1 find difference between two text files with one item per line
a. grep -Fxvf file1 file2
What the flags mean:

-F, --fixed-strings
              Interpret PATTERN as a list of fixed strings, separated by newlines, any of which is to be matched.    
-x, --line-regexp
              Select only those matches that exactly match the whole line.
-v, --invert-match
              Invert the sense of matching, to select non-matching lines.
-f FILE, --file=FILE
              Obtain patterns from FILE, one per line.  The empty file contains zero patter

b. diff "${file1}" "${file2}" | grep "<" | sed 's/^<//g'  > "${diff_file}"

		36.1.2
	36.2
37. Display 

38. korn shell ksh

    38.1 Tutorial

        38.1.1  intro
Step 2. Understand variables.

Hopefully, you already understand the concept of a variable. It is a place you can store a value to, and then do operations on "whatever is in this place",vs the value directly.
In shellscripts, a variable can contain a collection of letters and/or numbers [aka a 'string'] , as well as pure numbers.

You set a variable by using

variablename="some string here"
  OR
variablename=1234
You access what is IN a variable, by putting a dollar-sign in front of it.
echo $variablename
  OR
echo ${variablename}
If you have JUST a number in a variable, you can do math operations on it. But that comes later on in this tutorial.
Step 3. Put everything in appropriate variables

Well, okay, not EVERYTHING :-) But properly named variables make the script more easily readable. There isn't really a 'simple' example for this, since it is only "obvious" in large scripts. So either just take my word for it, or stop reading and go somewhere else now!
An example of "proper" variable naming practice:


#Okay, this script doesnt do anything useful, it is just for demo purposes.
# and normally, I would put in more safety checks, but this is a quickie.
INPUTFILE="$1"
USERLIST="$2"
OUTPUTFILE="$3"

count=0

while read username ; do
	grep $username $USERLIST >>$OUTPUTFILE
	count=$(($count+1))
done < $INPUTFILE
echo user count is $count	

While the script may not be totally readable to you yet, I think you'll agree it is a LOT clearer than the following;


i=0
while read line ; do
	grep $line $2 >> $3
	i=$(($i+1))
done <$1
echo $i

Note that '$1' means the first argument to your script.
'$*' means "all the arguments together
'$#' means "how many arguments are there?"

Step 4. Know your quotes

It is very important to know when, and what type, of quotes to use.
Quotes are generally used to group things together into a single entity.
Single-quotes are literal quotes.
Double-quotes can have their contents expanded
 echo "$PWD"
prints out your current directory
 echo '$PWD'
prints out the string $PWD
 echo $PWDplusthis
prints out NOTHING. There is no such variable "PWDplusthis
 echo "$PWD"plusthis
prints out your current directory, and the string "plusthis" immediately following it. You could also accomplish this with the alternate form of using variables,
 echo ${PWD}plusthis
There is also what is sometimes called the `back quote`, or ` backtick`: This is not used to quote things, but actually to evaluate and run commands.

        38.1.2 Ksh basics

This is a quickie page to run through basic "program flow control" commands, if you are completely new to shell programming. The basic ways to shape a program, are loops, and conditionals. Conditionals say "run this command, IF some condition is true". Loops say "repeat these commands" (usually, until some condition is met, and then you stop repeating).
Conditionals

IF

The basic type of condition is "if".
if [ $? -eq 0 ] ; then
	print we are okay
else
	print something failed
fi
IF the variable $? is equal to 0, THEN print out a message. Otherwise (else), print out a different message. FYI, "$?" checks the exit status of the last command run.
The final 'fi' is required. This is to allow you to group multiple things together. You can have multiple things between if and else, or between else and fi, or both.
You can even skip the 'else' altogether, if you dont need an alternate case.

if [ $? -eq 0 ] ; then
	print we are okay
	print We can do as much as we like here
fi
CASE

The case statement functions like 'switch' in some other languages. Given a particular variable, jump to a particular set of commands, based on the value of that variable.
While the syntax is similar to C on the surface, there are some major differences;

The variable being checked can be a string, not just a number
There is no "fall through" with ;;. You hit only one set of commands.. UNLESS you use ";&" instead of ";;'
To make up for no 'fall through', you can 'share' variable states
You can use WILDCARDS to match strings
echo input yes or no
read  answer
case $answer in
	yes|Yes|y)
		echo got a positive answer
		# the following ';;' is mandatory for every set
		# of comparative xxx)  that you do
		;;
	no)
		echo got a 'no'
		;;
	q*|Q*)
		#assume the user wants to quit
		exit
		;;
		
	*)
		echo This is the default clause. we are not sure why or
		echo what someone would be typing, but we could take
		echo action on it here
		;;
esac
Loops

WHILE

The basic loop is the 'while' loop; "while" something is true, keep looping.
There are two ways to stop the loop. The obvious way is when the 'something' is no longer true. The other way is with a 'break' command.

keeplooping=1;
while [[ $keeplooping -eq 1 ]] ; do
	read quitnow
	if [[ "$quitnow" = "yes" ]] ; then
		keeplooping=0
	fi
	if [[ "$quitnow" = "q" ]] ; then
		break;
	fi
done
UNTIL

The other kind of loop in ksh, is 'until'. The difference between them is that 'while' implies looping while something remains true.
'until', implies looping until something false, becomes true
until [[ $stopnow -eq 1 ]] ; do
	echo just run this once
	stopnow=1;
	echo we should not be here again.
done
FOR

A "for loop", is a "limited loop". It loops a specific number of times, to match a specific number of items. Once you start the loop, the number of times you will repeat is fixed.
The basic syntax is

for var in one two three ; do
	echo $var
done
Whatever name you put in place of 'var', will be updated by each value following "in". So the above loop will print out
one
two
three
But you can also have variables defining the item list. They will be checked ONLY ONCE, when you start the loop.
list="one two three"
for var in $list ; do
	echo $var
	# Note: Changing this does NOT affect the loop items
	list="nolist"
done
The two things to note are:
It stills prints out "one" "two" "three"
Do NOT quote "$list", if you want the 'for' command to use multiple items
If you used "$list" in the 'for' line, it would print out a SINGLE LINE, "one two three"

        38.1.3 Advanced variable usage

Braces

Sometimes, you want to immediately follow a variable with a string. This can cause issues if you use the typical grammar of "echo $a" to use a variable, since ksh by default, attempts to do "intelligent" parsing of variable names, but it cannot read your mind.
Compare the difference in output in the following lines:
two=2
print one$twothree
print one${two}three
There is no variable named "twothree", so ksh defaults it to an empty value, for the first print line. However, when you use braces to explicitly show ksh {this is the variable name}, it understands that you want the variable named "two", to be expanded in the middle of those other letters.
Arrays

Yes, you CAN have arrays in ksh, unlike old bourne shell. The syntax is as follows:

# This is an OPTIONAL way to quickly null out prior values
set -A array
#
array[1]="one"
array[2]="two"
array[3]="three"
three=3

print ${array[1]}
print ${array[2]}
print ${array[3]}
print ${array[three]}  #This is interpreted as array[3]

Note that ksh automatically dereferences names inbetween [], to be variable values. Unfortunately, ksh does not seem to handle associative arrays. (storing values indexed by a string 'abcd', rather than a number index)
Special variables

There are some "special" variables that ksh itself gives values to. Here are the ones I find interesting
PWD - always the current directory
RANDOM - a different number every time you access it
$$ - the current process id (of the script, not the user's shell)
PPID - the "parent process"s ID. (BUT NOT ALWAYS, FOR FUNCTIONS)
$? - exit status of last command run by the script
PS1 - your "prompt". "PS1='$PWD:> '" is interesting.
$1 to $9 - arguments 1 to 9 passed to your script or function (you can actually have higher, but you need to use braces for those)
Tweaks with variables

Both bourne shell and KSH have lots of strange little tweaks you can do with the ${} operator. 
The ones I like are below.
To give a default value if and ONLY if a variable is not already set, use this construct:


APP_DIR=${APP_DIR:-/usr/local/bin}

(KSH only)
You can also get funky, by running an actual command to generate the value. For example


DATESTRING=${DATESTRING:-$(date)}

(KSH only)
To count the number of characters contained in a variable string, use ${#varname}.


  echo num of chars in stringvar is ${#stringvar}

(KSH only)
To strip off characters from a variable's value, using shell wildcard matching: 
(note that using the doubled-char, means "greedy match": match the longest you can. Whereas the single char default is to just snip off the first match)

$ PATHNAME=/path/to/file

$ print ${PATHNAME#*/}
path/to/file
$ print ${PATHNAME##*/}
file

$ print ${PATHNAME%/*}
/path/to
$ print ${PATHNAME%%/*}
(nothing! It stripped away the entire path! :)

The above is useful for shell-internal replacements for "dirname" and "basename" commands; however, the operators have other uses as well.

        38.1.4 Ksh Functions

Functions are the key to writing just about ANY program that is longer than a page or so of text. Other languages may call functions something else. But essentially, its all a matter of breaking up a large program, into smaller, managable chunks. Ideally, functions are sort of like 'objects' for program flow. You pick a part of your program that is pretty much self-contained, and make it into its own 'function'

Why are functions critical?

Properly written functions can exist by themselves, and affect only small things external to themselves. You should DOCUMENT what things it changes external to itself. Then you can look very carefully just at the function, and determine whether it actually does what you think it does :-)

When your program isn't working properly(WHEN, not if), you can then put in little debug notes to yourself in the approximate section you think is broken. If you suspect a function is not working, then all you have to verify is

Is the INPUT to the function correct?
Is the OUTPUT from the function correct?
Once you have done that, you then know the entire function is correct, for that particular set of input(s), and you can look for errors elsewhere.

A trivial function


printmessage() {
	echo "Hello, this is the printmessage function"
}

printmessage

The first part, from the first "printmessage()" all the way through the final '}', is the function definition. It only defines what the function does, when you decide to call it. It does not DO anything, until you actually say "I want to call this function now".

You call a function in ksh, by pretending it is a regular command, as shown above. Just have the function name as the first part of your line. Or any other place commands go. For example,


echo The message is: `printmessage`

Remember: Just like its own separate shellscript. Which means if you access "$1" in a function, it is the first argument passed in to the function, not the shellscript.

Debugging your functions

If you are really really having difficulties, it should be easy to copy the entire function into another file, and test it separately from the main program.

This same type of modularity can be achived by making separate script files, instead of functions. In some ways, that is almost preferable, because it is then easier to test each part by itself. But functions run much faster than separate shellscripts.

A nice way to start a large project is to start with multiple, separate shellscripts, but then encapsulate them into functions in your main script, once you are happy with how they work.

CRITICAL ISSUE: exit vs return

THE main difference when changing between shellscripts and functions, is the use of "exit".

'exit' will exit the entire script, whether it is in a function or not.
'return' will just quit the function. Like 'exit', however, it can return the default "sucess" value of 0, or any number from 1-255 that you specify. You can then check the return value of a function, just in the same way you can check the return value of an external program, with the $? variable.

# This is just a dummy script. It does not DO anything

fatal(){
	echo FATAL ERROR
	# This will quit the 'fatal' function, and the entire script that
	# it is in!
	exit
}

lessthanfour(){
	if [[ "$1" = "" ]] ; then echo "hey, give me an argument" ; return 1; fi

	# we should use 'else' here, but this is just a demonstration
	if [[ $1 -lt 4 ]] ; then
		echo Argument is less than 4
		# We are DONE with this function. Dont do anything else in
		# here. But the shellscript will continue at the caller
		return
	fi

	echo Argument is equal to or GREATER than 4
	echo We could do other stuff if we wanted to now
}

echo note that the above functions are not even called. They are just
echo defined

A bare "return" in a shellscript is an error. It can only be used inside a function.

CRITICAL ISSUE: "scope" for function variables!

Be warned: Functions act almost just like external scripts... except that by default, all variables are SHARED between the same ksh process! If you change a variable name inside a function.... that variable's value will still be changed after you have left the function!! Run this script to see what I mean.
#!/bin/sh
# Acts the same with /bin/sh, or /bin/ksh, or /bin/bash
subfunc(){
        echo sub: var starts as $var
        var=2
        echo sub: var is now $var
}
var=1
echo var starts as $var, before calling function '"subfunc"'
subfunc  # calls the function
echo var after function is now $var
To avoid this behaviour, and give what is known as "local scope" to a variable, you can use the typeset command, to define a variable as local to the function.
#!/bin/ksh
# You must use a modern sh like /bin/ksh, or /bin/bash for this
subfunc(){
	typeset var
        echo sub: var starts as $var '(empty)'
        var=2
        echo sub: var is now $var
}
var=1
echo var starts as $var, before calling function '"subfunc"'
subfunc  # calls the function
echo var after function is now $var
Another exception to this is if you call a function in the 'background', or as part of a pipe (like echo val | function )
This makes the function be called in a separate ksh process, which cannot dynamically share variables back to the parent shell. Another way that this happens, is if you use backticks to call the function. This treats the function like an external call, and forks a new shell. This means the variable from the parent will not be updated. Eg:

func() {
        newval=$(($1 + 1))
        echo $newval
        echo in func: newval ends as $newval
}
newval=1
echo newval in main is $newval
output=`func $newval`
func $newval
echo output is : $output
echo newval finishes in main as $newval
CRITICAL ISSUE: performance

It is important to note that many sh-derivative shells (including bash!), actually call fork() to call a function routine.
KSH93 (or later) is optimized to not fork a new process for functions. Sadly, earlier versions do call fork().
So, if you are writing particularly resource-sensitive largescale shellscripts, be sure to use ksh93 or newer.

        38.1.5 Ksh built-in functions
Calling the many UNIX programs in /bin and elsewhere can be very useful. The one drawback is speed. Every time you call a separate program, there is a lot of overhead in starting it up. So the conciencious programmer always tries to use built-in functions over external ones. In particular, ksh programmers should always try use '[[ ]]' over '[ ]', except where [] is neccessary
The more useful functions in ksh I find are:

built-in 'typeset'
'read' and 'set' functions
built-in 'test': [[ ]] 
(But this is apparently NOT a part of POSIX!!)
Built-in fast math routines, via $(( ))
An overview for each of the above is given below
typeset

The man page for it gives full details on typeset; however, I find it useful simply to provide function-local variables.
val=1
func(){
	typeset val=2
	echo val in func is $val
}
func
echo after func, top level val is still $val
Read and Set

 read varname
will set the variable varname to have the value of the next line read in from standard input.
What often comes next, is

 set $varname
This sets the argument variables $1, $2, etc to be set as if the program were called with $varname as the argument string to the shellscript. So, if the value of varname is "first second third", then $1="first", $2="second", and $3="third".
Note that if you want to access "double-digit" arguments, you cannot use "$10". it will get interpreted as "$1,""0". To access argument #10 and higher you must explicitly define the limits of the variable string, with braces:

echo ${10}
This is also good to know, if you wish to follow a variable immediately followed by a string. Compare the output from the following lines:
a="A "
echo $astring
echo ${a}string

The test function

In brief, 'test' can let you check the status of files OR string values.
Here are the most common uses for it.
if [[ $? -ne 0 ]] ; then echo status is bad ; fi
if [[ "$var" != "good" ]] ; then echo var is not good ; fi
if [[ ! -f /file/name ]] ; then echo /file/name is not there ; fi
if [[ ! -d /dir/name ]] ; then echo /dir/name is not a directory ; fi
wildcards in test

Please note that [[]] is a special built-in version of test, that is almost, but not 100%, like the standard [].
One large difference is that filename wildcard expansion will be done with the standard test. In contrast, filename wildcard expansion does not work within [[]]. It does string wildcarding instead, and only on the right side. Additionally, it must not be quoted in any way (unlike case statements)

Examples:

if [[ a* == "astring" ]] ; then echo this does not work ; fi
if [[ "astring" == "a*" ]] ; then echo this does not work ; fi
if [[ "astring" == 'a*' ]] ; then echo this does not work ; fi
if [[ "astring" == a* ]] ; then echo this works ; fi
if [[ "astring" = a* ]] ; then echo this works ; fi
Built-in math

The math evaluator is very useful. Everything inside the double-parens gets evaluated with basic math functions. For example;

four=$((2 + 2))
eight=$(($four + 4))
print $(($four * $eight))
# Inside (()), the $ is optional, so the following works...
# but you might want to keep them for assorted reasons..
print $((four * eight))

# even hex to decimal conversions
print $((0xff))
This next example isnt exactly a math routine, but it is a very handy and fast builtin method to go the other way, from decimal to hex (since 'printf' is a ksh builtin function):

printf "%x\n" 255    #this prints out "ff"
printf "%.4x\n" 255  #or if you need a fixed field width number use this
Warning #1: Some versions of ksh allow you to use floating point with $(()). Most do NOT. Warning #2: ksh is very strict about what goes inside the (( )). Every variable you use must have a value, and it must be a numeric value. Otherwise, ksh will most likely treat that line in your script as a failed function call.
Limitations of built-in functions

Be wary of assumptions. Being "built in" is not always faster than an external progam, if an operation is complex. For example, it is trivial to write a shell-only equivalent of the trivial awk usage, "awk '{print $2}'", to print the second column. However, compare them on a long file:

# function to emulate awk '{print $2}'
sh_awk(){
	while read one two three ; do
	print $two
	done
}

# and now, compare the speed of the two methods

time sh_awk </usr/dict/words >/dev/null
time awk '{print $2}' </usr/dict/words >/dev/null

The awk version will be much much faster. This is because ksh scripts are interpreted, each and every time it executes a line. AWK, however, loads up its programming in one go, and figures out what it is doing ONE TIME. Once that overhead has been put aside, it then can repeat its instructions very fast.

        38.1.6 Redirection and Pipes

There are lots of strange and interesting ways to connect utilities together. Most of these you have probably already seen.
The standard redirect to file;


ls > /tmp/listing

and piping output from one command to another

ls | wc -l

But bourne-shell derivatives give you even more power than that.
Most properly written programs output in one of two ways.

Progress messages go to stdout, error messages go to stderr
Data goes to stdout, error AND progress messsages go to stderr
If you know which of the categories your utilities fall into, you can do interesting things.
Redirection

An uncommon program to use for this example is the "fuser" program under solaris. it gives you a long listing of what processes are using a particular file. For example:


$ fuser /bin/sh
/bin/sh:    13067tm   21262tm

If you wanted to see just the processes using that file, you might initially groan and wonder how best to parse it with awk or something. However, fuser actually splits up the data for you already. It puts the stuff you may not care about on stderr, and the meaty 'data' on stdout. So if you throw away stderr, with the '2>' special redirect, you get

$ fuser /bin/sh  2>/dev/null
    13067   21262

which is then trivially usable.
Unfortunately, not all programs are that straightforward :-) However, it is good to be aware of these things, and also of status returns. The 'grep' command actually returns a status based on whether it found a line. The status of the last command is stored in the '$?' variable. So if all you care about is, "is 'biggles' in /etc/hosts?" you can do the following:


grep biggles /etc/hosts >/dev/null
if [[ $? -eq 0 ]] ; then
	echo YES
else
	echo NO
fi
As usual, there are lots of other ways to accomplish this task, even using the same 'grep' command. However, this method has the advantage that it does not waste OS cycles with a temp file, nor does it waste memory with a potentially very long variable.
(If you were looking for something that could potentially match hundreds of lines, then var=`grep something /file/name` could get very long)
Inline redirection

You have seen redirection TO a file. But you can also redirect input, from a file. For programs that can take data in stdin, this is useful. The 'wc' can take a filename as an argument, or use stdin. So all the following are roughly equivalent in result, although internally, different things happen:
wc -l /etc/hosts
wc -l < /etc/hosts
cat /etc/hosts | wc -l
Additionally, if there are a some fixed lines you want to use, and you do not want to bother making a temporary file, you can pretend part of your script is a separate file!. This is done with the special '<<' redirect operator.

command << EOF
means, "run 'command', but make its stdin come from this file right here, until you see the string 'EOF'"
EOF is the traditional string. But you can actually use any unique string you want. Additionally, you can use variable expansion in this section!

DATE=`date`
HOST=`uname -n`
mailx -s 'long warning' root << EOF
Something went horribly wrong with system $HOST
at $DATE
EOF
if you do NOT want to use variable expansion, then use "EOF" rather than EOF.
Pipes

In case you missed it before, pipes take the output of one command, and put it on the input of another command. You can actually string these together, as seen here;
grep hostspec /etc/hosts| awk '{print $1}' | fgrep '^10.1.' | wc -l

This is a fairly easy way to find what entries in /etc/hosts both match a particular pattern in their name, AND have a particular IP address ranage.
The "disadvantage" to this, is that it is very wasteful. Whenever you use more than one pipe at a time, you should wonder if there is a better way to do it. And indeed for this case, there most certainly IS a better way:


grep '^10\.1\..*hostspec' /etc/hosts | wc -l

There is actually a way to do this with a single awk command. But this is not a lesson on how to use AWK!
Combining pipes and redirection

An interesting example of pipes with stdin/err and redirection is the "tar" command. If you use "tar cvf file.tar dirname", it will create a tar file, and print out all the names of the files in dirname it is putting in the tarfile. It is also possible to take the same 'tar' data, and dump it to stdout. This is useful if you want to compress at the same time you are archiving:
tar cf - dirname | compress > file.tar.Z
But it is important to note that pipes by default only take the data on stdout! So it is possible to get an interactive view of the process, by using
tar cvf - dirname | compress > file.tar.Z
stdout has been redirected to the pipe, but stderr is still being displayed to your terminal, so you will get a file-by-file progress report. Or of course, you could redirect it somewhere else, with
tar cvf - dirname 2>/tmp/tarfile.list | compress > file.tar.Z 
Indirect redirection (Inline files)

Additionally, there is a special type of pipes+redirection. This only works on systems with /dev/fd/X support. You can automatically generate a "fake" file as the result of a command that does not normally generate a file. The name of the fake files will be /dev/fd/{somenumberhere}
Here's an example that doesnt do anything useful

wc -l <(echo one line) <(echo another line)
wc will report that it saw two files, "/dev/fd/4", and "/dev/fd/5", and each "file" had 1 line each. From its own perspective, wc was called simply as
wc -l /dev/fd/4 /dev/fd/5
There are two useful components to this:

You can handle MULTIPLE commands' output at once
It's a quick-n-dirty way to create a pipeline out of a command that "requires" a filename (as long as it only processes its input in a single continuous stream).

        38.1.7 Other Stuff

And here's stuff that I cant fit anywhere else :-)
eval
Backticks
Text positioning/color/curses stuff
Number-based menus
Raw TCP access
Using arrow keys
Graphics and ksh
eval

The eval command is a way to pretend you type something directly. This is a very dangerous command. Think carefully before using it.
One way of using eval, is to use an external command to set variables that you do not know the name of beforehand. Or a GROUP of variables. A common use of this, is to set terminal-size variables on login:

eval `resize`
Backticks

There are ways to put the output of one command as the command line of another one. There are two methods of doing this that are basically equivalent:
echo This is the uptime: `uptime`
echo This is the uptime: $(uptime)
Technically, the second one is the POSIX-preferred one.
In addition to creating dynamic output, this is also very useful for setting variables:

datestring=`date`
echo "The current date is: $datestring"
Text positioning/color games

This is actually a huge topic, and almost deserves its own tutorial. But I'm just going to mention it briefly.
Some people may be familiar with the "curses" library. It is a way to manipulate and move around text on a screen, reguardless of what kind of "terminal" the user is using.

As mentioned, this is a potentially huge topic. So, I'm just going to give you a trivial example, and say "Go read the man-page on tput". Well, okay, actually, you have to read the "tput" manpage, AND either the "terminfo" or "termcap" manpage to figure out what magical 3-5 letter name to use. For example, it should tell you that "cup" is short for the "cursor_address" command. But you must use "cup", NOT "cursor_address", with tput.


tput init
tput clear
tput cup 3 2
print -n "Here is a clean screen, with these words near the top"
endline=`tput cols`
tput cup $(($endline - 2)) 
print "and now, back to you"
sleep 2


The above example clear the screen, prints the given line at a SPECIFIC place on the screen, then puts the cursor back down near the bottom of the screen for you.
PS: If you've been doing a lot of funky things with the screen, you might want to do a

tput reset
as the last thing before your shellscript exits.

Number-based menus

You dont have to build your own "choose a number" function: ksh has one already! But note that it returns the value of the line, not the number of the line.
 select word in one two three exit; do
	echo word is $word
	echo reply is $REPLY
	if [[ "$word" = "exit" ]] ; then
		break;
	fi
 done

This will print out a mini-menu like the following:
1) one
2) two
3) three
4) exit
#?
Note that this will loop between "do ... done" until you trigger a break somehow! (or until the user control-c's or whatever). So dont forget an exit condition!


Raw TCP access

Ksh88 has a built in virtual filesystem that looks like it is under /dev/tcp. You can use it to create connections to specific ports, if you know the IP address.
Here is a trivial example that just opens up a connection to an SMTP server. Note that the connection is half-duplex: You do NOT see data that you send to the other side.

#!/bin/ksh -p

MAILHOST=127.0.0.1
exec 3<>/dev/tcp/$MAILHOST/25 || exit 1

read -r BANNER <&3
echo BANNER is $BANNER
print -u3 HELO myhost.com
read -r REPLY <&3
echo REPLY is $REPLY
The output will look something like the following:

BANNER is 220 yourhost.domain.com ESMTP Sendmail 8.11.6+Sun/8.11.6; Tue, 3 Dec 2002 17:30:01 -0800 (PST)
REPLY is 250 yourhost.domain.com Hello localhost [127.0.0.1], pleased to meet you
Note that we use the "-r" flag to read. In this particular example, it is not neccessary. But in the general case, it will give you the data "raw". Be warned that if the shell cannot open the port, it will kill your entire script, with status 1, automatically

You can also dump the rest of the data waiting on the socket, to whereever you like, by doing

cat <&3 >somefile

Using arrow keys with ksh

For some reason,(older) ksh does not come with arrow key usage on the command line enabled. That being said, it is relatively easy to enable them, since it has some special hooks for the purpose.
You can cut-n-paste this into your .kshrc (if you have set ENV=$HOME/.kshrc), or directly to your prompt. Please note, however, that it presumes you have "emacs mode" enabled, otherwise it may not work properly:

set -o emacs
alias __A=`echo "\020"`     # up arrow == ^p == back a command
alias __B=`echo "\016"`     # dn arrow == ^n == down a command
alias __C=`echo "\006"`     # rt arrow == ^f == forward a character
alias __D=`echo "\002"`     # lft arrow == ^b == back a character
alias __H=`echo "\001"`     # home  == ^a == start of line
Sadly, for ksh93, the process is a bit messier, involving 'keybind' commands:
#ksh93 and later...
# put this in your kshrc
set -o emacs
typeset -A Keytable
trap 'eval "${Keytable[${.sh.edchar}]}"' KEYBD

function keybind # key action
{
	typeset key=$(print -f "%q" "$2")
	case $# in
	2)      Keytable[$1]=' .sh.edchar=${.sh.edmode}'"$key"
	;;
	1)      unset Keytable[$1]
	;;
	*)      print -u2 "Usage: $0 key [action]"
	;;
	esac
}
keybind $'\E[D' $'\002'
keybind $'\E[C' $'\006'
keybind $'\E[B' $'\016'
keybind $'\E[A' $'\020'
keybind $'\t'   $'\E\E'
Graphics and ksh

Not many people are aware of this, but there is actually a graphical version of ksh, called "dtksh". It was created as part of "CDE". Any of the modern UNIX(tm)es should come with it, in /usr/dt/bin/dtksh. If you are interested, take a look at some dtksh demos that someone else has written. And/or, you might see if you have a /usr/dt/share/examples/dtksh/ directory present on your machine.

    38.2 command history

        38.2.1 run last command
fc -s

        38.2.2 run cmd by number

        38.2.3

    38.3 kshrc

        38.3.1 GE linux example


de680136@ctds64-1:/vobs/pet_acq/source> cat /home/de680136/.kshrc                     
de680136@ctds64-1:/home/de680136/temp/tmux> cat /home/de680136/.kshrc
echo "Loading yizaq KSHRC for GE  Linux environment"
# Default to human readable figures
 alias df='df -h'
 alias du='du -h'

# Misc :)
# alias less='less -r'                          # raw control characters
 alias whence='type -a'                        # where, of a sort
 alias grep='grep --color'                     # show differences in colour

# Some shortcuts for different directory listings
 alias ls='ls -hF '
 alias dir='ls '
 alias ll='ls -l'                              # long list
 alias la='ls -A'                              # all but . and ..
 alias l='ls -CF'                              #


# Functions
# #########

# Some example functions
# function settitle() { echo -ne "\e]2;$@\a\e]1;$@\a"; }

## editor for crontab, cvs
EDITOR=vi
export EDITOR

alias ct=/usr/atria/bin/cleartool

HISTSIZE=5000  
export VIM=~

alias h="fc -l 0 5000"
alias vksh="vi ~/.kshrc"
alias hrl="fc -s"
FCEDIT=vi

alias cdpetacq="cd /vobs/pet_acq/source/"
alias cdpetacqT="cd /vobs/pet_acq_test/source/"
alias cdpetraw="cd /vobs/pet_raw/source/"
alias cdpetplat="cd /vobs/pet_platform/source/"
alias sview="ct setview yosi1_pac_col_ddg_4dev"


#screen
function scrn
{
        print "creating new screen session name $1"
         #screen -S $1
}
alias scrls="screen -ls"
alias scrr="screen -r"
alias scrdr="screen -d -r"
export PATH="$HOME/bin:$PATH"
export VIMRUNTIME=/home/de680136/.local/usr/share/vim/vim80/                  
alias vi=vim
alias resume_pack_main="vim /home/de680136/work/vim_sessions/pet_ack_main"
alias cmake="clearmake -C gnu "
alias gremake='clearmake -C gnu -I /vobs/gre/platform/mak'


    38.4 The fc Command

fc is a shell built-in command that provides a superset of the C shell history mechanism. You can use it to examine the most recent commands you entered, to edit one or more commands with your favorite "real" editor, and to run old commands with changes without having to type the entire command in again. We'll look at each of these uses.

The -l option to fc lists previous commands. It takes arguments that refer to commands in the history file. Arguments can be numbers or alphanumeric strings; numbers refer to the commands in the history file, while strings refer to the most recent command beginning with the string. fc treats arguments in a rather complex way:

If you give two arguments, they serve as the first and last commands to be shown.

If you specify one number argument, only the command with that number is shown.

With a single string argument, it searches for the most recent command starting with that string and shows you everything from that command to the most recent command.

If you specify no arguments, you will see the last 16 commands you entered. Thus, fc -l by itself is equivalent to the C shell history command, and indeed the Korn shell defines a built-in alias history as:

alias history=fc -l
As you will find out in Chapter 3 , this means that you can type history and the Korn shell will run the command fc -l .

A few examples should make these options clearer. Let's say you logged in and entered these commands:

ls -l
more myfile
vi myfile
wc -l myfile
pr myfile | lp -h
If you type fc -l (or history ) with no arguments, you will see the above list with command numbers, as in:

1	ls -l
2	more myfile
3	vi myfile
4	wc -l myfile
5	pr myfile | lp -h
The option -n suppresses the line numbers. If you want to see only commands 2 through 4, type fc -l 2 4 . If you want to see only the vi command, type fc -l 3 . To see everything from the vi command up to the present, type fc -l v . Finally, if you want to see commands between more and wc , you can type fc -l m w , fc -l m 4 , fc -l 2 4 , etc.

The -l option to fc is not particularly useful, except as a quick way of remembering what commands you typed recently. Use the history alias if you are an experienced C shell user.

The other important option to fc is -e for "edit." This is useful as an "escape hatch" from vi- and emacs-modes if you aren't used to either of those editors. You can specify the pathname of your favorite editor and edit commands from your history file; then when you have made the changes, the shell will actually execute the new lines.

Let's say your favorite editor is a little home-brew gem called zed . You could edit your commands by typing:

$ 
fc -e /usr/local/bin/zed
This seems like a lot of work just to fix a typo in your previous command; fortunately, there is a better way. You can set the environment variable FCEDIT to the pathname of the editor you want fc to use. If you put a line in your .profile or environment file saying:

FCEDIT=/usr/local/bin/zed
you will get zed when you invoke fc . FCEDIT defaults to the old line editor ed , so that the overall default is also ed .

fc is usually used to fix a recent command. Therefore it handles arguments a bit differently than it does for the fc -l variation above:

With no arguments, fc loads the editor with the most recent command.

With a numeric argument, fc loads the editor with the command with that number.

With a string argument, fc loads the most recent command starting with that string.

With two arguments to fc , the arguments specify the beginning and end of a range of commands, as above.

Remember that fc actually runs the command(s) after you edit them. Therefore the last-named choice can be dangerous. The Korn shell will attempt to execute all commands in the range you specify when you exit your editor. If you have typed in any multiline constructs (like those we will cover in Chapter 5, Flow Control ) the results could be even more dangerous. Although these might seem like valid ways of generating "instant shell programs," a far better strategy would be to direct the output of fc -l with the same arguments to a file; then edit that file and execute the commands when you're satisfied with them:

$ 
fc -l cp > lastcommands

$ 
vi lastcommands

$ 
. lastcommands
In this case, the shell will not try to execute the file when you leave the editor!

There is one final use for fc . If you specify the editor - (i.e., type fc -e - ), the Korn shell will skip the editing part and just run the command(s) specified by the argument(s). Why is this useful? For one thing, just typing fc -e - causes the previous command to repeat, just like the C shell !! command. The Korn shell provides the built-in alias r for this, so that if you type r and hit RETURN , you will repeat the last command.

This form of fc allows yet another type of argument, of the form old = new , meaning "change occurrences of old in the specified previous command to new and then run it." For example, if you wanted to run a complex command like the following on two sets of files:

$ 
tbl ch2.tbl | nroff -mS -Tepson > ch2.out
you can enter the command and then type fc -e - 2=3 . (You could also use the alias, r 2=3 .) This command would then run:

39. Docker

    39.1  Introduction
Docker overview
Estimated reading time: 10 minutes
Docker is an open platform for developing, shipping, and running applications. Docker enables you to separate your applications from your infrastructure so you can deliver software quickly. With Docker, you can manage your infrastructure in the same ways you manage your applications. By taking advantage of Docker’s methodologies for shipping, testing, and deploying code quickly, you can significantly reduce the delay between writing code and running it in production.

The Docker platform
Docker provides the ability to package and run an application in a loosely isolated environment called a container. The isolation and security allow you to run many containers simultaneously on a given host. Containers are lightweight because they don’t need the extra load of a hypervisor, but run directly within the host machine’s kernel. This means you can run more containers on a given hardware combination than if you were using virtual machines. You can even run Docker containers within host machines that are actually virtual machines!

Docker provides tooling and a platform to manage the lifecycle of your containers:

Develop your application and its supporting components using containers.
The container becomes the unit for distributing and testing your application.
When you’re ready, deploy your application into your production environment, as a container or an orchestrated service. This works the same whether your production environment is a local data center, a cloud provider, or a hybrid of the two.
Docker Engine
Docker Engine is a client-server application with these major components:

A server which is a type of long-running program called a daemon process (the dockerd command).

A REST API which specifies interfaces that programs can use to talk to the daemon and instruct it what to do.

A command line interface (CLI) client (the docker command).

Docker Engine Components Flow

The CLI uses the Docker REST API to control or interact with the Docker daemon through scripting or direct CLI commands. Many other Docker applications use the underlying API and CLI.

The daemon creates and manages Docker objects, such as images, containers, networks, and volumes.

Note: Docker is licensed under the open source Apache 2.0 license.
For more details, see Docker Architecture below.

What can I use Docker for?
Fast, consistent delivery of your applications

Docker streamlines the development lifecycle by allowing developers to work in standardized environments using local containers which provide your applications and services. Containers are great for continuous integration and continuous delivery (CI/CD) workflows.

Consider the following example scenario:

Your developers write code locally and share their work with their colleagues using Docker containers.
They use Docker to push their applications into a test environment and execute automated and manual tests.
When developers find bugs, they can fix them in the development environment and redeploy them to the test environment for testing and validation.
When testing is complete, getting the fix to the customer is as simple as pushing the updated image to the production environment.
Responsive deployment and scaling

Docker’s container-based platform allows for highly portable workloads. Docker containers can run on a developer’s local laptop, on physical or virtual machines in a data center, on cloud providers, or in a mixture of environments.

Docker’s portability and lightweight nature also make it easy to dynamically manage workloads, scaling up or tearing down applications and services as business needs dictate, in near real time.

Running more workloads on the same hardware

Docker is lightweight and fast. It provides a viable, cost-effective alternative to hypervisor-based virtual machines, so you can use more of your compute capacity to achieve your business goals. Docker is perfect for high density environments and for small and medium deployments where you need to do more with fewer resources.

Docker architecture
Docker uses a client-server architecture. The Docker client talks to the Docker daemon, which does the heavy lifting of building, running, and distributing your Docker containers. The Docker client and daemon can run on the same system, or you can connect a Docker client to a remote Docker daemon. The Docker client and daemon communicate using a REST API, over UNIX sockets or a network interface.

Docker Architecture Diagram

The Docker daemon
The Docker daemon (dockerd) listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes. A daemon can also communicate with other daemons to manage Docker services.

The Docker client
The Docker client (docker) is the primary way that many Docker users interact with Docker. When you use commands such as docker run, the client sends these commands to dockerd, which carries them out. The docker command uses the Docker API. The Docker client can communicate with more than one daemon.

Docker registries
A Docker registry stores Docker images. Docker Hub is a public registry that anyone can use, and Docker is configured to look for images on Docker Hub by default. You can even run your own private registry. If you use Docker Datacenter (DDC), it includes Docker Trusted Registry (DTR).

When you use the docker pull or docker run commands, the required images are pulled from your configured registry. When you use the docker push command, your image is pushed to your configured registry.

Docker objects
When you use Docker, you are creating and using images, containers, networks, volumes, plugins, and other objects. This section is a brief overview of some of those objects.

IMAGES

An image is a read-only template with instructions for creating a Docker container. Often, an image is based on another image, with some additional customization. For example, you may build an image which is based on the ubuntu image, but installs the Apache web server and your application, as well as the configuration details needed to make your application run.

You might create your own images or you might only use those created by others and published in a registry. To build your own image, you create a Dockerfile with a simple syntax for defining the steps needed to create the image and run it. Each instruction in a Dockerfile creates a layer in the image. When you change the Dockerfile and rebuild the image, only those layers which have changed are rebuilt. This is part of what makes images so lightweight, small, and fast, when compared to other virtualization technologies.

CONTAINERS

A container is a runnable instance of an image. You can create, start, stop, move, or delete a container using the Docker API or CLI. You can connect a container to one or more networks, attach storage to it, or even create a new image based on its current state.

By default, a container is relatively well isolated from other containers and its host machine. You can control how isolated a container’s network, storage, or other underlying subsystems are from other containers or from the host machine.

A container is defined by its image as well as any configuration options you provide to it when you create or start it. When a container is removed, any changes to its state that are not stored in persistent storage disappear.

Example docker run command

The following command runs an ubuntu container, attaches interactively to your local command-line session, and runs /bin/bash.

$ docker run -i -t ubuntu /bin/bash
When you run this command, the following happens (assuming you are using the default registry configuration):

If you do not have the ubuntu image locally, Docker pulls it from your configured registry, as though you had run docker pull ubuntu manually.

Docker creates a new container, as though you had run a docker container create command manually.

Docker allocates a read-write filesystem to the container, as its final layer. This allows a running container to create or modify files and directories in its local filesystem.

Docker creates a network interface to connect the container to the default network, since you did not specify any networking options. This includes assigning an IP address to the container. By default, containers can connect to external networks using the host machine’s network connection.

Docker starts the container and executes /bin/bash. Because the container is running interactively and attached to your terminal (due to the -i and -t flags), you can provide input using your keyboard while the output is logged to your terminal.

When you type exit to terminate the /bin/bash command, the container stops but is not removed. You can start it again or remove it.

SERVICES

Services allow you to scale containers across multiple Docker daemons, which all work together as a swarm with multiple managers and workers. Each member of a swarm is a Docker daemon, and the daemons all communicate using the Docker API. A service allows you to define the desired state, such as the number of replicas of the service that must be available at any given time. By default, the service is load-balanced across all worker nodes. To the consumer, the Docker service appears to be a single application. Docker Engine supports swarm mode in Docker 1.12 and higher.

The underlying technology
Docker is written in Go and takes advantage of several features of the Linux kernel to deliver its functionality.

Namespaces
Docker uses a technology called namespaces to provide the isolated workspace called the container. When you run a container, Docker creates a set of namespaces for that container.

These namespaces provide a layer of isolation. Each aspect of a container runs in a separate namespace and its access is limited to that namespace.

Docker Engine uses namespaces such as the following on Linux:

The pid namespace: Process isolation (PID: Process ID).
The net namespace: Managing network interfaces (NET: Networking).
The ipc namespace: Managing access to IPC resources (IPC: InterProcess Communication).
The mnt namespace: Managing filesystem mount points (MNT: Mount).
The uts namespace: Isolating kernel and version identifiers. (UTS: Unix Timesharing System).
Control groups
Docker Engine on Linux also relies on another technology called control groups (cgroups). A cgroup limits an application to a specific set of resources. Control groups allow Docker Engine to share available hardware resources to containers and optionally enforce limits and constraints. For example, you can limit the memory available to a specific container.

Union file systems
Union file systems, or UnionFS, are file systems that operate by creating layers, making them very lightweight and fast. Docker Engine uses UnionFS to provide the building blocks for containers. Docker Engine can use multiple UnionFS variants, including AUFS, btrfs, vfs, and DeviceMapper.

Container format
Docker Engine combines the namespaces, control groups, and UnionFS into a wrapper called a container format. The default container format is libcontainer. In the future, Docker may support other container formats by integrating with technologies such as BSD Jails or Solaris Zones.

        39.1.1  First steps


install (for mac there's a dmg, you'll need to create an account)
login and give permission

DL and run a hello-world docker:
docker run hello-world 

list local dockers:
docker ps -a
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                     PORTS               NAMES
a64e38b59914        ubuntu              "bash"              7 minutes ago       Exited (0) 7 minutes ago                       cranky_swartz
d9152c5c2208        hello-world         "/hello"            8 minutes ago       Exited (0) 8 minutes ago                       hungry_banach

search containers:
https://hub.docker.com/


list images
[i500695@C02X632CJGH6:2019-03-19 15:12:09:~:]561$ docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
ubuntu              latest              94e814e2efa8        7 days ago          88.9MB
hello-world         latest              fce289e99eb9        2 months ago        1.84kB

remove a container
docker rm container-id

remove all containers
docker rm $(docker ps -a -f status=exited -q)

create an ubunto docker, enter interactive shell, set its name, tie a volume to a local dir and set to remove container after exit:
docker run -it --name myUbuntu --rm -v ~/temp/docker:localtmp ubuntu bash

        39.1.2 Create a container
a. create a Dockerfile
[i500695@C02X632CJGH6:2019-03-19 16:19:02:~/work/code/docker/helloWorld:]576$ cat !$
cat Dockerfile
FROM ubuntu
CMD echo "hello yosi from your first docker container"

b. build container
[i500695@C02X632CJGH6:2019-03-19 16:20:49:~/work/code/docker/helloWorld:]578$ docker build -t my_first_container .
Sending build context to Docker daemon  2.048kB
Step 1/2 : FROM ubuntu
 ---> 94e814e2efa8
Step 2/2 : CMD echo "hello yosi from your first docker container"
 ---> Running in 29ddc1044d59
Removing intermediate container 29ddc1044d59
 ---> d6d436542d41
Successfully built d6d436542d41
Successfully tagged my_first_container:latest

run it:
[i500695@C02X632CJGH6:2019-03-19 16:22:00:~/work/code/docker/helloWorld:]580$ docker run my_first_container
hello yosi from your first docker container

c.
        39.1.3 cleanup 

            39.1.3.1 How To Remove Docker Containers, Images, Volumes, and Networks
https://linuxize.com/post/how-to-remove-docker-images-containers-volumes-and-networks/

Docker allows you to quickly build, test, and deploy applications as portable, self-sufficient containers that can run virtually anywhere.

Docker doesn’t remove unused objects such as containers, images, volumes, and networks unless you explicitly tell it to do so. As you work with Docker, you can easily accumulate a large number of unused objects that consume significant disk space and clutter the output produced by the Docker commands.

This guide serves as a “cheat sheet” to help Docker users keep their system organized, and to free disk space by removing unused Docker containers, images, volumes, and networks.

                39.1.3.1.1 Removing All Unused Objects

The docker system prune command will remove all stopped containers, all dangling images, and all unused networks:

docker system prune
You’ll be prompted to continue, use the -f or --force flag to bypass the prompt.


WARNING! This will remove:
        - all stopped containers
        - all networks not used by at least one container
        - all dangling images
        - all build cache
Are you sure you want to continue? [y/N]
If you also want to remove all unused volumes, pass the --volumes flag:

docker system prune --volumes
WARNING! This will remove:
        - all stopped containers
        - all networks not used by at least one container
        - all volumes not used by at least one container
        - all dangling images
        - all build cache
Are you sure you want to continue? [y/N] y

                39.1.3.1.2 Removing Docker Containers

Docker containers are not automatically removed when you stop them unless you start the container using the --rm flag.


Remove one or more containers

To remove one or more Docker images use the docker container rm command followed by the ID of the containers you want to remove.

You can get a list of all containers by passing the -a flag to the docker container ls command:

docker container ls -a
The output should look something like this:

CONTAINER ID        IMAGE                   COMMAND                  CREATED             STATUS                      PORTS               NAMES
cc3f2ff51cab        centos                  "/bin/bash"              2 months ago        Created                                         competent_nightingale
cd20b396a061        solita/ubuntu-systemd   "/bin/bash -c 'exec …"   2 months ago        Exited (137) 2 months ago                       systemd
fb62432cf3c1        ubuntu                  "/bin/bash"              3 months ago        Exited (130) 3 months ago                       jolly_mirzakhani
Once you know the CONTAINER ID of the containers you want to delete, pass it to the docker container rm command. For example, to remove the first two containers listed in the output above run:

docker container rm cc3f2ff51cab cd20b396a061
If you get an error similar to the following, it means that the container is running. You’ll need to stop the container before removing it.


Error response from daemon: You cannot remove a running container fc983ebf4771d42a8bd0029df061cb74dc12cb174530b2036987575b83442b47. Stop the container before attempting removal or force remove.
Remove all stopped containers

Before performing the removal command, you can get a list of all non-running (stopped) containers that will be removed using the following command:

docker container ls -a --filter status=exited --filter status=created 
To remove all stopped containers use the docker container prune command:

docker container prune
You’ll be prompted to continue, use the -f or --force flag to bypass the prompt.

WARNING! This will remove all stopped containers.
Are you sure you want to continue? [y/N] y
Remove containers using filters

The docker container prune command allows you to remove containers based on condition using the filtering flag --filter.


At the time of the writing of this article, the currently supported filters are until and label. You can use more than one filter by using multiple --filter flags.

For example, to remove all images that are created more than 12 hours ago, run:

docker container prune --filter "until=12h"
Stop and remove all containers

You can get a list of all Docker containers on your system using the docker container ls -aq command.

To stop all running containers use the docker container stop command followed by a list of all containers IDs.

docker container stop $(docker container ls -aq)
Once all containers are stopped, you can remove them using the docker container rm command followed by the containers ID list.


docker container rm $(docker container ls -aq)

                39.1.3.1.3 Removing Docker Images

Remove one or more images

To remove one or more Docker images use the docker images ls command to find the ID of the images you want to remove.


docker image ls
The output should look something like this:

REPOSITORY              TAG                 IMAGE ID            CREATED             SIZE
centos                  latest              75835a67d134        7 days ago          200MB
ubuntu                  latest              2a4cca5ac898        2 months ago        111MB
linuxize/fedora         latest              a45d6dca3361        3 months ago        311MB
java                    8-jre               e44d62cf8862        3 months ago        311MB
Once you’ve located the images you want to remove, pass their IMAGE ID to the docker image rm command. For example, to remove the first two images listed in the output above run:

docker image rm 75835a67d134 2a4cca5ac898
If you get an error similar to the one shown below, it means that an existing container uses the image. To remove the image, you will have to remove the container first.

Error response from daemon: conflict: unable to remove repository reference "centos" (must force) - container cd20b396a061 is using its referenced image 75835a67d134
Copy
Remove dangling images

Docker provides a docker image prune command that can be used to remove dangled and unused images.


A dangling image is an image that is not tagged and is not used by any container. To remove dangling images type:

docker image prune
You’ll be prompted to continue, use the -f or --force flag to bypass the prompt.

WARNING! This will remove all dangling images.
Are you sure you want to continue? [y/N] y
When removing dangling images, if the images build by you are not tagged, they will be removed too.
Remove all unused images

To remove all images which are not referenced by any existing container, not just the dangling ones, use the prune command with the -a flag:

docker image prune -a
WARNING! This will remove all images without at least one container associated to them.
Are you sure you want to continue? [y/N] y
Remove images using filters

With the docker image prune command, you can also remove images based on a certain condition using the filtering flag --filter.

At the time of the writing of this article, the currently supported filters are until and label. You can use more than one filter by using multiple --filter flags.


For example, to remove all images that are created more than 12 hours ago, you would run:

docker image prune -a --filter "until=12h"

                39.1.3.1.4 Removing Docker Volumes

Remove one or more volumes

To remove one or more Docker volumes use the docker volume ls command to find the ID of the volumes you want to remove.

docker volume ls
The output should look something like this:

DRIVER              VOLUME NAME
local               4e12af8913af888ba67243dec78419bf18adddc3c7a4b2345754b6db64293163
local               terano
Once you’ve found the VOLUME NAME of the volumes you want to remove, pass them to the docker volume rm command. For example, to remove the first volume listed in the output above, run:

docker volume rm 4e12af8913af888ba67243dec78419bf18adddc3c7a4b2345754b6db64293163
If you get an error similar to the one shown below, it means that an existing container uses the volume. To remove the volume, you will have to remove the container first.

Error response from daemon: remove 4e12af8913af888ba67243dec78419bf18adddc3c7a4b2345754b6db64293163: volume is in use - [c7188935a38a6c3f9f11297f8c98ce9996ef5ddad6e6187be62bad3001a66c8e]
Copy
Remove all unused volumes

To remove all unused volumes use the docker image prune command:

docker volume prune
You’ll be prompted to continue, use the -f or --force flag to bypass the prompt.

WARNING! This will remove all local volumes not used by at least one container.
Are you sure you want to continue? [y/N]

                39.1.3.1.5 Removing Docker Networks

Remove one or more networks

To remove one or more Docker networks use the docker network ls command to find the ID of the networks you want to remove.

docker network ls
The output should look something like this:

NETWORK ID          NAME                DRIVER              SCOPE
107b8ac977e3        bridge              bridge              local
ab998267377d        host                host                local
c520032c3d31        my-bridge-network   bridge              local
9bc81b63f740        none                null                local
Once you’ve located the networks you want to remove, pass their NETWORK ID to the docker network rm command. For example to remove the network with the name my-bridge-network run:

docker network rm c520032c3d31
If you get an error similar to the one shown below, it means that an existing container uses the network. To remove the network you will have to remove the container first.

Error response from daemon: network my-bridge-network id 6f5293268bb91ad2498b38b0bca970083af87237784017be24ea208d2233c5aa has active endpoints
Copy
Remove all unused network

Use the docker network prune command to remove all unused networks.

docker network prune
You’ll be prompted to continue, use the -f or --force flag to bypass the prompt.

WARNING! This will remove all networks not used by at least one container.
Are you sure you want to continue? [y/N] 
Remove networks using filters

With the docker network prune command you can remove networks based on condition using the filtering flag --filter.

At the time of the writing of this article, the currently supported filters are until and label. You can use more than one filter by using multiple --filter flags.

For example, to remove all networks that are created more than 12 hours ago, run:

docker network prune -a --filter "until=12h"

                39.1.3.1.6 Conclusion

In this guide, we have shown you some of the common commands for removing Docker containers, images, volumes, and networks.

You should also check out the official Docker documentation.

If you have any questions, please leave a comment below.

                39.1.3.1.7
            39.1.3.2
        39.1.4
    39.2 Cheatsheet
/Users/i500695/work/code/cheatsheets/awesome-cheatsheets/tools/docker.sh

[i500695@C02X632CJGH6:2019-03-19 15:22:57:~:]565$ docker run -it --name myUbuntu --rm -v ~/temp/docker:/localtmp ubuntu bash
root@8c3d6e5fe918:/# ls localtmp/

    39.3 docker-compose

        39.3.1  intro 


            39.3.1.1  https://docs.docker.com/compose/
Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application’s services. Then, with a single command, you create and start all the services from your configuration. To learn more about all the features of Compose, see the list of features.

Compose works in all environments: production, staging, development, testing, as well as CI workflows. You can learn more about each case in Common Use Cases.

Using Compose is basically a three-step process:

Define your app’s environment with a Dockerfile so it can be reproduced anywhere.

Define the services that make up your app in docker-compose.yml so they can be run together in an isolated environment.

Run docker-compose up and Compose starts and runs your entire app.

A docker-compose.yml looks like this:

version: '3'
services:
  web:
    build: .
    ports:
    - "5000:5000"
    volumes:
    - .:/code
    - logvolume01:/var/log
    links:
    - redis
  redis:
    image: redis
volumes:
  logvolume01: {}

            39.3.1.2 https://hackernoon.com/practical-introduction-to-docker-compose-d34e79c4c2b6

A Practical Introduction to Docker Compose

TL;DR
Docker containers opened a world of possibilities for the tech community, hassles in setting up new software were decreased unlike old times when a mess was to be sorted by a grievous format, it reduced the time to set up and use new software which eventually played a big part for techies to learn new things, roll it out in a container and scrap it when done. Things became easy, and the best thing its open source anyone and everyone can use it, comes with a little learning curve though.

Out of the myriad possibilities was the possibility of implementing complex technology stacks for our applications, which previously would have been the domain of experts. Today with the help of containers software engineers with sound understanding of the underlying systems can implement a complex stack and why not it’s the need of the hour, the figure of speech “Jack of all trades” got a fancy upgrade; “Master of some” based on the needs of the age. Simply put “T” shaped skills.

The possibility of defining a complex stack in a file and running it with a single command, pretty tempting huh. The guys at Docker Inc. choose to call it Docker compose.


In this article, we will use Docker’s example Voting App and deploy it using Docker compose.

Docker Compose
In the words of Docker Inc.

Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application’s services. Then, with a single command, you create and start all the services from your configuration.
The Voting App
Introducing the most favourite demonstration app for the Docker community “The Voting App”, as if it needs an introduction at all. This is a simple application based on micro-services architecture, consisting of 5 simple services.


Voting app architecture [https://github.com/docker/example-voting-app]
Voting-App: Frontend of the application written in Python, used by users to cast their votes.
Redis: In-memory database, used as intermediate storage.
Worker: .Net service, used to fetch votes from Redis and store in Postres database.
DB: PostgreSQL database, used as database.
Result-App: Frontend of the application written in Node.js, displays the voting results.
The Voting repo has a file called docker-compose.yml this file contains the configuration for creating the containers, exposing ports, binding volumes and connecting containers through networks required for the voting app to work. Sounds like a lot of pretty long docker run and docker network create commands otherwise, docker compose allows us to put all of that stuff in a single docker-compose file in yaml format.



Git clone and cd into the voting app repo.
git clone https://github.com/dockersamples/example-voting-app.git

dockersamples/example-voting-app
example-voting-app - Example Docker Compose app
github.com
Compose Time
With all of our application defined in a single compose file we can take a sigh of relief, chill and simply run the application. The beauty of compose lies in the fact that a single command creates all the services, wires up the networks(literally), mounts all volumes and exposes the ports. Its time to welcome the up command, its performs all of the aforementioned tasks.

$ docker-compose up 
After lots of “Pull complete”, hundreds of megabytes and few minutes (maybe more). . .

Voila, we have the voting app up and running.

Command docker ps lists all the running containers

$ docker ps -a --format="table {{.Names}}\t{{.Image}}\t{{.Ports}}" 
NAMES               IMAGE               PORTS
voting_worker_1     voting_worker      
db                  postgres:9.4        5432/tcp
voting_vote_1       voting_vote         0.0.0.0:5000->80/tcp
voting_result_1     voting_result       0.0.0.0:5858->5858/tcp, 0.0.0.0:5001->80/tcp
redis               redis:alpine        0.0.0.0:32768->6379/tcp
The above command displays all the running containers, respective images and the exposed port numbers.

The Voting app can be accessed on http://localhost:5000


Likewise the Voting results app can be accessed on http://localhost:5001


Each vote cast on the Voting app is first stored in the Redis in-memory database, the .Net worker service fetches the vote and stores it in the Postgres DB which is accessed by the Node.js frontend.

Compose Features
Compose provide the flexibility to use a project name to isolate the environments from each other, the project name is the base name of the directory that contains the project. In our voting app this is signified by the name of the containers voting_worker_1 where voting is the base name of the directory. We can set a custom project name using the -p flag followed by the custom name.

Compose preserves all volumes used by the services defined in the compose file, thus no data is lost when the containers are recreated using docker-compose up. Another cool feature is that only the containers which have changed are recreated, the containers whose state did not change remain untouched.

Another cool feature is the support for variables in the compose file, we can define variables in a .env file and use them in the docker-compose file. Here the POSTGRES_VERSION=9.4 can defined in the environment file or can be defined in the shell. It is used in the compose file in the following manner:

db:
  image: "postgres:${POSTGRES_VERSION}"
Command Cheat Sheet
Its easy as breeze to start, stop and play around with compose.

$ docker-compose up -d
$ docker-compose down
$ docker-compose start
$ docker-compose stop
$ docker-compose build
$ docker-compose logs -f db
$ docker-compose scale db=4
$ docker-compose events
$ docker-compose exec db bash
Summary
Docker Compose is a great tool to quickly deploy and scrap containers, the compose file can run seamlessly on any machine installed with docker-compose. Experimentation and learning technologies is just a Compose file away ;).

I hope this article helped in the understanding of Docker Compose. I’d love to hear about how you use Docker Compose in your projects. Clap if it increased your knowledge, help it reach more people.

        39.3.2
    39.4
40. cgroups

41. find, find command, 

    41.1 find cheatsheet
|       The Find command cheat sheet
|       
|       The Find Command Cheat Sheet
|       
|       1. Basic find command
|       # find -name "TestFile"
|       
|       2. Find Files Using Name and Ignoring Case
|       # find -iname "TestFile"
|       
|       3. Limit Search To Specific Directory Level Using mindepth and maxdepth
|       # find / -maxdepth 3 -name passwd
|       -maxdepth --> will go 3 directories below -- / 1st; /etc 2nd; /usr/bin 3rd
|       
|       # find / -mindepth 3 -maxdepth 5 -name passwd
|       will go 3 depths first and upto 5 -- so will not disply under /; /usr; /usr/bin
|       
|       4. Executing Commands on the Files Found by the Find Command.
|       user -exec {} /;
|       # find -iname "TestFile" -exec md5sum {} \;
|       
|       5. Inverting the match.
|       To inver the match use the "-not" switch
|       # find / -not -iname "TestFile"
|       
|       6. List inodes of the files
|       # ls -i1 test*
|       16187429 test-file-name
|       16187430 test-file-name
|       
|       # find -inum 16187430 -exec mv {} new-test-file-name \;
|       # ls -i1 *test*
|       16187430 new-test-file-name
|       16187429 test-file-name
|       
|       7. Find file based on the File-Permissions
|       You can :
|           * Find files that match exact permission
|           * Check whether the given permission matches, irrespective of other permission bits
|           * Search by giving octal / symbolic representation
|       
|       # find . -perm -g=r -type f -exec ls -l {} \;
|       Will display all files with group permission read. Not files with readonly group permission
|       
|       # find . -perm g=r -type f -exec ls -l {} \;
|       Will dispay files with 040 permission. i.e files with group read only permisison
|       
|       # find . -perm 040 -type f -exec ls -l {} \;
|       Will dispay files with 040 permission. i.e files with group read only permisison
|       
|       
|       8. Find all empty files (zero byte file) in your home directory and its subdirectory
|       # find ~ -empty
|       
|       Need to check diff
|       # find . -empty -not -name "test" -- name not equal to test
|       # find . -not -empty -name ".*"   -- no empty files
|       
|       9. Finding the Top 5 Big Files
|       # find /var -type f -exec ls -s {} \; | sort -n -r | head -5
|       
|       # find /var -type f -size +10M -exec ls -lh {} \;  
|       
|       10. Finding the Top 5 Small Files
|       # find /var -type f -exec ls -s {} \; | sort -n | head -5
|       This will most probably list zero byte files or empty files!!!  
|       
|       So to list the smaller files other than the ZERO byte files.
|       # find /var -not -empty -type f -exec ls -s {} \; | sort -n | head -5
|       
|       11. Find Files by Size
|       use the -size switch -- + for greater than; - for lesser than
|       Find files bigger than the given size
|       # find / -size +100M
|       
|       Find files smaller than the given size
|       # find / -size -100M
|       
|       Find files that matches the exact given size
|       # find / -size 100M
|       
|       Note: – means less than the give size, + means more than the given size, and no symbol means exact given size.
|       
|       12. Find files based on file types
|       use -type switch
|       
|       Find only the socket files.
|       # find . -type s
|       
|       Find all directories
|       # find . -type d
|       
|       Find only the normal files
|       # find . -type f
|       
|       Find all the hidden files
|       # find . -type f -name ".*"
|       Find all the hidden directories
|       # find -type d -name ".*"
|       
|       13. Creaing aliases:
|       use the alias command
|       # alias lslarge="find /var -type f -size +10M -exec ls -lh {} \;"
|       # lslarge -- will display greater than 10MB files in /var
|       
|       14. Find Files Based on Access / Modification / Change Time
|       
|       You can find files based on following three file time attribute.
|          1. Modification time of the file. Modification time gets updated when the file content modified. 
|          2. Access time of the file. Access time gets updated when the file accessed.
|          3. Change time of the file. Change time gets updated when the inode data changes.
|       
|       # min argument treats its argument as minutes. For example, min 60 = 60 minutes (1 hour).
|       # time argument treats its argument as 24 hours. For example, time 2 = 2*24 hours (2 days)
|       
|           * -mmin n File’s data was last modified n minutes ago.
|           * -mtime n File’s data was last modified n*24 hours ago.
|       
|           * -amin n File was last accessed n minutes ago
|           * -atime n File was last accessed n*24 hours ago
|       
|           * -cmin n File’s status was last changed n minutes ago. inode change
|           * -ctime n File’s status was last changed n*24 hours ago.
|       
|       14. Find files whose content got updated/Modified within last 1 hour/1Day
|       # find . -mmin -60  -- 
|       # find / -mtime -1 -- finds all the files (under root file system /) that got updated within the last 24 hours (1 day)
|       
|       15. Find files which got accessed before 1 hour/1Day
|       # find -amin -60  -- find files in the current directory and sub-directories, which got accessed within last 1 hour (60 minutes)
|       # find / -atime -1 --
|       
|       16. Find files which got changed exactly before 1 hour/1day :: inode change
|       # find . -cmin -60 --
|       # find / -ctime -1 -- finds all the files (under root file system /) that got changed within the last 24 hours (1 day).
|       
|       17. Restricting the search only to unhidden files.
|       # find . -mmin -15 \( ! -regex ".*/\..*" \)
|       Refer the regex statement
|       
|       18. Find files modified/accessed/changed after modifying a refrence file
|       Syntax: # find -newer FILE
|       
|       # find -newer /etc/passwd   -- Modified after 
|       # find -anewer /etc/hosts     -- Accessed after 
|       # find -cnewer /etc/fstab      -- Changed after
|       
|       19. Searching Only in the Current Filesystem
|       use -xdev switch -- Don’t descend directories on other filesystems.
|       
|       # find / -xdev -name "*.log"
|       Will search only /, not other mount points mounted under /.
|       
|       # find / -name "*.log"
|       This will search all FS starting from /.
|       
|       20. Using more than one { } in same command
|       # find -name "*.txt" cp {} {}.bkup \;
|       
|       21. Redirecting errors to /dev/null
|       # find -name "*.txt" 2>>/dev/null
|       
|       22. Substitute space with underscore in the file name.
|       # find . -type f -iname “*.mp3″ -exec mv “s/ /_/g” {} \;

    41.2 My useful find commands

        41.2.1 find recent files - newer than 1 hour / 1 day
find . -mmin -60 -type f
find . -mtime -1 -type f

        41.2.2 exclude directory
        ignode node_modules dir:
find . -path ./node_modules -prune -o  -type d

        41.2.3
    41.3
42.  rm, delete files

    42.1  rm -fr
-f force delete
-r recursive

    42.2 Delete All Files in a Directory Except One or Few Files with Extensions

        42.2.1 background


https://www.tecmint.com/delete-all-files-in-directory-except-one-few-file-extensions/
first, wildcards
* – matches zero or more characters
? – matches any single character
[seq] – matches any character in seq
[!seq] – matches any character not in seq

extended pattern matching operators are listed below, where pattern-list is a list containing one or more filenames, separated using the | character:

*(pattern-list) – matches zero or more occurrences of the specified patterns
?(pattern-list) – matches zero or one occurrence of the specified patterns
+(pattern-list) – matches one or more occurrences of the specified patterns
@(pattern-list) – matches one of the specified patterns
!(pattern-list) – matches anything except one of the given patterns

To use:
shopt -s extglob

add to .profile/.bashrc etc

        42.2.2 To delete all files in a directory except filename, type the command below:
$ rm -v !("filename")

        42.2.3 To delete all files with the exception of filename1 and filename2:
$ rm -v !("filename1"|"filename2") 

        42.2.4 The example below shows how to remove all files other than all .zip files interactively:
$ rm -i !(*.zip)

        42.2.5 delete all files in a directory apart from all .zip and .odt files as follows, while displaying what is being done:
$ rm -v !(*.zip|*.odt)

        42.2.6 Delete Files Using Linux find Command
Under this method, we can use find command exclusively with appropriate options or in conjunction with xargs command by employing a pipeline as in the forms below:

Under this method, we can use find command exclusively with appropriate options or in conjunction with xargs command by employing a pipeline as in the forms below:

$ find /directory/ -type f -not -name 'PATTERN' -delete
$ find /directory/ -type f -not -name 'PATTERN' -print0 | xargs -0 -I {} rm {}
$ find /directory/ -type f -not -name 'PATTERN' -print0 | xargs -0 -I {} rm [options] {}

            42.2.6.1 The following command will delete all files apart from .gz files in the current directory:
$ find . -type f -not -name '*.gz'-delete

            42.2.6.2 Using a pipeline and xargs, you can modify the case above as follows:
$ find . -type f -not -name '*gz' -print0 | xargs -0  -I {} rm -v {}

            42.2.6.3 wipe out all files excluding .gz, .odt, and .jpg files in the current directory:
$ find . -type f -not \(-name '*gz' -or -name '*odt' -or -name '*.jpg' \) -delete

            42.2.6.4
        42.2.7 Delete Files Using Bash GLOBIGNORE Variable
To employ this method, move into the directory that you wish to clean up, then set the GLOBIGNORE variable as follows:

$ cd test
$ GLOBIGNORE=*.odt:*.iso:*.txt
In this instance, all files other than .odt, .iso, and .txt files with be removed from the current directory.

Now run the command to clean up the directory:

$ rm -v *
Afterwards, turn off GLOBIGNORE variable:

$ unset GLOBIGNORE

        42.2.8
    42.3

43. ag silver searcher

    43.1  basics


    43.2 advanced

        43.2.1  https://www.alexwheeler.io/command-line/2018/08/02/ag.html

- search sub dir 
  [I500695@C02ZR8BSMD6N:2021-11-28 18:20:13:/Users/i500695/git/portal-cf-transport-service:]2003$ ag job src/transport/
src/transport/dto/creators-params.dto.ts
1:import {JobInfoDto} from './job-info.dto';
5:    importJobInfo: JobInfoDto<T>;

- search whole word match 
$ ag -w job src/transport/

- Search only TypeScript
$ ag --ts -w job src/transport/
full list of languages:
1$ ag --list-file-types

- just list matching files 
  [I500695@C02ZR8BSMD6N:2021-11-28 18:25:17:/Users/i500695/git/portal-cf-transport-service:]2011$ ag --ts -w job src/transport/ -l
src/transport/dto/creators-params.dto.ts
src/transport/dto/transport-result.dto.tshark

- ignore directory
  $ ag --ts -w job src/transport/ --ignore-dir interfaces

- pipe input like grep
[I500695@C02ZR8BSMD6N:2021-11-28 18:30:59:/Users/i500695/git/portal-cf-transport-service:]2019$ cat README.md  | ag docker
for running integration tests, it is needed to have a postgres running inside a docker container.
$ docker run --name local-postgres -e POSTGRES_USER=<your i username> -e POSTGRES_PASSWORD=<wanted local password> -d -p 5432:5432 postgres:9.6
$ docker ps -a
$ docker run --name local-redis -d -p 6379:6379 redis:4.0
this will create a docker container named "local-redis" in your environment.
$ docker start/stop local-redis
### Using docker-compose.yml
docker-compose up -d
docker-compose up -d --no-deps --build transport-service
[I500695@C02ZR8BSMD6N:2021-11-28 18:31:06:/Users/i500695/git/portal-cf-transport-service:]2020$ cat README.md  | grep docker
for running integration tests, it is needed to have a postgres running inside a docker container.
$ docker run --name local-postgres -e POSTGRES_USER=<your i username> -e POSTGRES_PASSWORD=<wanted local password> -d -p 5432:5432 postgres:9.6
$ docker ps -a
$ docker run --name local-redis -d -p 6379:6379 redis:4.0
this will create a docker container named "local-redis" in your environment.
$ docker start/stop local-redis
### Using docker-compose.yml
docker-compose up -d
docker-compose up -d --no-deps --build transport-service


- regexp search
[I500695@C02ZR8BSMD6N:2021-11-28 18:32:43:/Users/i500695/git/portal-cf-transport-service:]2023$ ag --ts -w "job.*dto" src/transport/ 
  src/transport/interfaces/transport.job.manager.interface.ts
3:import {JobInfoDto} from '../dto/job-info.dto';
9:    startJob<T>(transportType: { new(): T }, jobInfoDto: JobInfoDto<T>): Promise<TransportJobStatus>;

src/transport/interfaces/transport-broken-apps-handler.interface.ts
2:import {JobInfoDto} from '../dto/job-info.dto';
5:    preTransportAction<T>(jobInfoDto: JobInfoDto<T>, token: string): Promise<void>;
6:    concludeBrokenApps<T>(jobInfoDto: JobInfoDto<T>, token: string): Promise<BrokenEntity[]>;

- provide search context
[I500695@C02ZR8BSMD6N:2021-11-28 18:35:25:/Users/i500695/git/portal-cf-transport-service:]2030$ ag --ts -w ".*dto" -C 3
src/report/interfaces/report-repository.interface.ts
1-import {DeleteResult} from 'typeorm';
2:import {ReportKeyDto} from '../dto/report-key.dto';
3:import {ReportDto} from '../dto/report.dto';
4-import {ReportEntity} from '../model/report.entity';
5-
6-export interface IReportRepository {
7:    getLastReport(key: ReportKeyDto): Promise<ReportEntity>;
8-    getReports(tenantId: string, instanceId: string): Promise<ReportEntity[]>;
9:    createOrUpdateReport(report: ReportDto): Promise<ReportDto>;
10:    deleteReport(key: ReportKeyDto): Promise<DeleteResult>;
11-    deleteReports(tenantId: string, instanceId: string): Promise<DeleteResult>;
12-}
13-
        43.2.2

    43.3
44.
